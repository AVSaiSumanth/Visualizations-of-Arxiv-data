{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6fa8487",
   "metadata": {},
   "source": [
    ">> ### *Task 1*\n",
    "Access Arxiv and get metadata and store it in a file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d310d",
   "metadata": {},
   "source": [
    "Installing arxiv wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b95e1651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in c:\\users\\avss4\\anaconda3\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: feedparser in c:\\users\\avss4\\anaconda3\\lib\\site-packages (from arxiv) (6.0.10)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\avss4\\anaconda3\\lib\\site-packages (from feedparser->arxiv) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afaae82",
   "metadata": {},
   "source": [
    "Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca48020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d38d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['cs.DB','cs.GR','cs.RO','cs.ET']   #Required categories as mentioned in the task\n",
    "arxiv_metadata = []  #list to store the metadata collected\n",
    "\n",
    "#loop to fetch the data\n",
    "for i in categories:\n",
    "    results_generator = arxiv.Client(\n",
    "        page_size=5000, delay_seconds=3, num_retries=3).results(\n",
    "            arxiv.Search(\n",
    "                query=i,\n",
    "                sort_order=arxiv.SortOrder.Descending,\n",
    "            ))\n",
    "    \n",
    "    # for loop to filter the data with dates and categories\n",
    "    for paper in results_generator:\n",
    "        if (int(paper.published.year) >= 2018) and (int(paper.published.year) <= 2022) and paper.primary_category in categories :\n",
    "            arxiv_metadata.append(paper)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fa09bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[arxiv.Result(entry_id='http://arxiv.org/abs/1903.12469v1', updated=datetime.datetime(2019, 3, 29, 12, 28, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 29, 12, 28, 28, tzinfo=datetime.timezone.utc), title='Corrigendum to \"Counting Database Repairs that Satisfy Conjunctive Queries with Self-Joins\"', authors=[arxiv.Result.Author('Jef Wijsen')], summary='The helping Lemma 7 in [Maslowski and Wijsen, ICDT, 2014] is false. The lemma\\nis used in (and only in) the proof of Theorem 3 of that same paper. In this\\ncorrigendum, we provide a new proof for the latter theorem.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1903.12469v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.12469v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.02622v1', updated=datetime.datetime(2018, 5, 7, 17, 11, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 7, 17, 11, 39, tzinfo=datetime.timezone.utc), title='Provenance for Interactive Visualizations', authors=[arxiv.Result.Author('Fotis Psallidas'), arxiv.Result.Author('Eugene Wu')], summary='We highlight the connections between data provenance and interactive\\nvisualizations. To do so, we first incrementally add interactions to a\\nvisualization and show how these interactions are readily expressible in terms\\nof provenance. We then describe how an interactive visualization system that\\nnatively supports provenance can be easily extended with novel interactions.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.02622v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.02622v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.12531v1', updated=datetime.datetime(2019, 12, 28, 21, 58, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 28, 21, 58, 43, tzinfo=datetime.timezone.utc), title='A framework supporting imprecise queries and data', authors=[arxiv.Result.Author('Giacomo Bergami')], summary='This technical report provides some lightweight introduction and some generic\\nuse case scenarios motivating the definition of a database supporting\\nuncertainties in both queries and data. This technical report is only providing\\nthe logical framework, which implementation is going to be provided in the\\nfinal paper.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1912.12531v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.12531v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2105.03161v1', updated=datetime.datetime(2021, 5, 7, 10, 59, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 5, 7, 10, 59, 16, tzinfo=datetime.timezone.utc), title='Open Data Portal Germany (OPAL) Projektergebnisse', authors=[arxiv.Result.Author('Adrian Wilke'), arxiv.Result.Author('Axel-Cyrille Ngonga Ngomo')], summary='In the Open Data Portal Germany (OPAL) project, a pipeline of the following\\ndata refinement steps has been developed: requirements analysis, data\\nacquisition, analysis, conversion, integration and selection. 800,000 datasets\\nin DCAT format have been produced.', comment='in German', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2105.03161v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2105.03161v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1909.06494v1', updated=datetime.datetime(2019, 9, 14, 0, 36, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 9, 14, 0, 36, 13, tzinfo=datetime.timezone.utc), title='Transactional Smart Contracts in Blockchain Systems', authors=[arxiv.Result.Author('Victor Zakhary'), arxiv.Result.Author('Divyakant Agrawal'), arxiv.Result.Author('Amr El Abbadi')], summary='This paper presents TXSC, a framework that provides smart contract developers\\nwith transaction primitives. These primitives allow developers to write smart\\ncontracts without the need to reason about the anomalies that can arise due to\\nconcurrent smart contract function executions.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1909.06494v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1909.06494v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.08336v2', updated=datetime.datetime(2018, 2, 22, 22, 3, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 25, 10, 16, 48, tzinfo=datetime.timezone.utc), title='Big Data Visualization Tools', authors=[arxiv.Result.Author('Nikos Bikakis')], summary='Data visualization is the presentation of data in a pictorial or graphical\\nformat, and a data visualization tool is the software that generates this\\npresentation. Data visualization provides users with intuitive means to\\ninteractively explore and analyze data, enabling them to effectively identify\\ninteresting patterns, infer correlations and causalities, and supports\\nsense-making activities.', comment='This article appears in Encyclopedia of Big Data Technologies,\\n  Springer, 2018', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.GR', 'cs.HC', '97R50, 68P05, 68P15', 'E.1; H.2.8; H.5.2; H.4'], links=[arxiv.Result.Link('http://arxiv.org/abs/1801.08336v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.08336v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.09353v1', updated=datetime.datetime(2019, 1, 27, 11, 18, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 27, 11, 18, 6, tzinfo=datetime.timezone.utc), title='Subsumption of Weakly Well-Designed SPARQL Patterns is Undecidable', authors=[arxiv.Result.Author('Mark Kaminski'), arxiv.Result.Author('Egor V. Kostylev')], summary='Weakly well-designed SPARQL patterns is a recent generalisation of\\nwell-designed patterns, which preserve good computational properties but also\\ncapture almost all patterns that appear in practice. Subsumption is one of\\nstatic analysis problems for SPARQL, along with equivalence and containment. In\\nthis paper we show that subsumption is undecidable for weakly well-designed\\npatterns, which is in stark contrast to well-designed patterns, and to\\nequivalence and containment.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.09353v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.09353v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.08621v1', updated=datetime.datetime(2019, 3, 20, 17, 7, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 20, 17, 7, 11, tzinfo=datetime.timezone.utc), title='Column2Vec: Structural Understanding via Distributed Representations of Database Schemas', authors=[arxiv.Result.Author('Michael J. Mior'), arxiv.Result.Author('Alexander G. Ororbia II')], summary='We present Column2Vec, a distributed representation of database columns based\\non column metadata. Our distributed representation has several applications.\\nUsing known names for groups of columns (i.e., a table name), we train a model\\nto generate an appropriate name for columns in an unnamed table. We demonstrate\\nthe viability of our approach using schema information collected from open\\nsource applications on GitHub.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1903.08621v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.08621v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.04991v1', updated=datetime.datetime(2019, 10, 11, 6, 41, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 11, 6, 41, 9, tzinfo=datetime.timezone.utc), title='Sub-query Fragmentation for Query Analysis and Data Caching in the Distributed Environment', authors=[arxiv.Result.Author('Santhilata Kuppili Venkata'), arxiv.Result.Author('Katarzyna Musial')], summary='When data stores and users are distributed geographically, it is essential to\\norganize distributed data cache points at ideal locations to minimize data\\ntransfers. To answer this, we are developing an adaptive distributed data\\ncaching framework that can identify suitable data chunks to cache and move\\nacross a network of community cache locations.', comment='29 pages, 18 figures, preprint', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.04991v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.04991v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.00694v1', updated=datetime.datetime(2020, 6, 1, 3, 36, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 1, 3, 36, 27, tzinfo=datetime.timezone.utc), title='F-IVM: Learning over Fast-Evolving Relational Data', authors=[arxiv.Result.Author('Milos Nikolic'), arxiv.Result.Author('Haozhe Zhang'), arxiv.Result.Author('Ahmet Kara'), arxiv.Result.Author('Dan Olteanu')], summary='F-IVM is a system for real-time analytics such as machine learning\\napplications over training datasets defined by queries over fast-evolving\\nrelational databases. We will demonstrate F-IVM for three such applications:\\nmodel selection, Chow-Liu trees, and ridge linear regression.', comment='SIGMOD DEMO 2020, 5 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2006.00694v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.00694v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.08657v1', updated=datetime.datetime(2020, 8, 19, 20, 15, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 19, 20, 15, 32, tzinfo=datetime.timezone.utc), title='LMFAO: An Engine for Batches of Group-By Aggregates', authors=[arxiv.Result.Author('Maximilian Schleich'), arxiv.Result.Author('Dan Olteanu')], summary='LMFAO is an in-memory optimization and execution engine for large batches of\\ngroup-by aggregates over joins. Such database workloads capture the\\ndata-intensive computation of a variety of data science applications.\\n  We demonstrate LMFAO for three popular models: ridge linear regression with\\nbatch gradient descent, decision trees with CART, and clustering with Rk-means.', comment='4 pages, 4 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2008.08657v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.08657v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.07781v1', updated=datetime.datetime(2021, 6, 14, 22, 26, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 14, 22, 26, 22, tzinfo=datetime.timezone.utc), title='On Declare MAX-SAT and a finite Herbrand Base for data-aware logs', authors=[arxiv.Result.Author('Giacomo Bergami')], summary='This technical report provides some lightweight introduction motivating the\\ndefinition of an alignment of log traces against Data-Aware Declare Models\\npotentially containing correlation conditions. This technical report is only\\nproviding the intuition of the logical framework as a feasibility study for a\\nfuture formalization and experiment section.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LO'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.07781v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.07781v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.04265v1', updated=datetime.datetime(2018, 5, 11, 7, 52, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 11, 7, 52, 23, tzinfo=datetime.timezone.utc), title='Scripting Relational Database Engine Using Transducer', authors=[arxiv.Result.Author('Feng Tian')], summary='We allow database user to script a parallel relational database engine with a\\nprocedural language. Procedural language code is executed as a user defined\\nrelational query operator called transducer. Transducer is tightly integrated\\nwith relation engine, including query optimizer, query executor and can be\\nexecuted in parallel like other query operators. With transducer, we can\\nefficiently execute queries that are very difficult to express in SQL. As\\nexample, we show how to run time series and graph queries, etc, within a\\nparallel relational database.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.04265v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.04265v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1809.03445v1', updated=datetime.datetime(2018, 9, 10, 16, 33, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 9, 10, 16, 33, 11, tzinfo=datetime.timezone.utc), title='A collection of database industrial techniques and optimization approaches of database operations', authors=[arxiv.Result.Author('Jasper Kyle Catapang')], summary='Databases play an essential role in our society today. Databases are embedded\\nin sectors like corporations, institutions, and government organizations, among\\nothers. These databases are used for our video and audio streaming platforms,\\nsocial gaming, finances, cloud storage, e-commerce, healthcare, economy, etc.\\nIt is therefore imperative that we learn how to properly execute database\\noperations and efficiently implement methodologies so that we may optimize the\\nperformance of databases.', comment=None, journal_ref=None, doi='10.5281/zenodo.1439511', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.5281/zenodo.1439511', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1809.03445v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1809.03445v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.07693v3', updated=datetime.datetime(2022, 11, 25, 23, 12, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 16, 14, 9, 51, tzinfo=datetime.timezone.utc), title='Frequent Itemset Mining using QUBO', authors=[arxiv.Result.Author('Jonas Nüßlein')], summary='In this paper we propose a R-step approximation to solve frequent itemset\\nmining on quantum hardware like quantum annealing or QAOA. The idea is to\\nsearch for the set of items where the minimal 2-item frequency is maximal. This\\ncan be represented as a maximum clique problem.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.07693v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.07693v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.01349v1', updated=datetime.datetime(2019, 5, 3, 19, 36, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 3, 19, 36, 55, tzinfo=datetime.timezone.utc), title='Adaptive filter ordering in Spark', authors=[arxiv.Result.Author('Nikodimos Nikolaidis'), arxiv.Result.Author('Anastasios Gounaris')], summary='This report describes a technical methodology to render the Apache Spark\\nexecution engine adaptive. It presents the engineering solutions, which\\nspecifically target to adaptively reorder predicates in data streams with\\nevolving statistics. The system extension developed is available as an\\nopen-source prototype. Indicative experimental results show its overhead and\\nsensitivity to tuning parameters.', comment='9 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.01349v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.01349v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.10268v1', updated=datetime.datetime(2019, 8, 27, 15, 22, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 27, 15, 22, 28, tzinfo=datetime.timezone.utc), title='Answering Summation Queries for Numerical Attributes under Differential Privacy', authors=[arxiv.Result.Author('Yikai Wu'), arxiv.Result.Author('David Pujol'), arxiv.Result.Author('Ios Kotsogiannis'), arxiv.Result.Author('Ashwin Machanavajjhala')], summary='In this work we explore the problem of answering a set of sum queries under\\nDifferential Privacy. This is a little understood, non-trivial problem\\nespecially in the case of numerical domains. We show that traditional\\ntechniques from the literature are not always the best choice and a more\\nrigorous approach is necessary to develop low error algorithms.', comment='TPDP 2019, 7 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1908.10268v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.10268v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.01633v1', updated=datetime.datetime(2019, 11, 5, 6, 3, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 5, 6, 3, 16, tzinfo=datetime.timezone.utc), title='On the Importance of Location Privacy for Users of Location Based Applications', authors=[arxiv.Result.Author('Sina Shaham'), arxiv.Result.Author('Saba Rafieian'), arxiv.Result.Author('Ming Ding'), arxiv.Result.Author('Mahyar Shirvanimoghaddam'), arxiv.Result.Author('Zihuai Lin')], summary='Do people care about their location privacy while using location-based\\nservice apps? This paper aims to answer this question and several other\\nhypotheses through a survey, and review the privacy preservation techniques.\\nOur results indicate that privacy is indeed an influential factor in the\\nselection of location-based apps by users.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.01633v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.01633v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.08352v1', updated=datetime.datetime(2019, 12, 16, 21, 28, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 16, 21, 28, 44, tzinfo=datetime.timezone.utc), title='Manifesto for Improved Foundations of Relational Model', authors=[arxiv.Result.Author('Witold Litwin')], summary='Normalized relations extended with inherited attributes can be more faithful\\nto reality and support logical navigation free queries, properties available at\\npresent only through specific views. Adding inherited attributes can be\\nnonetheless always less procedural than to define any such views. Present\\nschemes should even typically suffice for relations with foreign keys.\\nImplementing extended relations on popular DBSs appears also simple. Relational\\nmodel should evolve accordingly, for benefit of likely millions of DBAs,\\nclients, developers.', comment='3 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1912.08352v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.08352v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.09617v2', updated=datetime.datetime(2020, 5, 20, 14, 9, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 19, 17, 46, 34, tzinfo=datetime.timezone.utc), title='Unlocking New York City Crime Insights using Relational Database Embeddings', authors=[arxiv.Result.Author('Apoorva Nitsure'), arxiv.Result.Author('Rajesh Bordawekar'), arxiv.Result.Author('Jose Neves')], summary='This version withdrawn by arXiv administrators because the author did not\\nhave the right to agree to our license at the time of submission.', comment='arXiv admin note: This version withdrawn by arXiv administrators\\n  because the author did not have the right to agree to our license at the time\\n  of submission', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2005.09617v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.09617v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.06300v1', updated=datetime.datetime(2020, 7, 13, 10, 37, 21, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 13, 10, 37, 21, tzinfo=datetime.timezone.utc), title='Synthetic Dataset Generation with Itemset-Based Generative Models', authors=[arxiv.Result.Author('Christian Lezcano'), arxiv.Result.Author('Marta Arias')], summary='This paper proposes three different data generators, tailored to\\ntransactional datasets, based on existing itemset-based generative models. All\\nthese generators are intuitive and easy to implement and show satisfactory\\nperformance. The quality of each generator is assessed by means of three\\ndifferent methods that capture how well the original dataset structure is\\npreserved.', comment='IEEE International Symposium on Software Reliability Engineering\\n  Workshops (ISSREW@RDSA 2019), Oct 2019', journal_ref=None, doi='10.1109/ISSREW.2019.00086', primary_category='cs.DB', categories=['cs.DB', 'cs.AI', '68T01'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/ISSREW.2019.00086', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2007.06300v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.06300v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.01934v1', updated=datetime.datetime(2020, 11, 3, 10, 50, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 3, 10, 50, 5, tzinfo=datetime.timezone.utc), title='Palette diagram: A Python package for visualization of collective categorical data', authors=[arxiv.Result.Author('Chihiro Noguchi'), arxiv.Result.Author('Tatsuro Kawamoto')], summary='Categorical data, wherein a numerical quantity is assigned to each category\\n(nominal variable), are ubiquitous in data science. A palette diagram is a\\nvisualization tool for a large number of categorical datasets, each comprising\\nseveral categories.', comment='2 pages, 1 figure', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.GR', 'cs.SI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2011.01934v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.01934v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2105.11926v1', updated=datetime.datetime(2021, 5, 21, 14, 9, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 5, 21, 14, 9, 35, tzinfo=datetime.timezone.utc), title='ConQuer-92 -- The revised report on the conceptual query language LISA-D', authors=[arxiv.Result.Author('H. A. Proper')], summary=\"In this report the conceptual query language ConQuer-92 is introduced. This\\nquery language serves as the backbone of InfoAssistant's query facilities.\\nFurthermore, this language can also be used for the specification of derivation\\nrules (e.g. subtype defining rules) and textual constraints in InfoModeler.\\nThis report is solely concerned with a formal definition, and the explanation\\nthereof, of ConQuer-92. The implementation of ConQuer-92 in SQL-92 will be\\ntreated in a separate report.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2105.11926v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2105.11926v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2105.12507v1', updated=datetime.datetime(2021, 5, 26, 12, 18, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 5, 26, 12, 18, 32, tzinfo=datetime.timezone.utc), title='Cost models for geo-distributed massively parallel streaming analytics', authors=[arxiv.Result.Author('Anna-Valentini Michailidou'), arxiv.Result.Author('Anastasios Gounaris'), arxiv.Result.Author('Konstantinos Tsichlas')], summary='This report is part of the DataflowOpt project on optimization of modern\\ndataflows and aims to introduce a data quality-aware cost model that covers the\\nfollowing aspects in combination: (1) heterogeneity in compute nodes, (2)\\ngeo-distribution, (3) massive parallelism, (4) complex DAGs and (5) streaming\\napplications. Such a cost model can be then leveraged to devise cost-based\\noptimization solutions that deal with task placement and operator\\nconfiguration.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2105.12507v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2105.12507v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2109.05142v1', updated=datetime.datetime(2021, 9, 11, 0, 2, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 9, 11, 0, 2, 2, tzinfo=datetime.timezone.utc), title='Discovering Technology Gaps using the IntSight Knowledge Navigator', authors=[arxiv.Result.Author('Aurpon Gupta'), arxiv.Result.Author('Subhasis Dasgupta'), arxiv.Result.Author('Snehasis Sinha'), arxiv.Result.Author('Amarnath Gupta')], summary='Knowledge analysis is an important application of knowledge graphs. In this\\npaper, we present a complex knowledge analysis problem that discovers the gaps\\nin the technology areas of interest to an organization. Our knowledge graph is\\ndeveloped on a heterogeneous data management platform. The analysis combines\\nsemantic search, graph analytics, and polystore query optimization.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2109.05142v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2109.05142v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2110.12996v1', updated=datetime.datetime(2021, 10, 25, 14, 39, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 10, 25, 14, 39, 42, tzinfo=datetime.timezone.utc), title='PREC: semantic translation of property graphs', authors=[arxiv.Result.Author('Julian Bruyat'), arxiv.Result.Author('Pierre-Antoine Champin'), arxiv.Result.Author('Lionel Médini'), arxiv.Result.Author('Frédérique Laforest')], summary='Converting property graphs to RDF graphs allows to enhance the\\ninteroperability of knowledge graphs. But existing tools perform the same\\nconversion for every graph, regardless of its content. In this paper, we\\npropose PREC, a user-configured conversion of property graphs to RDF graphs to\\nbetter capture the semantics of the content.', comment='6 pages, 1 figure, 1st Workshop on Squaring the Circles on Graphs at\\n  SEMANTiCS 2021', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2.5'], links=[arxiv.Result.Link('http://arxiv.org/abs/2110.12996v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2110.12996v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2201.05664v1', updated=datetime.datetime(2022, 1, 14, 20, 48, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 1, 14, 20, 48, 8, tzinfo=datetime.timezone.utc), title='Demonstration of PI2: Interactive Visualization Interface Generation for SQL Analysis in Notebook', authors=[arxiv.Result.Author('Jeffrey Tao'), arxiv.Result.Author('Yiru Chen'), arxiv.Result.Author('Eugene Wu')], summary='We demonstrate PI2, the first notebook extension that can automatically\\ngenerate interactive visualization interfaces during SQL-based analyses.', comment='arXiv admin note: text overlap with arXiv:2107.08203', journal_ref=\"SIGMOD '22: Proceedings of the 2022 International Conference on\\n  Management of Data\", doi='10.1145/3514221.3520153', primary_category='cs.DB', categories=['cs.DB', 'cs.HC', 'H.2; H.5.2'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3514221.3520153', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2201.05664v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2201.05664v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2202.06351v1', updated=datetime.datetime(2022, 2, 13, 16, 1, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 2, 13, 16, 1, 27, tzinfo=datetime.timezone.utc), title='Comparing Flexible Skylines And Top-k Queries: Which Is the Best Alternative?', authors=[arxiv.Result.Author('Flavio Rizzoglio')], summary='The question of how to get the best results out of the data we have is an\\neverlasting problem in data science. The two main approaches to tackle the\\nproblem are top-k queries and skyline queries. Since their introduction, a new\\nparadigm called flexible skylines has emerged. The aim of this survey is to\\nprovide a solid comparison between the new and the old approaches,\\nunderstanding and exploring their differences and similarities.', comment='7 pages, 1 figure', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2202.06351v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2202.06351v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2206.07278v1', updated=datetime.datetime(2022, 6, 15, 3, 38, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 6, 15, 3, 38, 1, tzinfo=datetime.timezone.utc), title='Nebula Graph: An open source distributed graph database', authors=[arxiv.Result.Author('Min Wu'), arxiv.Result.Author('Xinglu Yi'), arxiv.Result.Author('Hui Yu'), arxiv.Result.Author('Yu Liu'), arxiv.Result.Author('Yujue Wang')], summary='This paper introduces the recent work of Nebula Graph, an open-source,\\ndistributed, scalable, and native graph database. We present a system design\\ntrade-off and a comprehensive overview of Nebula Graph internals, including\\ngraph data models, partitioning strategies, secondary indexes, optimizer rules,\\nstorage-side transactions, graph query languages, observability, graph\\nprocessing frameworks, and visualization tool-kits. In addition, three sets of\\nlarge-scale graph b', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2206.07278v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2206.07278v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2207.09198v1', updated=datetime.datetime(2022, 7, 19, 11, 15, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 7, 19, 11, 15, 15, tzinfo=datetime.timezone.utc), title='Consistent Query Answering for Expressive Constraints under Tuple-Deletion Semantics', authors=[arxiv.Result.Author('Lorenzo Marconi'), arxiv.Result.Author('Riccardo Rosati')], summary='We study consistent query answering in relational databases. We consider an\\nexpressive class of schema constraints that generalizes both tuple-generating\\ndependencies and equality-generating dependencies. We establish the complexity\\nof consistent query answering and repair checking under tuple-deletion\\nsemantics for different fragments of the above constraint language. In\\nparticular, we identify new subclasses of constraints in which the above\\nproblems are tractable or even first-order rewritable.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2207.09198v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2207.09198v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2208.00782v1', updated=datetime.datetime(2022, 7, 26, 14, 36, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 7, 26, 14, 36, 23, tzinfo=datetime.timezone.utc), title='Tree edit distance for hierarchical data compatible with HMIL paradigm', authors=[arxiv.Result.Author('Břetislav Šopík'), arxiv.Result.Author('Tomáš Strenáčik')], summary='We define edit distance for hierarchically structured data compatible with\\nthe hierarchical multi-instance learning paradigm. Example of such data is\\ndataset represented in JSON format where inner Array objects are interpreted as\\nunordered bags of elements. We prove correct analytical properties of the\\ndefined distance.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2208.00782v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2208.00782v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2208.01613v1', updated=datetime.datetime(2022, 8, 2, 17, 44, 36, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 8, 2, 17, 44, 36, tzinfo=datetime.timezone.utc), title='Principles of Query Visualization', authors=[arxiv.Result.Author('Wolfgang Gatterbauer'), arxiv.Result.Author('Cody Dunne'), arxiv.Result.Author('H. V. Jagadish'), arxiv.Result.Author('Mirek Riedewald')], summary='Query Visualization (QV) is the problem of transforming a given query into a\\ngraphical representation that helps humans understand its meaning. This task is\\nnotably different from designing a Visual Query Language (VQL) that helps a\\nuser compose a query. This article discusses the principles of relational query\\nvisualization and its potential for simplifying user interactions with\\nrelational data.', comment='20 pages, 12 figures, preprint for IEEE Data Engineering Bulletin', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2208.01613v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2208.01613v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2208.09305v1', updated=datetime.datetime(2022, 8, 19, 12, 33, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 8, 19, 12, 33, 3, tzinfo=datetime.timezone.utc), title='Real and simulated CBM data interacting with an ESCAPE datalake', authors=[arxiv.Result.Author('E. Clerkin'), arxiv.Result.Author('P. -N. Kramp'), arxiv.Result.Author('P. -A. Loizeau'), arxiv.Result.Author('M. Szuba')], summary='Integration of the ESCAPE and CBM software environment. The ESCAPE datalake\\nare utilized by the CBM experiment for the storage, distribution and retrieval\\nof real SIS18 and simulated SIS100 particle physics data.', comment='4 pages, 6 figures', journal_ref='CBM Progress Report 2021', doi='10.15120/GSI-2022-00599', primary_category='cs.DB', categories=['cs.DB', 'hep-ex'], links=[arxiv.Result.Link('http://dx.doi.org/10.15120/GSI-2022-00599', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2208.09305v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2208.09305v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2210.15711v1', updated=datetime.datetime(2022, 10, 27, 18, 21, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 10, 27, 18, 21, 43, tzinfo=datetime.timezone.utc), title='Re-looking at the View Update Problem', authors=[arxiv.Result.Author('Terry Brennan')], summary='Relational databases have always had a means for creating a pseudo-table,\\ncalled a view, defined by a query. Views are like tables in most ways, except\\nthat they are read-only and cannot be updated. The problem of how to update\\nviews has attracted a lot of attention in the 1980s but is unsolved.\\n  The best approach from that time was by Bancilhon and Spyratos. I use one of\\ntheir overlooked theorems and find a number of simple solutions for common\\nrelational operators.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2210.15711v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2210.15711v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1803.01248v1', updated=datetime.datetime(2018, 3, 3, 21, 40, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 3, 3, 21, 40, 33, tzinfo=datetime.timezone.utc), title='Imprecise temporal associations and decision support systems', authors=[arxiv.Result.Author('Giovanni Vincenti')], summary='The quick and pervasive infiltration of decision support systems, artificial\\nintelligence, and data mining in consumer electronics and everyday life in\\ngeneral has been significant in recent years. Fields such as UX have been\\nfacilitating the integration of such technologies into software and hardware,\\nbut the back-end processing is still based on binary foundations. This article\\ndescribes an approach to mining for imprecise temporal associations among\\nevents in data streams, taking into account the very natural concept of\\napproximation. This type of association analysis is likely to lead to more\\nmeaningful and actionable decision support systems.', comment='6 pages, 3 figures', journal_ref=None, doi='10.1016/j.procs.2018.04.096', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1016/j.procs.2018.04.096', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1803.01248v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1803.01248v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1803.06071v2', updated=datetime.datetime(2021, 4, 26, 7, 48, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 3, 16, 4, 23, tzinfo=datetime.timezone.utc), title='Impacts of Dirty Data: and Experimental Evaluation', authors=[arxiv.Result.Author('Zhixin Qi'), arxiv.Result.Author('Hongzhi Wang'), arxiv.Result.Author('Jianzhong Li'), arxiv.Result.Author('Hong Gao')], summary='Data quality issues have attracted widespread attention due to the negative\\nimpacts of dirty data on data mining and machine learning results. The\\nrelationship between data quality and the accuracy of results could be applied\\non the selection of the appropriate algorithm with the consideration of data\\nquality and the determination of the data share to clean. However, rare\\nresearch has focused on exploring such relationship. Motivated by this, this\\npaper conducts an experimental comparison for the effects of missing,\\ninconsistent and conflicting data on classification and clustering algorithms.\\nBased on the experimental findings, we provide guidelines for algorithm\\nselection and data cleaning.', comment='22 pages, 192 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG', 'stat.ML'], links=[arxiv.Result.Link('http://arxiv.org/abs/1803.06071v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1803.06071v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1807.00602v1', updated=datetime.datetime(2018, 7, 2, 11, 27, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 7, 2, 11, 27, 51, tzinfo=datetime.timezone.utc), title='Semantic Query Language for Temporal Genealogical Trees', authors=[arxiv.Result.Author('Evgeniy Gryaznov')], summary='Computers play a crucial role in modern ancestry management, they are used to\\ncollect, store, analyze, sort and display genealogical data. However, current\\napplications do not take into account the kinship structure of a natural\\nlanguage.\\n  In this paper we propose a new domain-specific language KISP which is based\\non a formalization of English kinship system, for accessing and querying\\ntraditional genealogical trees. KISP is a dynamically typed LISP-like\\nprogramming language with a rich set of features, such as kinship term\\nreduction and temporal information expression.\\n  Our solution provides a user with a coherent genealogical framework that\\nallows for a natural navigation over any traditional family tree.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1807.00602v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1807.00602v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1807.05258v1', updated=datetime.datetime(2018, 7, 13, 19, 31, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 7, 13, 19, 31, 8, tzinfo=datetime.timezone.utc), title='QR2: A Third-party Query Reranking Service Over Web Databases', authors=[arxiv.Result.Author('Yeshwanth D. Gunasekaran'), arxiv.Result.Author('Abolfazl Asudeh'), arxiv.Result.Author('Sona Hasani'), arxiv.Result.Author('Nan Zhang'), arxiv.Result.Author('Ali Jaoua'), arxiv.Result.Author('Gautam Das')], summary='The ranked retrieval model has rapidly become the de-facto way for search\\nquery processing in web databases. Despite the extensive efforts on designing\\nbetter ranking mechanisms, in practice, many such databases fail to address the\\ndiverse and sometimes contradicting preferences of users. In this paper, we\\npresent QR2, a third-party service that uses nothing but the public search\\ninterface of a web database and enables the on-the-fly processing of queries\\nwith any user-specified ranking functions, no matter if the ranking function is\\nsupported by the database or not.', comment='34th IEEE International Conference on Data Engineering (ICDE Demo),\\n  2018', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1807.05258v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1807.05258v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.01620v1', updated=datetime.datetime(2018, 8, 5, 14, 10, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 5, 14, 10, 14, tzinfo=datetime.timezone.utc), title='Schema Integration on Massive Data Sources', authors=[arxiv.Result.Author('Tianbao Lia'), arxiv.Result.Author('Hongzhi Wang'), arxiv.Result.Author('Jianzhong Li'), arxiv.Result.Author('Hong Gao')], summary='As the fundamental phrase of collecting and analyzing data, data integration\\nis used in many applications, such as data cleaning, bioinformatics and pattern\\nrecognition. In big data era, one of the major problems of data integration is\\nto obtain the global schema of data sources since the global schema could be\\nhardly derived from massive data sources directly. In this paper, we attempt to\\nsolve such schema integration problem. For different scenarios, we develop\\nbatch and incremental schema integration algorithms. We consider the\\nrepresentation difference of attribute names in various data sources and\\npropose ED Join and Semantic Join algorithms to integrate attributes with\\ndifferent representations. Extensive experimental results demonstrate that the\\nproposed algorithms could integrate schemas efficiently and effectively.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.01620v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.01620v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.01621v1', updated=datetime.datetime(2018, 8, 5, 14, 11, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 5, 14, 11, 14, tzinfo=datetime.timezone.utc), title='Mining CFD Rules on Big Data', authors=[arxiv.Result.Author('Hongzhi Wang'), arxiv.Result.Author('Mingda Li'), arxiv.Result.Author('Jiawei Zhao'), arxiv.Result.Author('Jianzhong Li'), arxiv.Result.Author('Hong Gao')], summary='Current conditional functional dependencies (CFDs) discovery algorithms\\nalways need a well-prepared training data set. This makes them difficult to be\\napplied on large datasets which are always in low-quality. To handle the volume\\nissue of big data, we develop the sampling algorithms to obtain a small\\nrepresentative training set. For the low-quality issue of big data, we then\\ndesign the fault-tolerant rule discovery algorithm and the conflict resolution\\nalgorithm. We also propose parameter selection strategy for CFD discovery\\nalgorithm to ensure its effectiveness. Experimental results demonstrate that\\nour method could discover effective CFD rules on billion-tuple data within\\nreasonable time.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.01621v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.01621v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.05138v1', updated=datetime.datetime(2018, 8, 13, 20, 19, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 13, 20, 19, 56, tzinfo=datetime.timezone.utc), title='Database Operations in D4M.jl', authors=[arxiv.Result.Author('Lauren Milechin'), arxiv.Result.Author('Vijay Gadepally'), arxiv.Result.Author('Jeremy Kepner')], summary='Each step in the data analytics pipeline is important, including database\\ningest and query. The D4M-Accumulo database connector has allowed analysts to\\nquickly and easily ingest to and query from Apache Accumulo using MATLAB(R)/GNU\\nOctave syntax. D4M.jl, a Julia implementation of D4M, provides much of the\\nfunctionality of the original D4M implementation to the Julia community. In\\nthis work, we extend D4M.jl to include many of the same database capabilities\\nthat the MATLAB(R)/GNU Octave implementation provides. Here we will describe\\nthe D4M.jl database connector, demonstrate how it can be used, and show that it\\nhas comparable or better performance to the original implementation in\\nMATLAB(R)/GNU Octave.', comment='IEEE HPEC 2018. arXiv admin note: text overlap with arXiv:1708.02934', journal_ref=None, doi='10.1109/HPEC.2018.8547567', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/HPEC.2018.8547567', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1808.05138v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.05138v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.05448v1', updated=datetime.datetime(2018, 8, 16, 12, 42, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 16, 12, 42, 29, tzinfo=datetime.timezone.utc), title='Automatic Generation of a Hybrid Query Execution Engine', authors=[arxiv.Result.Author('Aleksei Kashuba'), arxiv.Result.Author('Hannes Mühleisen')], summary='The ever-increasing need for fast data processing demands new methods for\\nefficient query execution. Just-in-time query compilation techniques have been\\ndemonstrated to improve performance in a set of analytical tasks significantly.\\nIn this work, we investigate the possibility of adding this approach to\\nexisting database solutions and the benefits it provides. To that end, we\\ncreate a set of automated tools to create a runtime code generation engine and\\nintegrate such an engine into SQLite which is one of the most popular\\nrelational databases in the world and is used in a large variety of contexts.\\nSpeedups of up to 1.7x were observed in microbenchmarks with queries involving\\na large number of operations.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.05448v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.05448v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.07767v2', updated=datetime.datetime(2019, 1, 25, 20, 11, 34, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 22, 15, 12, 34, tzinfo=datetime.timezone.utc), title='The First Order Truth behind Undecidability of Regular Path Queries Determinacy', authors=[arxiv.Result.Author('Grzegorz Głuch'), arxiv.Result.Author('Jerzy Marcinkowski'), arxiv.Result.Author('Piotr Ostropolski-Nalewaja')], summary='In our paper [G{\\\\l}uch, Marcinkowski, Ostropolski-Nalewaja, LICS ACM, 2018]\\nwe have solved an old problem stated in [Calvanese, De Giacomo, Lenzerini,\\nVardi, SPDS ACM, 2000] showing that query determinacy is undecidable for\\nRegular Path Queries. Here a strong generalisation of this result is shown, and\\n-- we think -- a very unexpected one. We prove that no regularity is needed:\\ndeterminacy remains undecidable even for finite unions of conjunctive path\\nqueries.', comment='arXiv admin note: text overlap with arXiv:1802.01554', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.07767v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.07767v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.00326v1', updated=datetime.datetime(2018, 9, 30, 6, 42, 18, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 9, 30, 6, 42, 18, tzinfo=datetime.timezone.utc), title='Using Graph-Pattern Association Rules On Yago Knowledge Base', authors=[arxiv.Result.Author('Wahyudi'), arxiv.Result.Author('Masayu Leylia Khodra'), arxiv.Result.Author('Ary Setijadi Prihatmanto'), arxiv.Result.Author('Carmadi Machbub')], summary='We propose the use of Graph-Pattern Association Rules (GPARs) on the Yago\\nknowledge base. Extending association rules for itemsets, GPARS can help to\\ndiscover regularities between entities in knowledge bases. A rule-generated\\ngraph pattern (RGGP) algorithm was used for extracting rules from the Yago\\nknowledge base and a graph-pattern association rules algorithm for creating\\nassociation rules. Our research resulted in 1114 association rules, where the\\nvalue of standard confidence at 50.18% was better than partial completeness\\nassumption (PCA) confidence at 49.82%. Besides that the computation time for\\nstandard confidence was also better than for PCA confidence', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.00326v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.00326v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.04604v1', updated=datetime.datetime(2018, 10, 10, 15, 50, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 10, 15, 50, 3, tzinfo=datetime.timezone.utc), title='A Similarity Measure for Weaving Patterns in Textiles', authors=[arxiv.Result.Author('Sven Helmer'), arxiv.Result.Author('Vuong M. Ngo')], summary='We propose a novel approach for measuring the similarity between weaving\\npatterns that can provide similarity-based search functionality for textile\\narchives. We represent textile structures using hypergraphs and extract\\nmultisets of k-neighborhoods from these graphs. The resulting multisets are\\nthen compared using Jaccard coefficients, Hamming distances, and cosine\\nmeasures. We evaluate the different variants of our similarity measure\\nexperimentally, showing that it can be implemented efficiently and illustrating\\nits quality using it to cluster and query a data set containing more than a\\nthousand textile samples.', comment='10 papes, will be published in SIGIR 2015', journal_ref='SIGIR 2015', doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CV', 'cs.DL', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.04604v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.04604v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.09227v1', updated=datetime.datetime(2018, 10, 22, 12, 59, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 22, 12, 59, 51, tzinfo=datetime.timezone.utc), title='Knowledge Graph Completion to Predict Polypharmacy Side Effects', authors=[arxiv.Result.Author('Brandon Malone'), arxiv.Result.Author('Alberto García-Durán'), arxiv.Result.Author('Mathias Niepert')], summary='The polypharmacy side effect prediction problem considers cases in which two\\ndrugs taken individually do not result in a particular side effect; however,\\nwhen the two drugs are taken in combination, the side effect manifests. In this\\nwork, we demonstrate that multi-relational knowledge graph completion achieves\\nstate-of-the-art results on the polypharmacy side effect prediction problem.\\nEmpirical results show that our approach is particularly effective when the\\nprotein targets of the drugs are well-characterized. In contrast to prior work,\\nour approach provides more interpretable predictions and hypotheses for wet lab\\nvalidation.', comment='13th International Conference on Data Integration in the Life\\n  Sciences (DILS2018)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG', 'stat.ML'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.09227v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.09227v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.01304v1', updated=datetime.datetime(2019, 2, 4, 16, 52, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 4, 16, 52, 40, tzinfo=datetime.timezone.utc), title='Declarative Data Analytics: a Survey', authors=[arxiv.Result.Author('Nantia Makrynioti'), arxiv.Result.Author('Vasilis Vassalos')], summary='The area of declarative data analytics explores the application of the\\ndeclarative paradigm on data science and machine learning. It proposes\\ndeclarative languages for expressing data analysis tasks and develops systems\\nwhich optimize programs written in those languages. The execution engine can be\\neither centralized or distributed, as the declarative paradigm advocates\\nindependence from particular physical implementations. The survey explores a\\nwide range of declarative data analysis frameworks by examining both the\\nprogramming model and the optimization techniques used, in order to provide\\nconclusions on the current state of the art in the area and identify open\\nchallenges.', comment='36 pages, 2 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1902.01304v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.01304v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.02013v1', updated=datetime.datetime(2019, 2, 6, 3, 23, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 6, 3, 23, 51, tzinfo=datetime.timezone.utc), title='Finding the Transitive Closure of Functional Dependencies using Strategic Port Graph Rewriting', authors=[arxiv.Result.Author('János Varga')], summary='We present a new approach to the logical design of relational databases,\\nbased on strategic port graph rewriting. We show how to model relational\\nschemata as attributed port graphs and provide port graph rewriting rules to\\nperform computations on functional dependencies. Using these rules we present a\\nstrategic graph program to find the transitive closure of a set of functional\\ndependencies. This program is sound, complete and terminating, assuming that\\nthere are no cyclical dependencies in the schema.', comment='In Proceedings TERMGRAPH 2018, arXiv:1902.01510', journal_ref='EPTCS 288, 2019, pp. 50-62', doi='10.4204/EPTCS.288.5', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.4204/EPTCS.288.5', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1902.02013v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.02013v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.03338v2', updated=datetime.datetime(2019, 2, 12, 19, 7, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 9, 0, 23, 56, tzinfo=datetime.timezone.utc), title='WarpFlow: Exploring Petabytes of Space-Time Data', authors=[arxiv.Result.Author('Catalin Popescu'), arxiv.Result.Author('Deepak Merugu'), arxiv.Result.Author('Giao Nguyen'), arxiv.Result.Author('Shiva Shivakumar')], summary='WarpFlow is a fast, interactive data querying and processing system with a\\nfocus on petabyte-scale spatiotemporal datasets and Tesseract queries. With the\\nrapid growth in smartphones and mobile navigation services, we now have an\\nopportunity to radically improve urban mobility and reduce friction in how\\npeople and packages move globally every minute-mile, with data. WarpFlow speeds\\nup three key metrics for data engineers working on such datasets --\\ntime-to-first-result, time-to-full-scale-result, and time-to-trained-model for\\nmachine learning.', comment='11 pages, 12 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1902.03338v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.03338v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.03948v1', updated=datetime.datetime(2019, 2, 11, 15, 46, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 11, 15, 46, 54, tzinfo=datetime.timezone.utc), title='Scaling Big Data Platform for Big Data Pipeline', authors=[arxiv.Result.Author('Rebecca Wild'), arxiv.Result.Author('Matthew Hubbell'), arxiv.Result.Author('Jeremy Kepner')], summary='Monitoring and Managing High Performance Computing (HPC) systems and\\nenvironments generate an ever growing amount of data. Making sense of this data\\nand generating a platform where the data can be visualized for system\\nadministrators and management to proactively identify system failures or\\nunderstand the state of the system requires the platform to be as efficient and\\nscalable as the underlying database tools used to store and analyze the data.\\nIn this paper we will show how we leverage Accumulo, d4m, and Unity to generate\\na 3D visualization platform to monitor and manage the Lincoln Laboratory\\nSupercomputer systems and how we have had to retool our approach to scale with\\nour systems.', comment='Accepted to MIT Northeast Database Day 2019', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1902.03948v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.03948v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.04938v1', updated=datetime.datetime(2019, 2, 13, 14, 54, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 13, 14, 54, 40, tzinfo=datetime.timezone.utc), title='Snapshot Semantics for Temporal Multiset Relations (Extended Version)', authors=[arxiv.Result.Author('Anton Dignös'), arxiv.Result.Author('Boris Glavic'), arxiv.Result.Author('Xing Niu'), arxiv.Result.Author('Michael Böhlen'), arxiv.Result.Author('Johann Gamper')], summary='Snapshot semantics is widely used for evaluating queries over temporal data:\\ntemporal relations are seen as sequences of snapshot relations, and queries are\\nevaluated at each snapshot. In this work, we demonstrate that current\\napproaches for snapshot semantics over interval-timestamped multiset relations\\nare subject to two bugs regarding snapshot aggregation and bag difference. We\\nintroduce a novel temporal data model based on K-relations that overcomes these\\nbugs and prove it to correctly encode snapshot semantics. Furthermore, we\\npresent an efficient implementation of our model as a database middleware and\\ndemonstrate experimentally that our approach is competitive with native\\nimplementations and significantly outperforms such implementations on queries\\nthat involve aggregation.', comment='extended version of PVLDB paper', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1902.04938v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.04938v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1909.03291v1', updated=datetime.datetime(2019, 9, 7, 15, 42, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 9, 7, 15, 42, 22, tzinfo=datetime.timezone.utc), title='Compiling PL/SQL Away', authors=[arxiv.Result.Author('Christian Duta'), arxiv.Result.Author('Denis Hirn'), arxiv.Result.Author('Torsten Grust')], summary='\"PL/SQL functions are slow,\" is common developer wisdom that derives from the\\ntension between set-oriented SQL evaluation and statement-by-statement PL/SQL\\ninterpretation. We pursue the radical approach of compiling PL/SQL away,\\nturning interpreted functions into regular subqueries that can then be\\nefficiently evaluated together with their embracing SQL query, avoiding any\\nPL/SQL to SQL context switches. Input PL/SQL functions may exhibit arbitrary\\ncontrol flow. Iteration, in particular, is compiled into SQL-level recursion.\\nRDBMSs across the board reward this compilation effort with significant run\\ntime savings that render established developer lore questionable.', comment='6 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.PL'], links=[arxiv.Result.Link('http://arxiv.org/abs/1909.03291v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1909.03291v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1909.05859v1', updated=datetime.datetime(2019, 9, 12, 16, 23, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 9, 12, 16, 23, 56, tzinfo=datetime.timezone.utc), title='Simple-ML: Towards a Framework for Semantic Data Analytics Workflows', authors=[arxiv.Result.Author('Simon Gottschalk'), arxiv.Result.Author('Nicolas Tempelmeier'), arxiv.Result.Author('Günter Kniesel'), arxiv.Result.Author('Vasileios Iosifidis'), arxiv.Result.Author('Besnik Fetahu'), arxiv.Result.Author('Elena Demidova')], summary='In this paper we present the Simple-ML framework that we develop to support\\nefficient configuration, robustness and reusability of data analytics workflows\\nthrough the adoption of semantic technologies. We present semantic data models\\nthat lay the foundation for the framework development and discuss the data\\nanalytics workflows based on these models. Furthermore, we present an example\\ninstantiation of the Simple-ML data models for a real-world use case in the\\nmobility domain.', comment='SEMANTiCS 2019', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1909.05859v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1909.05859v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1806.06151v2', updated=datetime.datetime(2018, 6, 19, 9, 59, 49, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 6, 15, 23, 51, 52, tzinfo=datetime.timezone.utc), title='Efficient Data Perturbation for Privacy Preserving and Accurate Data Stream Mining', authors=[arxiv.Result.Author('M. A. P. Chamikara'), arxiv.Result.Author('P. Bertok'), arxiv.Result.Author('D. Liu'), arxiv.Result.Author('S. Camtepe'), arxiv.Result.Author('I. Khalil')], summary='The widespread use of the Internet of Things (IoT) has raised many concerns,\\nincluding the protection of private information. Existing privacy preservation\\nmethods cannot provide a good balance between data utility and privacy, and\\nalso have problems with efficiency and scalability. This paper proposes an\\nefficient data stream perturbation method (named as $P^2RoCAl$). $P^2RoCAl$\\noffers better data utility than similar methods: classification accuracies of\\n$P^2RoCAl$ perturbed data streams are very close to those of the original data\\nstreams. $P^2RoCAl$ also provides higher resilience against data reconstruction\\nattacks.', comment='Pervasive and Mobile Computing 2018', journal_ref=None, doi='10.1016/j.pmcj.2018.05.003', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1016/j.pmcj.2018.05.003', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1806.06151v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1806.06151v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1806.07728v1', updated=datetime.datetime(2018, 6, 20, 13, 45, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 6, 20, 13, 45, 24, tzinfo=datetime.timezone.utc), title='Parallelization of XPath Queries using Modern XQuery Processors', authors=[arxiv.Result.Author('Shigeyuki Sato'), arxiv.Result.Author('Wei Hao'), arxiv.Result.Author('Kiminori Matsuzaki')], summary='A practical and promising approach to parallelizing XPath queries was\\nproposed by Bordawekar et al. in 2009, which enables parallelization on top of\\nexisting XML database engines. Although they experimentally demonstrated the\\nspeedup by their approach, their practice has already been out of date because\\nthe software environment has largely changed with the capability of XQuery\\nprocessing. In this work, we implement their approach in two ways on top of a\\nstate-of-the-art XML database engine and experimentally demonstrate that our\\nimplementations can bring significant speedup on a commodity server.', comment='This is the full version of our publication to appear at ADBIS 2018\\n  as a short paper', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.PL'], links=[arxiv.Result.Link('http://arxiv.org/abs/1806.07728v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1806.07728v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1806.10078v2', updated=datetime.datetime(2018, 7, 3, 9, 46, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 6, 26, 15, 55, 37, tzinfo=datetime.timezone.utc), title='A General Framework for Anytime Approximation in Probabilistic Databases', authors=[arxiv.Result.Author('Maarten Van den Heuvel'), arxiv.Result.Author('Floris Geerts'), arxiv.Result.Author('Wolfgang Gatterbauer'), arxiv.Result.Author('Martin Theobald')], summary=\"Anytime approximation algorithms that compute the probabilities of queries\\nover probabilistic databases can be of great use to statistical learning tasks.\\nThose approaches have been based so far on either (i) sampling or (ii)\\nbranch-and-bound with model-based bounds. We present here a more general\\nbranch-and-bound framework that extends the possible bounds by using\\n'dissociation', which yields tighter bounds.\", comment='3 pages, 2 figures, submitted to StarAI 2018 Workshop', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1806.10078v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1806.10078v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1812.04379v2', updated=datetime.datetime(2020, 2, 3, 14, 54, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 12, 11, 13, 13, 38, tzinfo=datetime.timezone.utc), title='On the expressive power of linear algebra on graphs', authors=[arxiv.Result.Author('Floris Geerts')], summary='Most graph query languages are rooted in logic. By contrast, in this paper we\\nconsider graph query languages rooted in linear algebra. More specifically, we\\nconsider MATLANG, a matrix query language recently introduced, in which some\\nbasic linear algebra functionality is supported. We investigate the problem of\\ncharacterising equivalence of graphs, represented by their adjacency matrices,\\nfor various fragments of MATLANG. A complete picture is painted of the impact\\nof the linear algebra operations in MATLANG on their ability to distinguish\\ngraphs.', comment='51 pages, revised extended version of conference paper (International\\n  Conference on Database Theory 2019)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LO'], links=[arxiv.Result.Link('http://arxiv.org/abs/1812.04379v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1812.04379v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.00781v1', updated=datetime.datetime(2019, 5, 30, 20, 10, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 30, 20, 10, 14, tzinfo=datetime.timezone.utc), title='Learning Semantic Annotations for Tabular Data', authors=[arxiv.Result.Author('Jiaoyan Chen'), arxiv.Result.Author('Ernesto Jimenez-Ruiz'), arxiv.Result.Author('Ian Horrocks'), arxiv.Result.Author('Charles Sutton')], summary=\"The usefulness of tabular data such as web tables critically depends on\\nunderstanding their semantics. This study focuses on column type prediction for\\ntables without any meta data. Unlike traditional lexical matching-based\\nmethods, we propose a deep prediction model that can fully exploit a table's\\ncontextual semantics, including table locality features learned by a Hybrid\\nNeural Network (HNN), and inter-column semantics features learned by a\\nknowledge base (KB) lookup and query answering algorithm.It exhibits good\\nperformance not only on individual table sets, but also when transferring from\\none table set to another.\", comment='7 pages', journal_ref='IJCAI 2019', doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.00781v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.00781v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.05366v2', updated=datetime.datetime(2020, 9, 3, 17, 19, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 12, 20, 10, 29, tzinfo=datetime.timezone.utc), title='Geo-L: Linking Geospatial Data Made Easy', authors=[arxiv.Result.Author('Christian Zinke-Wehlmann'), arxiv.Result.Author('Amit Kirschenbaum')], summary='Geospatial Linked Data is an emerging domain with growing interest in\\nresearch and industry. There is an increasing number of publicly available\\ngeospatial Linked Data resources and they need to be interlinked and easily\\nintegrated with private and industrial Linked Data on the Web. The present\\npaper introduces Geo-L, a system for discovery of RDF spatial links based on\\ntopological relations. Experiments show that the proposed system improves\\nstate-of-the-art spatial linking processes in terms of mapping-time and\\n-accuracy, as well as concerning resources retrieval efficiency and robustness.', comment='18 pages, 10 figures, 2 tables', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.05366v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.05366v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.06085v1', updated=datetime.datetime(2019, 6, 14, 9, 16, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 14, 9, 16, 16, tzinfo=datetime.timezone.utc), title='DeepSPACE: Approximate Geospatial Query Processing with Deep Learning', authors=[arxiv.Result.Author('Dimitri Vorona'), arxiv.Result.Author('Andreas Kipf'), arxiv.Result.Author('Thomas Neumann'), arxiv.Result.Author('Alfons Kemper')], summary='The amount of the available geospatial data grows at an ever faster pace.\\nThis leads to the constantly increasing demand for processing power and storage\\nin order to provide data analysis in a timely manner. At the same time, a lot\\nof geospatial processing is visual and exploratory in nature, thus having\\nbounded precision requirements. We present DeepSPACE, a deep learning-based\\napproximate geospatial query processing engine which combines modest hardware\\nrequirements with the ability to answer flexible aggregation queries while\\nkeeping the required state to a few hundred KiBs.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.06085v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.06085v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.08574v1', updated=datetime.datetime(2019, 6, 20, 12, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 20, 12, 22, tzinfo=datetime.timezone.utc), title='Extracting Basic Graph Patterns from Triple Pattern Fragment Logs', authors=[arxiv.Result.Author('Nassopoulos Georges'), arxiv.Result.Author('Serrano-Alvarado Patricia'), arxiv.Result.Author('Molli Pascal'), arxiv.Result.Author('Desmontils Emmanuel')], summary='The Triple Pattern Fragment (TPF) approach is de-facto a new way to publish\\nLinked Data at low cost and with high server availability. However, data\\nproviders hosting TPF servers are not able to analyze the SPARQL queries they\\nexecute because they only receive and evaluate queries with one triple pattern.\\nIn this paper, we propose LIFT: an algorithm to extract Basic Graph Patterns\\n(BGPs) of executed queries from TPF server logs. Experiments show that LIFT\\nextracts BGPs with good precision and good recall generating limited noise.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.08574v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.08574v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.06258v1', updated=datetime.datetime(2018, 1, 19, 0, 2, 34, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 19, 0, 2, 34, tzinfo=datetime.timezone.utc), title='Towards a Theory of Data-Diff: Optimal Synthesis of Succinct Data Modification Scripts', authors=[arxiv.Result.Author('Tana Wattanawaroon'), arxiv.Result.Author('Stephen Macke'), arxiv.Result.Author('Aditya Parameswaran')], summary='This paper addresses the Data-Diff problem: given a dataset and a subsequent\\nversion of the dataset, find the shortest sequence of operations that\\ntransforms the dataset to the subsequent version, under a restricted family of\\noperations. We consider operations similar to SQL UPDATE, each with a condition\\n(WHERE) that matches a subset of tuples and a modifier (SET) that makes changes\\nto those matched tuples. We characterize the problem based on different\\nconstraints on the attributes and the allowed conditions and modifiers,\\nproviding complexity classification and algorithms in each case.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1801.06258v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.06258v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.08052v1', updated=datetime.datetime(2018, 1, 24, 16, 7, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 24, 16, 7, 39, tzinfo=datetime.timezone.utc), title='The Historic Development of the Zooarchaeological Database OssoBook and the xBook Framework for Scientific Databases', authors=[arxiv.Result.Author('Daniel Kaltenthaler'), arxiv.Result.Author('Johannes-Y. Lohrer')], summary='In this technical report, we describe the historic development of the\\nzooarchaeological database OssoBook and the resulting framework xBook, a\\ngeneric infrastructure for distributed, relational data management that is\\nmainly designed for the needs of scientific data. We describe the concepts of\\nthe architecture and its most important features. We especially point out the\\nServer-Client architecture, the synchronization process, the Launcher\\napplication, and the structure and features of the application.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1801.08052v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.08052v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1802.01554v1', updated=datetime.datetime(2018, 2, 5, 18, 33, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 2, 5, 18, 33, 16, tzinfo=datetime.timezone.utc), title='Can One Escape Red Chains? Regular Path Queries Determinacy is Undecidable', authors=[arxiv.Result.Author('Grzegorz Głuch'), arxiv.Result.Author('Jerzy Marcinkowski'), arxiv.Result.Author('Piotr Ostropolski-Nalewaja')], summary='For a given set of queries (which are expressions in some query language)\\n$\\\\mathcal{Q}=\\\\{Q_1$, $Q_2, \\\\ldots Q_k\\\\}$ and for another query $Q_0$ we say\\nthat $\\\\mathcal{Q}$ determines $Q_0$ if -- informally speaking -- for every\\ndatabase $\\\\mathbb D$, the information contained in the views\\n$\\\\mathcal{Q}({\\\\mathbb D})$ is sufficient to compute $Q_0({\\\\mathbb D})$. Query\\nDeterminacy Problem is the problem of deciding, for given $\\\\mathcal{Q}$ and\\n$Q_0$, whether $\\\\mathcal{Q}$ determines $Q_0$. Many versions of this problem,\\nfor different query languages, were studied in database theory. In this paper\\nwe solve a problem stated in [CGLV02] and show that Query Determinacy Problem\\nis undecidable for the Regular Path Queries -- the paradigmatic query language\\nof graph databases.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1802.01554v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1802.01554v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1802.08586v1', updated=datetime.datetime(2018, 2, 23, 15, 15, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 2, 23, 15, 15, 53, tzinfo=datetime.timezone.utc), title='Database Aggregation', authors=[arxiv.Result.Author('Francesco Belardinelli'), arxiv.Result.Author('Umberto Grandi')], summary='Knowledge can be represented compactly in a multitude ways, from a set of\\npropositional formulas, to a Kripke model, to a database. In this paper we\\nstudy the aggregation of information coming from multiple sources, each source\\nsubmitting a database modelled as a first-order relational structure. In the\\npresence of an integrity constraint, we identify classes of aggregators that\\nrespect it in the aggregated database, provided all individual databases\\nsatisfy it. We also characterise languages for first-order queries on which the\\nanswer to queries on the aggregated database coincides with the aggregation of\\nthe answers to the query obtained on each individual database. This\\ncontribution is meant to be a first step on the application of techniques from\\nrational choice theory to knowledge representation in databases.', comment='20 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1802.08586v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1802.08586v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1802.09984v2', updated=datetime.datetime(2018, 3, 20, 18, 27, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 2, 27, 16, 1, 36, tzinfo=datetime.timezone.utc), title='Formal Semantics of the Language Cypher', authors=[arxiv.Result.Author('Nadime Francis'), arxiv.Result.Author('Alastair Green'), arxiv.Result.Author('Paolo Guagliardo'), arxiv.Result.Author('Leonid Libkin'), arxiv.Result.Author('Tobias Lindaaker'), arxiv.Result.Author('Victor Marsault'), arxiv.Result.Author('Stefan Plantikow'), arxiv.Result.Author('Mats Rydberg'), arxiv.Result.Author('Martin Schuster'), arxiv.Result.Author('Petra Selmer'), arxiv.Result.Author('Andrés Taylor')], summary='Cypher is a query language for property graphs. It was originally designed\\nand implemented as part of the Neo4j graph database, and it is currently used\\nin a growing number of commercial systems, industrial applications and research\\nprojects. In this work, we provide denotational semantics of the core fragment\\nof the read-only part of Cypher, which features in particular pattern matching,\\nfiltering, and most relational operations on tables.', comment='22 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.PL'], links=[arxiv.Result.Link('http://arxiv.org/abs/1802.09984v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1802.09984v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1811.10855v1', updated=datetime.datetime(2018, 11, 27, 7, 54, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 11, 27, 7, 54, 43, tzinfo=datetime.timezone.utc), title='Data Management in Time-Domain Astronomy: Requirements and Challenges', authors=[arxiv.Result.Author('Chen Yang'), arxiv.Result.Author('Xiaofeng Meng'), arxiv.Result.Author('Zhihui Du'), arxiv.Result.Author('Zhiqiang Duan'), arxiv.Result.Author('Yongjie Du')], summary='In time-domain astronomy, we need to use the relational database to manage\\nstar catalog data. With the development of sky survey technology, the size of\\nstar catalog data is larger, and the speed of data generation is faster. So, in\\nthis paper, we make a systematic and comprehensive introduction to process the\\ndata in time-domain astronomy, and valuable research questions are detailed.\\nThen, we list candidate systems usually used in astronomy and point out the\\nadvantages and disadvantages of these systems. In addition, we present the key\\ntechniques needed to deal with astronomical data. Finally, we summarize the\\nchallenges faced by the design of our database prototype.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1811.10855v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1811.10855v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.07388v1', updated=datetime.datetime(2019, 1, 18, 13, 34, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 18, 13, 34, 26, tzinfo=datetime.timezone.utc), title='Improving the data quality in the research information systems', authors=[arxiv.Result.Author('Otmane Azeroual'), arxiv.Result.Author('Mohammad Abuosba')], summary='In order to introduce an integrated research information system, this will\\nprovide scientific institutions with the necessary information on research\\nactivities and research results in assured quality. Since data collection,\\nduplication, missing values, incorrect formatting, inconsistencies, etc. can\\narise in the collection of research data in different research information\\nsystems, which can have a wide range of negative effects on data quality, the\\nsubject of data quality should be treated with better results. This paper\\nexamines the data quality problems in research information systems and presents\\nthe new techniques that enable organizations to improve their quality of\\nresearch information.', comment='15(11), pp. 82-86. arXiv admin note: substantial text overlap with\\n  arXiv:1901.06208', journal_ref='International Journal of Computer Science and Information\\n  Security, IJCSIS, 2017', doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.07388v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.07388v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.08248v1', updated=datetime.datetime(2019, 1, 24, 6, 34, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 24, 6, 34, 15, tzinfo=datetime.timezone.utc), title='TigerGraph: A Native MPP Graph Database', authors=[arxiv.Result.Author('Alin Deutsch'), arxiv.Result.Author('Yu Xu'), arxiv.Result.Author('Mingxi Wu'), arxiv.Result.Author('Victor Lee')], summary=\"We present TigerGraph, a graph database system built from the ground up to\\nsupport massively parallel computation of queries and analytics.\\n  TigerGraph's high-level query language, GSQL, is designed for compatibility\\nwith SQL, while simultaneously allowing NoSQL programmers to continue thinking\\nin Bulk-Synchronous Processing (BSP) terms and reap the benefits of high-level\\nspecification.\\n  GSQL is sufficiently high-level to allow declarative SQL-style programming,\\nyet sufficiently expressive to concisely specify the sophisticated iterative\\nalgorithms required by modern graph analytics and traditionally coded in\\ngeneral-purpose programming languages like C++ and Java.\\n  We report very strong scale-up and scale-out performance over a benchmark we\\npublished on GitHub for full reproducibility.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.08248v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.08248v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.02949v2', updated=datetime.datetime(2019, 3, 15, 1, 33, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 7, 14, 46, 4, tzinfo=datetime.timezone.utc), title='SAVIME: A Multidimensional System for the Analysis and Visualization of Simulation Data', authors=[arxiv.Result.Author('Hermano Lustosa'), arxiv.Result.Author('Fabio Porto')], summary='Scientific applications produce a huge amount of data, which imposes serious\\nmanagement and analysis challenges. In particular, limitations in current\\ndatabase management systems prevent their adoption in simulation applications,\\nin which in-situ analysis libraries, in-transit I/O interfaces and scientific\\nformat files are preferred over DBMSs. In order to make simulation applications\\nbenefit from DBMS support, the author proposes the development of a system\\ncalled SAVIME in the context of his PhD thesis. SAVIME is an array database\\nsystem designed to manage numerical simulation data. In this document, the\\nauthor presents all work conducted so far and the current state of development.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1903.02949v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.02949v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.05228v1', updated=datetime.datetime(2019, 3, 12, 21, 29, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 12, 21, 29, 8, tzinfo=datetime.timezone.utc), title='Distributed Dependency Discovery', authors=[arxiv.Result.Author('Hemant Saxena'), arxiv.Result.Author('Lukasz Golab'), arxiv.Result.Author('Ihab F. Ilyas')], summary='We analyze the problem of discovering dependencies from distributed big data.\\nExisting (non-distributed) algorithms focus on minimizing computation by\\npruning the search space of possible dependencies. However, distributed\\nalgorithms must also optimize communication costs, especially in shared-nothing\\nsettings, leading to a more complex optimization space. To understand this\\nspace, we introduce six primitives shared by existing dependency discovery\\nalgorithms, corresponding to data processing steps separated by communication\\nbarriers. Through case studies, we show how the primitives allow us to analyze\\nthe design space and develop communication-optimized implementations. Finally,\\nwe support our analysis with an experimental evaluation on real datasets.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1903.05228v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.05228v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.10579v1', updated=datetime.datetime(2019, 3, 25, 20, 8, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 25, 20, 8, 22, tzinfo=datetime.timezone.utc), title='Categorical Data Integration for Computational Science', authors=[arxiv.Result.Author('Kristopher Brown'), arxiv.Result.Author('David I. Spivak'), arxiv.Result.Author('Ryan Wisnesky')], summary='Categorical Query Language is an open-source query and data integration\\nscripting language that can be applied to common challenges in the field of\\ncomputational science. We discuss how the structure-preserving nature of CQL\\ndata migrations protect those who publicly share data from the\\nmisinterpretation of their data. Likewise, this feature of CQL migrations\\nallows those who draw from public data sources to be sure only data which meets\\ntheir specification will actually be transferred. We argue some open problems\\nin the field of data sharing in computational science are addressable by\\nworking within this paradigm of functorial data migration. We demonstrate these\\ntools by integrating data from the Open Quantum Materials Database with some\\nalternative materials databases.', comment='10 pages, 5 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1903.10579v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.10579v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.05146v2', updated=datetime.datetime(2020, 11, 1, 11, 10, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 11, 12, 57, 17, tzinfo=datetime.timezone.utc), title='Analysis of Co-Occurrence Patterns in Data through Modular and Clan Decompositions of Gaifman Graphs', authors=[arxiv.Result.Author('Marie Ely Piceno'), arxiv.Result.Author('José Luis Balcázar')], summary='We argue that the existing knowledge about modular decomposition of graphs\\nand clan decomposition of 2-structures can be put to use advantageously in a\\ncontext of data analysis. We show how to obtain visual descriptions of\\nco-occurrence patterns by employing these decompositions on possibly\\ngeneralized Gaifman graphs associated to datasets. We provide both theoretical\\nadvances that connect the proposed process to other data mining aspects\\n(namely, closed set mining), as well as implemented algorithmics leading to an\\nopen-source tool that demonstrates our approach.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.05146v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.05146v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.10350v1', updated=datetime.datetime(2019, 10, 23, 4, 53, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 23, 4, 53, 16, tzinfo=datetime.timezone.utc), title='A Queue-oriented Transaction Processing Paradigm', authors=[arxiv.Result.Author('Thamir M. Qadah')], summary='Transaction processing has been an active area of research for several\\ndecades. A fundamental characteristic of classical transaction processing\\nprotocols is non-determinism, which causes them to suffer from performance\\nissues on modern computing environments such as main-memory databases using\\nmany-core, and multi-socket CPUs and distributed environments. Recent proposals\\nof deterministic transaction processing techniques have shown great potential\\nin addressing these performance issues. In this position paper, I argue for a\\nqueue-oriented transaction processing paradigm that leads to better design and\\nimplementation of deterministic transaction processing protocols. I support my\\napproach with extensive experimental evaluations and demonstrate significant\\nperformance gains.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.10350v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.10350v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.05667v1', updated=datetime.datetime(2020, 1, 16, 6, 22, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 16, 6, 22, 51, tzinfo=datetime.timezone.utc), title='Hardware-Conscious Stream Processing: A Survey', authors=[arxiv.Result.Author('Shuhao Zhang'), arxiv.Result.Author('Feng Zhang'), arxiv.Result.Author('Yingjun Wu'), arxiv.Result.Author('Bingsheng He'), arxiv.Result.Author('Paul Johns')], summary='Data stream processing systems (DSPSs) enable users to express and run stream\\napplications to continuously process data streams. To achieve real-time data\\nanalytics, recent researches keep focusing on optimizing the system latency and\\nthroughput. Witnessing the recent great achievements in the computer\\narchitecture community, researchers and practitioners have investigated the\\npotential of adoption hardware-conscious stream processing by better utilizing\\nmodern hardware capacity in DSPSs. In this paper, we conduct a systematic\\nsurvey of recent work in the field, particularly along with the following three\\ndirections: 1) computation optimization, 2) stream I/O optimization, and 3)\\nquery deployment. Finally, we advise on potential future research directions.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.05667v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.05667v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.08329v1', updated=datetime.datetime(2020, 1, 23, 1, 27, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 23, 1, 27, 41, tzinfo=datetime.timezone.utc), title='Leveraging Neighborhood Summaries for Efficient RDF Queries on RDBMS', authors=[arxiv.Result.Author('Lei Gai')], summary='Using structural informations to summarize graph-structured RDF data is\\nhelpful in tackling query performance issues. However, leveraging structural\\nindexes needs to revise or even redesign the internal of RDF systems. Given an\\nRDF dataset that have already been bulk loaded into a relational RDF system, we\\naim at improving the query performance on such systems. We do so by summarizing\\nneighborhood structures and encoding them into triples which can be managed\\nalong side the exist instance data. At query time, we optimally select the\\neffective structural patterns, and adding these patterns to the existing\\nqueries to gain an improved query performance. Empirical evaluations shown the\\neffectiveness of our method.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.08329v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.08329v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.11324v1', updated=datetime.datetime(2020, 1, 29, 7, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 29, 7, 30, tzinfo=datetime.timezone.utc), title='Proceedings of Symposium on Data Mining Applications 2014', authors=[arxiv.Result.Author('Basit Qureshi'), arxiv.Result.Author('Yasir Javed')], summary='The Symposium on Data Mining and Applications (SDMA 2014) is aimed to gather\\nresearchers and application developers from a wide range of data mining related\\nareas such as statistics, computational intelligence, pattern recognition,\\ndatabases, Big Data Mining and visualization. SDMA is organized by MEGDAM to\\nadvance the state of the art in data mining research field and its various real\\nworld applications. The symposium will provide opportunities for technical\\ncollaboration among data mining and machine learning researchers around the\\nSaudi Arabia, GCC countries and Middle-East region. Acceptance will be based\\nprimarily on originality, significance and quality of contribution.', comment='Proceedings of Symposium on Data Mining Applications 2014', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'J.7; B.8; E.4'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.11324v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.11324v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.11506v1', updated=datetime.datetime(2020, 1, 30, 7, 45, 18, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 30, 7, 45, 18, tzinfo=datetime.timezone.utc), title='Theoretical Model and Practical Considerations for Data Lineage Reconstruction', authors=[arxiv.Result.Author('Egor Pushkin')], summary=\"We live in a world driven by data. The amount of it outgrows anyone's ability\\nto oversee it or even observe its scope. Along with all the advances in the\\nspace of data management, there is still a significant lack of formalism and\\nstandardization around defining data ecosystems and processes occurring within\\nthose. In order to address the issue we propose a notation for data flow\\nmodeling and evaluate some of the most common applications of it based on\\nreal-world use cases. To facilitate future work, we provide detailed reference\\nof the data model we defined and consider potential programming paradigms.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.11506v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.11506v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.05134v1', updated=datetime.datetime(2020, 6, 9, 9, 21, 18, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 9, 9, 21, 18, tzinfo=datetime.timezone.utc), title='Dynamic Interleaving of Content and Structure for Robust Indexing of Semi-Structured Hierarchical Data (Extended Version)', authors=[arxiv.Result.Author('Kevin Wellenzohn'), arxiv.Result.Author('Michael H. Böhlen'), arxiv.Result.Author('Sven Helmer')], summary='We propose a robust index for semi-structured hierarchical data that supports\\ncontent-and-structure (CAS) queries specified by path and value predicates. At\\nthe heart of our approach is a novel dynamic interleaving scheme that merges\\nthe path and value dimensions of composite keys in a balanced way. We store\\nthese keys in our trie-based Robust Content-And-Structure index, which\\nefficiently supports a wide range of CAS queries, including queries with\\nwildcards and descendant axes. Additionally, we show important properties of\\nour scheme, such as robustness against varying selectivities, and demonstrate\\nimprovements of up to two orders of magnitude over existing approaches in our\\nexperimental evaluation.', comment=None, journal_ref=None, doi='10.14778/3401960.3401963', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3401960.3401963', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2006.05134v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.05134v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.08224v1', updated=datetime.datetime(2020, 6, 15, 8, 54, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 15, 8, 54, 22, tzinfo=datetime.timezone.utc), title=\"Needles in the 'Sheet'stack: Augmented Analytics to get Insights from Spreadsheets\", authors=[arxiv.Result.Author('Medha Atre'), arxiv.Result.Author('Anand Deshpande'), arxiv.Result.Author('Reshma Godse'), arxiv.Result.Author('Pooja Deokar'), arxiv.Result.Author('Sandip Moharir'), arxiv.Result.Author('Dhruva Ray'), arxiv.Result.Author('Akshay Chitlangia'), arxiv.Result.Author('Trupti Phadnis'), arxiv.Result.Author('Yugansh Goyal')], summary='Business intelligence (BI) tools for database analytics have come a long way\\nand nowadays also provide ready insights or visual query explorations, e.g.\\nQuickInsights by Microsoft Power BI, SpotIQ by ThoughtSpot, Zenvisage, etc. In\\nthis demo, we focus on providing insights by examining periodic spreadsheets of\\ndifferent reports (aka views), without prior knowledge of the schema of the\\ndatabase or reports, or data information. Such a solution is targeted at users\\nwithout the familiarity with the database schema or resources to conduct\\nanalytics in the contemporary way.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.HC', 'H.2.8'], links=[arxiv.Result.Link('http://arxiv.org/abs/2006.08224v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.08224v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.08842v1', updated=datetime.datetime(2020, 6, 16, 0, 40, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 16, 0, 40, 50, tzinfo=datetime.timezone.utc), title='Index Selection for NoSQL Database with Deep Reinforcement Learning', authors=[arxiv.Result.Author('Shun Yao'), arxiv.Result.Author('Hongzhi Wang'), arxiv.Result.Author('Yu Yan')], summary='We propose a new approach of NoSQL database index selection. For different\\nworkloads, we select different indexes and their different parameters to\\noptimize the database performance. The approach builds a deep reinforcement\\nlearning model to select an optimal index for a given fixed workload and adapts\\nto a changing workload. Experimental results show that, Deep Reinforcement\\nLearning Index Selection Approach (DRLISA) has improved performance to varying\\ndegrees according to traditional single index structures.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2006.08842v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.08842v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.00412v1', updated=datetime.datetime(2021, 6, 1, 11, 52, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 1, 11, 52, 59, tzinfo=datetime.timezone.utc), title='Curating Covid-19 data in Links', authors=[arxiv.Result.Author('Vashti Galpin'), arxiv.Result.Author('James Cheney')], summary='Curated scientific databases play an important role in the scientific\\nendeavour and support is needed for the significant effort that goes into their\\ncreation and maintenance. This demonstration and case study illustrate how\\ncuration support has been developed in the Links cross-tier programming\\nlanguage, a functional, strongly typed language with language-integrated query\\nand support for temporal databases. The chosen case study uses weekly released\\nCovid-19 fatality figures from the Scottish government which exhibit updates to\\npreviously released data. This data allows the capture and query of update\\nprovenance in our prototype. This demonstration will highlight the potential\\nfor language-integrated support for curation to simplify and streamline\\nprototyping of web-applications in support of scientific databases', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DL', 'cs.PL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.00412v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.00412v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.07837v1', updated=datetime.datetime(2021, 6, 15, 2, 6, 34, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 15, 2, 6, 34, tzinfo=datetime.timezone.utc), title='A Survey on Mining and Analysis of Uncertain Graphs', authors=[arxiv.Result.Author('Suman Banerjee')], summary='\\\\emph{Uncertain Graph} (also known as \\\\emph{Probabilistic Graph}) is a\\ngeneric model to represent many real\\\\mbox{-}world networks from social to\\nbiological. In recent times analysis and mining of uncertain graphs have drawn\\nsignificant attention from the researchers of the data management community.\\nSeveral noble problems have been introduced and efficient methodologies have\\nbeen developed to solve those problems. Hence, there is a need to summarize the\\nexisting results on this topic in a self\\\\mbox{-}organized way. In this paper,\\nwe present a comprehensive survey on uncertain graph mining focusing on mainly\\nthree aspects: (i) different problems studied, (ii) computational challenges\\nfor solving those problems, and (iii) proposed methodologies. Finally, we list\\nout important future research directions.', comment='46 Pages, 2 Figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.SI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.07837v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.07837v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.11456v2', updated=datetime.datetime(2021, 6, 25, 3, 18, 36, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 22, 0, 17, 6, tzinfo=datetime.timezone.utc), title='Querying in the Age of Graph Databases and Knowledge Graphs', authors=[arxiv.Result.Author('Marcelo Arenas'), arxiv.Result.Author('Claudio Gutierrez'), arxiv.Result.Author('Juan F. Sequeda')], summary='Graphs have become the best way we know of representing knowledge. The\\ncomputing community has investigated and developed the support for managing\\ngraphs by means of digital technology. Graph databases and knowledge graphs\\nsurface as the most successful solutions to this program. The goal of this\\ndocument is to provide a conceptual map of the data management tasks underlying\\nthese developments, paying particular attention to data models and query\\nlanguages for graphs.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.11456v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.11456v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.15664v1', updated=datetime.datetime(2021, 6, 29, 18, 14, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 29, 18, 14, 26, tzinfo=datetime.timezone.utc), title='Is 2NF a Stable Normal Form?', authors=[arxiv.Result.Author('Amir Sapir'), arxiv.Result.Author('Ariel Sapir')], summary='Traditionally, it was accepted that a relational database can be normalized\\nstep-by-step, from a set of un-normalized tables to tables in $1NF$, then to\\n$2NF$, then to $3NF$, then (possibly) to $BCNF$. The rule applied to a table in\\n$1NF$ in order to transform it to a set of tables in $2NF$ seems to be too\\nstraightforward to pose any difficulty.\\n  However, we show that, depending on the set of functional dependencies, it is\\nimpossible to reach $2NF$ and stop there; one must, in these cases, perform the\\nnormalization from $1NF$ to $3NF$ as an indecomposable move. The minimal setup\\nto exhibit the phenomena requires a single composite key, and two partially\\noverlapping chains of transitive dependencies.', comment='10 pages, 1 figure', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', '68P15', 'H.2.1; H.2.5'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.15664v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.15664v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.00465v1', updated=datetime.datetime(2018, 4, 2, 12, 8, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 2, 12, 8, 1, tzinfo=datetime.timezone.utc), title='Database as a Service - Current Issues and Its Future', authors=[arxiv.Result.Author('Xi Zheng')], summary='With the prevalence of applications in cloud, Database as a Service (DBaaS)\\nbecomes a promising method to provide cloud applications with reliable and\\nflexible data storage services. It provides a number of interesting features to\\ncloud developers, however, it suffers a few drawbacks: long learning curve and\\ndevelopment cycle, lacking of in-depth support for NoSQL, lacking of flexible\\nconfiguration for security and privacy, and high cost models. In this paper, we\\ninvestigate these issues among current DBaaS providers and propose a novel\\nTrinity Model that can significantly reduce the learning curves, improve the\\nsecurity and privacy, and accelerate database design and development. We\\nfurther elaborate our ongoing and future work on developing large real-world\\nSaaS projects using this new DBaaS model.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1804.00465v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.00465v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.05892v1', updated=datetime.datetime(2018, 4, 16, 18, 54, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 16, 18, 54, 11, tzinfo=datetime.timezone.utc), title='Accelerating Human-in-the-loop Machine Learning: Challenges and Opportunities', authors=[arxiv.Result.Author('Doris Xin'), arxiv.Result.Author('Litian Ma'), arxiv.Result.Author('Jialin Liu'), arxiv.Result.Author('Stephen Macke'), arxiv.Result.Author('Shuchen Song'), arxiv.Result.Author('Aditya Parameswaran')], summary='Development of machine learning (ML) workflows is a tedious process of\\niterative experimentation: developers repeatedly make changes to workflows\\nuntil the desired accuracy is attained. We describe our vision for a\\n\"human-in-the-loop\" ML system that accelerates this process: by intelligently\\ntracking changes and intermediate results over time, such a system can enable\\nrapid iteration, quick responsive feedback, introspection and debugging, and\\nbackground execution and automation. We finally describe Helix, our preliminary\\nattempt at such a system that has already led to speedups of up to 10x on\\ntypical iterative workflows against competing systems.', comment=\"to be published in SIGMOD '18 DEEM Workshop\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1804.05892v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.05892v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.08834v3', updated=datetime.datetime(2018, 7, 12, 20, 42, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 24, 4, 4, 27, tzinfo=datetime.timezone.utc), title='Measuring and Computing Database Inconsistency via Repairs', authors=[arxiv.Result.Author('Leopoldo Bertossi')], summary='We propose a generic numerical measure of inconsistency of a database with\\nrespect to a set of integrity constraints. It is based on an abstract repair\\nsemantics. A particular inconsistency measure associated to cardinality-repairs\\nis investigated; and we show that it can be computed via answer-set programs.\\n  Keywords: Integrity constraints in databases, inconsistent databases,\\ndatabase repairs, inconsistency measure.', comment='Submission as short paper; to appear in Proc. Scalable Uncertainty\\n  Management, SUM 2018. Abstract and keywords added', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LO'], links=[arxiv.Result.Link('http://arxiv.org/abs/1804.08834v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.08834v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.00680v1', updated=datetime.datetime(2018, 5, 2, 8, 53, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 2, 8, 53, 35, tzinfo=datetime.timezone.utc), title='BUDAMAF: Data Management in Cloud Federations', authors=[arxiv.Result.Author('Evangelos Psomakelis'), arxiv.Result.Author('Konstantinos Tserpes'), arxiv.Result.Author('Dimosthenis Anagnostopoulos'), arxiv.Result.Author('Theodora Varvarigou')], summary='Data management has always been a multi-domain problem even in the simplest\\ncases. It involves, quality of service, security, resource management, cost\\nmanagement, incident identification, disaster avoidance and/or recovery, as\\nwell as many other concerns. In our case, this situation gets ever more\\ncomplicated because of the divergent nature of a cloud federation like BASMATI.\\nIn this federation, the BASMATI Unified Data Management Framework (BUDaMaF),\\ntries to create an automated uniform way of managing all the data transactions,\\nas well as the data stores themselves, in a polyglot multi-cloud, consisting of\\na plethora of different machines and data store systems.', comment='11 pages, 2 figures, conference', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC', 'cs.PF'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.00680v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.00680v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.05235v2', updated=datetime.datetime(2018, 8, 11, 14, 6, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 14, 15, 37, 2, tzinfo=datetime.timezone.utc), title='Decomposition of quantitative Gaifman graphs as a data analysis tool', authors=[arxiv.Result.Author('José Luis Balcázar'), arxiv.Result.Author('Marie Ely Piceno'), arxiv.Result.Author('Laura Rodríguez-Navas')], summary='We argue the usefulness of Gaifman graphs of first-order relational\\nstructures as an exploratory data analysis tool. We illustrate our approach\\nwith cases where the modular decompositions of these graphs reveal interesting\\nfacts about the data. Then, we introduce generalized notions of Gaifman graphs,\\nenhanced with quantitative information, to which we can apply more general,\\nexisting decomposition notions via 2-structures; thus enlarging the analytical\\ncapabilities of the scheme. The very essence of Gaifman graphs makes this\\napproach immediately appropriate for the multirelational data framework.', comment='Accepted for presentation at: Intelligent Data Analysis 2018', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.05235v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.05235v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.09157v1', updated=datetime.datetime(2018, 5, 21, 23, 29, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 21, 23, 29, 9, tzinfo=datetime.timezone.utc), title='A New Finitely Controllable Class of Tuple Generating Dependencies: The Triangularly-Guarded Class', authors=[arxiv.Result.Author('Vernon Asuncion'), arxiv.Result.Author('Yan Zhang')], summary='In this paper we introduce a new class of tuple-generating dependencies\\n(TGDs) called triangularly-guarded (TG) TGDs. We show that conjunctive query\\nanswering under this new class of TGDs is decidable since this new class of\\nTGDs also satisfies the finite controllability (FC) property. We further show\\nthat this new class strictly contains some other decidable classes such as\\nweak-acyclic, guarded, sticky and shy. In this sense, the class TG provides a\\nunified representation of all these aforementioned classes of TGDs.', comment='Submitted for review. arXiv admin note: substantial text overlap with\\n  arXiv:1804.05997', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LO'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.09157v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.09157v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1809.00677v2', updated=datetime.datetime(2018, 12, 18, 11, 16, 34, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 9, 3, 18, 5, 12, tzinfo=datetime.timezone.utc), title='Learned Cardinalities: Estimating Correlated Joins with Deep Learning', authors=[arxiv.Result.Author('Andreas Kipf'), arxiv.Result.Author('Thomas Kipf'), arxiv.Result.Author('Bernhard Radke'), arxiv.Result.Author('Viktor Leis'), arxiv.Result.Author('Peter Boncz'), arxiv.Result.Author('Alfons Kemper')], summary='We describe a new deep learning approach to cardinality estimation. MSCN is a\\nmulti-set convolutional network, tailored to representing relational query\\nplans, that employs set semantics to capture query features and true\\ncardinalities. MSCN builds on sampling-based estimation, addressing its\\nweaknesses when no sampled tuples qualify a predicate, and in capturing\\njoin-crossing correlations. Our evaluation of MSCN using a real-world dataset\\nshows that deep learning significantly enhances the quality of cardinality\\nestimation, which is the core problem in query optimization.', comment='CIDR 2019. https://github.com/andreaskipf/learnedcardinalities', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1809.00677v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1809.00677v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1809.03261v1', updated=datetime.datetime(2018, 9, 10, 12, 11, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 9, 10, 12, 11, 29, tzinfo=datetime.timezone.utc), title='The Skiplist-Based LSM Tree', authors=[arxiv.Result.Author('Aron Szanto')], summary='Log-Structured Merge (LSM) Trees provide a tiered data storage and retrieval\\nparadigm that is attractive for write-optimized data systems. Maintaining an\\nefficient buffer in memory and deferring updates past their initial write-time,\\nthe structure provides quick operations over hot data. Because each layer of\\nthe structure is logically separate from the others, the structure is also\\nconducive to opportunistic and granular optimization. In this paper, we\\nintroduce the Skiplist-Based LSM Tree (sLSM), a novel system in which the\\nmemory buffer of the LSM is composed of a sequence of skiplists. We develop\\ntheoretical and experimental results that demonstrate that the breadth of\\ntuning parameters inherent to the sLSM allows it broad flexibility for\\nexcellent performance across a wide variety of workloads.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1809.03261v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1809.03261v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1809.03822v1', updated=datetime.datetime(2018, 9, 11, 12, 39, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 9, 11, 12, 39, 17, tzinfo=datetime.timezone.utc), title='Integration of Relational and Graph Databases Functionally', authors=[arxiv.Result.Author('Jaroslav Pokorny')], summary='A significant category of NoSQL approaches is known as graph da-tabases. They\\nare usually represented by one property graph. We introduce a functional\\napproach to modelling relations and property graphs. Single-valued and\\nmultivalued functions will be sufficient in this case. Then, a typed\\n{\\\\lambda}-calculus, i.e., the language of lambda terms, will be used as a data\\nmanipulation lan-guage. Some integration options at the query language level\\nare discussed.', comment='In the Pre-proceedings of the Semantics in Big Data Management\\n  workshop, IFIP W.G. 2.6 on Database, Tuesday 18th September 2018 at IFIP\\n  World Computer Congress 2018 - Poznan, Poland', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1809.03822v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1809.03822v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1809.10286v3', updated=datetime.datetime(2019, 1, 22, 17, 14, 34, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 9, 27, 0, 57, 33, tzinfo=datetime.timezone.utc), title='Repair-Based Degrees of Database Inconsistency: Computation and Complexity', authors=[arxiv.Result.Author('Leopoldo Bertossi')], summary='We propose a generic numerical measure of the inconsistency of a database\\nwith respect to a set of integrity constraints. It is based on an abstract\\nrepair semantics. In particular, an inconsistency measure associated to\\ncardinality-repairs is investigated in detail. More specifically, it is shown\\nthat it can be computed via answer-set programs, but sometimes its computation\\ncan be intractable in data complexity. However, polynomial-time deterministic\\nand randomized approximations are exhibited. The behavior of this measure under\\nsmall updates is analyzed, obtaining fixed-parameter tractability results.\\nFurthermore, alternative inconsistency measures are proposed and discussed.', comment='Some editing made and some new paragraphs added', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1809.10286v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1809.10286v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1809.10856v2', updated=datetime.datetime(2018, 10, 15, 3, 15, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 9, 28, 4, 47, 54, tzinfo=datetime.timezone.utc), title='Answering Analytical Queries on Text Data with Temporal Term Histograms', authors=[arxiv.Result.Author('Kai Lin'), arxiv.Result.Author('Subhasis Dasgupta'), arxiv.Result.Author('Amarnath Gupta')], summary=\"Temporal text, i.e., time-stamped text data are found abundantly in a variety\\nof data sources like newspapers, blogs and social media posts. While today's\\ndata management systems provide facilities for searching full-text data, they\\ndo not provide any simple primitives for performing analytical operations with\\ntext. This paper proposes the temporal term histograms (TTH) as an intermediate\\nprimitive that can be used for analytical tasks. We propose an algebra, with\\noperators and equivalence rules for TTH and present a reference implementation\\non a relational database system.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1809.10856v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1809.10856v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.00079v4', updated=datetime.datetime(2021, 1, 27, 22, 47, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 29, 20, 17, 48, tzinfo=datetime.timezone.utc), title='Query the model: precomputations for efficient inference with Bayesian Networks', authors=[arxiv.Result.Author('Cigdem Aslay'), arxiv.Result.Author('Martino Ciaperoni'), arxiv.Result.Author('Aristides Gionis'), arxiv.Result.Author('Michael Mathioudakis')], summary='Variable Elimination is a fundamental algorithm for probabilistic inference\\nover Bayesian networks. In this paper, we propose a novel materialization\\nmethod for Variable Elimination, which can lead to significant efficiency gains\\nwhen answering inference queries. We evaluate our technique using real-world\\nBayesian networks. Our results show that a modest amount of materialization can\\nlead to significant improvements in the running time of queries. Furthermore,\\nin comparison with junction tree methods that also rely on materialization, our\\napproach achieves comparable efficiency during inference using significantly\\nlighter materialization.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.00079v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.00079v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.01614v3', updated=datetime.datetime(2019, 6, 6, 8, 38, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 2, 18, 39, 16, tzinfo=datetime.timezone.utc), title='Persistent Memory I/O Primitives', authors=[arxiv.Result.Author('Alexander van Renen'), arxiv.Result.Author('Lukas Vogel'), arxiv.Result.Author('Viktor Leis'), arxiv.Result.Author('Thomas Neumann'), arxiv.Result.Author('Alfons Kemper')], summary=\"I/O latency and throughput is one of the major performance bottlenecks for\\ndisk-based database systems. Upcoming persistent memory (PMem) technologies,\\nlike Intel's Optane DC Persistent Memory Modules, promise to bridge the gap\\nbetween NAND-based flash (SSD) and DRAM, and thus eliminate the I/O bottleneck.\\nIn this paper, we provide one of the first performance evaluations of PMem in\\nterms of bandwidth and latency. Based on the results, we develop guidelines for\\nefficient PMem usage and two essential I/O primitives tuned for PMem: log\\nwriting and block flushing.\", comment='7 pages, 6 figures, DaMoN 2019', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.01614v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.01614v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.03403v1', updated=datetime.datetime(2019, 4, 6, 9, 48, 57, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 6, 9, 48, 57, tzinfo=datetime.timezone.utc), title='Inconsistency Measures for Relational Databases', authors=[arxiv.Result.Author('Francesco Parisi'), arxiv.Result.Author('John Grant')], summary='In this paper, building on work done on measuring inconsistency in knowledge\\nbases, we introduce inconsistency measures for databases. In particular,\\nfocusing on databases with denial constraints, we first consider the natural\\napproach of virtually transforming a database into a propositional knowledge\\nbase and then applying well-known measures. However, using this method, tuples\\nand constraints are equally considered in charge of inconsistencies. Then, we\\nintroduce a version of inconsistency measures blaming database tuples only,\\ni.e., treating integrity constraints as irrefutable statements.\\n  We analyze the compliance of database inconsistency measures with standard\\nrationality postulates and find interesting relationships between measures.\\nFinally, we investigate the complexity of the inconsistency measurement problem\\nas well as of the problems of deciding whether the inconsistency is lower than,\\ngreater than, or equal to a given threshold.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.03403v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.03403v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.03604v1', updated=datetime.datetime(2019, 4, 7, 8, 22, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 7, 8, 22, 53, tzinfo=datetime.timezone.utc), title='BriskStream: Scaling Data Stream Processing on Shared-Memory Multicore Architectures', authors=[arxiv.Result.Author('Shuhao Zhang'), arxiv.Result.Author('Jiong He'), arxiv.Result.Author('Amelie Chi Zhou'), arxiv.Result.Author('Bingsheng He')], summary=\"We introduce BriskStream, an in-memory data stream processing system (DSPSs)\\nspecifically designed for modern shared-memory multicore architectures.\\nBriskStream's key contribution is an execution plan optimization paradigm,\\nnamely RLAS, which takes relative-location (i.e., NUMA distance) of each pair\\nof producer-consumer operators into consideration. We propose a branch and\\nbound based approach with three heuristics to resolve the resulting nontrivial\\noptimization problem. The experimental evaluations demonstrate that BriskStream\\nyields much higher throughput and better scalability than existing DSPSs on\\nmulti-core architectures when processing different types of workloads.\", comment=\"To appear in SIGMOD'19\", journal_ref='ACM SIGMOD/PODS International Conference on Management of Data\\n  2019', doi='10.1145/3299869.3300067', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3299869.3300067', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1904.03604v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.03604v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.03934v1', updated=datetime.datetime(2019, 4, 8, 10, 26, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 8, 10, 26, 35, tzinfo=datetime.timezone.utc), title='On matrices and $K$-relations', authors=[arxiv.Result.Author('Robert Brijder'), arxiv.Result.Author('Marc Gyssens'), arxiv.Result.Author('Jan Van den Bussche')], summary='We show that the matrix query language $\\\\mathsf{MATLANG}$ corresponds to a\\nnatural fragment of the positive relational algebra on $K$-relations. The\\nfragment is defined by introducing a composition operator and restricting\\n$K$-relation arities to two. We then proceed to show that $\\\\mathsf{MATLANG}$\\ncan express all matrix queries expressible in the positive relational algebra\\non $K$-relations, when intermediate arities are restricted to three. Thus we\\noffer an analogue, in a model with numerical data, to the situation in\\nclassical logic, where the algebra of binary relations is equivalent to\\nfirst-order logic with three variables.', comment='17 pages, 3 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LO'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.03934v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.03934v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.08223v1', updated=datetime.datetime(2019, 4, 17, 12, 29, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 17, 12, 29, 28, tzinfo=datetime.timezone.utc), title='Estimating Cardinalities with Deep Sketches', authors=[arxiv.Result.Author('Andreas Kipf'), arxiv.Result.Author('Dimitri Vorona'), arxiv.Result.Author('Jonas Müller'), arxiv.Result.Author('Thomas Kipf'), arxiv.Result.Author('Bernhard Radke'), arxiv.Result.Author('Viktor Leis'), arxiv.Result.Author('Peter Boncz'), arxiv.Result.Author('Thomas Neumann'), arxiv.Result.Author('Alfons Kemper')], summary='We introduce Deep Sketches, which are compact models of databases that allow\\nus to estimate the result sizes of SQL queries. Deep Sketches are powered by a\\nnew deep learning approach to cardinality estimation that can capture\\ncorrelations between columns, even across tables. Our demonstration allows\\nusers to define such sketches on the TPC-H and IMDb datasets, monitor the\\ntraining process, and run ad-hoc queries against trained sketches. We also\\nestimate query cardinalities with HyPer and PostgreSQL to visualize the gains\\nover traditional cardinality estimators.', comment=\"To appear in SIGMOD'19\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.08223v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.08223v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.12626v1', updated=datetime.datetime(2019, 4, 18, 5, 27, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 18, 5, 27, 9, tzinfo=datetime.timezone.utc), title='tsmp: An R Package for Time Series with Matrix Profile', authors=[arxiv.Result.Author('Francisco Bischoff'), arxiv.Result.Author('Pedro Pereira Rodrigues')], summary='This article describes tsmp, an R package that implements the matrix profile\\nconcept for time series. The tsmp package is a toolkit that allows all-pairs\\nsimilarity joins, motif, discords and chains discovery, semantic segmentation,\\netc. Here we describe how the tsmp package may be used by showing some of the\\nuse-cases from the original articles and evaluate the algorithm speed in the R\\nenvironment. This package can be downloaded at\\nhttps://CRAN.R-project.org/package=tsmp.', comment=None, journal_ref=None, doi='10.32614/RJ-2020-021', primary_category='cs.DB', categories=['cs.DB', 'cs.IR'], links=[arxiv.Result.Link('http://dx.doi.org/10.32614/RJ-2020-021', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1904.12626v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.12626v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.13164v1', updated=datetime.datetime(2019, 4, 30, 11, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 30, 11, 29, tzinfo=datetime.timezone.utc), title='Learning Restricted Regular Expressions with Interleaving', authors=[arxiv.Result.Author('Chunmei Dong'), arxiv.Result.Author('Yeting Li'), arxiv.Result.Author('Haiming Chen')], summary='The advantages for the presence of an XML schema for XML documents are\\nnumerous. However, many XML documents in practice are not accompanied by a\\nschema or by a valid schema. Relax NG is a popular and powerful schema\\nlanguage, which supports the unconstrained interleaving operator. Focusing on\\nthe inference of Relax NG, we propose a new subclass of regular expressions\\nwith interleaving and design a polynomial inference algorithm. Then we\\nconducted a series of experiments based on large-scale real data and on three\\nXML data corpora, and experimental results show that our subclass has a better\\npracticality than previous ones, and the regular expressions inferred by our\\nalgorithm are more precise.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.13164v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.13164v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.01135v1', updated=datetime.datetime(2019, 5, 3, 12, 10, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 3, 12, 10, 2, tzinfo=datetime.timezone.utc), title='On the Impact of Memory Allocation on High-Performance Query Processing', authors=[arxiv.Result.Author('Dominik Durner'), arxiv.Result.Author('Viktor Leis'), arxiv.Result.Author('Thomas Neumann')], summary='Somewhat surprisingly, the behavior of analytical query engines is crucially\\naffected by the dynamic memory allocator used. Memory allocators highly\\ninfluence performance, scalability, memory efficiency and memory fairness to\\nother processes. In this work, we provide the first comprehensive experimental\\nanalysis on the impact of memory allocation for high-performance query engines.\\nWe test five state-of-the-art dynamic memory allocators and discuss their\\nstrengths and weaknesses within our DBMS. The right allocator can increase the\\nperformance of TPC-DS (SF 100) by 2.7x on a 4-socket Intel Xeon server.', comment=None, journal_ref='DaMoN 2019', doi='10.1145/3329785.3329918', primary_category='cs.DB', categories=['cs.DB', 'cs.PF'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3329785.3329918', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1905.01135v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.01135v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.01294v1', updated=datetime.datetime(2019, 5, 1, 23, 39, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 1, 23, 39, 42, tzinfo=datetime.timezone.utc), title='RedisGraph GraphBLAS Enabled Graph Database', authors=[arxiv.Result.Author('Pieter Cailliau'), arxiv.Result.Author('Tim Davis'), arxiv.Result.Author('Vijay Gadepally'), arxiv.Result.Author('Jeremy Kepner'), arxiv.Result.Author('Roi Lipman'), arxiv.Result.Author('Jeffrey Lovitz'), arxiv.Result.Author('Keren Ouaknine')], summary='RedisGraph is a Redis module developed by Redis Labs to add graph database\\nfunctionality to the Redis database. RedisGraph represents connected data as\\nadjacency matrices. By representing the data as sparse matrices and employing\\nthe power of GraphBLAS (a highly optimized library for sparse matrix\\noperations), RedisGraph delivers a fast and efficient way to store, manage and\\nprocess graphs. Initial benchmarks indicate that RedisGraph is significantly\\nfaster than comparable graph databases.', comment='Accepted to IEEE IPDPS 2019 GrAPL workshop', journal_ref=None, doi='10.1109/IPDPSW.2019.00054', primary_category='cs.DB', categories=['cs.DB', 'cs.PF'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/IPDPSW.2019.00054', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1905.01294v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.01294v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.01306v1', updated=datetime.datetime(2019, 5, 3, 16, 13, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 3, 16, 13, 52, tzinfo=datetime.timezone.utc), title='Big Data Model \"Entity and Features\"', authors=[arxiv.Result.Author('Nataliya Shakhovska'), arxiv.Result.Author('Uyrii Bolubash'), arxiv.Result.Author('Oleh Veres')], summary='The article deals with the problem which led to Big Data. Big Data\\ninformation technology is the set of methods and means of processing different\\ntypes of structured and unstructured dynamic large amounts of data for their\\nanalysis and use of decision support. Features of NoSQL databases and\\ncategories are described. The developed Big Data Model \"Entity and Features\"\\nallows determining the distance between the sources of data on the availability\\nof information about a particular entity. The information structure of Big Data\\nhas been devised. It became a basis for further research and for concentrating\\non a problem of development of diverse data without their preliminary\\nintegration.', comment='8 pages, 4 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.01306v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.01306v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.04767v1', updated=datetime.datetime(2019, 5, 12, 18, 27, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 12, 18, 27, 44, tzinfo=datetime.timezone.utc), title='Moving Processing to Data: On the Influence of Processing in Memory on Data Management', authors=[arxiv.Result.Author('Tobias Vincon'), arxiv.Result.Author('Andreas Koch'), arxiv.Result.Author('Ilia Petrov')], summary='Near-Data Processing refers to an architectural hardware and software\\nparadigm, based on the co-location of storage and compute units. Ideally, it\\nwill allow to execute application-defined data- or compute-intensive operations\\nin-situ, i.e. within (or close to) the physical data storage. Thus, Near-Data\\nProcessing seeks to minimize expensive data movement, improving performance,\\nscalability, and resource-efficiency. Processing-in-Memory is a sub-class of\\nNear-Data processing that targets data processing directly within memory (DRAM)\\nchips. The effective use of Near-Data Processing mandates new architectures,\\nalgorithms, interfaces, and development toolchains.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.04767v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.04767v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.00146v1', updated=datetime.datetime(2019, 6, 29, 4, 45, 37, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 29, 4, 45, 37, tzinfo=datetime.timezone.utc), title='DataPop: Knowledge Base Population using Distributed Voice Enabled Devices', authors=[arxiv.Result.Author('Elena Montes'), arxiv.Result.Author('Monique Shotande'), arxiv.Result.Author('Daniel Helm'), arxiv.Result.Author('Christan Grant')], summary='Data scientists are constantly creating methods to efficiently and accurately\\npopulate big data sets for use in large-scale applications. Many recent efforts\\nutilize crowd-sourcing and textual interfaces. In this paper, we propose a new\\nmethod of curating data; namely, creating a multi-device Amazon Alexa Skill in\\nthe form of a research trivia game. Users experience a synchronized gaming\\nexperience with other Amazon Echo users, competing against one another while\\nfilling in gaps of a connected knowledge base. This allows for full\\nexploitation of the speed improvement offered by voice interface technology in\\na game-based format.', comment='7 pages, 2 references, unsubmitted', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1907.00146v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.00146v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.05618v1', updated=datetime.datetime(2019, 7, 12, 8, 35, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 12, 8, 35, 23, tzinfo=datetime.timezone.utc), title='Detecting coherent explorations in SQL workloads', authors=[arxiv.Result.Author('Veronika Peralta'), arxiv.Result.Author('Patrick Marcel'), arxiv.Result.Author('Willeme Verdeaux'), arxiv.Result.Author('Aboubakar Sidikhy Diakhaby')], summary='This paper presents a proposal aiming at better understanding a workload of\\nSQL queries and detecting coherent explorations hidden within the workload. In\\nparticular, our work investigates SQLShare [11], a database-as-a-service\\nplatform targeting scientists and data scientists with minimal database\\nexperience, whose workload was made available to the research community.\\nAccording to the authors of [11], this workload is the only one containing\\nprimarily ad-hoc hand-written queries over user-uploaded datasets. We analyzed\\nthis workload by extracting features that characterize SQL queries and we show\\nhow to use these features to separate sequences of SQL queries into meaningful\\nexplorations. We ran several tests over various query workloads to validate\\nempirically our approach.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1907.05618v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.05618v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.09535v1', updated=datetime.datetime(2019, 7, 22, 19, 11, 49, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 22, 19, 11, 49, tzinfo=datetime.timezone.utc), title='Association rule mining and itemset-correlation based variants', authors=[arxiv.Result.Author('Niels Mündler')], summary='Association rules express implication formed relations among attributes in\\ndatabases of itemsets. The apriori algorithm is presented, the basis for most\\nassociation rule mining algorithms. It works by pruning away rules that need\\nnot be evaluated based on the user specified minimum support confidence.\\nAdditionally, variations of the algorithm are presented that enable it to\\nhandle quantitative attributes and to extract rules about generalizations of\\nitems, but preserve the downward closure property that enables pruning.\\nIntertransformation of the extensions is proposed for special cases.', comment='IEEE format, 6 pages, 4 figures, seminar paper', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1907.09535v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.09535v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.07924v3', updated=datetime.datetime(2019, 10, 1, 2, 7, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 20, 17, 23, tzinfo=datetime.timezone.utc), title='Data Management for Causal Algorithmic Fairness', authors=[arxiv.Result.Author('Babak Salimi'), arxiv.Result.Author('Bill Howe'), arxiv.Result.Author('Dan Suciu')], summary='Fairness is increasingly recognized as a critical component of machine\\nlearning systems. However, it is the underlying data on which these systems are\\ntrained that often reflects discrimination, suggesting a data management\\nproblem. In this paper, we first make a distinction between associational and\\ncausal definitions of fairness in the literature and argue that the concept of\\nfairness requires causal reasoning. We then review existing works and identify\\nfuture opportunities for applying data management techniques to causal\\nalgorithmic fairness.', comment='arXiv admin note: text overlap with arXiv:1902.08283', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1908.07924v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.07924v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.11543v1', updated=datetime.datetime(2019, 11, 24, 2, 40, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 24, 2, 40, 9, tzinfo=datetime.timezone.utc), title='Schema Matching using Machine Learning', authors=[arxiv.Result.Author('Tanvi Sahay'), arxiv.Result.Author('Ankita Mehta'), arxiv.Result.Author('Shruti Jadon')], summary='Schema Matching is a method of finding attributes that are either similar to\\neach other linguistically or represent the same information. In this project,\\nwe take a hybrid approach at solving this problem by making use of both the\\nprovided data and the schema name to perform one to one schema matching and\\nintroduce the creation of a global dictionary to achieve one to many schema\\nmatching. We experiment with two methods of one to one matching and compare\\nboth based on their F-scores, precision, and recall. We also compare our method\\nwith the ones previously suggested and highlight differences between them.', comment='7 pages, 2 figures, 2 tables', journal_ref=None, doi='10.1109/SPIN48934.2020.9071272', primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.IR'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/SPIN48934.2020.9071272', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1911.11543v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.11543v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.00580v1', updated=datetime.datetime(2019, 12, 2, 5, 5, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 2, 5, 5, 39, tzinfo=datetime.timezone.utc), title='Multi-version Indexing in Flash-based Key-Value Stores', authors=[arxiv.Result.Author('Pulkit A. Misra'), arxiv.Result.Author('Jeffrey S. Chase'), arxiv.Result.Author('Johannes Gehrke'), arxiv.Result.Author('Alvin R. Lebeck')], summary='Maintaining multiple versions of data is popular in key-value stores since it\\nincreases concurrency and improves performance. However, designing a\\nmulti-version key-value store entails several challenges, such as additional\\ncapacity for storing extra versions and an indexing mechanism for mapping\\nversions of a key to their values. We present SkimpyFTL, a FTL-integrated\\nmulti-version key-value store that exploits the remap-on-write property of\\nflash-based SSDs for multi-versioning and provides a tradeoff between memory\\ncapacity and lookup latency for indexing.', comment='7 pages, 6 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.OS'], links=[arxiv.Result.Link('http://arxiv.org/abs/1912.00580v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.00580v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.00084v2', updated=datetime.datetime(2020, 4, 27, 16, 29, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 31, 22, 47, 43, tzinfo=datetime.timezone.utc), title='Approximate Summaries for Why and Why-not Provenance (Extended Version)', authors=[arxiv.Result.Author('Seokki Lee'), arxiv.Result.Author('Bertram Ludaescher'), arxiv.Result.Author('Boris Glavic')], summary='Why and why-not provenance have been studied extensively in recent years.\\nHowever, why-not provenance, and to a lesser degree why provenance, can be very\\nlarge resulting in severe scalability and usability challenges. In this paper,\\nwe introduce a novel approximate summarization technique for provenance which\\novercomes these challenges. Our approach uses patterns to encode (why-not)\\nprovenance concisely. We develop techniques for efficiently computing\\nprovenance summaries balancing informativeness, conciseness, and completeness.\\nTo achieve scalability, we integrate sampling techniques into provenance\\ncapture and summarization. Our approach is the first to scale to large datasets\\nand to generate comprehensive and meaningful summaries.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.00084v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.00084v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.05589v1', updated=datetime.datetime(2020, 2, 13, 16, 6, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 13, 16, 6, 39, tzinfo=datetime.timezone.utc), title='Explainable Queries over Event Logs', authors=[arxiv.Result.Author('Sylvain Hallé')], summary='Added value can be extracted from event logs generated by business processes\\nin various ways. However, although complex computations can be performed over\\nevent logs, the result of such computations is often difficult to explain; in\\nparticular, it is hard to determine what parts of an input log actually matters\\nin the production of that result. This paper describes how an existing log\\nprocessing library, called BeepBeep, can be extended in order to provide a form\\nof provenance: individual output events produced by a query can be precisely\\ntraced back to the data elements of the log that contribute to (i.e. \"explain\")\\nthe result.', comment='10 pages, submitted to IJCNN 2020', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.05589v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.05589v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.06039v1', updated=datetime.datetime(2020, 2, 14, 14, 2, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 14, 14, 2, 29, tzinfo=datetime.timezone.utc), title='Benchmarking Knowledge Graphs on the Web', authors=[arxiv.Result.Author('Michael Röder'), arxiv.Result.Author('Mohamed Ahmed Sherif'), arxiv.Result.Author('Muhammad Saleem'), arxiv.Result.Author('Felix Conrads'), arxiv.Result.Author('Axel-Cyrille Ngonga Ngomo')], summary='The growing interest in making use of Knowledge Graphs for developing\\nexplainable artificial intelligence, there is an increasing need for a\\ncomparable and repeatable comparison of the performance of Knowledge\\nGraph-based systems. History in computer science has shown that a main driver\\nto scientific advances, and in fact a core element of the scientific method as\\na whole, is the provision of benchmarks to make progress measurable. This paper\\ngives an overview of benchmarks used to evaluate systems that process Knowledge\\nGraphs.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.06039v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.06039v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.10283v1', updated=datetime.datetime(2020, 2, 24, 14, 35, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 24, 14, 35, 2, tzinfo=datetime.timezone.utc), title='The Knowledge Graph Track at OAEI -- Gold Standards, Baselines, and the Golden Hammer Bias', authors=[arxiv.Result.Author('Sven Hertling'), arxiv.Result.Author('Heiko Paulheim')], summary='The Ontology Alignment Evaluation Initiative (OAEI) is an annual evaluation\\nof ontology matching tools. In 2018, we have started the Knowledge Graph track,\\nwhose goal is to evaluate the simultaneous matching of entities and schemas of\\nlarge-scale knowledge graphs. In this paper, we discuss the design of the track\\nand two different strategies of gold standard creation. We analyze results and\\nexperiences obtained in first editions of the track, and, by revealing a hidden\\ntask, we show that all tools submitted to the track (and probably also to other\\ntracks) suffer from a bias which we name the golden hammer bias.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.10283v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.10283v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.11771v1', updated=datetime.datetime(2020, 2, 26, 19, 57, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 26, 19, 57, 59, tzinfo=datetime.timezone.utc), title='Distributed Cross-Blockchain Transactions', authors=[arxiv.Result.Author('Dongfang Zhao'), arxiv.Result.Author('Tonglin Li')], summary='The interoperability across multiple or many blockchains would play a\\ncritical role in the forthcoming blockchain-based data management paradigm. In\\nparticular, how to ensure the ACID properties of those transactions across an\\narbitrary number of blockchains remains an open problem in both academic and\\nindustry: Existing solutions either work for only two blockchains or requires a\\ncentralized component, neither of which would meet the scalability requirement\\nin practice. This short paper shares our vision and some early results toward\\nscalable cross-blockchain transactions. Specifically, we design two distributed\\ncommit protocols and, both analytically and experimentally, demonstrate their\\neffectiveness.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.11771v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.11771v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.00054v1', updated=datetime.datetime(2020, 2, 28, 20, 34, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 28, 20, 34, 9, tzinfo=datetime.timezone.utc), title='An Empirical Study on the Design and Evolution of NoSQL Database Schemas', authors=[arxiv.Result.Author('Stefanie Scherzinger'), arxiv.Result.Author('Sebastian Sidortschuck')], summary='We study how software engineers design and evolve their domain model when\\nbuilding applications against NoSQL data stores. Specifically, we target Java\\nprojects that use object-NoSQL mappers to interface with schema-free NoSQL data\\nstores. Given the source code of ten real-world database applications, we\\nextract the implicit NoSQL database schema. We capture the sizes of the\\nschemas, and investigate whether the schema is denormalized, as is recommended\\npractice in data modeling for NoSQL data stores. Further, we analyze the entire\\nproject history, and with it, the evolution history of the NoSQL database\\nschema. In doing so, we conduct the so far largest empirical study on NoSQL\\nschema design and evolution.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.SE'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.00054v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.00054v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.00965v1', updated=datetime.datetime(2020, 3, 2, 15, 25, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 2, 15, 25, 15, tzinfo=datetime.timezone.utc), title='Distribution Constraints: The Chase for Distributed Data', authors=[arxiv.Result.Author('Gaetano Geck'), arxiv.Result.Author('Frank Neven'), arxiv.Result.Author('Thomas Schwentick')], summary='This paper introduces a declarative framework to specify and reason about\\ndistributions of data over computing nodes in a distributed setting. More\\nspecifically, it proposes distribution constraints which are tuple and equality\\ngenerating dependencies (tgds and egds) extended with node variables ranging\\nover computing nodes. In particular, they can express co-partitioning\\nconstraints and constraints about range-based data distributions by using\\ncomparison atoms. The main technical contribution is the study of the\\nimplication problem of distribution constraints. While implication is\\nundecidable in general, relevant fragments of so-called data-full constraints\\nare exhibited for which the corresponding implication problems are complete for\\nEXPTIME, PSPACE and NP. These results yield bounds on deciding\\nparallel-correctness for conjunctive queries in the presence of distribution\\nconstraints.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.00965v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.00965v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.01331v1', updated=datetime.datetime(2020, 3, 3, 4, 48, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 3, 4, 48, 40, tzinfo=datetime.timezone.utc), title='Data Migration using Datalog Program Synthesis', authors=[arxiv.Result.Author('Yuepeng Wang'), arxiv.Result.Author('Rushi Shah'), arxiv.Result.Author('Abby Criswell'), arxiv.Result.Author('Rong Pan'), arxiv.Result.Author('Isil Dillig')], summary='This paper presents a new technique for migrating data between different\\nschemas. Our method expresses the schema mapping as a Datalog program and\\nautomatically synthesizes a Datalog program from simple input-output examples\\nto perform data migration. This approach can transform data between different\\ntypes of schemas (e.g., relational-to-graph, document-to-relational) and\\nperforms synthesis efficiently by leveraging the semantics of Datalog. We\\nimplement the proposed technique as a tool called Dynamite and show its\\neffectiveness by evaluating Dynamite on 28 realistic data migration scenarios.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.PL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.01331v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.01331v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.02446v1', updated=datetime.datetime(2020, 3, 5, 6, 8, 25, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 5, 6, 8, 25, tzinfo=datetime.timezone.utc), title='LAQP: Learning-based Approximate Query Processing', authors=[arxiv.Result.Author('Meifan Zhang'), arxiv.Result.Author('Hongzhi Wang')], summary='Querying on big data is a challenging task due to the rapid growth of data\\namount. Approximate query processing (AQP) is a way to meet the requirement of\\nfast response. In this paper, we propose a learning-based AQP method called the\\nLAQP. The LAQP builds an error model learned from the historical queries to\\npredict the sampling-based estimation error of each new query. It makes a\\ncombination of the sampling-based AQP, the pre-computed aggregations and the\\nlearned error model to provide high-accurate query estimations with a small\\noff-line sample. The experimental results indicate that our LAQP outperforms\\nthe sampling-based AQP, the pre-aggregation-based AQP and the most recent\\nlearning-based AQP method.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.02446v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.02446v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.03505v1', updated=datetime.datetime(2020, 3, 7, 3, 34, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 7, 3, 34, 14, tzinfo=datetime.timezone.utc), title='Data Management for Context-Aware Computing', authors=[arxiv.Result.Author('Wenwei Xue'), arxiv.Result.Author('Hungkeng Pung'), arxiv.Result.Author('Wenlong Ng'), arxiv.Result.Author('Tao Gu')], summary='We envisage future context-aware applications will dynamically adapt their\\nbehaviors to various context data from sources in wide-area networks, such as\\nthe Internet. Facing the changing context and the sheer number of context\\nsources, a data management system that supports effective source organization\\nand efficient data lookup becomes crucial to the easy development of\\ncontext-aware applications. In this paper, we propose the design of a new\\ncontext data management system that is equipped with query processing\\ncapabilities. We encapsulate the context sources into physical spaces belonging\\nto different context spaces and organize them as peers in semantic overlay\\nnetworks. Initial evaluation results of an experimental system prototype\\ndemonstrate the effectiveness of our design', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.03505v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.03505v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.08215v1', updated=datetime.datetime(2020, 3, 17, 2, 50, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 17, 2, 50, 13, tzinfo=datetime.timezone.utc), title='Multi-dimensional Skyline Query to Find Best Shopping Mall for Customers', authors=[arxiv.Result.Author('Md Amiruzzaman'), arxiv.Result.Author('Suphanut Jamonnak')], summary='This paper presents a new application for multi-dimensional Skyline query.\\nThe idea presented in this paper can be used to find best shopping malls based\\non users requirements. A web-based application was used to simulate the problem\\nand proposed solution. Also, a mathematical definition was developed to define\\nthe problem and show how multi-dimensional Skyline query can be used to solve\\ncomplex problems, such as, finding shopping malls using multiple different\\ncriteria. The idea of this paper can be used in other fields, where different\\ncriteria should be considered.', comment='This paper is accepted for publication in IEEE CDMA 2020 conference', journal_ref='2020 6th Conference on Data Science and Machine Learning\\n  Applications (CDMA)', doi='10.1109/CDMA47397.2020.00018', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/CDMA47397.2020.00018', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2003.08215v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.08215v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.11124v1', updated=datetime.datetime(2020, 3, 24, 21, 38, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 24, 21, 38, 55, tzinfo=datetime.timezone.utc), title='Implementing Suffix Array Algorithm Using Apache Big Table Data Implementation', authors=[arxiv.Result.Author('Piero Giacomelli')], summary='In this paper we will describe a new approach on the well-known suffix-array\\nalgorithm using Big Table Data Technology. We will demonstrate how it is\\npossible to refactor a well-known algorithm coupled by taking advantage of an\\nhigh-performance distributed datastore, to illustrate the advantages of using\\ndatastore cloud related technology for storing large text sequences and\\nretrieving them. A case study using DNA strings, considered one of the most\\ndifficult pattern matching problem, will be described and evaluated to\\ndemonstrate the potentiality of this implementation. Further discussion on\\nperformances and other big data related issues will be described as well as new\\npossible lines of research in big data technology for precise medical\\napplications.', comment='Paper prepared for a conference but never submitted', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'math.CO'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.11124v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.11124v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.01833v1', updated=datetime.datetime(2020, 4, 4, 1, 47, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 4, 1, 47, 35, tzinfo=datetime.timezone.utc), title='On the Efficient Design of LSM Stores', authors=[arxiv.Result.Author('Martin Weise')], summary='In the last decade, key-value data storage systems have gained significantly\\nmore interest from academia and industry. These systems face numerous\\nchallenges concerning storage space- and read optimization. There exists a\\nlarge potential for improving current solutions by introducing new management\\ntechniques and algorithms.\\n  In this paper we give an overview of the basic concept of key-value data\\nstorage systems and provide an explanation for bottlenecks. Further we\\nintroduce two new memory management algorithms and a improved index structure.\\nFinally, these solutions are compared to each other and discussed.', comment='7 pages, 3 figures, course material', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.01833v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.01833v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.10247v3', updated=datetime.datetime(2020, 6, 15, 13, 48, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 21, 19, 20, 2, tzinfo=datetime.timezone.utc), title='GGDs: Graph Generating Dependencies', authors=[arxiv.Result.Author('Larissa C. Shimomura'), arxiv.Result.Author('George Fletcher'), arxiv.Result.Author('Nikolay Yakovets')], summary='We propose Graph Generating Dependencies (GGDs), a new class of dependencies\\nfor property graphs. Extending the expressivity of state of the art constraint\\nlanguages, GGDs can express both tuple- and equality-generating dependencies on\\nproperty graphs, both of which find broad application in graph data management.\\nWe provide the formal definition of GGDs, analyze the validation problem for\\nGGDs, and demonstrate the practical utility of GGDs.', comment='5 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.10247v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.10247v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.00993v1', updated=datetime.datetime(2020, 5, 3, 5, 41, 36, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 3, 5, 41, 36, tzinfo=datetime.timezone.utc), title='An Algebraic Approach for High-level Text Analytics', authors=[arxiv.Result.Author('Xiuwen Zheng'), arxiv.Result.Author('Amarnath Gupta')], summary='Text analytical tasks like word embedding, phrase mining, and topic modeling,\\nare placing increasing demands as well as challenges to existing database\\nmanagement systems.\\n  In this paper, we provide a novel algebraic approach based on associative\\narrays. Our data model and algebra can bring together relational operators and\\ntext operators, which enables interesting optimization opportunities for hybrid\\ndata sources that have both relational and textual data. We demonstrate its\\nexpressive power in text analytics using several real-world tasks.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2005.00993v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.00993v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.01389v1', updated=datetime.datetime(2020, 5, 4, 11, 9, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 4, 11, 9, 11, tzinfo=datetime.timezone.utc), title='Knowledge Graph Validation', authors=[arxiv.Result.Author('Elwin Huaman'), arxiv.Result.Author('Elias Kärle'), arxiv.Result.Author('Dieter Fensel')], summary='Knowledge graphs (KGs) have shown to be an important asset of large companies\\nlike Google and Microsoft. KGs play an important role in providing structured\\nand semantically rich information, making them available to people and\\nmachines, and supplying accurate, correct and reliable knowledge. To do so a\\ncritical task is knowledge validation, which measures whether statements from\\nKGs are semantically correct and correspond to the so-called \"real\" world. In\\nthis paper, we provide an overview and review of the state-of-the-art\\napproaches, methods and tools on knowledge validation for KGs, as well as an\\nevaluation of them. As a result, we demonstrate a lack of reproducibility of\\ntools results, give insights, and state our future research direction.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2005.01389v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.01389v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.08483v2', updated=datetime.datetime(2022, 10, 5, 7, 41, 12, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 18, 6, 48, 45, tzinfo=datetime.timezone.utc), title='Improving Reverse k Nearest Neighbors Queries', authors=[arxiv.Result.Author('Lixin Ye')], summary='The reverse $k$ nearest neighbor query finds all points that have the query\\npoint as one of their $k$ nearest neighbors, where the $k$NN query finds the\\n$k$ closest points to its query point. Based on conics, we propose an efficent\\nR$k$NN verification method. By using the proposed verification method, we\\nimplement an efficient R$k$NN algorithm on VoR-tree, which has a computational\\ncomplexity of $O(k^{1.5}\\\\cdot log\\\\,k)$. The comparative experiments are\\nconducted between our algorithm and other two state-of-the-art R$k$NN\\nalgorithms. The experimental results indicate that the efficiency of our\\nalgorithm is significantly higher than its competitors.', comment='arXiv admin note: text overlap with arXiv:1911.02788 by other authors', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2005.08483v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.08483v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.01946v1', updated=datetime.datetime(2020, 7, 3, 21, 50, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 3, 21, 50, 35, tzinfo=datetime.timezone.utc), title='CICLAD: A Fast and Memory-efficient Closed Itemset Miner for Streams', authors=[arxiv.Result.Author('Tomas Martin'), arxiv.Result.Author('Guy Francoeur'), arxiv.Result.Author('Petko Valtchev')], summary=\"Mining association rules from data streams is a challenging task due to the\\n(typically) limited resources available vs. the large size of the result.\\nFrequent closed itemsets (FCI) enable an efficient first step, yet current FCI\\nstream miners are not optimal on resource consumption, e.g. they store a large\\nnumber of extra itemsets at an additional cost. In a search for a better\\nstorage-efficiency trade-off, we designed Ciclad,an intersection-based\\nsliding-window FCI miner. Leveraging in-depth insights into FCI evolution, it\\ncombines minimal storage with quick access. Experimental results indicate\\nCiclad's memory imprint is much lower and its performances globally better than\\ncompetitor methods.\", comment='KDD20', journal_ref=None, doi='10.1145/3394486.3403232', primary_category='cs.DB', categories=['cs.DB', 'cs.LG', 'stat.ML'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3394486.3403232', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2007.01946v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.01946v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.08876v2', updated=datetime.datetime(2021, 6, 16, 18, 53, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 17, 10, 10, 4, tzinfo=datetime.timezone.utc), title='Tractability Beyond $β$-Acyclicity for Conjunctive Queries with Negation', authors=[arxiv.Result.Author('Matthias Lanzinger')], summary='Numerous fundamental database and reasoning problems are known to be NP-hard\\nin general but tractable on instances where the underlying hypergraph structure\\nis $\\\\beta$-acyclic. Despite the importance of many of these problems, there has\\nbeen little success in generalizing these results beyond acyclicity.\\n  In this paper, we take on this challenge and propose nest-set width, a novel\\ngeneralization of hypergraph $\\\\beta$-acyclicity. We demonstrate that nest-set\\nwidth has desirable properties and algorithmic significance. In particular,\\nevaluation of boolean conjunctive queries with negation is tractable for\\nclasses with bounded nest-set width. Furthermore, propositional satisfiability\\nis fixed-parameter tractable when parameterized by nest-set width.', comment=None, journal_ref=None, doi='10.1145/3452021.3458308', primary_category='cs.DB', categories=['cs.DB', 'H.2.4'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3452021.3458308', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2007.08876v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.08876v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.12799v2', updated=datetime.datetime(2020, 8, 19, 1, 53, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 24, 23, 13, 27, tzinfo=datetime.timezone.utc), title='Score-Based Explanations in Data Management and Machine Learning', authors=[arxiv.Result.Author('Leopoldo Bertossi')], summary='We describe some approaches to explanations for observed outcomes in data\\nmanagement and machine learning. They are based on the assignment of numerical\\nscores to predefined and potentially relevant inputs. More specifically, we\\nconsider explanations for query answers in databases, and for results from\\nclassification models. The described approaches are mostly of a causal and\\ncounterfactual nature. We argue for the need to bring domain and semantic\\nknowledge into score computations; and suggest some ways to do this.', comment=\"Companion paper for a tutorial at the Scalable Uncertainty Management\\n  Conference (SUM'20). To appear in Proc. SUM'20. Minor fixes made\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2007.12799v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.12799v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.05032v1', updated=datetime.datetime(2020, 9, 10, 17, 53, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 10, 17, 53, 19, tzinfo=datetime.timezone.utc), title='GeoSPARQL+: Syntax, Semantics and System for Integrated Querying of Graph, Raster and Vector Data -- Technical Report', authors=[arxiv.Result.Author('Timo Homburg'), arxiv.Result.Author('Steffen Staab'), arxiv.Result.Author('Daniel Janke')], summary='We introduce an approach to semantically represent and query raster data in a\\nSemantic Web graph. We extend the GeoSPARQL vocabulary and query language to\\nsupport raster data as a new type of geospatial data. We define new filter\\nfunctions and illustrate our approach using several use cases on real-world\\ndata sets. Finally, we describe a prototypical implementation and validate the\\nfeasibility of our approach.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.05032v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.05032v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.05798v1', updated=datetime.datetime(2020, 9, 12, 14, 6, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 12, 14, 6, 39, tzinfo=datetime.timezone.utc), title='A Simple and Efficient Framework for Identifying Relation-gaps in Ontologies', authors=[arxiv.Result.Author('Subhashree S'), arxiv.Result.Author('P Sreenivasa Kumar')], summary='Though many ontologies have huge number of classes, one cannot find a good\\nnumber of object properties connecting the classes in most of the cases. Adding\\nobject properties makes an ontology richer and more applicable for tasks such\\nas Question Answering. In this context, the question of which two classes\\nshould be considered for discovering object properties becomes very important.\\nWe address the above question in this paper. We propose a simple machine\\nlearning framework which exhibits low time complexity and yet gives promising\\nresults with respect to both precision as well as number of class-pairs\\nretrieved.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.05798v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.05798v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.09895v2', updated=datetime.datetime(2020, 9, 22, 13, 29, 34, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 2, 16, 24, 1, tzinfo=datetime.timezone.utc), title='Data mining and time series segmentation via extrema: preliminary investigations', authors=[arxiv.Result.Author('Michel Fliess'), arxiv.Result.Author('Cédric Join')], summary='Time series segmentation is one of the many data mining tools. This paper, in\\nFrench, takes local extrema as perceptually interesting points (PIPs). The\\nblurring of those PIPs by the quick fluctuations around any time series is\\ntreated via an additive decomposition theorem, due to Cartier and Perrin, and\\nalgebraic estimation techniques, which are already useful in automatic control\\nand signal processing. Our approach is validated by several computer\\nillustrations. They underline the importance of the choice of a threshold for\\nthe extrema detection.', comment='13th International Conference on Modeling, Optimization and\\n  Simulation (MOSIM 2020), Agadir (Morocco), 12-14 November 2020, in French', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.09895v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.09895v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.11543v1', updated=datetime.datetime(2020, 9, 24, 8, 21, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 24, 8, 21, 10, tzinfo=datetime.timezone.utc), title='Compressed Key Sort and Fast Index Reconstruction', authors=[arxiv.Result.Author('Yongsik Kwon'), arxiv.Result.Author('Cheol Ryu'), arxiv.Result.Author('Sang Kyun Cha'), arxiv.Result.Author('Arthur H. Lee'), arxiv.Result.Author('Kunsoo Park'), arxiv.Result.Author('Bongki Moon')], summary='In this paper we propose an index key compression scheme based on the notion\\nof distinction bits by proving that the distinction bits of index keys are\\nsufficient information to determine the sorted order of the index keys\\ncorrectly. While the actual compression ratio may vary depending on the\\ncharacteristics of datasets (an average of 2.76 to one compression ratio was\\nobserved in our experiments), the index key compression scheme leads to\\nsignificant performance improvements during the reconstruction of large-scale\\nindexes. Our index key compression can be effectively used in database\\nreplication and index recovery of modern main-memory database systems.', comment='26 pages and 13 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.11543v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.11543v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.14821v2', updated=datetime.datetime(2020, 10, 1, 0, 58, 37, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 30, 17, 51, 37, tzinfo=datetime.timezone.utc), title='A Systematic Method for On-The-Fly Denormalization of Relational Databases', authors=[arxiv.Result.Author('Sareen Shah')], summary='Normalized relational databases are a common method for storing data, but\\npulling out usable denormalized data for consumption generally requires either\\ndirect access to the source data or creation of an appropriate view or table by\\na database administrator. End-users are thus limited in their ability to\\nexplore and use data that is stored in this manner. Presented here is a method\\nfor performing automated denormalization of relational databases at run-time,\\nwithout requiring access to source data or ongoing intervention by a database\\nadministrator. Furthermore, this method does not require a restructure of the\\ndatabase itself and so it can be flexibly applied as a layer on top of already\\nexisting databases.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2.1'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.14821v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.14821v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.03090v4', updated=datetime.datetime(2021, 12, 15, 18, 22, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 6, 23, 40, 3, tzinfo=datetime.timezone.utc), title='Validating UTF-8 In Less Than One Instruction Per Byte', authors=[arxiv.Result.Author('John Keiser'), arxiv.Result.Author('Daniel Lemire')], summary='The majority of text is stored in UTF-8, which must be validated on\\ningestion. We present the lookup algorithm, which outperforms UTF-8 validation\\nroutines used in many libraries and languages by more than 10 times using\\ncommonly available SIMD instructions. To ensure reproducibility, our work is\\nfreely available as open source software.', comment=None, journal_ref='Software: Practice and Experience 51 (5), 2021', doi='10.1002/spe.2920', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1002/spe.2920', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2010.03090v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.03090v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.08807v1', updated=datetime.datetime(2020, 10, 17, 15, 7, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 17, 15, 7, 45, tzinfo=datetime.timezone.utc), title='MithraDetective: A System for Cherry-picked Trendlines Detection', authors=[arxiv.Result.Author('Yoko Nagafuchi'), arxiv.Result.Author('Yin Lin'), arxiv.Result.Author('Kaushal Mamgain'), arxiv.Result.Author('Abolfazl Asudeh'), arxiv.Result.Author('H. V. Jagadish'), arxiv.Result.Author('You'), arxiv.Result.Author('Wu'), arxiv.Result.Author('Cong Yu')], summary=\"Given a data set, misleading conclusions can be drawn from it by\\ncherry-picking selected samples. One important class of conclusions is a trend\\nderived from a data set of values over time. Our goal is to evaluate whether\\nthe 'trends' described by the extracted samples are representative of the true\\nsituation represented in the data. We demonstrate MithraDetective, a system to\\ncompute a support score to indicate how cherry-picked a statement is; that is,\\nwhether the reported trend is well-supported by the data. The system can also\\nbe used to discover more supported alternatives. MithraDetective provides an\\ninteractive visual interface for both tasks.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.08807v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.08807v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.12243v3', updated=datetime.datetime(2022, 3, 24, 19, 51, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 23, 9, 4, 23, tzinfo=datetime.timezone.utc), title='An analysis of the SIGMOD 2014 Programming Contest: Complex queries on the LDBC social network graph', authors=[arxiv.Result.Author('Márton Elekes'), arxiv.Result.Author('János Benjamin Antal'), arxiv.Result.Author('Gábor Szárnyas')], summary='This report contains an analysis of the queries defined in the SIGMOD 2014\\nProgramming Contest. We first describe the data set, then present the queries,\\nproviding graphical illustrations for them and pointing out their caveats. Our\\nintention is to document our lessons learnt and simplify the work of those who\\nwill attempt to create a solution to this contest. We also demonstrate the\\ninfluence of this contest by listing followup works which used these queries as\\ninspiration to design better algorithms or to define interesting graph queries.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2.5; G.2.2'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.12243v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.12243v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.04838v1', updated=datetime.datetime(2020, 11, 10, 0, 10, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 10, 0, 10, 43, tzinfo=datetime.timezone.utc), title='Answer Graph: Factorization Matters in Large Graphs', authors=[arxiv.Result.Author('Zahid Abul-Basher'), arxiv.Result.Author('Nikolay Yakovets'), arxiv.Result.Author('Parke Godfrey'), arxiv.Result.Author('Stanley Clark'), arxiv.Result.Author('Mark Chignell')], summary=\"Our answer-graph method to evaluate SPARQL conjunctive queries (CQs) finds a\\nfactorized answer set first, an answer graph, and then finds the embedding\\ntuples from this. This approach can reduce greatly the cost to evaluate CQs.\\nThis affords a second advantage: we can construct a cost-based planner. We\\npresent the answer-graph approach, and overview our prototype system,\\nWireframe. We then offer proof of concept via a micro-benchmark over the YAGO2s\\ndataset with two prevalent shapes of queries, snowflake and diamond. We compare\\nWireframe's performance over these against PostgreSQL, Virtuoso, MonetDB, and\\nNeo4J to illustrate the performance advantages of our answer-graph approach.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2011.04838v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.04838v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.09314v1', updated=datetime.datetime(2020, 11, 18, 14, 31, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 18, 14, 31, 17, tzinfo=datetime.timezone.utc), title='First-Order Rewritability of Frontier-Guarded Ontology-Mediated Queries', authors=[arxiv.Result.Author('Pablo Barcelo'), arxiv.Result.Author('Gerald Berger'), arxiv.Result.Author('Carsten Lutz'), arxiv.Result.Author('Andreas Pieris')], summary='We focus on ontology-mediated queries (OMQs) based on (frontier-)guarded\\nexistential rules and (unions of) conjunctive queries, and we investigate the\\nproblem of FO-rewritability, i.e., whether an OMQ can be rewritten as a\\nfirst-order query. We adopt two different approaches. The first approach\\nemploys standard two-way alternating parity tree automata. Although it does not\\nlead to a tight complexity bound, it provides a transparent solution based on\\nwidely known tools. The second approach relies on a sophisticated automata\\nmodel, known as cost automata. This allows us to show that our problem is\\n2ExpTime-complete. In both approaches, we provide semantic characterizations of\\nFO-rewritability that are of independent interest.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2011.09314v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.09314v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.02454v1', updated=datetime.datetime(2020, 12, 4, 8, 18, 48, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 4, 8, 18, 48, tzinfo=datetime.timezone.utc), title='Data Lakes for Digital Humanities', authors=[arxiv.Result.Author('Jérôme Darmont'), arxiv.Result.Author('Cécile Favre'), arxiv.Result.Author('Sabine Loudcher'), arxiv.Result.Author('Camille Noûs')], summary='Traditional data in Digital Humanities projects bear various formats\\n(structured, semi-structured, textual) and need substantial transformations\\n(encoding and tagging, stemming, lemmatization, etc.) to be managed and\\nanalyzed. To fully master this process, we propose the use of data lakes as a\\nsolution to data siloing and big data variety problems. We describe data lake\\nprojects we currently run in close collaboration with researchers in humanities\\nand social sciences and discuss the lessons learned running these projects.', comment='Data and Digital Humanities Track', journal_ref='2nd International Digital Tools & Uses Congress (DTUC 2020), Oct\\n  2020, Hammamet, Tunisia. pp.38-41', doi='10.1145/3423603.3424004', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3423603.3424004', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2012.02454v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.02454v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.02619v3', updated=datetime.datetime(2020, 12, 8, 11, 17, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 4, 14, 26, 21, tzinfo=datetime.timezone.utc), title='Computational Complexity of Three Central Problems in Itemset Mining', authors=[arxiv.Result.Author('Christian Bessiere'), arxiv.Result.Author('Mohamed-Bachir Belaid'), arxiv.Result.Author('Nadjib Lazaar')], summary='Itemset mining is one of the most studied tasks in knowledge discovery. In\\nthis paper we analyze the computational complexity of three central itemset\\nmining problems. We prove that mining confident rules with a given item in the\\nhead is NP-hard. We prove that mining high utility itemsets is NP-hard. We\\nfinally prove that mining maximal or closed itemsets is coNP-hard as soon as\\nthe users can specify constraints on the kind of itemsets they are interested\\nin.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.CC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.02619v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.02619v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.07108v1', updated=datetime.datetime(2020, 12, 13, 17, 25, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 13, 17, 25, 26, tzinfo=datetime.timezone.utc), title='Knowledge Graph Management on the Edge', authors=[arxiv.Result.Author('Weiqin Xu'), arxiv.Result.Author('Olivier Curé'), arxiv.Result.Author('Philippe Calvez')], summary='Edge computing emerges as an innovative platform for services requiring low\\nlatency decision making. Its success partly depends on the existence of\\nefficient data management systems. We consider that knowledge graph management\\nsystems have a key role to play in this context due to their data integration\\nand reasoning features. In this paper, we present SuccinctEdge, a compact,\\ndecompression-free, self-index, in-memory RDF store that can answer SPARQL\\nqueries, including those requiring reasoning services associated to some\\nontology. We provide details on its design and implementation before\\ndemonstrating its efficiency on real-world and synthetic datasets.', comment='13 pages, 14 figures, 3 tables', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.07108v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.07108v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.07309v1', updated=datetime.datetime(2020, 12, 14, 7, 49, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 14, 7, 49, 4, tzinfo=datetime.timezone.utc), title='Event Data Quality: A Survey', authors=[arxiv.Result.Author('Ruihong Huang'), arxiv.Result.Author('Jianmin Wang')], summary='Event data are prevalent in diverse domains such as financial trading,\\nbusiness workflows and industrial IoT nowadays. An event is often characterized\\nby several attributes denoting the meaning associated with the corresponding\\noccurrence time/duration. From traditional operational systems in enterprises\\nto online systems for Web services, event data is generated from physical world\\nuninterruptedly. However, due to the variety and veracity features of Big data,\\nevent data generated from heterogeneous and dirty sources could have very\\ndifferent event representations and data quality issues. In this work, we\\nsummarize several typical works on studying data quality issues of event data,\\nincluding: (1) event matching, (2) event error detection, (3) event data\\nrepair, and (4) approximate pattern matching.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.07309v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.07309v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.11269v3', updated=datetime.datetime(2021, 5, 3, 18, 52, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 21, 11, 54, 35, tzinfo=datetime.timezone.utc), title='A Journey to the Frontiers of Query Rewritability', authors=[arxiv.Result.Author('Piotr Ostropolski-Nalewaja'), arxiv.Result.Author('Jerzy Marcinkowski'), arxiv.Result.Author('David Carral'), arxiv.Result.Author('Sebastian Rudolph')], summary='This paper is about (first order) query rewritability in the context of\\ntheory-mediated query answering. The starting point of our journey is the\\nFUS/FES conjecture, saying that if a theory is core-terminating (FES) and\\nadmits query rewriting (BDD, FUS) then it is uniformly bounded. We show that\\nthis conjecture is true for a wide class of \"local\" BDD theories. Then we ask\\nhow non-local can a BDD theory actually be and we discover phenomena which we\\nthink are quite counter-intuitive.', comment='Removed faulty observation, fixed everything that depended on it', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.11269v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.11269v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.13677v1', updated=datetime.datetime(2020, 12, 26, 4, 45, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 26, 4, 45, 40, tzinfo=datetime.timezone.utc), title='Toward Compact Data from Big Data', authors=[arxiv.Result.Author('Song-Kyoo'), arxiv.Result.Author('Kim')], summary='Bigdata is a dataset of which size is beyond the ability of handling a\\nvaluable raw material that can be refined and distilled into valuable specific\\ninsights. Compact data is a method that optimizes the big dataset that gives\\nbest assets without handling complex bigdata. The compact dataset contains the\\nmaximum knowledge patterns at fine grained level for effective and personalized\\nutilization of bigdata systems without bigdata. The compact data method is a\\ntailor-made design which depends on problem situations. Various compact data\\ntechniques have been demonstrated into various data-driven research area in the\\npaper.', comment='This paper has been accepted in the 2020 IEEE-ICITIS Conference', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LG', 'stat.AP'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.13677v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.13677v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2101.00170v3', updated=datetime.datetime(2022, 5, 2, 4, 50, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 1, 1, 5, 41, 44, tzinfo=datetime.timezone.utc), title='Visualization Techniques with Data Cubes: Utilizing Concurrency for Complex Data', authors=[arxiv.Result.Author('Daniel Szelogowski')], summary='With web and mobile platforms becoming more prominent devices utilized in\\ndata analysis, there are currently few systems which are not without flaw. In\\norder to increase the performance of these systems and decrease errors of data\\noversimplification, we seek to understand how other programming languages can\\nbe used across these platforms which provide data and type safety, as well as\\nutilizing concurrency to perform complex data manipulation tasks.', comment='11 pages, 4 figures Update: Revised format to align closer to IEEE\\n  standards', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2; E.5; H.5'], links=[arxiv.Result.Link('http://arxiv.org/abs/2101.00170v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00170v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2101.00171v2', updated=datetime.datetime(2021, 4, 19, 22, 34, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 1, 1, 5, 42, 38, tzinfo=datetime.timezone.utc), title='Optimizing Data Cube Visualization for Web Applications: Performance and User-Friendly Data Aggregation', authors=[arxiv.Result.Author('Daniel Szelogowski')], summary='Current open source applications which allow for cross-platform data\\nvisualization of OLAP cubes feature issues of high overhead and inconsistency\\ndue to data oversimplification. To improve upon this issue, there is a need to\\ncut down the number of pipelines that the data must travel between for these\\naggregation operations and create a single, unified application which performs\\nefficiently without sacrificing data, and allows for ease of usability and\\nextension.', comment='12 pages, 2 figures, 3 tables Update: Revised format to align closer\\n  to IEEE standards', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2; E.5; H.5'], links=[arxiv.Result.Link('http://arxiv.org/abs/2101.00171v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00171v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2101.02174v1', updated=datetime.datetime(2021, 1, 6, 18, 22, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 1, 6, 18, 22, 52, tzinfo=datetime.timezone.utc), title='Efficient Discovery of Approximate Order Dependencies', authors=[arxiv.Result.Author('Reza Karegar'), arxiv.Result.Author('Parke Godfrey'), arxiv.Result.Author('Lukasz Golab'), arxiv.Result.Author('Mehdi Kargar'), arxiv.Result.Author('Divesh Srivastava'), arxiv.Result.Author('Jaroslaw Szlichta')], summary='Order dependencies (ODs) capture relationships between ordered domains of\\nattributes. Approximate ODs (AODs) capture such relationships even when there\\nexist exceptions in the data. During automated discovery of ODs, validation is\\nthe process of verifying whether an OD holds. We present an algorithm for\\nvalidating approximate ODs with significantly improved runtime performance over\\nexisting methods for AODs, and prove that it is correct and has optimal\\nruntime. By replacing the validation step in a leading algorithm for\\napproximate OD discovery with ours, we achieve orders-of-magnitude improvements\\nin performance.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2101.02174v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.02174v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2101.06637v1', updated=datetime.datetime(2021, 1, 17, 10, 17, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 1, 17, 10, 17, 6, tzinfo=datetime.timezone.utc), title='AMALGAM: A Matching Approach to fairfy tabuLar data with knowledGe grAph Model', authors=[arxiv.Result.Author('Rabia Azzi'), arxiv.Result.Author('Gayo Diallo')], summary='In this paper we present AMALGAM, a matching approach to fairify tabular data\\nwith the use of a knowledge graph. The ultimate goal is to provide fast and\\nefficient approach to annotate tabular data with entities from a background\\nknowledge. The approach combines lookup and filtering services combined with\\ntext pre-processing techniques. Experiments conducted in the context of the\\n2020 Semantic Web Challenge on Tabular Data to Knowledge Graph Matching with\\nboth Column Type Annotation and Cell Type Annotation tasks showed promising\\nresults.', comment='10 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2101.06637v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.06637v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2102.06139v1', updated=datetime.datetime(2021, 2, 11, 17, 28, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 2, 11, 17, 28, 52, tzinfo=datetime.timezone.utc), title='A GeoSPARQL Compliance Benchmark', authors=[arxiv.Result.Author('Milos Jovanovik'), arxiv.Result.Author('Timo Homburg'), arxiv.Result.Author('Mirko Spasić')], summary='We propose a series of tests that check for the compliance of RDF\\ntriplestores with the GeoSPARQL standard. The purpose of the benchmark is to\\ntest how many of the requirements outlined in the standard a tested system\\nsupports and to push triplestores forward in achieving a full GeoSPARQL\\ncompliance. This topic is of concern because the support of GeoSPARQL varies\\ngreatly between different triplestore implementations, and such support is of\\ngreat importance for the domain of geospatial RDF data. Additionally, we\\npresent a comprehensive comparison of triplestores, providing an insight into\\ntheir current GeoSPARQL support.', comment=None, journal_ref='ISPRS International Journal of Geo-Information. 2021; 10(7):487', doi='10.3390/ijgi10070487', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.3390/ijgi10070487', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2102.06139v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2102.06139v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2102.06563v3', updated=datetime.datetime(2021, 8, 30, 21, 34, 25, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 2, 12, 15, 0, 51, tzinfo=datetime.timezone.utc), title='Querying collections of tree-structured records in the presence of within-record referential constraints', authors=[arxiv.Result.Author('Foto N. Afrati'), arxiv.Result.Author('Matthew Damigos')], summary='In this paper, we consider a tree-structured data model used in many\\ncommercial databases like Dremel, F1, JSON stores. We define identity and\\nreferential constraints within each tree-structured record. The query language\\nis a variant of SQL and flattening is used as an evaluation mechanism. We\\ninvestigate querying in the presence of these constraints, and point out the\\nchallenges that arise from taking them into account during query evaluation.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2102.06563v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2102.06563v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2102.08228v1', updated=datetime.datetime(2021, 2, 16, 15, 44, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 2, 16, 15, 44, 27, tzinfo=datetime.timezone.utc), title='Data provenance, curation and quality in metrology', authors=[arxiv.Result.Author('James Cheney'), arxiv.Result.Author('Adriane Chapman'), arxiv.Result.Author('Joy Davidson'), arxiv.Result.Author('Alistair Forbes')], summary='Data metrology -- the assessment of the quality of data -- particularly in\\nscientific and industrial settings, has emerged as an important requirement for\\nthe UK National Physical Laboratory (NPL) and other national metrology\\ninstitutes. Data provenance and data curation are key components for emerging\\nunderstanding of data metrology. However, to date provenance research has had\\nlimited visibility to or uptake in metrology. In this work, we summarize a\\nscoping study carried out with NPL staff and industrial participants to\\nunderstand their current and future needs for provenance, curation and data\\nquality. We then survey provenance technology and standards that are relevant\\nto metrology. We analyse the gaps between requirements and the current state of\\nthe art.', comment=None, journal_ref=None, doi='10.1142/9789811242380_0009', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1142/9789811242380_0009', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2102.08228v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2102.08228v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2102.08942v1', updated=datetime.datetime(2021, 2, 17, 18, 56, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 2, 17, 18, 56, 3, tzinfo=datetime.timezone.utc), title='A Survey on Locality Sensitive Hashing Algorithms and their Applications', authors=[arxiv.Result.Author('Omid Jafari'), arxiv.Result.Author('Preeti Maurya'), arxiv.Result.Author('Parth Nagarkar'), arxiv.Result.Author('Khandker Mushfiqul Islam'), arxiv.Result.Author('Chidambaram Crushev')], summary='Finding nearest neighbors in high-dimensional spaces is a fundamental\\noperation in many diverse application domains. Locality Sensitive Hashing (LSH)\\nis one of the most popular techniques for finding approximate nearest neighbor\\nsearches in high-dimensional spaces. The main benefits of LSH are its\\nsub-linear query performance and theoretical guarantees on the query accuracy.\\nIn this survey paper, we provide a review of state-of-the-art LSH and\\nDistributed LSH techniques. Most importantly, unlike any other prior survey, we\\npresent how Locality Sensitive Hashing is utilized in different application\\ndomains.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2.4'], links=[arxiv.Result.Link('http://arxiv.org/abs/2102.08942v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2102.08942v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2102.13370v1', updated=datetime.datetime(2021, 2, 26, 9, 41, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 2, 26, 9, 41, 4, tzinfo=datetime.timezone.utc), title='Fast Distributed Complex Join Processing', authors=[arxiv.Result.Author('Hao Zhang'), arxiv.Result.Author('Miao Qiao'), arxiv.Result.Author('Jeffrey Xu Yu'), arxiv.Result.Author('Hong Cheng')], summary='In this work, we study the problem of co-optimize communication,\\npre-computing, and computation cost in one-round multi-way join evaluation. We\\npropose a multi-way join approach ADJ (Adaptive Distributed Join) for complex\\njoin which finds one optimal query plan to process by exploring cost-effective\\npartial results in terms of the trade-off between pre-computing, communication,\\nand computation.We analyze the input relations for a given join query and find\\none optimal over a set of query plans in some specific form, with high-quality\\ncost estimation by sampling. Our extensive experiments confirm that ADJ\\noutperforms the existing multi-way join methods by up to orders of magnitude.', comment='Long Version', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2102.13370v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2102.13370v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2103.02145v1', updated=datetime.datetime(2021, 3, 3, 2, 56, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 3, 3, 2, 56, 46, tzinfo=datetime.timezone.utc), title='Enhancing the Interactivity of Dataframe Queries by Leveraging Think Time', authors=[arxiv.Result.Author('Doris Xin'), arxiv.Result.Author('Devin Petersohn'), arxiv.Result.Author('Dixin Tang'), arxiv.Result.Author('Yifan Wu'), arxiv.Result.Author('Joseph E. Gonzalez'), arxiv.Result.Author('Joseph M. Hellerstein'), arxiv.Result.Author('Anthony D. Joseph'), arxiv.Result.Author('Aditya G. Parameswaran')], summary='We propose opportunistic evaluation, a framework for accelerating\\ninteractions with dataframes. Interactive latency is critical for iterative,\\nhuman-in-the-loop dataframe workloads for supporting exploratory data analysis.\\nOpportunistic evaluation significantly reduces interactive latency by 1)\\nprioritizing computation directly relevant to the interactions and 2)\\nleveraging think time for asynchronous background computation for non-critical\\noperators that might be relevant to future interactions. We show, through\\nempirical analysis, that current user behavior presents ample opportunities for\\noptimization, and the solutions we propose effectively harness such\\nopportunities.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2103.02145v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2103.02145v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2103.03314v3', updated=datetime.datetime(2021, 11, 12, 6, 48, 25, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 3, 4, 20, 35, 53, tzinfo=datetime.timezone.utc), title='Consistent Answers of Aggregation Queries using SAT Solvers', authors=[arxiv.Result.Author('Akhil A. Dixit'), arxiv.Result.Author('Phokion G. Kolaitis')], summary='The framework of database repairs and consistent answers to queries is a\\nprincipled approach to managing inconsistent databases. We describe the first\\nsystem able to compute the consistent answers of general aggregation queries\\nwith the COUNT(A), COUNT(*), SUM(A), MIN(A), and MAX(A) operators, and with or\\nwithout grouping constructs. Our system uses reductions to optimization\\nversions of Boolean satisfiability (SAT) and then leverages powerful SAT\\nsolvers. We carry out an extensive set of experiments on both synthetic and\\nreal-world data that demonstrate the usefulness and scalability of this\\napproach.', comment='18 pages, 10 figures, 7 tables', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2103.03314v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2103.03314v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2103.07978v1', updated=datetime.datetime(2021, 3, 14, 17, 21, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 3, 14, 17, 21, 26, tzinfo=datetime.timezone.utc), title='Putting Data Science Pipelines on the Edge', authors=[arxiv.Result.Author('Ali Akoglu'), arxiv.Result.Author('Genoveva Vargas-Solar')], summary='This paper proposes a composable \"Just in Time Architecture\" for Data Science\\n(DS) Pipelines named JITA-4DS and associated resource management techniques for\\nconfiguring disaggregated data centers (DCs). DCs under our approach are\\ncomposable based on vertical integration of the application,\\nmiddleware/operating system, and hardware layers customized dynamically to meet\\napplication Service Level Objectives (SLO - application-aware management).\\nThereby, pipelines utilize a set of flexible building blocks that can be\\ndynamically and automatically assembled and re-assembled to meet the dynamic\\nchanges in the workload\\'s SLOs. To assess disaggregated DC\\'s, we study how to\\nmodel and validate their performance in large-scale settings.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2103.07978v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2103.07978v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2103.15816v1', updated=datetime.datetime(2021, 3, 27, 8, 26, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 3, 27, 8, 26, 45, tzinfo=datetime.timezone.utc), title='Some Results of Experimental Check of The Model of the Object Innovativeness Quantitative Evaluation', authors=[arxiv.Result.Author('V. K. Ivanov')], summary='The paper presents the results of the experiments that were conducted to\\nconfirm the main ideas of the proposed approach to determining the objects\\ninnovativeness. This approach assumed that the product life cycle of whose\\ndescriptions are placed in different data warehouses is adequate. The proposed\\nformal model allows us to calculate the quantitative value of the additive\\nevaluation criterion of objects innovativeness. The obtained experimental data\\nmake it possible to evaluate the adopted approach correctness.', comment='10 pages, in Russian', journal_ref='Information and Innovations. 2020. Vol. 15, N 3', doi='10.31432/1994-2443-2020-15-3-27-36', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.31432/1994-2443-2020-15-3-27-36', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2103.15816v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2103.15816v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2104.13605v1', updated=datetime.datetime(2021, 4, 28, 7, 31, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 4, 28, 7, 31, 20, tzinfo=datetime.timezone.utc), title='A Linked Data Application Framework to Enable Rapid Prototyping', authors=[arxiv.Result.Author('Markus Schröder'), arxiv.Result.Author('Christian Jilek'), arxiv.Result.Author('Andreas Dengel')], summary='Application developers, in our experience, tend to hesitate when dealing with\\nlinked data technologies. To reduce their initial hurdle and enable rapid\\nprototyping, we propose in this paper a framework for building linked data\\napplications. Our approach especially considers the participation of web\\ndevelopers and non-technical users without much prior knowledge about linked\\ndata concepts. Web developers are supported with bidirectional RDF to JSON\\nconversions and suitable CRUD endpoints. Non-technical users can browse\\nwebsites generated from JSON data by means of a template language. A\\nprototypical open source implementation demonstrates its capabilities.', comment='5 pages, demo', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2104.13605v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2104.13605v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2105.05130v1', updated=datetime.datetime(2021, 5, 11, 15, 39, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 5, 11, 15, 39, 55, tzinfo=datetime.timezone.utc), title='Towards a Model for LSH', authors=[arxiv.Result.Author('Li Wang')], summary='As data volumes continue to grow, clustering and outlier detection algorithms\\nare becoming increasingly time-consuming. Classical index structures for\\nneighbor search are no longer sustainable due to the \"curse of dimensionality\".\\nInstead, approximated index structures offer a good opportunity to\\nsignificantly accelerate the neighbor search for clustering and outlier\\ndetection and to have the lowest possible error rate in the results of the\\nalgorithms. Locality-sensitive hashing is one of those. We indicate directions\\nto model the properties of LSH.', comment='arXiv admin note: text overlap with arXiv:2103.01888', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2105.05130v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2105.05130v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2105.05443v3', updated=datetime.datetime(2021, 12, 27, 8, 56, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 5, 12, 5, 47, 7, tzinfo=datetime.timezone.utc), title='A Nearly Instance-optimal Differentially Private Mechanism for Conjunctive Queries', authors=[arxiv.Result.Author('Wei Dong'), arxiv.Result.Author('Ke Yi')], summary='Releasing the result size of conjunctive queries and graph pattern queries\\nunder differential privacy (DP) has received considerable attention in the\\nliterature, but existing solutions do not offer any optimality guarantees. We\\nprovide the first DP mechanism for this problem with a fairly strong notion of\\noptimality, which can be considered as a natural relaxation of\\ninstance-optimality to a constant.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2105.05443v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2105.05443v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2107.00729v3', updated=datetime.datetime(2021, 10, 20, 9, 47, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 7, 1, 20, 9, 56, tzinfo=datetime.timezone.utc), title='Essence of Factual Knowledge', authors=[arxiv.Result.Author('Ruoyu Wang'), arxiv.Result.Author('Daniel Sun'), arxiv.Result.Author('Guoqiang Li'), arxiv.Result.Author('Raymond Wong'), arxiv.Result.Author('Shiping Chen')], summary='Knowledge bases are collections of domain-specific and commonsense facts.\\nRecently, the sizes of KBs are rocketing due to automatic extraction for\\nknowledge and facts. For example, the number of facts in WikiData is up to 974\\nmillion! According to our observation, current KBs, especially domain KBs, show\\nstrong relevance in relations according to some topics. These patterns can be\\nused to conclude and infer for part of facts in the KBs. Therefore, the\\noriginal KBs can be minimzed by extracting patterns and essential facts.', comment='4 pages, 1 figure', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2107.00729v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2107.00729v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2107.02885v1', updated=datetime.datetime(2021, 7, 5, 14, 3, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 7, 5, 14, 3, 53, tzinfo=datetime.timezone.utc), title='Data Lake Ingestion Management', authors=[arxiv.Result.Author('Yan Zhao'), arxiv.Result.Author('Imen Megdiche'), arxiv.Result.Author('Franck Ravat')], summary='Data Lake (DL) is a Big Data analysis solution which ingests raw data in\\ntheir native format and allows users to process these data upon usage. Data\\ningestion is not a simple copy and paste of data, it is a complicated and\\nimportant phase to ensure that ingested data are findable, accessible,\\ninteroperable and reusable at all times. Our solution is threefold. Firstly, we\\npropose a metadata model that includes information about external data sources,\\ndata ingestion processes, ingested data, dataset veracity and dataset security.\\nSecondly, we present the algorithms that ensure the ingestion phase (data\\nstorage and metadata instanciation). Thirdly, we introduce a developed metadata\\nmanagement system whereby users can easily consult different elements stored in\\nDL.', comment='12 pages, 4 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2107.02885v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2107.02885v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2107.03997v1', updated=datetime.datetime(2021, 7, 8, 17, 42, 57, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 7, 8, 17, 42, 57, tzinfo=datetime.timezone.utc), title='Probabilistic Trace Alignment', authors=[arxiv.Result.Author('Giacomo Bergami'), arxiv.Result.Author('Fabrizio Maria Maggi'), arxiv.Result.Author('Marco Montali'), arxiv.Result.Author('Rafael Peñaloza')], summary='Alignments provide sophisticated diagnostics that pinpoint deviations in a\\ntrace with respect to a process model and their severity. However, approaches\\nbased on trace alignments use crisp process models as reference and recent\\nprobabilistic conformance checking approaches check the degree of conformance\\nof an event log with respect to a stochastic process model instead of finding\\ntrace alignments. In this paper, for the first time, we provide a conformance\\nchecking approach based on trace alignments using stochastic Workflow nets.\\nConceptually, this requires to handle the two possibly contrasting forces of\\nthe cost of the alignment on the one hand and the likelihood of the model trace\\nwith respect to which the alignment is computed on the other.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LO'], links=[arxiv.Result.Link('http://arxiv.org/abs/2107.03997v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2107.03997v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2107.04027v1', updated=datetime.datetime(2021, 7, 5, 7, 56, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 7, 5, 7, 56, 27, tzinfo=datetime.timezone.utc), title='goldMEDAL : une nouvelle contribution {à} la mod{é}lisation g{é}n{é}rique des m{é}tadonn{é}es des lacs de donn{é}es', authors=[arxiv.Result.Author('Etienne Scholly'), arxiv.Result.Author('Pegdwendé Sawadogo'), arxiv.Result.Author('Pengfei Liu'), arxiv.Result.Author('Javier Espinosa-Oviedo'), arxiv.Result.Author('Cécile Favre'), arxiv.Result.Author('Sabine Loudcher'), arxiv.Result.Author('Jérôme Darmont'), arxiv.Result.Author('Camille Noûs')], summary='We summarize here a paper published in 2021 in the DOLAP international\\nworkshop DOLAP associated with the EDBT and ICDT conferences. We propose\\ngoldMEDAL, a generic metadata model for data lakes based on four concepts and a\\nthree-level modeling: conceptual, logical and physical.', comment=\"in French. 17e journ{\\\\'e}es Business Intelligence et Big Data (EDA\\n  2021), Jul 2021, Toulouse, France\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2107.04027v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2107.04027v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2107.08297v2', updated=datetime.datetime(2021, 9, 27, 16, 43, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 7, 17, 18, 8, 51, tzinfo=datetime.timezone.utc), title='Spatial Data Generators', authors=[arxiv.Result.Author('Tin Vu'), arxiv.Result.Author('Sara Migliorini'), arxiv.Result.Author('Ahmed Eldawy'), arxiv.Result.Author('Alberto Belussi')], summary='This gem describes a standard method for generating synthetic spatial data\\nthat can be used in benchmarking and scalability tests. The goal is to improve\\nthe reproducibility and increase the trust in experiments on synthetic data by\\nusing standard widely acceptable dataset distributions. In addition, this\\narticle describes how to assign a unique identifier to each synthetic dataset\\nthat can be shared in papers for reproducibility of results. Finally, this gem\\nprovides a supplementary material that gives a reference implementation for all\\nthe provided distributions.', comment='9 pages', journal_ref='1st ACM SIGSPATIAL International Workshop on Spatial Gems\\n  (SpatialGems 2019)', doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2107.08297v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2107.08297v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2107.08814v1', updated=datetime.datetime(2021, 7, 14, 6, 28, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 7, 14, 6, 28, 42, tzinfo=datetime.timezone.utc), title='MARC: Mining Association Rules from datasets by using Clustering models', authors=[arxiv.Result.Author('Shadi Al Shehabi'), arxiv.Result.Author('Abdullatif Baba')], summary='Association rules are useful to discover relationships, which are mostly\\nhidden, between the different items in large datasets. Symbolic models are the\\nprincipal tools to extract association rules. This basic technique is\\ntime-consuming, and it generates a big number of associated rules. To overcome\\nthis drawback, we suggest a new method, called MARC, to extract the more\\nimportant association rules of two important levels: Type I, and Type II. This\\napproach relies on a multi-topographic unsupervised neural network model as\\nwell as clustering quality measures that evaluate the success of a given\\nnumerical classification model to behave as a natural symbolic model.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2107.08814v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2107.08814v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2107.11592v2', updated=datetime.datetime(2021, 8, 4, 16, 8, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 7, 24, 12, 20, 36, tzinfo=datetime.timezone.utc), title='Blockchain Transaction Processing', authors=[arxiv.Result.Author('Suyash Gupta'), arxiv.Result.Author('Mohammad Sadoghi')], summary='A blockchain is an append-only linked-list of blocks, which is maintained at\\neach participating node. Each block records a set of transactions and their\\nassociated metadata. Blockchain transactions act on the identical ledger data\\nstored at each node. Blockchain was first perceived by Satoshi Nakamoto as a\\npeer-to-peer digital-commodity (also known as crypto-currency) exchange system.\\nBlockchains received traction due to their inherent property of\\nimmutability-once a block is accepted, it cannot be reverted.', comment=None, journal_ref='Encyclopedia of Big Data Technologies 2019', doi='10.1007/978-3-319-77525-8_333', primary_category='cs.DB', categories=['cs.DB', 'cs.CR', 'cs.DC'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-319-77525-8_333', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2107.11592v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2107.11592v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2107.12373v1', updated=datetime.datetime(2021, 7, 25, 20, 29, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 7, 25, 20, 29, 28, tzinfo=datetime.timezone.utc), title='Relational Boosted Regression Trees', authors=[arxiv.Result.Author('Sonia Cromp'), arxiv.Result.Author('Alireza Samadian'), arxiv.Result.Author('Kirk Pruhs')], summary='Many tasks use data housed in relational databases to train boosted\\nregression tree models. In this paper, we give a relational adaptation of the\\ngreedy algorithm for training boosted regression trees. For the subproblem of\\ncalculating the sum of squared residuals of the dataset, which dominates the\\nruntime of the boosting algorithm, we provide a $(1 + \\\\epsilon)$-approximation\\nusing the tensor sketch technique. Employing this approximation within the\\nrelational boosted regression trees algorithm leads to learning similar model\\nparameters, but with asymptotically better runtime.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2107.12373v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2107.12373v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2107.13923v1', updated=datetime.datetime(2021, 7, 29, 12, 0, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 7, 29, 12, 0, 11, tzinfo=datetime.timezone.utc), title='Machine Learning over Static and Dynamic Relational Data', authors=[arxiv.Result.Author('Ahmet Kara'), arxiv.Result.Author('Milos Nikolic'), arxiv.Result.Author('Dan Olteanu'), arxiv.Result.Author('Haozhe Zhang')], summary='This tutorial overviews principles behind recent works on training and\\nmaintaining machine learning models over relational data, with an emphasis on\\nthe exploitation of the relational data structure to improve the runtime\\nperformance of the learning task.\\n  The tutorial has the following parts:\\n  1) Database research for data science\\n  2) Three main ideas to achieve performance improvements\\n  2.1) Turn the ML problem into a DB problem\\n  2.2) Exploit structure of the data and problem\\n  2.3) Exploit engineering tools of a DB researcher\\n  3) Avenues for future research', comment='arXiv admin note: text overlap with arXiv:2008.07864', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2107.13923v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2107.13923v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2107.14274v1', updated=datetime.datetime(2021, 7, 29, 18, 42, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 7, 29, 18, 42, 47, tzinfo=datetime.timezone.utc), title='Interactive Region-of-Interest Discovery using Exploratory Feedback', authors=[arxiv.Result.Author('Behrooz Omidvar-Tehrani')], summary=\"In this paper, we propose a geospatial data management framework called\\nIRIDEF which captures and analyzes user's exploratory feedback for an enriched\\nguidance mechanism in the context of interactive analysis. We discuss that\\nexploratory feedback can be a proxy for decision-making feedback when the\\nlatter is scarce or unavailable. IRIDEF identifies regions of interest (ROIs)\\nvia exploratory feedback and highlights a few interesting and out-of-sight POIs\\nin each ROI. These highlights enable the user to shape up his/her future\\ninteractions with the system. We detail the components of our proposed\\nframework in the form of a data analysis pipeline and present the aspects of\\nefficiency and effectiveness for each component. We also discuss evaluation\\nplans and future directions for IRIDEF.\", comment='12 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2107.14274v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2107.14274v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2108.04727v1', updated=datetime.datetime(2021, 8, 10, 14, 44, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 8, 10, 14, 44, 59, tzinfo=datetime.timezone.utc), title='Crowdsourced Databases and Sui Generis Rights', authors=[arxiv.Result.Author('Gonçalo Simões de Almeida'), arxiv.Result.Author('Gonçalo Faria Abreu')], summary='In this study we propose a new concept of databases (crowdsourced databases),\\nadding a new conceptual approach to the debate on legal protection of databases\\nin Europe. We also summarise the current legal framework and current indexing\\nand web scraping practices - it would not be prudent to suggest a new theory\\nwithout contextualising it in the legal and practical context in which it is\\ndeveloped.', comment='21 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2108.04727v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2108.04727v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2110.01778v1', updated=datetime.datetime(2021, 10, 5, 1, 38, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 10, 5, 1, 38, 24, tzinfo=datetime.timezone.utc), title='Version Reconciliation for Collaborative Databases', authors=[arxiv.Result.Author('Nalin Ranjan'), arxiv.Result.Author('Zechao Shang'), arxiv.Result.Author('Aaron J. Elmore'), arxiv.Result.Author('Sanjay Krishnan')], summary='We propose MindPalace, a prototype of a versioned database for efficient\\ncollaborative data management. MindPalace supports offline collaboration, where\\nusers work independently without real-time correspondence. The core of\\nMindPalace is a critical step of offline collaboration: reconciling divergent\\nbranches made by simultaneous data manipulation. We formalize the concept of\\nauto-mergeability, a condition under which branches may be reconciled without\\nhuman intervention, and propose an efficient framework for determining whether\\ntwo branches are auto-mergeable and identifying particular records for manual\\nreconciliation.', comment=\"Full version of a paper to appear in SoCC '21\", journal_ref=None, doi='10.1145/3472883.3486980', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3472883.3486980', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2110.01778v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2110.01778v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2110.03028v1', updated=datetime.datetime(2021, 10, 6, 19, 21, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 10, 6, 19, 21, 51, tzinfo=datetime.timezone.utc), title='Reconsidering Optimistic Algorithms for Relational DBMS', authors=[arxiv.Result.Author('Malcolm Crowe'), arxiv.Result.Author('Fritz Laux')], summary='At DBKDA 2019, we demonstrated that StrongDBMS with simple but rigorous\\noptimistic algorithms, provides better performance in situations of high\\nconcurrency than major commercial database management systems (DBMS). The\\ndemonstration was convincing but the reasons for its success were not fully\\nanalysed. There is a brief account of the results below. In this short\\ncontribution, we wish to discuss the reasons for the results. The analysis\\nleads to a strong criticism of all DBMS algorithms based on locking, and based\\non these results, it is not fanciful to suggest that it is time to re-engineer\\nexisting DBMS.', comment='4 pages, 3 figures, conference paper', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2110.03028v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2110.03028v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2111.04556v1', updated=datetime.datetime(2021, 11, 8, 15, 12, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 11, 8, 15, 12, 33, tzinfo=datetime.timezone.utc), title='Time- and Space-Efficient Regular Path Queries on Graphs', authors=[arxiv.Result.Author('Diego Arroyuelo'), arxiv.Result.Author('Aidan Hogan'), arxiv.Result.Author('Gonzalo Navarro'), arxiv.Result.Author('Javiel Rojas-Ledesma')], summary='We introduce a time- and space-efficient technique to solve regularpath\\nqueries over labeled graphs. We combine a bit-parallel simula-tion of the\\nGlushkov automaton of the regular expression with thering index introduced by\\nArroyuelo et al., exploiting its wavelettree representation of the triples in\\norder to efficiently reach thestates of the product graph that are relevant for\\nthe query. Ourquery algorithm is able to simultaneously process several\\nautoma-ton states, as well as several graph nodes/labels. Our\\nexperimentalresults show that our representation uses 3-5 times less space\\nthanthe alternatives in the literature, while generally outperformingthem in\\nquery times (1.67 times faster than the next best).', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://arxiv.org/abs/2111.04556v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2111.04556v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2111.12487v1', updated=datetime.datetime(2021, 11, 24, 13, 31, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 11, 24, 13, 31, 3, tzinfo=datetime.timezone.utc), title='Distributed Evaluation of Graph Queries using Recursive Relational Algebra', authors=[arxiv.Result.Author('Sarah Chlyah'), arxiv.Result.Author('Pierre Genevès'), arxiv.Result.Author('Nabil Layaïda')], summary='We present a system called Dist-$\\\\mu$-RA for the distributed evaluation of\\nrecursive graph queries. Dist-$\\\\mu$-RA builds on the recursive relational\\nalgebra and extends it with evaluation plans suited for the distributed\\nsetting. The goal is to offer expressivity for high-level queries while\\nproviding efficiency at scale and reducing communication costs. Experimental\\nresults on both real and synthetic graphs show the effectiveness of the\\nproposed approach compared to existing systems.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2111.12487v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2111.12487v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2111.12835v1', updated=datetime.datetime(2021, 11, 24, 23, 6, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 11, 24, 23, 6, 2, tzinfo=datetime.timezone.utc), title='SchemaDB: Structures in Relational Datasets', authors=[arxiv.Result.Author('Cody James Christopher'), arxiv.Result.Author('Kristen Moore'), arxiv.Result.Author('David Liebowitz')], summary='In this paper we introduce the SchemaDB data-set; a collection of relational\\ndatabase schemata in both sql and graph formats. Databases are not commonly\\nshared publicly for reasons of privacy and security, so schemata are not\\navailable for study. Consequently, an understanding of database structures in\\nthe wild is lacking, and most examples found publicly belong to common\\ndevelopment frameworks or are derived from textbooks or engine benchmark\\ndesigns. SchemaDB contains 2,500 samples of relational schemata found in public\\nrepositories which we have standardised to MySQL syntax. We provide our\\ngathering and transformation methodology, summary statistics, and structural\\nanalysis, and discuss potential downstream research tasks in several domains.', comment='Draft', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2111.12835v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2111.12835v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2112.00288v1', updated=datetime.datetime(2021, 12, 1, 5, 47, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 12, 1, 5, 47, 39, tzinfo=datetime.timezone.utc), title='Operation-based Collaborative Data Sharing for Distributed Systems', authors=[arxiv.Result.Author('Masato Takeichi')], summary='Collaborative Data Sharing raises a fundamental issue in distributed systems.\\nSeveral strategies have been proposed for making shared data consistent between\\npeers in such a way that the shared part of their local data become equal. Most\\nof the proposals rely on state-based semantics. But this suffers from a lack of\\ndescriptiveness in conflict-free features of synchronization required for\\nflexible network connections. Recent applications tend to use non-permanent\\nconnection with mobile devices or allow temporary breakaways from the system,\\nfor example. To settle ourselves in conflict-free data sharing, we propose a\\nnovel scheme \"Operation-based Collaborative Data Sharing\" that enables\\nconflict-free strategies for synchronization based on operational semantics.', comment='11 pages, 4 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC', 'cs.PL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2112.00288v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2112.00288v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2112.01132v1', updated=datetime.datetime(2021, 12, 2, 11, 12, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 12, 2, 11, 12, 23, tzinfo=datetime.timezone.utc), title='A Practical Dynamic Programming Approach to Datalog Provenance Computation', authors=[arxiv.Result.Author('Yann Ramusat'), arxiv.Result.Author('Silviu Maniu'), arxiv.Result.Author('Pierre Senellart')], summary=\"We establish a translation between a formalism for dynamic programming over\\nhypergraphs and the computation of semiring-based provenance for Datalog\\nprograms. The benefit of this translation is a new method for computing\\nprovenance for a specific class of semirings. Theoretical and practical\\noptimizations lead to an efficient implementation using \\\\textsc{Souffl\\\\'e}, a\\nstate-of-the-art Datalog interpreter. Experimental results on real-world data\\nsuggest this approach to be efficient in practical contexts, even competing\\nwith our previous dedicated solutions for computing provenance in annotated\\ngraph databases. The cost overhead compared to plain Datalog evaluation is\\nfairly moderate in many cases of interest.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2112.01132v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2112.01132v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2201.03643v1', updated=datetime.datetime(2022, 1, 10, 20, 55, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 1, 10, 20, 55, 51, tzinfo=datetime.timezone.utc), title='Designing a Visual Tool for Property Graph Schema Extraction and Refinement: An Expert Study', authors=[arxiv.Result.Author('Nimo Beeren')], summary='The design space of visual tools that aim to help people create schemas for\\nproperty graphs is explored. Interviews are conducted with experts in the\\ndomain of property graphs and data management in general. Through this\\ncollaboration, we determine how a schema extraction tool can provide value.\\nThese insights are used to establish design requirements and design a UI\\nprototype, which are then relayed back to the experts. Positive reactions were\\nreceived, which encourage future work in the property graph schema space.', comment='15 pages, 4 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2201.03643v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2201.03643v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2201.03832v1', updated=datetime.datetime(2022, 1, 11, 8, 25, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 1, 11, 8, 25, 6, tzinfo=datetime.timezone.utc), title='Parallel Acyclic Joins with Canonical Edge Covers', authors=[arxiv.Result.Author('Yufei Tao')], summary='In PODS\\'21, Hu presented an algorithm in the massively parallel computation\\n(MPC) model that processes any acyclic join with an asymptotically optimal\\nload. In this paper, we present an alternative analysis of her algorithm. The\\nnovelty of our analysis is in the revelation of a new mathematical structure --\\nwhich we name \"canonical edge cover\" -- for acyclic hypergraphs. We prove\\nnon-trivial properties for canonical edge covers that offer us a\\ngraph-theoretic perspective about why Hu\\'s algorithm works.', comment=\"Accepted to ICDT'22\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2201.03832v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2201.03832v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2201.04233v1', updated=datetime.datetime(2022, 1, 11, 23, 0, 18, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 1, 11, 23, 0, 18, tzinfo=datetime.timezone.utc), title='Finding Your Way Through the Jungle of Big Data Architectures', authors=[arxiv.Result.Author('Torsten Priebe'), arxiv.Result.Author('Sebastian Neumaier'), arxiv.Result.Author('Stefan Markus')], summary='This paper presents a systematic review of common analytical data\\narchitectures based on DAMA-DMBOK and ArchiMate. The paper is work in progress\\nand provides a first view on Gartner\\'s Logical Data Warehouse paradigm, Data\\nFabric and Dehghani\\'s Data Mesh proposal as well as their interdependencies. It\\nfurthermore sketches the way forward how this work can be extended by covering\\nmore architecture paradigms (incl. classic Data Warehouse, Data Vault, Data\\nLake, Lambda and Kappa architectures) and introducing a template with among\\nothers \"context\", \"problem\" and \"solution\" descriptions, leading ultimately to\\na pattern system providing guidance for choosing the right architecture\\nparadigm for the right situation.', comment='3 pages, 4 figures', journal_ref='2021 IEEE International Conference on Big Data (IEEE BigData 2021)', doi='10.1109/BigData52589.2021.9671862', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/BigData52589.2021.9671862', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2201.04233v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2201.04233v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2201.04899v1', updated=datetime.datetime(2022, 1, 13, 11, 41, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 1, 13, 11, 41, 28, tzinfo=datetime.timezone.utc), title='An outline of multi objective optimization in databases with focus on flexible skyline queries', authors=[arxiv.Result.Author('Matteo Savino')], summary='The problem of optimizing across different, conceivably conflicting, criteria\\nis called multi-objective optimization and it is widely spread across many\\nfields. This is a recurring problem in database queries when there is the need\\nof obtaining the best objects from a very large data set. In this article, I\\nincluded a complete review of the main approaches typically used to achieve\\nmulti-criteria optimization. Starting from ranking queries and skylines and\\nthen proceeding to more advanced methods, this paper aims to define a clear\\noutline of multi-objective optimization in databases. In particular, the\\nflexible skyline paradigm is considered and thoroughly discussed as it\\novercomes many of the critical issues that arise with other methods.', comment='8 pages, 1 figure, 1 table, 5 examples', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2201.04899v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2201.04899v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2201.05096v2', updated=datetime.datetime(2022, 1, 14, 11, 53, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 1, 11, 13, 15, 8, tzinfo=datetime.timezone.utc), title='Flexible Skyline: one query to rule them all', authors=[arxiv.Result.Author('Giacomo Vinati')], summary='The most common archetypes to identify relevant information in large datasets\\nand find the bestoptions according to some preferences or user criteria, are\\nthe top-k queries (ranking method based ona score function defined over the\\nrecords attributes) and skyline queries (based on Pareto dominance oftuples).\\nDespite their large diffusion, both approaches have their pros and cons. In\\nthis survey paper, a comparison is made between these methods and the Flexible\\nSkylines, which is a framework that combines the ranking and skyline approaches\\nusing the novel concept ofF-dominanceto a set of monotone scoring function F.', comment='10 pages, 6 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2201.05096v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2201.05096v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2201.09018v1', updated=datetime.datetime(2022, 1, 22, 10, 50, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 1, 22, 10, 50, 17, tzinfo=datetime.timezone.utc), title='Comparison of 6 different approaches to outclass Top-k queries and Skyline queries', authors=[arxiv.Result.Author('Martino Manzolini')], summary='Topk queries and skyline queries have well explored limitations which recent\\nresearch have tried to complete through new techniques. In this survey, after\\nresuming such limitations, we consider Restricted Skyline Queries, ORD and ORU\\napproach, Krepresentative minimization queries, Skyline ordering queries, UTK\\nqueries approach and Skyrank that aim to overcome them. After introducing and\\ncomparing their main concepts, pros and cons, we briefly report the algorithms\\nand confront some of the experimental data collected from the bibliography. To\\nconclude the paper, we summarize the results presented with a short guide on\\nhow to select the best approach according to specific needs.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2201.09018v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2201.09018v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2201.10217v2', updated=datetime.datetime(2022, 1, 30, 17, 24, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 1, 25, 10, 18, 42, tzinfo=datetime.timezone.utc), title=\"Poisson's CDF applied to Flexible Skylines\", authors=[arxiv.Result.Author('Jaime Pons Garrido')], summary=\"The evolution of skyline and ranking queries has created new archetypes like\\nflexible skylines, which have proven to be an efficient method to select\\nrelevant data from large datasets using multi objective optimization. This\\npaper aims to study the possible applications of Poisson distribution mass\\nfunction as a monotonic scoring function in flexible skyline processes,\\nespecially those featuring schemas whose attributes can be translated to\\nconstant mean rates. Moreover, a method to express users's requirement by means\\nof the F-dominant set of tuples will be proposed using parametrical variations\\nin F[1], simultaneously, algorithm construction and potential applications will\\nbe studied.\", comment='10 pages, 3 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2201.10217v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2201.10217v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2201.12038v4', updated=datetime.datetime(2022, 2, 9, 22, 8, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 1, 28, 11, 6, 42, tzinfo=datetime.timezone.utc), title='A survey on flexible/restricted skyline and their applicability', authors=[arxiv.Result.Author('Davide Canali')], summary=\"Skyline and Top-k are two of the most important methods to extract\\ninformation from datasets, but both come with their drawbacks, that's why\\nlately some new technics that try to mix the features of the two have been\\nstudied. In this survey three new operators are analysed, F-Skyline, ORU/ORD,\\nand ${\\\\epsilon}$-Skyline. After giving the main ideas behind those and their\\nproperties, they are compered on 3 fundamental features such as\\npersonalization, cardinality control, and generalization to guide the user to\\nchoose the best one for any task.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2201.12038v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2201.12038v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2202.01546v1', updated=datetime.datetime(2022, 2, 3, 12, 12, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 2, 3, 12, 12, 14, tzinfo=datetime.timezone.utc), title='QueryER: A Framework for Fast Analysis-Aware Deduplication over Dirty Data', authors=[arxiv.Result.Author('Giorgos Alexiou'), arxiv.Result.Author('George Papastefanatos'), arxiv.Result.Author('Vassilis Stamatopoulos'), arxiv.Result.Author('Georgia Koutrika'), arxiv.Result.Author('Nectarios Koziris')], summary='In this work, we explore the problem of correctly and efficiently answering\\ncomplex SPJ queries issued directly on top of dirty data. We introduce QueryER,\\na framework that seamlessly integrates Entity Resolution into Query Processing.\\nQueryER executes analysis-aware deduplication by weaving ER operators into the\\nquery plan. The experimental evaluation of our approach exhibits that it adapts\\nto the workload and scales on both real and synthetic datasets.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2202.01546v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2202.01546v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2202.01550v1', updated=datetime.datetime(2022, 2, 3, 12, 22, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 2, 3, 12, 22, 19, tzinfo=datetime.timezone.utc), title='Multi-Objective Optimization, different approach to query a database', authors=[arxiv.Result.Author('Matteo Cordioli')], summary=\"The datasets available nowadays are very rich and complex, but how do we\\nreach the information we are looking for? In this survey, two different\\napproaches to query a dataset are analyzed and algorithms for each type are\\nexplained. Specifically, the TA and NRA have been analyzed for the Top-K query\\nand the Basic Block Nested Loops has been examined for the skyline query.\\nMoreover, it's explained the core idea behind the Prioritized and Flexible\\nskyline. In the end, the pros and cons of each type of analyzed query have been\\nevaluated based on different criteria.\", comment='7 pages, 11 references', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2202.01550v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2202.01550v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2202.02619v1', updated=datetime.datetime(2022, 2, 5, 19, 40, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 2, 5, 19, 40, 4, tzinfo=datetime.timezone.utc), title=\"A bird's eye view on Multi-Objective Optimization techniques in Relational Databases\", authors=[arxiv.Result.Author('Giuseppe Tortorelli')], summary='Multi-objective optimization is the problem of optimizing simultaneously\\nmultiple objective functions and several techniques exist to deal with this\\nproblem. This paper aims to present the main methods that can be used to solve\\nthis issue in the context of relational databases. In particular, this work\\nexamines Top-k query to get the k best result from a dataset and Skyline query\\nthat provides a more general overview of the best results. We also discuss\\nFlexible-skyline, a new method designed to improve upon the previous\\ntechniques, mitigating their shortcomings. For each method, we describe the\\nmain characteristics and present an overview of the algorithms implementing\\nsuch thecniques, while comparing advantages and disadvantages.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2202.02619v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2202.02619v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2202.05689v1', updated=datetime.datetime(2022, 2, 11, 15, 21, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 2, 11, 15, 21, 24, tzinfo=datetime.timezone.utc), title='Conservative Extensions for Existential Rules', authors=[arxiv.Result.Author('Jean Christoph Jung'), arxiv.Result.Author('Carsten Lutz'), arxiv.Result.Author('Jerzy Marcinkowski')], summary='We study the problem to decide, given sets T1,T2 of tuple-generating\\ndependencies (TGDs), also called existential rules, whether T2 is a\\nconservative extension of T1. We consider two natural notions of conservative\\nextension, one pertaining to answers to conjunctive queries over databases and\\none to homomorphisms between chased databases. Our main results are that these\\nproblems are undecidable for linear TGDs, undecidable for guarded TGDs even\\nwhen T1 is empty, and decidable for frontier-one TGDs.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LO'], links=[arxiv.Result.Link('http://arxiv.org/abs/2202.05689v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2202.05689v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2202.06430v1', updated=datetime.datetime(2022, 2, 13, 22, 36, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 2, 13, 22, 36, 16, tzinfo=datetime.timezone.utc), title='Giving the Right Answer: a Brief Overview on How to Extend Ranking and Skyline Queries', authors=[arxiv.Result.Author('Sergio Cuzzucoli')], summary='To retrieve the best results in a database we use Top-K queries and Skyline\\nqueries but some problems arise. The formers rely too much on user preferences,\\nwhich are difficult to quantify and may skew the fetching of the data, while\\nthe latters tend to output too much data. In this paper, we explore three\\ndifferent branches of research that seek to overcome such limitations:\\nFlexible/Restricted Skylines, Skyline Ordering/Ranking, and Regret\\nMinimization. We analyze how they work and we make comparisons among them to\\nguide the reader to choose the approach that best fits their use cases.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2202.06430v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2202.06430v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2202.09552v1', updated=datetime.datetime(2022, 2, 19, 8, 52, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 2, 19, 8, 52, 11, tzinfo=datetime.timezone.utc), title='A survey on making skylines more flexible', authors=[arxiv.Result.Author('Cem Cebeci')], summary='Top-$k$ queries and skylines are the two most common approaches to finding\\nthe most interesting entries in a homogeneous multi-dimensional dataset.\\nHowever, both of these strategies have some shortcomings. Top-$k$ queries are\\nvery challenging to specify precisely and skylines are not customizable to\\nspecific scenarios, on top of having unpredictable output cardinalities. We\\ndescribe some alternative methods aimed at addressing the shortcomings of\\ntop-$k$ queries and skylines and compare all approaches to illustrate which of\\nthe desired properties each of them possesses.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2202.09552v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2202.09552v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2202.10502v1', updated=datetime.datetime(2022, 2, 21, 19, 26, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 2, 21, 19, 26, 52, tzinfo=datetime.timezone.utc), title='Flexible Skylines: Customizing Skyline Queries Catching Desired Preferences', authors=[arxiv.Result.Author('Giuseppe Montanaro')], summary=\"The techniques most extensively used to retrieve interesting data from\\ndata-sets are the Skyline and the Top-k queries. Sadly, they are not enough for\\nfacing modern problems, so the needing of something more usable and reliable\\nhas come. In this survey we are going to explore Flexible Skylines which are\\nproposed to overcame the old fashion techniques' problems by extending the\\nconcept of dominance. After, we are going to compare this approach with the old\\nand new ones evaluating pros and cons. Finally, we will see some interesting\\napplications.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2202.10502v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2202.10502v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2202.10785v1', updated=datetime.datetime(2022, 2, 22, 10, 15, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 2, 22, 10, 15, 29, tzinfo=datetime.timezone.utc), title='Comparing the latest ranking techniques: pros and cons of flexible skylines, regret minimization and skyline ranking queries', authors=[arxiv.Result.Author('Davide Foini')], summary='Long-established ranking approaches, such as top-k and skyline queries, have\\nbeen thoroughly discussed and their drawbacks are well acknowledged. New\\ntechniques have been developed in recent years that try to combine traditional\\nones to overcome their limitations. In this paper we focus our attention on\\nsome of them: flexible skylines, regret minimization and skyline ranking\\nqueries, because, while these new methods are promising and have shown\\ninteresting results, a comparison between them is still not available. After a\\nshort introduction of each approach, we discuss analogies and differences\\nbetween them with the advantages and disadvantages of every technique debated.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2202.10785v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2202.10785v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2202.12618v1', updated=datetime.datetime(2022, 2, 25, 11, 10, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 2, 25, 11, 10, 50, tzinfo=datetime.timezone.utc), title='Getting the best from skylines and top-k queries', authors=[arxiv.Result.Author('Marco Costanzo')], summary='Top-k and skylines are two important techniques that can be used to extract\\nthe best objects from a set. Both the approaches have well-known pros and cons:\\na quite big limitation of skyline queries is the impossibility to control the\\ncardinality of the output and the difficulty in specifying a trade-off among\\nattributes, whereas the ranking queries allow so. On the other hand, the usage\\nof ranking implies that ranking functions need to be specified by users and\\nrenouncing the simplicity of skylines. Flexible/ restricted skylines present a\\nnew approach to tackle this problem, combining the best characteristics of both\\ntechniques making use of a new flexible relation of dominance.', comment='24 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2202.12618v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2202.12618v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2203.00331v2', updated=datetime.datetime(2022, 3, 3, 10, 2, 37, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 3, 1, 10, 12, 39, tzinfo=datetime.timezone.utc), title='Counting stars: a survey on flexible Skyline Query approaches', authors=[arxiv.Result.Author('Alessandro Del Giudice')], summary='Nowadays, as the quantity of data to process began to rise, so did the need\\nfor a method to discern what pieces of information could be useful for the\\nuser; in response, researchers focused their efforts on improving the already\\nexisting ranking methods or creating new ones starting from them. This survey\\nwill be presented a small list of some of the most known and/or most recent\\nsolutions proposed, with some possible applications for them, concerning a\\nstate of the art restricted to around the last ten years, comparing their\\nperformance with the traditional one top-k and skyline queries.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2203.00331v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.00331v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2203.09236v1', updated=datetime.datetime(2022, 3, 17, 10, 56, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 3, 17, 10, 56, 30, tzinfo=datetime.timezone.utc), title='Weighing the techniques for data optimization in a database', authors=[arxiv.Result.Author('Anagha Radhakrishnan')], summary='A set of preferred records can be obtained from a large database in a\\nmulti-criteria setting using various computational methods which either depend\\non the concept of dominance or on the concept of utility or scoring function\\nbased on the attributes of the database record. A skyline approach relies on\\nthe dominance relationship between different data points to discover\\ninteresting data from a huge database. On the other hand, ranking queries make\\nuse of specific scoring functions to rank tuples in a database. An experimental\\nevaluation of datasets can provides us with information on the effectiveness of\\neach of these methods.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2203.09236v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.09236v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2203.09271v2', updated=datetime.datetime(2022, 3, 20, 16, 36, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 3, 17, 11, 53, 21, tzinfo=datetime.timezone.utc), title='A flexible solution to embrace Ranking and Skyline queries approaches', authors=[arxiv.Result.Author('Simone Censuales')], summary='The multi-objective optimization problem has always been the main objective\\nof the principal traditional approaches, such as Ranking queries and Skyline\\nqueries. The conventional idea was to either use one or the other, trying to\\nexploit both ranking queries advantages when it comes to taking into account\\nuser preferences, and skyline queries points of strength when the main\\nobjective was to obtain interesting results from a dataset in a simple, yet\\neffective fashion, both of them showing limitations when entering specific\\nfields of interest.', comment='12 pages, 8 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://arxiv.org/abs/2203.09271v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.09271v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2204.04628v2', updated=datetime.datetime(2023, 4, 25, 6, 55, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 4, 10, 8, 7, 33, tzinfo=datetime.timezone.utc), title='A Skyline and ranking query odyssey: a journey from skyline and ranking queries up to f-skyline queries', authors=[arxiv.Result.Author('Giuseppe Sorrentino')], summary='Skyline and ranking queries are two of the most used tools to manage large\\ndata sets. The former is based on non-dominance, while the latter on a scoring\\nfunction. Despite their effectiveness, they have some drawbacks like the result\\nsize or the need for a utility function that must be taken into account. To do\\nthis, in the last years, new kinds of queries, called flexible skyline queries,\\nhave been developed. In the present article, a description of skyline and\\nranking queries, f-skyline queries and a comparison among them are provided to\\nhighlight the improvements achieved and how some limitations have been\\novercome.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2204.04628v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2204.04628v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2204.04898v1', updated=datetime.datetime(2022, 4, 11, 6, 53, 36, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 4, 11, 6, 53, 36, tzinfo=datetime.timezone.utc), title='PM4Py-GPU: a High-Performance General-Purpose Library for Process Mining', authors=[arxiv.Result.Author('Alessandro Berti'), arxiv.Result.Author('Minh Phan Nghia'), arxiv.Result.Author('Wil M. P. van der Aalst')], summary='Open-source process mining provides many algorithms for the analysis of event\\ndata which could be used to analyze mainstream processes (e.g., O2C, P2P, CRM).\\nHowever, compared to commercial tools, they lack the performance and struggle\\nto analyze large amounts of data. This paper presents PM4Py-GPU, a Python\\nprocess mining library based on the NVIDIA RAPIDS framework. Thanks to the\\ndataframe columnar storage and the high level of parallelism, a significant\\nspeed-up is achieved on classic process mining computations and processing\\nactivities.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2204.04898v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2204.04898v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2204.06078v1', updated=datetime.datetime(2022, 4, 12, 20, 42, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 4, 12, 20, 42, 28, tzinfo=datetime.timezone.utc), title='Understanding the compromise between skyline and ranking queries', authors=[arxiv.Result.Author('Marco Tonnarelli')], summary='Skyline and Ranking queries have gained great popularity in the recent years.\\nThese two techniques are crucial for multi-criteria decision support\\napplications, which are now more popular than ever before. Skyline and Ranking\\nqueries are, however, affected by well-known limitations. In the past recent\\nyears, the database community provided numerous studies in this field with the\\naim to overcome the weaknesses of these two approaches. This survey introduces\\nthe reader to Skyline and Ranking queries, explaining the concepts on which\\nthey are based, with the intent to present the compromise between the two\\ntechniques: flexible skylines.', comment='13 pages, 7 figures, 4 tables', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2204.06078v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2204.06078v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2204.08941v1', updated=datetime.datetime(2022, 4, 19, 15, 19, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 4, 19, 15, 19, 35, tzinfo=datetime.timezone.utc), title='CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex', authors=[arxiv.Result.Author('Immanuel Trummer')], summary=\"CodexDB is an SQL processing engine whose internals can be customized via\\nnatural language instructions. CodexDB is based on OpenAI's GPT-3 Codex model\\nwhich translates text into code. It is a framework on top of GPT-3 Codex that\\ndecomposes complex SQL queries into a series of simple processing steps,\\ndescribed in natural language. Processing steps are enriched with user-provided\\ninstructions and descriptions of database properties. Codex translates the\\nresulting text into query processing code. An early prototype of CodexDB is\\nable to generate correct code for a majority of queries of the WikiSQL\\nbenchmark and can be customized in various ways.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CL', 'cs.LG', 'H.2.4'], links=[arxiv.Result.Link('http://arxiv.org/abs/2204.08941v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2204.08941v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2204.10655v1', updated=datetime.datetime(2022, 4, 22, 11, 54, 38, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 4, 22, 11, 54, 38, tzinfo=datetime.timezone.utc), title='Use of Context in Data Quality Management: a Systematic Literature Review', authors=[arxiv.Result.Author('Flavia Serra'), arxiv.Result.Author('Veronika Peralta'), arxiv.Result.Author('Adriana Marotta'), arxiv.Result.Author('Patrick Marcel')], summary='The importance of context in data quality (DQ) was shown many years ago and\\nnowadays is widely accepted. Early approaches and surveys defined DQ as\\n\\\\textit{fitness for use} and showed the influence of context on DQ. This paper\\npresents a Systematic Literature Review (SLR) for investigating how context is\\ntaken into account in recent proposals for DQ management. We specifically\\npresent the planning and execution of the SLR, the analysis criteria and our\\nresults reflecting the relationship between context and DQ in the state of the\\nart and, particularly, how that context is defined and used for DQ management.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2204.10655v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2204.10655v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2204.11137v3', updated=datetime.datetime(2023, 2, 2, 9, 29, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 4, 23, 20, 5, 35, tzinfo=datetime.timezone.utc), title='Evaluating regular path queries under the all-shortest paths semantics', authors=[arxiv.Result.Author('Domagoj Vrgoč')], summary='The purpose of this report is to explain how the textbook breadth-first\\nsearch algorithm (BFS) can be modified in order to also create a compact\\nrepresentation of all shortest paths connecting a single source node to all the\\nnodes reachable from it. From this representation, all these paths can also be\\nefficiently enumerated. We then apply this algorithm to solve a similar problem\\nin edge labelled graphs, where paths also have an additional restriction that\\ntheir edge labels form a word belonging to a regular language. Namely, we solve\\nthe problem of evaluating regular path queries (RPQs) under the all-shortest\\npaths semantics.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2204.11137v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2204.11137v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2205.00285v1', updated=datetime.datetime(2022, 4, 30, 14, 55, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 4, 30, 14, 55, 33, tzinfo=datetime.timezone.utc), title='Flexible skyline: overview and applicability', authors=[arxiv.Result.Author('Carlo Bellacoscia')], summary=\"Ranking (or top-k) and skyline queries are the most popular approaches used\\nto extract interesting data from large datasets. The first one is based on a\\nscoring function to evaluate and rank tuples. Its computation is fast, but it\\nis sensitive to the choice of the evaluating function. Skyline queries are\\nbased on the idea of dominance and the result is the set of all non-dominated\\ntuples. This is a very interesting approach, but it can't allow to control the\\ncardinality of the output. Recent researches discovered more techniques to\\ncompensate for these drawbacks. In particular, this paper will focus on the\\nflexible skyline approach.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2205.00285v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2205.00285v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2205.01428v1', updated=datetime.datetime(2022, 5, 3, 11, 33, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 5, 3, 11, 33, 50, tzinfo=datetime.timezone.utc), title='Filtering and Sampling Object-Centric Event Logs', authors=[arxiv.Result.Author('Alessandro Berti')], summary='The scalability of process mining techniques is one of the main challenges to\\ntackling the massive amount of event data produced every day in enterprise\\ninformation systems. To this purpose, filtering and sampling techniques are\\nproposed to keep a subset of the behavior of the original log and make the\\napplication of process mining techniques feasible. While techniques for\\nfiltering/sampling traditional event logs have been already proposed,\\nfiltering/sampling object-centric event logs is more challenging as the number\\nof factors (events, objects, object types) to consider is significantly higher.\\nThis paper provides some techniques to filter/sample object-centric event logs.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2205.01428v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2205.01428v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2205.02190v2', updated=datetime.datetime(2022, 9, 13, 20, 1, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 5, 4, 17, 13, 8, tzinfo=datetime.timezone.utc), title='Ontology-Mediated Querying on Databases of Bounded Cliquewidth', authors=[arxiv.Result.Author('Carsten Lutz'), arxiv.Result.Author('Leif Sabellek'), arxiv.Result.Author('Lukas Schulze')], summary='We study the evaluation of ontology-mediated queries (OMQs) on databases of\\nbounded cliquewidth from the viewpoint of parameterized complexity theory. As\\nthe ontology language, we consider the description logics $\\\\mathcal{ALC}$ and\\n$\\\\mathcal{ALCI}$ as well as the guarded two-variable fragment GF$_2$ of\\nfirst-order logic. Queries are atomic queries (AQs), conjunctive queries (CQs),\\nand unions of CQs. All studied OMQ problems are fixed-parameter linear (FPL)\\nwhen the parameter is the size of the OMQ plus the cliquewidth. Our main\\ncontribution is a detailed analysis of the dependence of the running time on\\nthe parameter, exhibiting several interesting effects.', comment=None, journal_ref=None, doi='10.24963/kr.2022/25', primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://dx.doi.org/10.24963/kr.2022/25', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2205.02190v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2205.02190v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2205.15547v1', updated=datetime.datetime(2022, 5, 31, 5, 40, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 5, 31, 5, 40, 28, tzinfo=datetime.timezone.utc), title='Discovery of Keys for Graphs [Extended Version]', authors=[arxiv.Result.Author('Morteza Alipourlangouri'), arxiv.Result.Author('Fei Chiang')], summary='Keys for graphs uses the topology and value constraints needed to uniquely\\nidentify entities in a graph database. They have been studied to support object\\nidentification, knowledge fusion, data deduplication, and social network\\nreconciliation. In this paper, we present our algorithm to mine keys over\\ngraphs. Our algorithm discovers keys in a graph via frequent subgraph\\nexpansion. We present two properties that define a meaningful key, including\\nminimality and support. Lastly, using real-world graphs, we experimentally\\nverify the efficiency of our algorithm on real world graphs.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2205.15547v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2205.15547v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2206.00623v1', updated=datetime.datetime(2022, 6, 1, 16, 48, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 6, 1, 16, 48, 47, tzinfo=datetime.timezone.utc), title='P4DB -- The Case for In-Network OLTP (Extended Technical Report)', authors=[arxiv.Result.Author('Matthias Jasny'), arxiv.Result.Author('Lasse Thostrup'), arxiv.Result.Author('Tobias Ziegler'), arxiv.Result.Author('Carsten Binnig')], summary='In this paper we present a new approach for distributed DBMSs called P4DB,\\nthat uses a programmable switch to accelerate OLTP workloads. The main idea of\\nP4DB is that it implements a transaction processing engine on top of a\\nP4-programmable switch. The switch can thus act as an accelerator in the\\nnetwork, especially when it is used to store and process hot (contended) tuples\\non the switch. In our experiments, we show that P4DB hence provides significant\\nbenefits compared to traditional DBMS architectures and can achieve a speedup\\nof up to 8x.', comment='Extended Technical Report for: P4DB - The Case for In-Network OLTP', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2206.00623v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2206.00623v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2206.01643v1', updated=datetime.datetime(2022, 6, 3, 15, 38, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 6, 3, 15, 38, 33, tzinfo=datetime.timezone.utc), title='ChaTEAU: A Universal Toolkit for Applying the Chase', authors=[arxiv.Result.Author('Tanja Auge'), arxiv.Result.Author('Nic Scharlau'), arxiv.Result.Author('Andreas Görres'), arxiv.Result.Author('Jakob Zimmer'), arxiv.Result.Author('Andreas Heuer')], summary='What do applications like semantic optimization, data exchange and\\nintegration, answering queries under dependencies, query reformulation with\\nconstraints, and data cleaning have in common? All these applications can be\\nprocessed by the Chase, a family of algorithms for reasoning with constraints.\\nWhile the theory of the Chase is well understood, existing implementations are\\nconfined to specific use cases and application scenarios, making it difficult\\nto reuse them in other settings. ChaTEAU overcomes this limitation: It takes\\nthe logical core of the Chase, generalizes it, and provides a software library\\nfor different Chase applications in a single toolkit.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LO'], links=[arxiv.Result.Link('http://arxiv.org/abs/2206.01643v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2206.01643v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2206.02736v1', updated=datetime.datetime(2022, 6, 6, 16, 46, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 6, 6, 16, 46, 4, tzinfo=datetime.timezone.utc), title='Comparing modern techniques for querying data starting from top-k and skyline queries', authors=[arxiv.Result.Author('Fabio Patella')], summary='To make intelligent decisions over complex data by discovering a set of\\ninteresting options is something that has become very important for users of\\nmodern applications. Consequently, researchers are studying new techniques to\\novercome limitations of traditional ways of querying data from databases as\\ntop-k queries and skyline queries. Over the past few years new methods have\\nbeen developed as Flexible Skylines, Regret Minimization and Skyline\\nordering/ranking. The aim of this survey is to describe these techniques and\\nsome their possible variants comparing them and explaining how they improve\\ntraditional methods.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2206.02736v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2206.02736v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2206.12753v1', updated=datetime.datetime(2022, 6, 26, 0, 8, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 6, 26, 0, 8, 6, tzinfo=datetime.timezone.utc), title='Spatiotemporal Data Mining: A Survey', authors=[arxiv.Result.Author('Arun Sharma'), arxiv.Result.Author('Zhe Jiang'), arxiv.Result.Author('Shashi Shekhar')], summary='Spatiotemporal data mining aims to discover interesting, useful but\\nnon-trivial patterns in big spatial and spatiotemporal data. They are used in\\nvarious application domains such as public safety, ecology, epidemiology, earth\\nscience, etc. This problem is challenging because of the high societal cost of\\nspurious patterns and exorbitant computational cost. Recent surveys of\\nspatiotemporal data mining need update due to rapid growth. In addition, they\\ndid not adequately survey parallel techniques for spatiotemporal data mining.\\nThis paper provides a more up-to-date survey of spatiotemporal data mining\\nmethods. Furthermore, it has a detailed survey of parallel formulations of\\nspatiotemporal data mining.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CV', 'cs.DC', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2206.12753v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2206.12753v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2206.14415v1', updated=datetime.datetime(2022, 6, 29, 6, 4, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 6, 29, 6, 4, 19, tzinfo=datetime.timezone.utc), title='Performance Analysis: Discovering Semi-Markov Models From Event Logs', authors=[arxiv.Result.Author('Anna Kalenkova'), arxiv.Result.Author('Lewis Mitchell'), arxiv.Result.Author('Matthew Roughan')], summary='Process mining methods and tools are largely used in industry to monitor and\\nimprove operational processes. This paper presents a new technique to analyze\\nperformance characteristics of processes using event data. Based on event\\nsequences and their timestamps, semi-Markov models are discovered. The\\ndiscovered models are further used for performance what-if analysis of the\\nprocesses. The paper studies a trade-off between the order of models discovered\\nand accuracy of representing performance information. The proposed discovery\\nand analysis techniques are implemented and tested on real-world event data.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2206.14415v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2206.14415v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2207.14163v1', updated=datetime.datetime(2022, 7, 21, 5, 35, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 7, 21, 5, 35, 2, tzinfo=datetime.timezone.utc), title='Towards Specificationless Monitoring of Provenance-Emitting Systems', authors=[arxiv.Result.Author('Martin Stoffers'), arxiv.Result.Author('Alexander Weinert')], summary='Monitoring often requires insight into the monitored system as well as\\nconcrete specifications of expected behavior. More and more systems, however,\\nprovide information about their inner procedures by emitting provenance\\ninformation in a W3C-standardized graph format.\\n  In this work, we present an approach to monitor such provenance data for\\nanomalous behavior by performing spectral graph analysis on slices of the\\nconstructed provenance graph and by comparing the characteristics of each slice\\nwith those of a sliding window over recently seen slices. We argue that this\\napproach not only simplifies the monitoring of heterogeneous distributed\\nsystems, but also enables applying a host of well-studied techniques to monitor\\nsuch systems.', comment='Accepted for publication as a short paper at Runtime Verification\\n  2022', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.GL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2207.14163v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2207.14163v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2208.02697v1', updated=datetime.datetime(2022, 8, 4, 14, 51, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 8, 4, 14, 51, 35, tzinfo=datetime.timezone.utc), title='WShEx: A language to describe and validate Wikibase entities', authors=[arxiv.Result.Author('Jose Emilio Labra Gayo')], summary='Wikidata is one of the most successful Semantic Web projects. Its underlying\\nWikibase data model departs from RDF with the inclusion of several features\\nlike qualifiers and references, built-in datatypes, etc. Those features are\\nserialized to RDF for content negotiation, RDF dumps and in the SPARQL\\nendpoint. Wikidata adopted the entity schemas namespace using the ShEx language\\nto describe and validate the RDF serialization of Wikidata entities. In this\\npaper we propose WShEx, a language inspired by ShEx that directly supports the\\nWikibase data model and can be used to describe and validate Wikibase entities.\\nThe paper presents a the abstract syntax and semantic of the WShEx language.', comment='arXiv admin note: substantial text overlap with arXiv:2110.11709', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2208.02697v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2208.02697v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2208.03823v1', updated=datetime.datetime(2022, 8, 7, 21, 25, 7, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 8, 7, 21, 25, 7, tzinfo=datetime.timezone.utc), title='Automatically Finding Optimal Index Structure', authors=[arxiv.Result.Author('Supawit Chockchowwat'), arxiv.Result.Author('Wenjie Liu'), arxiv.Result.Author('Yongjoo Park')], summary='Existing learned indexes (e.g., RMI, ALEX, PGM) optimize the internal\\nregressor of each node, not the overall structure such as index height, the\\nsize of each layer, etc. In this paper, we share our recent findings that we\\ncan achieve significantly faster lookup speed by optimizing the structure as\\nwell as internal regressors. Specifically, our approach (called AirIndex)\\nexpresses the end-to-end lookup time as a novel objective function, and\\nsearches for optimal design decisions using a purpose-built optimizer. In our\\nexperiments with state-of-the-art methods, AirIndex achieves 3.3x-7.7x faster\\nlookup for the data stored on local SSD, and 1.4x-3.0x faster lookup for the\\ndata on Azure Cloud Storage.', comment='5 pages, to be published in AIDB at VLDB 2022', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2208.03823v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2208.03823v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2208.09672v1', updated=datetime.datetime(2022, 8, 20, 12, 34, 18, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 8, 20, 12, 34, 18, tzinfo=datetime.timezone.utc), title='Comparing graph data science libraries for querying and analysing datasets: towards data science queries on graphs', authors=[arxiv.Result.Author('Genoveva Vargas-Solar'), arxiv.Result.Author('Pierre Marrec'), arxiv.Result.Author('Mirian Halfeld Ferrari Alves')], summary='This paper presents an experimental study to compare analysis tools with\\nmanagement systems for querying and analysing graphs. Our experiment compares\\nclassic graph navigational operations queries where analytics tools and\\nmanagement systems adopt different execution strategies. Then, our experiment\\naddresses data science pipelines with clustering and prediction models applied\\nto graphs. In this kind of experiment, we underline the interest in combining\\nboth approaches and the interest of relying on a parallel execution platform\\nfor executing queries.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2208.09672v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2208.09672v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2208.09673v1', updated=datetime.datetime(2022, 8, 20, 12, 40, 25, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 8, 20, 12, 40, 25, tzinfo=datetime.timezone.utc), title='Graph analytics workflows enactment on just in time data centres, Position Paper', authors=[arxiv.Result.Author('Ali Akoglu'), arxiv.Result.Author('José-Luis Zechinelli-Martini'), arxiv.Result.Author('Hamamache Kheddouci'), arxiv.Result.Author('Genoveva Vargas-Solar')], summary='This paper discusses our vision of multirole-capable decision-making systems\\nacross a broad range of Data Science (DS) workflows working on graphs through\\ndisaggregated data centres. Our vision is that an alternative is possible to\\nwork on a disaggregated solution for the provision of computational services\\nunder the notion of a disaggregated data centre. We define this alternative as\\na virtual entity that dynamically provides resources crosscutting the layers of\\nedge, fog and data centre according to the workloads submitted by the workflows\\nand their Service Level Objectives.', comment='arXiv admin note: text overlap with arXiv:2103.07978', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2208.09673v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2208.09673v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2208.10415v1', updated=datetime.datetime(2022, 8, 22, 15, 53, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 8, 22, 15, 53, 39, tzinfo=datetime.timezone.utc), title='NLDS-QL: From natural language data science questions to queries on graphs: analysing patients conditions & treatments', authors=[arxiv.Result.Author('Genoveva Vargas-Solar'), arxiv.Result.Author('Karim Dao'), arxiv.Result.Author('Mirian Halfeld Ferrari Alves')], summary='This paper introduces NLDS-QL, a translator of data science questions\\nexpressed in natural language (NL) into data science queries on graph\\ndatabases. Our translator is based on a simplified NL described by a grammar\\nthat specifies sentences combining keywords to refer to operations on graphs\\nwith the vocabulary of the graph schema. The demonstration proposed in this\\npaper shows NLDS-QL in action within a scenario to explore and analyse a graph\\nbase on patient diagnoses generated with the open-source Synthea.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2208.10415v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2208.10415v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2209.02089v1', updated=datetime.datetime(2022, 9, 5, 18, 24, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 9, 5, 18, 24, 59, tzinfo=datetime.timezone.utc), title='Compressing integer lists with Contextual Arithmetic Trits', authors=[arxiv.Result.Author('Yann Barsamian'), arxiv.Result.Author('André Chailloux')], summary='Inverted indexes allow to query large databases without needing to search in\\nthe database at each query. An important line of research is to construct the\\nmost efficient inverted indexes, both in terms of compression ratio and time\\nefficiency. In this article, we show how to use trit encoding, combined with\\ncontextual methods for computing inverted indexes. We perform an extensive\\nstudy of different variants of these methods and show that our method\\nconsistently outperforms the Binary Interpolative Method -- which is one of the\\ngolden standards in this topic -- with respect to compression size. We apply\\nour methods to a variety of datasets and make available the source code that\\nproduced the results, together with all our datasets.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IT', 'math.IT'], links=[arxiv.Result.Link('http://arxiv.org/abs/2209.02089v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2209.02089v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2209.10475v1', updated=datetime.datetime(2022, 9, 21, 16, 15, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 9, 21, 16, 15, 47, tzinfo=datetime.timezone.utc), title='Designing PIDs for Reproducible Science Using Time-Series Data', authors=[arxiv.Result.Author('Wen Ting Maria Tu'), arxiv.Result.Author('Stephen Makonin')], summary='As part of the investigation done by the IEEE Standards Association P2957\\nWorking Group, called Big Data Governance and Metadata Management, the use of\\npersistent identifiers (PIDs) is looked at for tackling the problem of\\nreproducible research and science. This short paper proposes a preliminary\\nmethod using PIDs to reproduce research results using time-series data.\\nFurthermore, we feel it is possible to use the methodology and design for other\\ntypes of datasets.', comment='Submitted to MTSR 2022 - 16th International Conference on Metadata\\n  and Semantics Research', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2209.10475v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2209.10475v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2210.02237v1', updated=datetime.datetime(2022, 10, 5, 13, 17, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 10, 5, 13, 17, 23, tzinfo=datetime.timezone.utc), title='Dimensional Data KNN-Based Imputation', authors=[arxiv.Result.Author('Yuzhao Yang'), arxiv.Result.Author('Jérôme Darmont'), arxiv.Result.Author('Franck Ravat'), arxiv.Result.Author('Olivier Teste')], summary='Data Warehouses (DWs) are core components of Business Intelligence (BI).\\nMissing data in DWs have a great impact on data analyses. Therefore, missing\\ndata need to be completed. Unlike other existing data imputation methods mainly\\nadapted for facts, we propose a new imputation method for dimensions. This\\nmethod contains two steps: 1) a hierarchical imputation and 2) a k-nearest\\nneighbors (KNN) based imputation. Our solution has the advantage of taking into\\naccount the DW structure and dependency constraints. Experimental assessments\\nvalidate our method in terms of effectiveness and efficiency.', comment=None, journal_ref='26th European Conference on Advances in Databases and Information\\n  Systems (ADBIS 2022), Sep 2020, Turin, Italy. pp.315-329', doi='10.1007/978-3-031-15740-0_23', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-031-15740-0_23', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2210.02237v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2210.02237v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2210.13136v1', updated=datetime.datetime(2022, 10, 24, 11, 42, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 10, 24, 11, 42, 42, tzinfo=datetime.timezone.utc), title='Path association rule mining', authors=[arxiv.Result.Author('Yuya Sasaki')], summary='Graph association rule mining is a data mining technique used for discovering\\nregularities in graph data. In this study, we propose a novel concept, {\\\\it\\npath association rule mining}, to discover the correlations of path patterns\\nthat frequently appear in a given graph. Reachability path patterns (i.e.,\\nexistence of paths from a vertex to another vertex) are applied in our concept\\nto discover diverse regularities. We show that the problem is NP-hard, and we\\ndevelop an efficient algorithm in which the anti-monotonic property is used on\\npath patterns. Subsequently, we develop approximation and parallelization\\ntechniques to efficiently and scalably discover rules. We use real-life graphs\\nto experimentally verify the effective', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2210.13136v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2210.13136v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2210.17086v1', updated=datetime.datetime(2022, 10, 31, 6, 23, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 10, 31, 6, 23, 5, tzinfo=datetime.timezone.utc), title='EEMARQ: Efficient Lock-Free Range Queries with Memory Reclamation', authors=[arxiv.Result.Author('Gali Sheffi'), arxiv.Result.Author('Pedro Ramalhete'), arxiv.Result.Author('Erez Petrank')], summary='Multi-Version Concurrency Control (MVCC) is a common mechanism for achieving\\nlinearizable range queries in database systems and concurrent data-structures.\\nThe core idea is to keep previous versions of nodes to serve range queries,\\nwhile still providing atomic reads and updates. Existing concurrent\\ndata-structure implementations, that support linearizable range queries, are\\neither slow, use locks, or rely on blocking reclamation schemes. We present\\nEEMARQ, the first scheme that uses MVCC with lock-free memory reclamation to\\nobtain a fully lock-free data-structure supporting linearizable inserts,\\ndeletes, contains, and range queries. Evaluation shows that EEMARQ outperforms\\nexisting solutions across most workloads, with lower space overhead and while\\nproviding full lock freedom.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2210.17086v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2210.17086v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2211.05416v1', updated=datetime.datetime(2022, 11, 10, 8, 46, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 11, 10, 8, 46, 47, tzinfo=datetime.timezone.utc), title='Wikidata-lite for Knowledge Extraction and Exploration', authors=[arxiv.Result.Author('Phuc Nguyen'), arxiv.Result.Author('Hideaki Takeda')], summary='Wikidata is the largest collaborative general knowledge graph supported by a\\nworldwide community. It includes many helpful topics for knowledge exploration\\nand data science applications. However, due to the enormous size of Wikidata,\\nit is challenging to retrieve a large amount of data with millions of results,\\nmake complex queries requiring large aggregation operations, or access too many\\nstatement references. This paper introduces our preliminary works on\\nWikidata-lite, a toolkit to build a database offline for knowledge extraction\\nand exploration, e.g., retrieving item information, statements, provenances, or\\nsearching entities by their keywords and attributes. Wikidata-lite has high\\nperformance and memory efficiency, much faster than the official Wikidata\\nSPARQL endpoint for big queries. The Wikidata-lite repository is available at\\nhttps://github.com/phucty/wikidb.', comment='3 pages, workshop paper', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2211.05416v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2211.05416v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2211.12996v2', updated=datetime.datetime(2022, 12, 14, 0, 22, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 11, 22, 17, 46, 56, tzinfo=datetime.timezone.utc), title='Converting OpenStreetMap Data to Road Networks for Downstream Applications', authors=[arxiv.Result.Author('Md Kaisar Ahmed')], summary='We study how to convert OpenStreetMap data to road networks for downstream\\napplications. OpenStreetMap data has different formats. Extensible Markup\\nLanguage (XML) is one of them. OSM data consist of nodes, ways, and relations.\\nWe process OSM XML data to extract the information of nodes and ways to obtain\\nthe map of streets of the Memphis area. We can use this map for different\\ndownstream applications.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CY', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2211.12996v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2211.12996v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2211.13170v1', updated=datetime.datetime(2022, 11, 23, 17, 47, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 11, 23, 17, 47, 42, tzinfo=datetime.timezone.utc), title='The World of Graph Databases from An Industry Perspective', authors=[arxiv.Result.Author('Yuanyuan Tian')], summary='Rapidly growing social networks and other graph data have created a high\\ndemand for graph technologies in the market. A plethora of graph databases,\\nsystems, and solutions have emerged, as a result. On the other hand, graph has\\nlong been a well studied area in the database research community. Despite the\\nnumerous surveys on various graph research topics, there is a lack of survey on\\ngraph technologies from an industry perspective. The purpose of this paper is\\nto provide the research community with an industrial perspective on the graph\\ndatabase landscape, so that graph researcher can better understand the industry\\ntrend and the challenges that the industry is facing, and work on solutions to\\nhelp address these problems.', comment='8 papers, 3 figures, to appear in SIGMOD Record', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2211.13170v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2211.13170v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2212.03294v1', updated=datetime.datetime(2022, 12, 6, 19, 47, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 12, 6, 19, 47, 10, tzinfo=datetime.timezone.utc), title='Cube Interestingness: Novelty, Relevance, Peculiarity and Surprise', authors=[arxiv.Result.Author('Dimos Gkitsakis'), arxiv.Result.Author('Spyridon Kaloudis'), arxiv.Result.Author('Eirini Mouselli'), arxiv.Result.Author('Veronika Peralta'), arxiv.Result.Author('Patrick Marcel'), arxiv.Result.Author('Panos Vassiliadis')], summary='In this paper, we discuss methods to assess the interestingness of a query in\\nan environment of data cubes. We assume a hierarchical multidimensional\\ndatabase, storing data cubes and level hierarchies. We provide a systematic\\ntaxonomy of the dimensions of interestingness, and specifically, relevance,\\nsurprise, novelty, and peculiarity. We propose specific measures and algorithms\\nfor assessing the different dimensions of cube query interestingness in a\\nquantitative fashion.', comment='61 pages, 10 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2212.03294v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2212.03294v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2212.05595v1', updated=datetime.datetime(2022, 11, 26, 20, 24, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 11, 26, 20, 24, 15, tzinfo=datetime.timezone.utc), title='A new PCA-based utility measure for synthetic data evaluation', authors=[arxiv.Result.Author('F. K. Dankar'), arxiv.Result.Author('M. K. Ibrahim')], summary='Data synthesis is a privacy enhancing technology aiming to produce realistic\\nand timely data when real data is hard to obtain. Utility of synthetic data\\ngenerators (SDGs) has been investigated through different utility metrics.\\nThese metrics have been found to generate conflicting conclusions making direct\\ncomparison of SDGs surprisingly difficult. Moreover, prior research found no\\ncorrelation between popular metrics, concluding they tackle different\\nutility-dimensions. This paper aggregates four popular utility metrics\\n(representing different utility dimensions) into one using\\nprincipal-component-analysis and checks whether the new measure can generate\\nsynthetic data that perform well in real-life. The new measure is used to\\ncompare four well-recognized SDGs.', comment='20 pages, 5 figures, 8 tables, 1 appendix', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', '68Txx', 'I.0'], links=[arxiv.Result.Link('http://arxiv.org/abs/2212.05595v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2212.05595v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2212.05682v1', updated=datetime.datetime(2022, 12, 12, 3, 38, 34, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 12, 12, 3, 38, 34, tzinfo=datetime.timezone.utc), title='Privacy-Preserving Record Linkage', authors=[arxiv.Result.Author('Dinusha Vatsalan'), arxiv.Result.Author('Dimitrios Karapiperis'), arxiv.Result.Author('Vassilios S. Verykios')], summary='Given several databases containing person-specific data held by different\\norganizations, Privacy-Preserving Record Linkage (PPRL) aims to identify and\\nlink records that correspond to the same entity/individual across different\\ndatabases based on the matching of personal identifying attributes, such as\\nname and address, without revealing the actual values in these attributes due\\nto privacy concerns. This reference work entry defines the PPRL problem,\\nreviews the literature and key findings, and discusses applications and\\nresearch challenges.', comment='PP. 1 - 10', journal_ref='Springer Encyclopedia of Big Data Technologies, 2022', doi='10.1007/978-3-319-63962-8_17-2', primary_category='cs.DB', categories=['cs.DB', 'cs.CR'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-319-63962-8_17-2', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2212.05682v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2212.05682v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2212.06043v1', updated=datetime.datetime(2022, 11, 3, 3, 32, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 11, 3, 3, 32, 22, tzinfo=datetime.timezone.utc), title='Introducing Hermes: Executing Clinical Quality Language (CQL) at over 66 Million Resources per Second (inexpensively)', authors=[arxiv.Result.Author('Angelo Kastroulis'), arxiv.Result.Author('Paolo Bonfini'), arxiv.Result.Author('Anastasios Litsas')], summary=\"Clinical Quality Language (CQL) has emerged as a standard for rule\\nrepresentation in Clinical Decision Support (CDS) and Electronic Clinical\\nQuality Measurement (eCQM) in healthcare. While open-source reference\\nimplementations and a few commercial engines exist, there is still a market\\nneed for high-performance engines that can execute CQL queries on the scales of\\nmillions of patients. We introduce the \\\\Hermes{} engine as the world's fastest\\ncommercial CQL execution engine.\", comment='9 pages, 9 figures, 2 appendices', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC', 'cs.PF'], links=[arxiv.Result.Link('http://arxiv.org/abs/2212.06043v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2212.06043v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2212.09271v2', updated=datetime.datetime(2022, 12, 20, 17, 3, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 12, 19, 6, 52, 13, tzinfo=datetime.timezone.utc), title='Very Large Language Model as a Unified Methodology of Text Mining', authors=[arxiv.Result.Author('Meng Jiang')], summary='Text data mining is the process of deriving essential information from\\nlanguage text. Typical text mining tasks include text categorization, text\\nclustering, topic modeling, information extraction, and text summarization.\\nVarious data sets are collected and various algorithms are designed for the\\ndifferent types of tasks. In this paper, I present a blue sky idea that very\\nlarge language model (VLLM) will become an effective unified methodology of\\ntext mining. I discuss at least three advantages of this new methodology\\nagainst conventional methods. Finally I discuss the challenges in the design\\nand development of VLLM techniques for text mining.', comment='4 pages, 3 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2212.09271v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2212.09271v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1803.00055v2', updated=datetime.datetime(2018, 3, 12, 21, 9, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 2, 28, 20, 0, 33, tzinfo=datetime.timezone.utc), title='Deep Reinforcement Learning for Join Order Enumeration', authors=[arxiv.Result.Author('Ryan Marcus'), arxiv.Result.Author('Olga Papaemmanouil')], summary='Join order selection plays a significant role in query performance. However,\\nmodern query optimizers typically employ static join enumeration algorithms\\nthat do not receive any feedback about the quality of the resulting plan.\\nHence, optimizers often repeatedly choose the same bad plan, as they do not\\nhave a mechanism for \"learning from their mistakes\". In this paper, we argue\\nthat existing deep reinforcement learning techniques can be applied to address\\nthis challenge. These techniques, powered by artificial neural networks, can\\nautomatically improve decision making by incorporating feedback from their\\nsuccesses and failures. Towards this goal, we present ReJOIN, a\\nproof-of-concept join enumerator, and present preliminary results indicating\\nthat ReJOIN can match or outperform the PostgreSQL optimizer in terms of plan\\nquality and join enumeration efficiency.', comment=None, journal_ref='aiDM@SIGMOD 2018 Proceedings of the First International Workshop\\n  on Exploiting Artificial Intelligence Techniques for Data Management', doi='10.1145/3211954.3211957', primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3211954.3211957', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1803.00055v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1803.00055v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1803.01384v2', updated=datetime.datetime(2019, 3, 24, 15, 9, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 3, 4, 17, 8, 45, tzinfo=datetime.timezone.utc), title='Data Curation with Deep Learning [Vision]', authors=[arxiv.Result.Author('Saravanan Thirumuruganathan'), arxiv.Result.Author('Nan Tang'), arxiv.Result.Author('Mourad Ouzzani'), arxiv.Result.Author('AnHai Doan')], summary='Data curation - the process of discovering, integrating, and cleaning data -\\nis one of the oldest, hardest, yet inevitable data management problems. Despite\\ndecades of efforts from both researchers and practitioners, it is still one of\\nthe most time consuming and least enjoyable work of data scientists. In most\\norganizations, data curation plays an important role so as to fully unlock the\\nvalue of big data. Unfortunately, the current solutions are not keeping up with\\nthe ever-changing data ecosystem, because they often require substantially high\\nhuman cost. Meanwhile, deep learning is making strides in achieving remarkable\\nsuccesses in multiple areas, such as image recognition, natural language\\nprocessing, and speech recognition. In this vision paper, we explore how some\\nof the fundamental innovations in deep learning could be leveraged to improve\\nexisting data curation solutions and to help build new ones. In particular, we\\nprovide a thorough overview of the current deep learning landscape, and\\nidentify interesting research opportunities and dispel common myths. We hope\\nthat the synthesis of these important domains will unleash a series of research\\nactivities that will lead to significantly improved solutions for many data\\ncuration tasks.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1803.01384v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1803.01384v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1803.01390v1', updated=datetime.datetime(2018, 3, 4, 17, 38, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 3, 4, 17, 38, 51, tzinfo=datetime.timezone.utc), title='Comparing Downward Fragments of the Relational Calculus with Transitive Closure on Trees', authors=[arxiv.Result.Author('Jelle Hellings'), arxiv.Result.Author('Marc Gyssens'), arxiv.Result.Author('Yuqing Wu'), arxiv.Result.Author('Dirk Van Gucht'), arxiv.Result.Author('Jan Van den Bussche'), arxiv.Result.Author('Stijn Vansummeren'), arxiv.Result.Author('George H. L. Fletcher')], summary='Motivated by the continuing interest in the tree data model, we study the\\nexpressive power of downward navigational query languages on trees and chains.\\nBasic navigational queries are built from the identity relation and edge\\nrelations using composition and union. We study the effects on relative\\nexpressiveness when we add transitive closure, projections, coprojections,\\nintersection, and difference; this for boolean queries and path queries on\\nlabeled and unlabeled structures. In all cases, we present the complete Hasse\\ndiagram. In particular, we establish, for each query language fragment that we\\nstudy on trees, whether it is closed under difference and intersection.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1803.01390v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1803.01390v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1803.01969v2', updated=datetime.datetime(2018, 7, 13, 23, 11, 49, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 3, 6, 0, 48, 59, tzinfo=datetime.timezone.utc), title='Moment-Based Quantile Sketches for Efficient High Cardinality Aggregation Queries', authors=[arxiv.Result.Author('Edward Gan'), arxiv.Result.Author('Jialin Ding'), arxiv.Result.Author('Kai Sheng Tai'), arxiv.Result.Author('Vatsal Sharan'), arxiv.Result.Author('Peter Bailis')], summary='Interactive analytics increasingly involves querying for quantiles over\\nsub-populations of high cardinality datasets. Data processing engines such as\\nDruid and Spark use mergeable summaries to estimate quantiles, but summary\\nmerge times can be a bottleneck during aggregation. We show how a compact and\\nefficiently mergeable quantile sketch can support aggregation workloads. This\\ndata structure, which we refer to as the moments sketch, operates with a small\\nmemory footprint (200 bytes) and computationally efficient (50ns) merges by\\ntracking only a set of summary statistics, notably the sample moments. We\\ndemonstrate how we can efficiently and practically estimate quantiles using the\\nmethod of moments and the maximum entropy principle, and show how the use of a\\ncascade further improves query time for threshold predicates. Empirical\\nevaluation on real-world datasets shows that the moments sketch can achieve\\nless than 1 percent error with 15 times less merge overhead than comparable\\nsummaries, improving end query time in the MacroBase engine by up to 7 times\\nand the Druid engine by up to 60 times.', comment='Technical Report for paper to be published in VLDB 2018', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1803.01969v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1803.01969v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1803.03716v1', updated=datetime.datetime(2018, 3, 9, 23, 7, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 3, 9, 23, 7, 9, tzinfo=datetime.timezone.utc), title='TRAJEDI: Trajectory Dissimilarity', authors=[arxiv.Result.Author('Pedram Gharani'), arxiv.Result.Author('Kenrick Fernande'), arxiv.Result.Author('Vineet Raghu')], summary='The vast increase in our ability to obtain and store trajectory data\\nnecessitates trajectory analytics techniques to extract useful information from\\nthis data. Pair-wise distance functions are a foundation building block for\\ncommon operations on trajectory datasets including constrained SELECT queries,\\nk-nearest neighbors, and similarity and diversity algorithms. The accuracy and\\nperformance of these operations depend heavily on the speed and accuracy of the\\nunderlying trajectory distance function, which is in turn affected by\\ntrajectory calibration. Current methods either require calibrated data, or\\nperform calibration of the entire relevant dataset first, which is expensive\\nand time consuming for large datasets. We present TRAJEDI, a calibrationaware\\npair-wise distance calculation scheme that outperforms naive approaches while\\npreserving accuracy. We also provide analyses of parameter tuning to trade-off\\nbetween speed and accuracy. Our scheme is usable with any diversity, similarity\\nor k-nearest neighbor algorithm.', comment=None, journal_ref=None, doi='10.1007/978-3-319-98923-5_8', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-319-98923-5_8', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1803.03716v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1803.03716v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1803.04562v2', updated=datetime.datetime(2018, 7, 24, 21, 35, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 3, 12, 22, 54, 11, tzinfo=datetime.timezone.utc), title='Bias in OLAP Queries: Detection, Explanation, and Removal', authors=[arxiv.Result.Author('Babak Salimi'), arxiv.Result.Author('Johannes Gehrke'), arxiv.Result.Author('Dan Suciu')], summary='On line analytical processing (OLAP) is an essential element of\\ndecision-support systems. OLAP tools provide insights and understanding needed\\nfor improved decision making. However, the answers to OLAP queries can be\\nbiased and lead to perplexing and incorrect insights. In this paper, we propose\\nHypDB, a system to detect, explain, and to resolve bias in decision-support\\nqueries. We give a simple definition of a \\\\emph{biased query}, which performs a\\nset of independence tests on the data to detect bias. We propose a novel\\ntechnique that gives explanations for bias, thus assisting an analyst in\\nunderstanding what goes on. Additionally, we develop an automated method for\\nrewriting a biased query into an unbiased query, which shows what the analyst\\nintended to examine. In a thorough evaluation on several real datasets we show\\nboth the quality and the performance of our techniques, including the\\ncompletely automatic discovery of the revolutionary insights from a famous 1973\\ndiscrimination case.', comment='This paper is an extended version of a paper presented at SIGMOD 2018', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1803.04562v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1803.04562v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1803.05277v1', updated=datetime.datetime(2018, 3, 14, 13, 44, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 3, 14, 13, 44, 53, tzinfo=datetime.timezone.utc), title='Constant delay algorithms for regular document spanners', authors=[arxiv.Result.Author('Fernando Florenzano'), arxiv.Result.Author('Cristian Riveros'), arxiv.Result.Author('Martin Ugarte'), arxiv.Result.Author('Stijn Vansummeren'), arxiv.Result.Author('Domagoj Vrgoc')], summary=\"Regular expressions and automata models with capture variables are core tools\\nin rule-based information extraction. These formalisms, also called regular\\ndocument spanners, use regular languages in order to locate the data that a\\nuser wants to extract from a text document, and then store this data into\\nvariables. Since document spanners can easily generate large outputs, it is\\nimportant to have good evaluation algorithms that can generate the extracted\\ndata in a quick succession, and with relatively little precomputation time.\\nTowards this goal, we present a practical evaluation algorithm that allows\\nconstant delay enumeration of a spanner's output after a precomputation phase\\nthat is linear in the document. While the algorithm assumes that the spanner is\\nspecified in a syntactic variant of variable set automata, we also study how it\\ncan be applied when the spanner is specified by general variable set automata,\\nregex formulas, or spanner algebras. Finally, we study the related problem of\\ncounting the number of outputs of a document spanner, providing a fine grained\\nanalysis of the classes of document spanners that support efficient enumeration\\nof their results.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.FL'], links=[arxiv.Result.Link('http://arxiv.org/abs/1803.05277v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1803.05277v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1803.06445v3', updated=datetime.datetime(2019, 2, 12, 16, 16, 36, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 3, 17, 2, 0, 47, tzinfo=datetime.timezone.utc), title='Datalog: Bag Semantics via Set Semantics', authors=[arxiv.Result.Author('Leopoldo Bertossi'), arxiv.Result.Author('Georg Gottlob'), arxiv.Result.Author('Reinhard Pichler')], summary='Duplicates in data management are common and problematic. In this work, we\\npresent a translation of Datalog under bag semantics into a well-behaved\\nextension of Datalog, the so-called {\\\\em warded Datalog}$^\\\\pm$, under set\\nsemantics. From a theoretical point of view, this allows us to reason on bag\\nsemantics by making use of the well-established theoretical foundations of set\\nsemantics. From a practical point of view, this allows us to handle the bag\\nsemantics of Datalog by powerful, existing query engines for the required\\nextension of Datalog. This use of Datalog$^\\\\pm$ is extended to give a set\\nsemantics to duplicates in Datalog$^\\\\pm$ itself. We investigate the properties\\nof the resulting Datalog$^\\\\pm$ programs, the problem of deciding\\nmultiplicities, and expressibility of some bag operations. Moreover, the\\nproposed translation has the potential for interesting applications such as to\\nMultiset Relational Algebra and the semantic web query language SPARQL with bag\\nsemantics.', comment='Extended version of paper appearing in Proc. ICDT 2019', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LO'], links=[arxiv.Result.Link('http://arxiv.org/abs/1803.06445v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1803.06445v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1803.06632v2', updated=datetime.datetime(2018, 7, 4, 13, 32, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 3, 18, 9, 57, 34, tzinfo=datetime.timezone.utc), title='A Guided FP-growth algorithm for multitude-targeted mining of big data', authors=[arxiv.Result.Author('Lior Shabtay'), arxiv.Result.Author('Rami Yaari'), arxiv.Result.Author('Itai Dattner')], summary='In this paper we present the GFP-growth (Guided FP-growth) algorithm, a novel\\nmethod for multitude-targeted mining: finding the count of a given large list\\nof itemsets in large data. The GFP-growth algorithm is designed to focus on the\\nspecific multitude itemsets of interest and optimizes the time and memory\\ncosts. We prove that the GFP-growth algorithm yields the exact frequency-counts\\nfor the required itemsets. We show that for a number of different problems, a\\nsolution can be devised which takes advantage of the efficient implementation\\nof multitude-targeted mining for boosting the performance. In particular, we\\nstudy in detail the problem of generating the minority-class rules from\\nimbalanced data, a scenario that appears in many real-life domains such as\\nmedical applications, failure prediction, network and cyber security, and\\nmaintenance. We develop the Minority-Report Algorithm that uses the GFP-growth\\nfor boosting performance. We prove some theoretical properties of the\\nMinority-Report Algorithm and demonstrate its performance gain using\\nsimulations and real data.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1803.06632v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1803.06632v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1803.07847v1', updated=datetime.datetime(2018, 3, 21, 10, 50, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 3, 21, 10, 50, 26, tzinfo=datetime.timezone.utc), title='On-demand Relational Concept Analysis', authors=[arxiv.Result.Author('Alexandre Bazin'), arxiv.Result.Author('Jessie Carbonnel'), arxiv.Result.Author('Marianne Huchard'), arxiv.Result.Author('Giacomo Kahn')], summary='Formal Concept Analysis and its associated conceptual structures have been\\nused to support exploratory search through conceptual navigation. Relational\\nConcept Analysis (RCA) is an extension of Formal Concept Analysis to process\\nrelational datasets. RCA and its multiple interconnected structures represent\\ngood candidates to support exploratory search in relational datasets, as they\\nare enabling navigation within a structure as well as between the connected\\nstructures. However, building the entire structures does not present an\\nefficient solution to explore a small localised area of the dataset, for\\ninstance to retrieve the closest alternatives to a given query. In these cases,\\ngenerating only a concept and its neighbour concepts at each navigation step\\nappears as a less costly alternative. In this paper, we propose an algorithm to\\ncompute a concept and its neighbourhood in extended concept lattices. The\\nconcepts are generated directly from the relational context family, and possess\\nboth formal and relational attributes. The algorithm takes into account two RCA\\nscaling operators. We illustrate it on an example.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.SE'], links=[arxiv.Result.Link('http://arxiv.org/abs/1803.07847v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1803.07847v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1803.08604v1', updated=datetime.datetime(2018, 3, 22, 22, 39, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 3, 22, 22, 39, 32, tzinfo=datetime.timezone.utc), title='Learning State Representations for Query Optimization with Deep Reinforcement Learning', authors=[arxiv.Result.Author('Jennifer Ortiz'), arxiv.Result.Author('Magdalena Balazinska'), arxiv.Result.Author('Johannes Gehrke'), arxiv.Result.Author('S. Sathiya Keerthi')], summary='Deep reinforcement learning is quickly changing the field of artificial\\nintelligence. These models are able to capture a high level understanding of\\ntheir environment, enabling them to learn difficult dynamic tasks in a variety\\nof domains. In the database field, query optimization remains a difficult\\nproblem. Our goal in this work is to explore the capabilities of deep\\nreinforcement learning in the context of query optimization. At each state, we\\nbuild queries incrementally and encode properties of subqueries through a\\nlearned representation. The challenge here lies in the formation of the state\\ntransition function, which defines how the current subquery state combines with\\nthe next query operation (action) to yield the next state. As a first step in\\nthis direction, we focus the state representation problem and the formation of\\nthe state transition function. We describe our approach and show preliminary\\nresults. We further discuss how we can use the state representation to improve\\nquery optimization using reinforcement learning.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1803.08604v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1803.08604v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1803.09010v8', updated=datetime.datetime(2021, 12, 1, 20, 29, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 3, 23, 23, 22, 18, tzinfo=datetime.timezone.utc), title='Datasheets for Datasets', authors=[arxiv.Result.Author('Timnit Gebru'), arxiv.Result.Author('Jamie Morgenstern'), arxiv.Result.Author('Briana Vecchione'), arxiv.Result.Author('Jennifer Wortman Vaughan'), arxiv.Result.Author('Hanna Wallach'), arxiv.Result.Author('Hal Daumé III'), arxiv.Result.Author('Kate Crawford')], summary='The machine learning community currently has no standardized process for\\ndocumenting datasets, which can lead to severe consequences in high-stakes\\ndomains. To address this gap, we propose datasheets for datasets. In the\\nelectronics industry, every component, no matter how simple or complex, is\\naccompanied with a datasheet that describes its operating characteristics, test\\nresults, recommended uses, and other information. By analogy, we propose that\\nevery dataset be accompanied with a datasheet that documents its motivation,\\ncomposition, collection process, recommended uses, and so on. Datasheets for\\ndatasets will facilitate better communication between dataset creators and\\ndataset consumers, and encourage the machine learning community to prioritize\\ntransparency and accountability.', comment='Published in CACM in December, 2021', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1803.09010v8', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1803.09010v8', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1803.09627v1', updated=datetime.datetime(2018, 3, 23, 7, 48, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 3, 23, 7, 48, 15, tzinfo=datetime.timezone.utc), title='GreyCat: Efficient What-If Analytics for Data in Motion at Scale', authors=[arxiv.Result.Author('Thomas Hartmann'), arxiv.Result.Author('Francois Fouquet'), arxiv.Result.Author('Assaad Moawad'), arxiv.Result.Author('Romain Rouvoy'), arxiv.Result.Author('Yves Le Traon')], summary='Over the last few years, data analytics shifted from a descriptive era,\\nconfined to the explanation of past events, to the emergence of predictive\\ntechniques. Nonetheless, existing predictive techniques still fail to\\neffectively explore alternative futures, which continuously diverge from\\ncurrent situations when exploring the effects of what-if decisions. Enabling\\nprescriptive analytics therefore calls for the design of scalable systems that\\ncan cope with the complexity and the diversity of underlying data models. In\\nthis article, we address this challenge by combining graphs and time series\\nwithin a scalable storage system that can organize a massive amount of\\nunstructured and continuously changing data into multi-dimensional data models,\\ncalled Many-Worlds Graphs. We demonstrate that our open source implementation,\\nGreyCat, can efficiently fork and update thousands of parallel worlds composed\\nof millions of timestamped nodes, such as what-if exploration.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1803.09627v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1803.09627v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1803.09930v3', updated=datetime.datetime(2018, 6, 26, 4, 17, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 3, 27, 7, 13, 49, tzinfo=datetime.timezone.utc), title='Worst-Case Optimal Join Algorithms: Techniques, Results, and Open Problems', authors=[arxiv.Result.Author('Hung Q. Ngo')], summary='Worst-case optimal join algorithms are the class of join algorithms whose\\nruntime match the worst-case output size of a given join query. While the first\\nprovably worst-case optimal join algorithm was discovered relatively recently,\\nthe techniques and results surrounding these algorithms grow out of decades of\\nresearch from a wide range of areas, intimately connecting graph theory,\\nalgorithms, information theory, constraint satisfaction, database theory, and\\ngeometric inequalities. These ideas are not just paperware: in addition to\\nacademic project implementations, two variations of such algorithms are the\\nwork-horse join algorithms of commercial database and data analytics engines.\\n  This paper aims to be a brief introduction to the design and analysis of\\nworst-case optimal join algorithms. We discuss the key techniques for proving\\nruntime and output size bounds. We particularly focus on the fascinating\\nconnection between join algorithms and information theoretic inequalities, and\\nthe idea of how one can turn a proof into an algorithm. Finally, we conclude\\nwith a representative list of fundamental open problems in this area.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://arxiv.org/abs/1803.09930v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1803.09930v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1803.10901v1', updated=datetime.datetime(2018, 3, 29, 2, 15, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 3, 29, 2, 15, 3, tzinfo=datetime.timezone.utc), title='Statistical Validity and Consistency of Big Data Analytics: A General Framework', authors=[arxiv.Result.Author('Bikram Karmakar'), arxiv.Result.Author('Indranil Mukhopadhyay')], summary='Informatics and technological advancements have triggered generation of huge\\nvolume of data with varied complexity in its management and analysis. Big Data\\nanalytics is the practice of revealing hidden aspects of such data and making\\ninferences from it. Although storage, retrieval and management of Big Data seem\\npossible through efficient algorithm and system development, concern about\\nstatistical consistency remains to be addressed in view of its specific\\ncharacteristics. Since Big Data does not conform to standard analytics, we need\\nproper modification of the existing statistical theory and tools. Here we\\npropose, with illustrations, a general statistical framework and an algorithmic\\nprinciple for Big Data analytics that ensure statistical accuracy of the\\nconclusions. The proposed framework has the potential to push forward\\nadvancement of Big Data analytics in the right direction. The\\npartition-repetition approach proposed here is broad enough to encompass all\\npractical data analytic problems.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC', 'stat.ME'], links=[arxiv.Result.Link('http://arxiv.org/abs/1803.10901v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1803.10901v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1807.00971v1', updated=datetime.datetime(2018, 7, 3, 4, 15, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 7, 3, 4, 15, 8, tzinfo=datetime.timezone.utc), title='Analytics for the Internet of Things: A Survey', authors=[arxiv.Result.Author('Eugene Siow'), arxiv.Result.Author('Thanassis Tiropanis'), arxiv.Result.Author('Wendy Hall')], summary='The Internet of Things (IoT) envisions a world-wide, interconnected network\\nof smart physical entities. These physical entities generate a large amount of\\ndata in operation and as the IoT gains momentum in terms of deployment, the\\ncombined scale of those data seems destined to continue to grow. Increasingly,\\napplications for the IoT involve analytics. Data analytics is the process of\\nderiving knowledge from data, generating value like actionable insights from\\nthem. This article reviews work in the IoT and big data analytics from the\\nperspective of their utility in creating efficient, effective and innovative\\napplications and services for a wide spectrum of domains. We review the broad\\nvision for the IoT as it is shaped in various communities, examine the\\napplication of data analytics across IoT domains, provide a categorisation of\\nanalytic approaches and propose a layered taxonomy from IoT data to analytics.\\nThis taxonomy provides us with insights on the appropriateness of analytical\\ntechniques, which in turn shapes a survey of enabling technology and\\ninfrastructure for IoT analytics. Finally, we look at some tradeoffs for\\nanalytics in the IoT that can shape future research.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1807.00971v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1807.00971v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1807.01706v1', updated=datetime.datetime(2018, 7, 4, 13, 21, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 7, 4, 13, 21, 19, tzinfo=datetime.timezone.utc), title='Mining Periodic Patterns with a MDL Criterion', authors=[arxiv.Result.Author('Esther Galbrun'), arxiv.Result.Author('Peggy Cellier'), arxiv.Result.Author('Nikolaj Tatti'), arxiv.Result.Author('Alexandre Termier'), arxiv.Result.Author('Bruno Crémilleux')], summary='The quantity of event logs available is increasing rapidly, be they produced\\nby industrial processes, computing systems, or life tracking, for instance. It\\nis thus important to design effective ways to uncover the information they\\ncontain. Because event logs often record repetitive phenomena, mining periodic\\npatterns is especially relevant when considering such data. Indeed, capturing\\nsuch regularities is instrumental in providing condensed representations of the\\nevent sequences.\\n  We present an approach for mining periodic patterns from event logs while\\nrelying on a Minimum Description Length (MDL) criterion to evaluate candidate\\npatterns. Our goal is to extract a set of patterns that suitably characterises\\nthe periodic structure present in the data. We evaluate the interest of our\\napproach on several real-world event log datasets.', comment=\"This report extends the conference version (at ECML-PKDD'18) with\\n  technical details, numerous examples, and additional experiments\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1807.01706v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1807.01706v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1807.04035v1', updated=datetime.datetime(2018, 7, 11, 9, 36, 34, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 7, 11, 9, 36, 34, tzinfo=datetime.timezone.utc), title='Modeling Data Lake Metadata with a Data Vault', authors=[arxiv.Result.Author('Iuri Nogueira'), arxiv.Result.Author('Maram Romdhane'), arxiv.Result.Author('Jérôme Darmont')], summary='With the rise of big data, business intelligence had to find solutions for\\nmanaging even greater data volumes and variety than in data warehouses, which\\nproved ill-adapted. Data lakes answer these needs from a storage point of view,\\nbut require managing adequate metadata to guarantee an efficient access to\\ndata. Starting from a multidimensional metadata model designed for an\\nindustrial heritage data lake presenting a lack of schema evolutivity, we\\npropose in this paper to use ensemble modeling, and more precisely a data\\nvault, to address this issue. To illustrate the feasibility of this approach,\\nwe instantiate our metadata conceptual model into relational and\\ndocument-oriented logical and physical models, respectively. We also compare\\nthe physical models in terms of metadata storage and query response time.', comment=None, journal_ref='22nd International Database Engineering & Applications Symposium\\n  (IDEAS 2018), Jun 2018, Villa San Giovanni, Italy. ACM, pp.253-261, 2018,\\n  http://confsys.encs.concordia.ca/IDEAS/ideas18/ideas18.php', doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1807.04035v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1807.04035v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1807.07346v1', updated=datetime.datetime(2018, 7, 19, 11, 29, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 7, 19, 11, 29, 40, tzinfo=datetime.timezone.utc), title='Indexing Execution Patterns in Workflow Provenance Graphs through Generalized Trie Structures', authors=[arxiv.Result.Author('Esteban García-Cuesta'), arxiv.Result.Author('José M. Gómez-Pérez')], summary='Over the last years, scientific workflows have become mature enough to be\\nused in a production style. However, despite the increasing maturity, there is\\nstill a shortage of tools for searching, adapting, and reusing workflows that\\nhinders a more generalized adoption by the scientific communities. Indeed, due\\nto the limited availability of machine-readable scientific metadata and the\\nheterogeneity of workflow specification formats and representations, new ways\\nto leverage alternative sources of information that complement existing\\napproaches are needed. In this paper we address such limitations by applying\\nstatistically enriched generalized trie structures to exploit workflow\\nexecution provenance information in order to assist the analysis, indexing and\\nsearch of scientific workflows. Our method bridges the gap between the\\ndescription of what a workflow is supposed to do according to its specification\\nand related metadata and what it actually does as recorded in its provenance\\nexecution trace. In doing so, we also prove that the proposed method\\noutperforms SPARQL 1.1 Property Paths for querying provenance graphs.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1807.07346v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1807.07346v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1807.08461v1', updated=datetime.datetime(2018, 7, 23, 7, 47, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 7, 23, 7, 47, 32, tzinfo=datetime.timezone.utc), title='A Cache-based Optimizer for Querying Enhanced Knowledge Bases', authors=[arxiv.Result.Author('Wei Emma Zhang'), arxiv.Result.Author('Quan Z. Sheng'), arxiv.Result.Author('Schahram Dustdar')], summary='With recent emerging technologies such as the Internet of Things (IoT),\\ninformation collection on our physical world and environment can be achieved at\\na much higher granularity and such detailed knowledge will play a critical role\\nin improving the productivity, operational effectiveness, decision making, and\\nin identifying new business models for economic growth. Efficient discovery and\\nquerying such knowledge remains a key challenge due to the limited capability\\nand high latency of connections to the interfaces of knowledge bases, e.g., the\\nSPARQL endpoints. In this article, we present a querying system on SPARQL\\nendpoints for knowledge bases that performs queries faster than the\\nstate-of-the-art systems. Our system features a cache-based optimization scheme\\nto improve querying performance by prefetching and caching the results of\\npredicted potential queries. The evaluations on query sets from SPARQL\\nendpoints of DBpedia and Linked GeoData showcase the effectiveness of our\\napproach.', comment='9 pages, 3 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1807.08461v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1807.08461v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1807.08709v1', updated=datetime.datetime(2018, 7, 23, 16, 38, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 7, 23, 16, 38, 5, tzinfo=datetime.timezone.utc), title='The Vadalog System: Datalog-based Reasoning for Knowledge Graphs', authors=[arxiv.Result.Author('Luigi Bellomarini'), arxiv.Result.Author('Georg Gottlob'), arxiv.Result.Author('Emanuel Sallinger')], summary=\"Over the past years, there has been a resurgence of Datalog-based systems in\\nthe database community as well as in industry. In this context, it has been\\nrecognized that to handle the complex knowl\\\\-edge-based scenarios encountered\\ntoday, such as reasoning over large knowledge graphs, Datalog has to be\\nextended with features such as existential quantification. Yet, Datalog-based\\nreasoning in the presence of existential quantification is in general\\nundecidable. Many efforts have been made to define decidable fragments. Warded\\nDatalog+/- is a very promising one, as it captures PTIME complexity while\\nallowing ontological reasoning. Yet so far, no implementation of Warded\\nDatalog+/- was available. In this paper we present the Vadalog system, a\\nDatalog-based system for performing complex logic reasoning tasks, such as\\nthose required in advanced knowledge graphs. The Vadalog system is Oxford's\\ncontribution to the VADA research programme, a joint effort of the universities\\nof Oxford, Manchester and Edinburgh and around 20 industrial partners. As the\\nmain contribution of this paper, we illustrate the first implementation of\\nWarded Datalog+/-, a high-performance Datalog+/- system utilizing an aggressive\\ntermination control strategy. We also provide a comprehensive experimental\\nevaluation.\", comment='Extended version of VLDB paper\\n  <https://doi.org/10.14778/3213880.3213888>', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1807.08709v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1807.08709v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1807.08712v1', updated=datetime.datetime(2018, 7, 23, 16, 40, 37, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 7, 23, 16, 40, 37, tzinfo=datetime.timezone.utc), title='Data Science with Vadalog: Bridging Machine Learning and Reasoning', authors=[arxiv.Result.Author('Luigi Bellomarini'), arxiv.Result.Author('Ruslan R. Fayzrakhmanov'), arxiv.Result.Author('Georg Gottlob'), arxiv.Result.Author('Andrey Kravchenko'), arxiv.Result.Author('Eleonora Laurenza'), arxiv.Result.Author('Yavor Nenov'), arxiv.Result.Author('Stephane Reissfelder'), arxiv.Result.Author('Emanuel Sallinger'), arxiv.Result.Author('Evgeny Sherkhonov'), arxiv.Result.Author('Lianlong Wu')], summary='Following the recent successful examples of large technology companies, many\\nmodern enterprises seek to build knowledge graphs to provide a unified view of\\ncorporate knowledge and to draw deep insights using machine learning and\\nlogical reasoning. There is currently a perceived disconnect between the\\ntraditional approaches for data science, typically based on machine learning\\nand statistical modelling, and systems for reasoning with domain knowledge. In\\nthis paper we present a state-of-the-art Knowledge Graph Management System,\\nVadalog, which delivers highly expressive and efficient logical reasoning and\\nprovides seamless integration with modern data science toolkits, such as the\\nJupyter platform. We demonstrate how to use Vadalog to perform traditional data\\nwrangling tasks, as well as complex logical and probabilistic reasoning. We\\nargue that this is a significant step forward towards combining machine\\nlearning and reasoning in data science.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1807.08712v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1807.08712v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1807.08804v1', updated=datetime.datetime(2018, 7, 14, 14, 46, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 7, 14, 14, 46, 3, tzinfo=datetime.timezone.utc), title='GPU-based Commonsense Paradigms Reasoning for Real-Time Query Answering and Multimodal Analysis', authors=[arxiv.Result.Author('Nguyen Ha Tran'), arxiv.Result.Author('Erik Cambria')], summary='We utilize commonsense knowledge bases to address the problem of real- time\\nmultimodal analysis. In particular, we focus on the problem of multimodal\\nsentiment analysis, which consists in the simultaneous analysis of different\\nmodali- ties, e.g., speech and video, for emotion and polarity detection. Our\\napproach takes advantages of the massively parallel processing power of modern\\nGPUs to enhance the performance of feature extraction from different\\nmodalities. In addition, in order to ex- tract important textual features from\\nmultimodal sources we generate domain-specific graphs based on commonsense\\nknowledge and apply GPU-based graph traversal for fast feature detection. Then,\\npowerful ELM classifiers are applied to build the senti- ment analysis model\\nbased on the extracted features. We conduct our experiments on the YouTube\\ndataset and achieve an accuracy of 78% which outperforms all previous systems.\\nIn term of processing speed, our method shows improvements of several orders of\\nmagnitude for feature extraction compared to CPU-based counterparts.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.PF'], links=[arxiv.Result.Link('http://arxiv.org/abs/1807.08804v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1807.08804v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1807.09887v1', updated=datetime.datetime(2018, 7, 25, 22, 39, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 7, 25, 22, 39, 52, tzinfo=datetime.timezone.utc), title='Compiling Database Application Programs', authors=[arxiv.Result.Author('Mohammad Dashti'), arxiv.Result.Author('Sachin Basil John'), arxiv.Result.Author('Thierry Coppey'), arxiv.Result.Author('Amir Shaikhha'), arxiv.Result.Author('Vojin Jovanovic'), arxiv.Result.Author('Christoph Koch')], summary='There is a trend towards increased specialization of data management software\\nfor performance reasons. In this paper, we study the automatic specialization\\nand optimization of database application programs -- sequences of queries and\\nupdates, augmented with control flow constructs as they appear in database\\nscripts, UDFs, transactional workloads and triggers in languages such as\\nPL/SQL. We show how to build an optimizing compiler for database application\\nprograms using generative programming and state-of-the-art compiler technology.\\n  We evaluate a hand-optimized low-level implementation of TPC-C, and identify\\nthe key optimization techniques that account for its good performance. Our\\ncompiler fully automates these optimizations and, applied to this benchmark,\\noutperforms the manually optimized baseline by a factor of two. By selectively\\ndisabling some of the optimizations in the compiler, we derive a clinical and\\nprecise way of obtaining insight into their individual performance\\ncontributions.', comment='16 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2.4'], links=[arxiv.Result.Link('http://arxiv.org/abs/1807.09887v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1807.09887v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1807.10792v1', updated=datetime.datetime(2018, 7, 27, 18, 42, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 7, 27, 18, 42, 59, tzinfo=datetime.timezone.utc), title='NDBench: Benchmarking Microservices at Scale', authors=[arxiv.Result.Author('Ioannis Papapanagiotou'), arxiv.Result.Author('Vinay Chella')], summary='Software vendors often report performance numbers for the sweet spot or\\nrunning on specialized hardware with specific workload parameters and without\\nrealistic failures. Accurate benchmarks at the persistence layer are crucial,\\nas failures may cause unrecoverable errors such as data loss, inconsistency or\\ncorruption. To accurately evaluate data stores and other microservices at\\nNetflix, we developed Netflix Data Benchmark (NDBench), a Cloud benchmark tool.\\nIt can be deployed in a loosely-coupled fashion with the ability to dynamically\\nchange the benchmark parameters at runtime so we can rapidly iterate on\\ndifferent tests and failure modes. NDBench offers pluggable patterns and loads,\\nsupport for pluggable client APIs, and was designed to run continually. This\\ndesign enabled us to test long-running maintenance jobs that may affect the\\nperformance, test numerous different systems under adverse conditions, and\\nuncover long-term issues like memory leaks or heap pressure.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1807.10792v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1807.10792v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1807.11104v1', updated=datetime.datetime(2018, 7, 29, 19, 39, 38, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 7, 29, 19, 39, 38, tzinfo=datetime.timezone.utc), title='DataJoint: A Simpler Relational Data Model', authors=[arxiv.Result.Author('Dimitri Yatsenko'), arxiv.Result.Author('Edgar Y. Walker'), arxiv.Result.Author('Andreas S. Tolias')], summary=\"The relational data model offers unrivaled rigor and precision in defining\\ndata structure and querying complex data. Yet the use of relational databases\\nin scientific data pipelines is limited due to their perceived unwieldiness. We\\npropose a simplified and conceptually refined relational data model named\\nDataJoint. The model includes a language for schema definition, a language for\\ndata queries, and diagramming notation for visualizing entities and\\nrelationships among them. The model adheres to the principle of entity\\nnormalization, which requires that all data -- both stored and derived -- must\\nbe represented by well-formed entity sets. DataJoint's data query language is\\nan algebra on entity sets with five operators that provide matching\\ncapabilities to those of other relational query languages with greater clarity\\ndue to entity normalization. Practical implementations of DataJoint have been\\nadopted in neuroscience labs for fluent interaction with scientific data\\npipelines.\", comment='26 pages, 5 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', '68P15'], links=[arxiv.Result.Link('http://arxiv.org/abs/1807.11104v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1807.11104v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1807.11149v1', updated=datetime.datetime(2018, 7, 30, 2, 37, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 7, 30, 2, 37, 44, tzinfo=datetime.timezone.utc), title='To Ship or Not to (Function) Ship (Extended version)', authors=[arxiv.Result.Author('Feilong Liu'), arxiv.Result.Author('Niranjan Kamat'), arxiv.Result.Author('Spyros Blanas'), arxiv.Result.Author('Arnab Nandi')], summary=\"Sampling is often used to reduce query latency for interactive big data\\nanalytics. The established parallel data processing paradigm relies on function\\nshipping, where a coordinator dispatches queries to worker nodes and then\\ncollects the results. The commoditization of high-performance networking makes\\ndata shipping possible, where the coordinator directly reads data in the\\nworkers' memory using RDMA while workers process other queries. In this work,\\nwe explore when to use function shipping or data shipping for interactive query\\nprocessing with sampling. Whether function shipping or data shipping should be\\npreferred depends on the amount of data transferred, the current CPU\\nutilization, the sampling method and the number of queries executed over the\\ndata set. The results show that data shipping is up to 6.5x faster when\\nperforming clustered sampling with heavily-utilized workers.\", comment='4 pages, 3 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1807.11149v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1807.11149v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1807.11634v1', updated=datetime.datetime(2018, 7, 31, 2, 31, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 7, 31, 2, 31, 39, tzinfo=datetime.timezone.utc), title='Interactive Summarization and Exploration of Top Aggregate Query Answers', authors=[arxiv.Result.Author('Yuhao Wen'), arxiv.Result.Author('Xiaodan Zhu'), arxiv.Result.Author('Sudeepa Roy'), arxiv.Result.Author('Jun Yang')], summary='We present a system for summarization and interactive exploration of\\nhigh-valued aggregate query answers to make a large set of possible answers\\nmore informative to the user. Our system outputs a set of clusters on the\\nhigh-valued query answers showing their common properties such that the\\nclusters are diverse as much as possible to avoid repeating information, and\\ncover a certain number of top original answers as indicated by the user.\\nFurther, the system facilitates interactive exploration of the query answers by\\nhelping the user (i) choose combinations of parameters for clustering, (ii)\\ninspect the clusters as well as the elements they contain, and (iii) visualize\\nhow changes in parameters affect clustering. We define optimization problems,\\nstudy their complexity, explore properties of the solutions investigating the\\nsemi-lattice structure on the clusters, and propose efficient algorithms and\\noptimizations to achieve these goals. We evaluate our techniques experimentally\\nand discuss our prototype with a graphical user interface that facilitates this\\ninteractive exploration. A user study is conducted to evaluate the usability of\\nour approach.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1807.11634v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1807.11634v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.00024v1', updated=datetime.datetime(2018, 7, 31, 18, 48, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 7, 31, 18, 48, 32, tzinfo=datetime.timezone.utc), title='Improve3C: Data Cleaning on Consistency and Completeness with Currency', authors=[arxiv.Result.Author('Xiaoou Ding'), arxiv.Result.Author('Hongzhi Wang'), arxiv.Result.Author('Jiaxuan Su'), arxiv.Result.Author('Jianzhong Li'), arxiv.Result.Author('Hong Gao')], summary='Data quality plays a key role in big data management today. With the\\nexplosive growth of data from a variety of sources, the quality of data is\\nfaced with multiple problems. Motivated by this, we study the multiple data\\nquality improvement on completeness, consistency and currency in this paper.\\nFor the proposed problem, we introduce a 4-step framework, named Improve3C, for\\ndetection and quality improvement on incomplete and inconsistent data without\\ntimestamps. We compute and achieve a relative currency order among records\\nderived from given currency constraints, according to which inconsistent and\\nincomplete data can be repaired effectively considering the temporal impact.\\nFor both effectiveness and efficiency consideration, we carry out inconsistent\\nrepair ahead of incomplete repair. Currency-related consistency distance is\\ndefined to measure the similarity between dirty records and clean ones more\\naccurately. In addition, currency orders are treated as an important feature in\\nthe training process of incompleteness repair. The solution algorithms are\\nintroduced in detail with examples. A thorough experiment on one real-life data\\nand a synthetic one verifies that the proposed method can improve the\\nperformance of dirty data cleaning with multiple quality problems which are\\nhard to be cleaned by the existing approaches effectively.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.00024v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.00024v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.00986v1', updated=datetime.datetime(2018, 8, 2, 18, 47, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 2, 18, 47, 27, tzinfo=datetime.timezone.utc), title='Diversification on Big Data in Query Processing', authors=[arxiv.Result.Author('Meifan Zhang'), arxiv.Result.Author('Hongzhi Wang'), arxiv.Result.Author('Jianzhong Li'), arxiv.Result.Author('Hong Gao')], summary=\"Recently, in the area of big data, some popular applications such as web\\nsearch engines and recommendation systems, face the problem to diversify\\nresults during query processing. In this sense, it is both significant and\\nessential to propose methods to deal with big data in order to increase the\\ndiversity of the result set. In this paper, we firstly define a set's diversity\\nand an element's ability to improve the set's overall diversity. Based on these\\ndefinitions, we propose a diversification framework which has good performance\\nin terms of effectiveness and efficiency. Also, this framework has theoretical\\nguarantee on probability of success. Secondly, we design implementation\\nalgorithms based on this framework for both numerical and string data. Thirdly,\\nfor numerical and string data respectively, we carry out extensive experiments\\non real data to verify the performance of our proposed framework, and also\\nperform scalability experiments on synthetic data.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.00986v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.00986v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.01624v1', updated=datetime.datetime(2018, 8, 5, 14, 13, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 5, 14, 13, 29, tzinfo=datetime.timezone.utc), title='On the Fairness of Quality-based Data Markets', authors=[arxiv.Result.Author('Dan Zhang'), arxiv.Result.Author('Hongzhi Wang'), arxiv.Result.Author('Xiaoou Ding'), arxiv.Result.Author('Yice Zhang'), arxiv.Result.Author('Jianzhong Li'), arxiv.Result.Author('Hong Gao')], summary='For data pricing, data quality is a factor that must be considered. To keep\\nthe fairness of data market from the aspect of data quality, we proposed a fair\\ndata market that considers data quality while pricing. To ensure fairness, we\\nfirst design a quality-driven data pricing strategy. Then based on the\\nstrategy, a fairness assurance mechanism for quality-driven data marketplace is\\nproposed. In this mechanism, we ensure that savvy consumers cannot cheat the\\nsystem and users can verify each consumption with Trusted Third Party (TTP)\\nthat they are charged properly. Based on this mechanism, we develop a fair\\nquality-driven data market system. Extensive experiments are performed to\\nverify the effectiveness of proposed techniques. Experimental results show that\\nour quality-driven data pricing strategy could assign a reasonable price to the\\ndata according to data quality and the fairness assurance mechanism could\\neffectively protect quality-driven data pricing from potential cheating.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.01624v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.01624v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.01703v1', updated=datetime.datetime(2018, 8, 6, 1, 1, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 6, 1, 1, 50, tzinfo=datetime.timezone.utc), title='The Bases of Association Rules of High Confidence', authors=[arxiv.Result.Author('Oren Segal'), arxiv.Result.Author('Justin Cabot-Miller'), arxiv.Result.Author('Kira Adaricheva'), arxiv.Result.Author('J. B. Nation'), arxiv.Result.Author('Anuar Sharafudinov')], summary='We develop a new approach for distributed computing of the association rules\\nof high confidence in a binary table. It is derived from the D-basis algorithm\\nin K. Adaricheva and J.B. Nation (TCS 2017), which is performed on multiple\\nsub-tables of a table given by removing several rows at a time. The set of\\nrules is then aggregated using the same approach as the D-basis is retrieved\\nfrom a larger set of implications. This allows to obtain a basis of association\\nrules of high confidence, which can be used for ranking all attributes of the\\ntable with respect to a given fixed attribute using the relevance parameter\\nintroduced in K. Adaricheva et al. (Proceedings of ICFCA-2015). This paper\\nfocuses on the technical implementation of the new algorithm. Some testing\\nresults are performed on transaction data and medical data.', comment='Presented at DTMN, Sydney, Australia, July 28, 2018', journal_ref='David C.Wyld et al. (Eds) : CSITY, DTMN, NWCOM, SIGPRO - 2018, pp.\\n  39-51, 2018', doi='10.5121/csit.2018.81104', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.5121/csit.2018.81104', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1808.01703v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.01703v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.03196v2', updated=datetime.datetime(2019, 1, 10, 20, 33, 48, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 9, 15, 30, 6, tzinfo=datetime.timezone.utc), title='Learning to Optimize Join Queries With Deep Reinforcement Learning', authors=[arxiv.Result.Author('Sanjay Krishnan'), arxiv.Result.Author('Zongheng Yang'), arxiv.Result.Author('Ken Goldberg'), arxiv.Result.Author('Joseph Hellerstein'), arxiv.Result.Author('Ion Stoica')], summary='Exhaustive enumeration of all possible join orders is often avoided, and most\\noptimizers leverage heuristics to prune the search space. The design and\\nimplementation of heuristics are well-understood when the cost model is roughly\\nlinear, and we find that these heuristics can be significantly suboptimal when\\nthere are non-linearities in cost. Ideally, instead of a fixed heuristic, we\\nwould want a strategy to guide the search space in a more data-driven\\nway---tailoring the search to a specific dataset and query workload.\\nRecognizing the link between classical Dynamic Programming enumeration methods\\nand recent results in Reinforcement Learning (RL), we propose a new method for\\nlearning optimized join search strategies. We present our RL-based DQ\\noptimizer, which currently optimizes select-project-join blocks. We implement\\nthree versions of DQ to illustrate the ease of integration into existing\\nDBMSes: (1) A version built on top of Apache Calcite, (2) a version integrated\\ninto PostgreSQL, and (3) a version integrated into SparkSQL. Our extensive\\nevaluation shows that DQ achieves plans with optimization costs and query\\nexecution times competitive with the native query optimizer in each system, but\\ncan execute significantly faster after learning (often by orders of magnitude).', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.03196v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.03196v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.03537v1', updated=datetime.datetime(2018, 8, 10, 13, 44, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 10, 13, 44, 26, tzinfo=datetime.timezone.utc), title='Optimizing error of high-dimensional statistical queries under differential privacy', authors=[arxiv.Result.Author('Ryan McKenna'), arxiv.Result.Author('Gerome Miklau'), arxiv.Result.Author('Michael Hay'), arxiv.Result.Author('Ashwin Machanavajjhala')], summary='Differentially private algorithms for answering sets of predicate counting\\nqueries on a sensitive database have many applications. Organizations that\\ncollect individual-level data, such as statistical agencies and medical\\ninstitutions, use them to safely release summary tabulations. However, existing\\ntechniques are accurate only on a narrow class of query workloads, or are\\nextremely slow, especially when analyzing more than one or two dimensions of\\nthe data. In this work we propose HDMM, a new differentially private algorithm\\nfor answering a workload of predicate counting queries, that is especially\\neffective for higher-dimensional datasets. HDMM represents query workloads\\nusing an implicit matrix representation and exploits this compact\\nrepresentation to efficiently search (a subset of) the space of differentially\\nprivate algorithms for one that answers the input query workload with high\\naccuracy. We empirically show that HDMM can efficiently answer queries with\\nlower error than state-of-the-art techniques on a variety of low and high\\ndimensional datasets.', comment=None, journal_ref='PVLDB, 11 (10): 1206-1219, 2018', doi='10.14778/3231751.3231769', primary_category='cs.DB', categories=['cs.DB', 'cs.CR'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3231751.3231769', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1808.03537v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.03537v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.03555v3', updated=datetime.datetime(2019, 5, 24, 15, 27, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 10, 14, 18, 40, tzinfo=datetime.timezone.utc), title='Ektelo: A Framework for Defining Differentially-Private Computations', authors=[arxiv.Result.Author('Dan Zhang'), arxiv.Result.Author('Ryan McKenna'), arxiv.Result.Author('Ios Kotsogiannis'), arxiv.Result.Author('George Bissias'), arxiv.Result.Author('Michael Hay'), arxiv.Result.Author('Ashwin Machanavajjhala'), arxiv.Result.Author('Gerome Miklau')], summary='The adoption of differential privacy is growing but the complexity of\\ndesigning private, efficient and accurate algorithms is still high. We propose\\na novel programming framework and system, Ektelo, for implementing both\\nexisting and new privacy algorithms. For the task of answering linear counting\\nqueries, we show that nearly all existing algorithms can be composed from\\noperators, each conforming to one of a small number of operator classes. While\\npast programming frameworks have helped to ensure the privacy of programs, the\\nnovelty of our framework is its significant support for authoring accurate and\\nefficient (as well as private) programs.\\n  After describing the design and architecture of the Ektelo system, we show\\nthat Ektelo is expressive, allows for safer implementations through code reuse,\\nand that it allows both privacy novices and experts to easily design\\nalgorithms. We demonstrate the use of Ektelo by designing several new\\nstate-of-the-art algorithms.', comment='Journal version under submission', journal_ref=None, doi='10.1145/3183713.3196921', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3183713.3196921', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1808.03555v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.03555v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.04663v2', updated=datetime.datetime(2019, 5, 29, 15, 6, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 14, 12, 48, 15, tzinfo=datetime.timezone.utc), title='Evaluating Datalog via Tree Automata and Cycluits', authors=[arxiv.Result.Author('Antoine Amarilli'), arxiv.Result.Author('Pierre Bourhis'), arxiv.Result.Author('Mikaël Monet'), arxiv.Result.Author('Pierre Senellart')], summary='We investigate parameterizations of both database instances and queries that\\nmake query evaluation fixed-parameter tractable in combined complexity. We show\\nthat clique-frontier-guarded Datalog with stratified negation (CFG-Datalog)\\nenjoys bilinear-time evaluation on structures of bounded treewidth for programs\\nof bounded rule size. Such programs capture in particular conjunctive queries\\nwith simplicial decompositions of bounded width, guarded negation fragment\\nqueries of bounded CQ-rank, or two-way regular path queries. Our result is\\nshown by translating to alternating two-way automata, whose semantics is\\ndefined via cyclic provenance circuits (cycluits) that can be tractably\\nevaluated.', comment='56 pages, 63 references. Journal version of \"Combined Tractability of\\n  Query Evaluation via Tree Automata and Cycluits (Extended Version)\" at\\n  arXiv:1612.04203. Up to the stylesheet, page/environment numbering, and\\n  possible minor publisher-induced changes, this is the exact content of the\\n  journal paper that will appear in Theory of Computing Systems. Update wrt\\n  version 1: latest reviewer feedback', journal_ref=None, doi='10.1007/s00224-018-9901-2', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/s00224-018-9901-2', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1808.04663v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.04663v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.05199v5', updated=datetime.datetime(2019, 1, 10, 6, 54, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 15, 17, 44, 25, tzinfo=datetime.timezone.utc), title='A Blockchain Database Application Platform', authors=[arxiv.Result.Author('Muhammad Muzammal'), arxiv.Result.Author('Qiang Qu'), arxiv.Result.Author('Bulat Nasrulin'), arxiv.Result.Author('Anders Skovsgaard')], summary='A blockchain is a decentralised linked data structure that is characterised\\nby its inherent resistance to data modification, but it is deficient in search\\nqueries, primarily due to its inferior data formatting. A distributed database\\nis also a decentralised data structure which features quick query processing\\nand well-designed data formatting but suffers from data reliability. In this\\ndemonstration, we showcase a blockchain database application platform developed\\nby integrating the blockchain with the database, i.e. we demonstrate a system\\nthat has the decentralised, distributed and audibility features of the\\nblockchain and quick query processing and well-designed data structure of the\\ndistributed databases. The system features a tamper-resistant, consistent and\\ncost-effective multi-active database and an effective and reliable data-level\\ndisaster recovery backup. The system is demonstrated in practice as a\\nmulti-active database along with the data-level disaster recovery backup\\nfeature.', comment='the draft will be reshaped', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.05199v5', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.05199v5', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.05215v1', updated=datetime.datetime(2018, 8, 15, 13, 47, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 15, 13, 47, 9, tzinfo=datetime.timezone.utc), title='Vis4DD: A visualization system that supports Data Quality Visual Assessment', authors=[arxiv.Result.Author('João Marcelo Borovina Josko'), arxiv.Result.Author('João Eduardo Ferreira')], summary='Data quality assessment process is essential to ensure reliable analytical\\noutcomes. This process depends on human supervision-driven approaches since it\\nis impossible to determine a defect based only on data. Visualization systems\\nbelong to a class of supervised tools that can make data defect pattern\\nvisible. However, their considerable design knowledge encodings and imple-\\nmentations provide little support design to data quality visual assessment. To\\ncover this gap, this work reports the design approach of V is4DD visualization\\nsystem based on patterns of data defects structures and assessment tasks. An\\nexploratory case study used this web-based system to explore which and how\\nvisual-interactive properties facilitate visual detection of data defect.', comment='6 pages, 3 figures, Proceedings of the satellite events on 32nd.\\n  Brazilian Symposium on Databases', journal_ref='32th Brazilian Symposium on Databases - Demo and Applications\\n  (2017) pp. 46-51', doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.05215v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.05215v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.05752v1', updated=datetime.datetime(2018, 8, 16, 2, 1, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 16, 2, 1, 11, tzinfo=datetime.timezone.utc), title='PUG: A Framework and Practical Implementation for Why & Why-Not Provenance (extended version)', authors=[arxiv.Result.Author('Seokki Lee'), arxiv.Result.Author('Bertram Ludaescher'), arxiv.Result.Author('Boris Glavic')], summary='Explaining why an answer is (or is not) returned by a query is important for\\nmany applications including auditing, debugging data and queries, and answering\\nhypothetical questions about data. In this work, we present the first practical\\napproach for answering such questions for queries with negation (first- order\\nqueries). Specifically, we introduce a graph-based provenance model that, while\\nsyntactic in nature, supports reverse reasoning and is proven to encode a wide\\nrange of provenance models from the literature. The implementation of this\\nmodel in our PUG (Provenance Unification through Graphs) system takes a\\nprovenance question and Datalog query as an input and generates a Datalog\\nprogram that computes an explanation, i.e., the part of the provenance that is\\nrelevant to answer the question. Furthermore, we demonstrate how a desirable\\nfactorization of provenance can be achieved by rewriting an input query. We\\nexperimentally evaluate our approach demonstrating its efficiency.', comment='Extended version of VLDB journal article of the same name. arXiv\\n  admin note: text overlap with arXiv:1701.05699', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.05752v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.05752v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.06298v1', updated=datetime.datetime(2018, 8, 20, 3, 57, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 20, 3, 57, 16, tzinfo=datetime.timezone.utc), title='FedMark: A Marketplace for Federated Data on the Web', authors=[arxiv.Result.Author('Tobias Grubenmann'), arxiv.Result.Author('Abraham Bernstein'), arxiv.Result.Author('Dmitry Moor'), arxiv.Result.Author('Sven Seuken')], summary='The Web of Data (WoD) has experienced a phenomenal growth in the past. This\\ngrowth is mainly fueled by tireless volunteers, government subsidies, and open\\ndata legislations. The majority of commercial data has not made the transition\\nto the WoD, yet. The problem is that it is not clear how publishers of\\ncommercial data can monetize their data in this new setting. Advertisement,\\nwhich is one of the main financial engines of the World Wide Web, cannot be\\napplied to the Web of Data as such unwanted data can easily be filtered out,\\nautomatically. This raises the question how the WoD can (i) maintain its grow\\nwhen subsidies disappear and (ii) give commercial data providers financial\\nincentives to share their wealth of data. In this paper, we propose a\\nmarketplace for the WoD as a solution for this data monetization problem. Our\\napproach allows a customer to transparently buy data from a combination of\\ndifferent providers. To that end, we introduce two different approaches for\\ndeciding which data elements to buy and compare their performance. We also\\nintroduce FedMark, a prototypical implementation of our marketplace that\\nrepresents a first step towards an economically viable WoD beyond subsidies.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.06298v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.06298v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.06907v2', updated=datetime.datetime(2019, 1, 19, 3, 46, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 17, 18, 3, 26, tzinfo=datetime.timezone.utc), title='The variable quality of metadata about biological samples used in biomedical experiments', authors=[arxiv.Result.Author('Rafael S. Gonçalves'), arxiv.Result.Author('Mark A. Musen')], summary='We present an analytical study of the quality of metadata about samples used\\nin biomedical experiments. The metadata under analysis are stored in two\\nwell-known databases: BioSample---a repository managed by the National Center\\nfor Biotechnology Information (NCBI), and BioSamples---a repository managed by\\nthe European Bioinformatics Institute (EBI). We tested whether 11.4M sample\\nmetadata records in the two repositories are populated with values that fulfill\\nthe stated requirements for such values. Our study revealed multiple anomalies\\nin the metadata. Most metadata field names and their values are not\\nstandardized or controlled. Even simple binary or numeric fields are often\\npopulated with inadequate values of different data types. By clustering\\nmetadata field names, we discovered there are often many distinct ways to\\nrepresent the same aspect of a sample. Overall, the metadata we analyzed reveal\\nthat there is a lack of principled mechanisms to enforce and validate metadata\\nrequirements. The significant aberrancies that we found in the metadata are\\nlikely to impede search and secondary use of the associated datasets.', comment='arXiv admin note: text overlap with arXiv:1708.01286', journal_ref=None, doi='10.1038/sdata.2019.21', primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://dx.doi.org/10.1038/sdata.2019.21', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1808.06907v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.06907v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.07603v1', updated=datetime.datetime(2018, 8, 23, 1, 52, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 23, 1, 52, 45, tzinfo=datetime.timezone.utc), title='Privacy-Preserving Synthetic Datasets Over Weakly Constrained Domains', authors=[arxiv.Result.Author('Luke Rodriguez'), arxiv.Result.Author('Bill Howe')], summary='Techniques to deliver privacy-preserving synthetic datasets take a sensitive\\ndataset as input and produce a similar dataset as output while maintaining\\ndifferential privacy. These approaches have the potential to improve data\\nsharing and reuse, but they must be accessible to non-experts and tolerant of\\nrealistic data. Existing approaches make an implicit assumption that the active\\ndomain of the dataset is similar to the global domain, potentially violating\\ndifferential privacy.\\n  In this paper, we present an algorithm for generating differentially private\\nsynthetic data over the large, weakly constrained domains we find in realistic\\nopen data situations. Our algorithm models the unrepresented domain\\nanalytically as a probability distribution to adjust the output and compute\\nnoise, avoiding the need to compute the full domain explicitly. We formulate\\nthe tradeoff between privacy and utility in terms of a \"tolerance for\\nrandomness\" parameter that does not require users to inspect the data to set.\\nFinally, we show that the algorithm produces sensible results on real datasets.', comment='Submitted to TDPD18', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.07603v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.07603v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.08355v1', updated=datetime.datetime(2018, 8, 25, 5, 14, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 25, 5, 14, 31, tzinfo=datetime.timezone.utc), title='Database-Agnostic Workload Management', authors=[arxiv.Result.Author('Shrainik Jain'), arxiv.Result.Author('Jiaqi Yan'), arxiv.Result.Author('Thierry Cruane'), arxiv.Result.Author('Bill Howe')], summary='We present a system to support generalized SQL workload analysis and\\nmanagement for multi-tenant and multi-database platforms. Workload analysis\\napplications are becoming more sophisticated to support database\\nadministration, model user behavior, audit security, and route queries, but the\\nmethods rely on specialized feature engineering, and therefore must be\\ncarefully implemented and reimplemented for each SQL dialect, database system,\\nand application. Meanwhile, the size and complexity of workloads are increasing\\nas systems centralize in the cloud. We model workload analysis and management\\ntasks as variations on query labeling, and propose a system design that can\\nsupport general query labeling routines across multiple applications and\\ndatabase backends. The design relies on the use of learned vector embeddings\\nfor SQL queries as a replacement for application-specific syntactic features,\\nreducing custom code and allowing the use of off-the-shelf machine learning\\nalgorithms for labeling. The key hypothesis, for which we provide evidence in\\nthis paper, is that these learned features can outperform conventional feature\\nengineering on representative machine learning tasks. We present the design of\\na database-agnostic workload management and analytics service, describe\\npotential applications, and show that separating workload representation from\\nlabeling tasks affords new capabilities and can outperform existing solutions\\nfor representative tasks, including workload sampling for index recommendation\\nand user labeling for security audits.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.08355v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.08355v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.08634v2', updated=datetime.datetime(2018, 8, 28, 13, 9, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 26, 22, 28, 36, tzinfo=datetime.timezone.utc), title='Rule Module Inheritance with Modification Restrictions', authors=[arxiv.Result.Author('Felix Burgstaller'), arxiv.Result.Author('Bernd Neumayr'), arxiv.Result.Author('Emanuel Sallinger'), arxiv.Result.Author('Michael Schrefl')], summary='Adapting rule sets to different settings, yet avoiding uncontrolled\\nproliferation of variations, is a key challenge of rule management. One\\nfundamental concept to foster reuse and simplify adaptation is inheritance.\\nBuilding on rule modules, i.e., rule sets with input and output schema, we\\nformally define inheritance of rule modules by incremental modification in\\nsingle inheritance hierarchies. To avoid uncontrolled proliferation of\\nmodifications, we introduce formal modification restrictions which flexibly\\nregulate the degree to which a child module may be modified in comparison to\\nits parent. As concrete rule language, we employ Datalog+/- which can be\\nregarded a common logical core of many rule languages. We evaluate the approach\\nby a proof-of-concept prototype.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.08634v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.08634v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.08822v2', updated=datetime.datetime(2019, 3, 1, 14, 28, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 27, 12, 42, 38, tzinfo=datetime.timezone.utc), title='A Monotone Preservation Result for Boolean Queries Expressed as a Containment of Conjunctive Queries', authors=[arxiv.Result.Author('Dimitri Surinx'), arxiv.Result.Author('Jan Van den Bussche')], summary='When a relational database is queried, the result is normally a relation.\\nSome queries, however, only require a yes/no answer; such queries are often\\ncalled boolean queries. It is customary in database theory to express boolean\\nqueries by testing nonemptiness of query expressions. Another interesting way\\nfor expressing boolean queries are containment statements of the form $Q_1\\n\\\\subseteq Q_2$ where $Q_1$ and $Q_2$ are query expressions. Here, for any input\\ninstance $I$, the boolean query result is $\\\\mathit{true}$ if $Q_1(I)$ is a\\nsubset of $Q_2(I)$ and $\\\\mathit{false}$ otherwise.\\n  In the present paper we will focus on nonemptiness and containment statements\\nabout conjunctive queries. The main goal is to investigate the monotone\\nfragment of the containments of conjunctive queries. In particular, we show a\\npreservation like result for this monotone fragment. That is, we show that, in\\nexpressive power, the monotone containments of conjunctive queries are exactly\\nequal to conjunctive queries under nonemptiness.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.08822v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.08822v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.08896v2', updated=datetime.datetime(2019, 1, 7, 18, 14, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 27, 15, 56, tzinfo=datetime.timezone.utc), title='Efficient Data Ingestion and Query Processing for LSM-Based Storage Systems', authors=[arxiv.Result.Author('Chen Luo'), arxiv.Result.Author('Michael J. Carey')], summary='In recent years, the Log Structured Merge (LSM) tree has been widely adopted\\nby NoSQL and NewSQL systems for its superior write performance. Despite its\\npopularity, however, most existing work has focused on LSM-based key-value\\nstores with only a primary LSM-tree index; auxiliary structures, which are\\ncritical for supporting ad-hoc queries, have received much less attention. In\\nthis paper, we focus on efficient data ingestion and query processing for\\ngeneral-purpose LSM-based storage systems. We first propose and evaluate a\\nseries of optimizations for efficient batched point lookups, significantly\\nimproving the range of applicability of LSM-based secondary indexes. We then\\npresent several new and efficient maintenance strategies for LSM-based storage\\nsystems. Finally, we have implemented and experimentally evaluated the proposed\\ntechniques in the context of the Apache AsterixDB system, and we present the\\nresults here.', comment='15 pages, 23 figures, to appear in VLDB 2019', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.08896v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.08896v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.08983v3', updated=datetime.datetime(2019, 7, 10, 16, 22, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 27, 18, 11, 16, tzinfo=datetime.timezone.utc), title='NeuralCubes: Deep Representations for Visual Data Exploration', authors=[arxiv.Result.Author('Zhe Wang'), arxiv.Result.Author('Dylan Cashman'), arxiv.Result.Author('Mingwei Li'), arxiv.Result.Author('Jixian Li'), arxiv.Result.Author('Matthew Berger'), arxiv.Result.Author('Joshua A. Levine'), arxiv.Result.Author('Remco Chang'), arxiv.Result.Author('Carlos Scheidegger')], summary='Visual exploration of large multidimensional datasets has seen tremendous\\nprogress in recent years, allowing users to express rich data queries that\\nproduce informative visual summaries, all in real time. Techniques based on\\ndata cubes are some of the most promising approaches. However, these techniques\\nusually require a large memory footprint for large datasets. To tackle this\\nproblem, we present NeuralCubes: neural networks that predict results for\\naggregate queries, similar to data cubes. NeuralCubes learns a function that\\ntakes as input a given query, for instance, a geographic region and temporal\\ninterval, and outputs the result of the query. The learned function serves as a\\nreal-time, low-memory approximator for aggregation queries. NeuralCubes models\\nare small enough to be sent to the client side (e.g. the web browser for a\\nweb-based application) for evaluation, enabling data exploration of large\\ndatasets without database/network connection. We demonstrate the effectiveness\\nof NeuralCubes through extensive experiments on a variety of datasets and\\ndiscuss how NeuralCubes opens up opportunities for new types of visualization\\nand interaction.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.08983v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.08983v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.09267v2', updated=datetime.datetime(2019, 3, 20, 4, 9, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 27, 6, 2, 49, tzinfo=datetime.timezone.utc), title='Creating a surrogate commuter network from Australian Bureau of Statistics census data', authors=[arxiv.Result.Author('Kristopher M. Fair'), arxiv.Result.Author('Cameron Zachreson'), arxiv.Result.Author('Mikhail Prokopenko')], summary='Between the 2011 and 2016 national censuses, the Australian Bureau of\\nStatistics changed its anonymity policy compliance system for the distribution\\nof census data. The new method has resulted in dramatic inconsistencies when\\ncomparing low-resolution data to aggregated high-resolution data. Hence,\\naggregated totals do not match true totals, and the mismatch gets worse as the\\ndata resolution gets finer. Here, we address several aspects of this\\ninconsistency with respect to the 2016 usual-residence to place-of-work travel\\ndata. We introduce a re-sampling system that rectifies many of the artifacts\\nintroduced by the new ABS protocol, ensuring a higher level of consistency\\nacross partition sizes. We offer a surrogate high-resolution 2016 commuter\\ndataset that reduces the difference between aggregated and true commuter totals\\nfrom ~34% to only ~7%, which is on the order of the discrepancy across\\npartition resolutions in data from earlier years.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.09267v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.09267v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.00511v2', updated=datetime.datetime(2018, 11, 29, 16, 16, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 1, 2, 58, 34, tzinfo=datetime.timezone.utc), title='Chasing Similarity: Distribution-aware Aggregation Scheduling (Extended Version)', authors=[arxiv.Result.Author('Feilong Liu'), arxiv.Result.Author('Ario Salmasi'), arxiv.Result.Author('Spyros Blanas'), arxiv.Result.Author('Anastasios Sidiropoulos')], summary='Parallel aggregation is a ubiquitous operation in data analytics that is\\nexpressed as GROUP BY in SQL, reduce in Hadoop, or segment in TensorFlow.\\nParallel aggregation starts with an optional local pre-aggregation step and\\nthen repartitions the intermediate result across the network. While local\\npre-aggregation works well for low-cardinality aggregations, the network\\ncommunication cost remains significant for high-cardinality aggregations even\\nafter local pre-aggregation. The problem is that the repartition-based\\nalgorithm for high-cardinality aggregation does not fully utilize the network.\\n  In this work, we first formulate a mathematical model that captures the\\nperformance of parallel aggregation. We prove that finding optimal aggregation\\nplans from a known data distribution is NP-hard, assuming the Small Set\\nExpansion conjecture. We propose GRASP, a GReedy Aggregation Scheduling\\nProtocol that decomposes parallel aggregation into phases. GRASP is\\ndistribution-aware as it aggregates the most similar partitions in each phase\\nto reduce the transmitted data size in subsequent phases. In addition, GRASP\\ntakes the available network bandwidth into account when scheduling aggregations\\nin each phase to maximize network utilization. The experimental evaluation on\\nreal data shows that GRASP outperforms repartition-based aggregation by 3.5x\\nand LOOM by 2.0x.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.00511v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.00511v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.01037v1', updated=datetime.datetime(2018, 10, 2, 2, 31, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 2, 2, 31, 42, tzinfo=datetime.timezone.utc), title='Heterogeneous Replica for Query on Cassandra', authors=[arxiv.Result.Author('Jialin Qiao'), arxiv.Result.Author('Xiangdong Huang'), arxiv.Result.Author('Lei Rui'), arxiv.Result.Author('Jianmin Wang')], summary='Cassandra is a popular structured storage system with high-performance,\\nscalability and high availability, and is usually used to store data that has\\nsome sortable attributes. When deploying and configuring Cassandra, it is\\nimportant to design a suitable schema of column families for accelerating the\\ntarget queries. However, one schema is only suitable for a part of queries, and\\nleaves other queries with high latency.\\n  In this paper, we propose a new replica mechanism, called heterogeneous\\nreplica, to reduce the query latency greatly while ensuring high write\\nthroughput and data recovery. With this replica mechanism, different replica\\nhas the same dataset while having different serialization on disk. By\\nimplementing the heterogeneous replica mechanism on Cassandra, we show that the\\nread performance of Cassandra can be improved by two orders of magnitude with\\nTPC-H data set.', comment='6 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.01037v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.01037v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.01794v3', updated=datetime.datetime(2019, 2, 17, 19, 31, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 3, 15, 31, 36, tzinfo=datetime.timezone.utc), title='VStore: A Data Store for Analytics on Large Videos', authors=[arxiv.Result.Author('Tiantu Xu'), arxiv.Result.Author('Luis Materon Botelho'), arxiv.Result.Author('Felix Xiaozhu Lin')], summary='We present VStore, a data store for supporting fast, resource-efficient\\nanalytics over large archival videos. VStore manages video ingestion, storage,\\nretrieval, and consumption. It controls video formats along the video data\\npath. It is challenged by i) the huge combinatorial space of video format\\nknobs; ii) the complex impacts of these knobs and their high profiling cost;\\niii) optimizing for multiple resource types. It explores an idea called\\nbackward derivation of configuration: in the opposite direction along the video\\ndata path, VStore passes the video quantity and quality expected by analytics\\nbackward to retrieval, to storage, and to ingestion. In this process, VStore\\nderives an optimal set of video formats, optimizing for different resources in\\na progressive manner. VStore automatically derives large, complex\\nconfigurations consisting of more than one hundred knobs over tens of video\\nformats. In response to queries, VStore selects video formats catering to the\\nexecuted operators and the target accuracy. It streams video data from disks\\nthrough decoder to operators. It runs queries as fast as 362x of video\\nrealtime.', comment=None, journal_ref=None, doi='10.1145/3302424.3303971', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3302424.3303971', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1810.01794v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.01794v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.01816v1', updated=datetime.datetime(2018, 10, 3, 16, 9, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 3, 16, 9, 14, tzinfo=datetime.timezone.utc), title='Shrinkwrap: Differentially-Private Query Processing in Private Data Federations', authors=[arxiv.Result.Author('Johes Bater'), arxiv.Result.Author('Xi He'), arxiv.Result.Author('William Ehrich'), arxiv.Result.Author('Ashwin Machanavajjhala'), arxiv.Result.Author('Jennie Rogers')], summary='A private data federation is a set of autonomous databases that share a\\nunified query interface offering in-situ evaluation of SQL queries over the\\nunion of the sensitive data of its members. Owing to privacy concerns, these\\nsystems do not have a trusted data collector that can see all their data and\\ntheir member databases cannot learn about individual records of other engines.\\nFederations currently achieve this goal by evaluating queries obliviously using\\nsecure multiparty computation. This hides the intermediate result cardinality\\nof each query operator by exhaustively padding it. With cascades of such\\noperators, this padding accumulates to a blow-up in the output size of each\\noperator and a proportional loss in query performance. Hence, existing private\\ndata federations do not scale well to complex SQL queries over large datasets.\\n  We introduce Shrinkwrap, a private data federation that offers data owners a\\ndifferentially private view of the data held by others to improve their\\nperformance over oblivious query processing. Shrinkwrap uses computational\\ndifferential privacy to minimize the padding of intermediate query results,\\nachieving up to 35X performance improvement over oblivious query processing.\\nWhen the query needs differentially private output, Shrinkwrap provides a\\ntrade-off between result accuracy and query evaluation performance.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.01816v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.01816v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.03367v2', updated=datetime.datetime(2021, 5, 20, 11, 54, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 8, 11, 3, 40, tzinfo=datetime.timezone.utc), title='Split-Correctness in Information Extraction', authors=[arxiv.Result.Author('Johannes Doleschal'), arxiv.Result.Author('Benny Kimelfeld'), arxiv.Result.Author('Wim Martens'), arxiv.Result.Author('Frank Neven'), arxiv.Result.Author('Matthias Niewerth')], summary='Programs for extracting structured information from text, namely information\\nextractors, often operate separately on document segments obtained from a\\ngeneric splitting operation such as sentences, paragraphs, k-grams, HTTP\\nrequests, and so on. An automated detection of this behavior of extractors,\\nwhich we refer to as split-correctness, would allow text analysis systems to\\ndevise query plans with parallel evaluation on segments for accelerating the\\nprocessing of large documents. Other applications include the incremental\\nevaluation on dynamic content, where re-evaluation of information extractors\\ncan be restricted to revised segments, and debugging, where developers of\\ninformation extractors are informed about potential boundary crossing of\\ndifferent semantic components. We propose a new formal framework for\\nsplit-correctness within the formalism of document spanners. Our analysis\\nstudies the complexity of split-correctness over regular spanners. We also\\ndiscuss different variants of split-correctness, for instance, in the presence\\nof black-box extractors with split constraints.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.03367v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.03367v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.03386v1', updated=datetime.datetime(2018, 10, 8, 11, 50, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 8, 11, 50, 20, tzinfo=datetime.timezone.utc), title='Consistent Query Answering for Primary Keys in Logspace', authors=[arxiv.Result.Author('Paraschos Koutris'), arxiv.Result.Author('Jef Wijsen')], summary='We study the complexity of consistent query answering on databases that may\\nviolate primary key constraints. A repair of such a database is any consistent\\ndatabase that can be obtained by deleting a minimal set of tuples. For every\\nBoolean query q, CERTAINTY(q) is the problem that takes a database as input and\\nasks whether q evaluates to true on every repair. In [KW17], the authors show\\nthat for every self-join-free Boolean conjunctive query q, the problem\\nCERTAINTY(q) is either in P or coNP-complete, and it is decidable which of the\\ntwo cases applies. In this paper, we sharpen this result by showing that for\\nevery self-join-free Boolean conjunctive query q, the problem CERTAINTY(q) is\\neither expressible in symmetric stratified Datalog or coNP-complete. Since\\nsymmetric stratified Datalog is in L, we thus obtain a complexity-theoretic\\ndichotomy between L and coNP-complete. Another new finding of practical\\nimportance is that CERTAINTY(q) is on the logspace side of the dichotomy for\\nqueries q where all join conditions express foreign-to-primary key matches,\\nwhich is undoubtedly the most common type of join condition.', comment='51 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CC', 'cs.LO'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.03386v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.03386v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.05497v1', updated=datetime.datetime(2018, 10, 11, 1, 16, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 11, 1, 16, 31, tzinfo=datetime.timezone.utc), title='Probabilistic Blocking with An Application to the Syrian Conflict', authors=[arxiv.Result.Author('Rebecca C. Steorts'), arxiv.Result.Author('Anshumali Shrivastava')], summary='Entity resolution seeks to merge databases as to remove duplicate entries\\nwhere unique identifiers are typically unknown. We review modern blocking\\napproaches for entity resolution, focusing on those based upon locality\\nsensitive hashing (LSH). First, we introduce $k$-means locality sensitive\\nhashing (KLSH), which is based upon the information retrieval literature and\\nclusters similar records into blocks using a vector-space representation and\\nprojections. Second, we introduce a subquadratic variant of LSH to the\\nliterature, known as Densified One Permutation Hashing (DOPH). Third, we\\npropose a weighted variant of DOPH. We illustrate each method on an application\\nto a subset of the ongoing Syrian conflict, giving a discussion of each method.', comment='16 pages, 3 figures. arXiv admin note: substantial text overlap with\\n  arXiv:1510.07714, arXiv:1710.02690', journal_ref='Steorts R.C., Shrivastava A. (2018) Probabilistic Blocking with an\\n  Application to the Syrian Conflict. PSD (2018)', doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG', 'stat.AP', 'stat.ML'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.05497v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.05497v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.05570v1', updated=datetime.datetime(2018, 10, 12, 15, 16, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 12, 15, 16, 47, tzinfo=datetime.timezone.utc), title='Characterization and extraction of condensed representation of correlated patterns based on formal concept analysis', authors=[arxiv.Result.Author('Souad Bouasker')], summary='Correlated pattern mining has increasingly become an important task in data\\nmining since these patterns allow conveying knowledge about meaningful and\\nsurprising relations among data. Frequent correlated patterns were thoroughly\\nstudied in the literature. In this thesis, we propose to benefit from both\\nfrequent correlated as well as rare correlated patterns according to the bond\\ncorrelation measure. We propose to extract a subset without information loss of\\nthe sets of frequent correlated and of rare correlated patterns, this subset is\\ncalled ``Condensed Representation``. In this regard, we are based on the\\nnotions derived from the Formal Concept Analysis FCA, specifically the\\nequivalence classes associated to a closure operator fbond dedicated to the\\nbond measure, to introduce new concise representations of both frequent\\ncorrelated and rare correlated patterns.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.05570v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.05570v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.06021v1', updated=datetime.datetime(2018, 10, 14, 11, 59, 18, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 14, 11, 59, 18, tzinfo=datetime.timezone.utc), title='DPASF: A Flink Library for Streaming Data preprocessing', authors=[arxiv.Result.Author('Alejandro Alcalde-Barros'), arxiv.Result.Author('Diego García-Gil'), arxiv.Result.Author('Salvador García'), arxiv.Result.Author('Francisco Herrera')], summary='Data preprocessing techniques are devoted to correct or alleviate errors in\\ndata. Discretization and feature selection are two of the most extended data\\npreprocessing techniques. Although we can find many proposals for static Big\\nData preprocessing, there is little research devoted to the continuous Big Data\\nproblem. Apache Flink is a recent and novel Big Data framework, following the\\nMapReduce paradigm, focused on distributed stream and batch data processing. In\\nthis paper we propose a data stream library for Big Data preprocessing, named\\nDPASF, under Apache Flink. We have implemented six of the most popular data\\npreprocessing algorithms, three for discretization and the rest for feature\\nselection. The algorithms have been tested using two Big Data datasets.\\nExperimental results show that preprocessing can not only reduce the size of\\nthe data, but to maintain or even improve the original accuracy in a short\\ntime. DPASF contains useful algorithms when dealing with Big Data data streams.\\nThe preprocessing algorithms included in the library are able to tackle Big\\nDatasets efficiently and to correct imperfections in the data.', comment='19 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG', 'stat.ML'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.06021v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.06021v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.06742v2', updated=datetime.datetime(2019, 2, 23, 23, 26, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 15, 22, 59, 14, tzinfo=datetime.timezone.utc), title='Assessing and Remedying Coverage for a Given Dataset', authors=[arxiv.Result.Author('Abolfazl Asudeh'), arxiv.Result.Author('Zhongjun Jin'), arxiv.Result.Author('H. V. Jagadish')], summary='Data analysis impacts virtually every aspect of our society today. Often,\\nthis analysis is performed on an existing dataset, possibly collected through a\\nprocess that the data scientists had limited control over. The existing data\\nanalyzed may not include the complete universe, but it is expected to cover the\\ndiversity of items in the universe. Lack of adequate coverage in the dataset\\ncan result in undesirable outcomes such as biased decisions and algorithmic\\nracism, as well as creating vulnerabilities such as opening up room for\\nadversarial attacks.\\n  In this paper, we assess the coverage of a given dataset over multiple\\ncategorical attributes. We first provide efficient techniques for traversing\\nthe combinatorial explosion of value combinations to identify any regions of\\nattribute space not adequately covered by the data. Then, we determine the\\nleast amount of additional data that must be obtained to resolve this lack of\\nadequate coverage. We confirm the value of our proposal through both\\ntheoretical analyses and comprehensive experiments on real data.', comment='in ICDE 2019', journal_ref=None, doi='10.1109/ICDE.2019.00056', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/ICDE.2019.00056', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1810.06742v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.06742v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.07355v1', updated=datetime.datetime(2018, 10, 17, 2, 22, 34, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 17, 2, 22, 34, tzinfo=datetime.timezone.utc), title='Optimization of Indexing Based on k-Nearest Neighbor Graph for Proximity Search in High-dimensional Data', authors=[arxiv.Result.Author('Masajiro Iwasaki'), arxiv.Result.Author('Daisuke Miyazaki')], summary='Searching for high-dimensional vector data with high accuracy is an\\ninevitable search technology for various types of data. Graph-based indexes are\\nknown to reduce the query time for high-dimensional data. To further improve\\nthe query time by using graphs, we focused on the indegrees and outdegrees of\\ngraphs. While a sufficient number of incoming edges (indegrees) are\\nindispensable for increasing search accuracy, an excessive number of outgoing\\nedges (outdegrees) should be suppressed so as to not increase the query time.\\nTherefore, we propose three degree-adjustment methods: static degree adjustment\\nof not only outdegrees but also indegrees, dynamic degree adjustment with which\\noutdegrees are determined by the search accuracy users require, and path\\nadjustment to remove edges that have alternative search paths to reduce\\noutdegrees. We also show how to obtain optimal degree-adjustment parameters and\\nthat our methods outperformed previous methods for image and textual data.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.07355v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.07355v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.08047v1', updated=datetime.datetime(2018, 10, 18, 13, 42, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 18, 13, 42, 4, tzinfo=datetime.timezone.utc), title='Finding Average Regret Ratio Minimizing Set in Database', authors=[arxiv.Result.Author('Sepanta Zeighami'), arxiv.Result.Author('Raymong Chi-Wing Wong')], summary='Selecting a certain number of data points (or records) from a database which\\n\"best\" satisfy users\\' expectations is a very prevalent problem with many\\napplications. One application is a hotel booking website showing a certain\\nnumber of hotels on a single page. However, this problem is very challenging\\nsince the selected points should \"collectively\" satisfy the expectation of all\\nusers. Showing a certain number of data points to a single user could decrease\\nthe satisfaction of a user because the user may not be able to see his/her\\nfavorite point which could be found in the original database. In this paper, we\\nwould like to find a set of k points such that on average, the satisfaction\\n(ratio) of a user is maximized. This problem takes into account the probability\\ndistribution of the users and considers the satisfaction (ratio) of all users,\\nwhich is more reasonable in practice, compared with the existing studies that\\nonly consider the worst-case satisfaction (ratio) of the users, which may not\\nreflect the whole population and is not useful in some applications. Motivated\\nby this, in this paper, we propose algorithms for this problem. Finally, we\\nconducted experiments to show the effectiveness and the efficiency of the\\nalgorithms.', comment=\"Submitted to ICDE '19\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.08047v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.08047v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.08062v3', updated=datetime.datetime(2019, 7, 8, 22, 47, 18, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 18, 13, 57, 49, tzinfo=datetime.timezone.utc), title='Modeling and In-Database Management of Relational, Data-Aware Processes (Extended Version)', authors=[arxiv.Result.Author('Diego Calvanese'), arxiv.Result.Author('Marco Montali'), arxiv.Result.Author('Fabio Patrizi'), arxiv.Result.Author('Andrey Rivkin')], summary='During the last two decades, it has been increasingly acknowledged that the\\nengineering of information systems usually requires a huge effort in\\nintegrating master data and business processes. This has led to a plethora of\\nproposals, both from academia and the industry. However, such approaches\\ntypically come with ad-hoc abstractions to represent and interact with the data\\ncomponent. This has a twofold disadvantage. On the one hand, they cannot be\\nused to effortlessly enrich an existing relational database with dynamics. On\\nthe other hand, they generally do not allow for integrated modelling,\\nverification, and enactment. We attack these two challenges by proposing a\\ndeclarative approach, fully grounded in SQL, that supports the agile modelling\\nof relational data-aware processes directly on top of relational databases. We\\nshow how this approach can be automatically translated into a concrete\\nprocedural SQL dialect, executable directly inside any relational database\\nengine. The translation exploits an in-database representation of process\\nstates that, in turn, is used to handle, at once, process enactment with or\\nwithout logging of the executed instances, as well as process verification. The\\napproach has been implemented in a working prototype.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.08062v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.08062v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.08755v4', updated=datetime.datetime(2019, 4, 19, 9, 6, 37, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 20, 5, 58, 48, tzinfo=datetime.timezone.utc), title='Property Graph Type System and Data Definition Language', authors=[arxiv.Result.Author('Mingxi Wu')], summary='Property graph manages data by vertices and edges. Each vertex and edge can\\nhave a property map, storing ad hoc attribute and its value. Label can be\\nattached to vertices and edges to group them. While this schema-less\\nmethodology is very flexible for data evolvement and for managing explosive\\ngraph element, it has two shortcomings-- 1) data dependency 2) less\\ncompression. Both problems can be solved by a schema based approach. In this\\npaper, a type system used to model property graph is defined. Based on the type\\nsystem, the associated data definition language (DDL) is proposed and multiple\\ngraph instances created under this type system is discussed.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.08755v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.08755v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.08833v2', updated=datetime.datetime(2019, 5, 29, 5, 10, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 20, 17, 54, 37, tzinfo=datetime.timezone.utc), title='MinJoin: Efficient Edit Similarity Joins via Local Hash Minima', authors=[arxiv.Result.Author('Haoyu Zhang'), arxiv.Result.Author('Qin Zhang')], summary='We study the problem of computing similarity joins under edit distance on a\\nset of strings. Edit similarity joins is a fundamental problem in databases,\\ndata mining and bioinformatics. It finds important applications in data\\ncleaning and integration, collaborative filtering, genome sequence assembly,\\netc. This problem has attracted significant attention in the past two decades.\\nHowever, all previous algorithms either cannot scale well to long strings and\\nlarge similarity thresholds, or suffer from imperfect accuracy.\\n  In this paper we propose a new algorithm for edit similarity joins using a\\nnovel string partition based approach. We show mathematically that with high\\nprobability our algorithm achieves a perfect accuracy, and runs in linear time\\nplus a data-dependent verification step. Experiments on real world datasets\\nshow that our algorithm significantly outperforms the state-of-the-art\\nalgorithms for edit similarity joins, and achieves perfect accuracy on all the\\ndatasets that we have tested.', comment='Accepted to KDD 2019, full version, 22 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.08833v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.08833v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.09378v1', updated=datetime.datetime(2018, 10, 22, 15, 53, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 22, 15, 53, 22, tzinfo=datetime.timezone.utc), title='biggy: An Implementation of Unified Framework for Big Data Management System', authors=[arxiv.Result.Author('Yao Wu'), arxiv.Result.Author('Henan Guan')], summary='Various tools, softwares and systems are proposed and implemented to tackle\\nthe challenges in big data on different emphases, e.g., data analysis, data\\ntransaction, data query, data storage, data visualization, data privacy. In\\nthis paper, we propose datar, a new prospective and unified framework for Big\\nData Management System (BDMS) from the point of system architecture by\\nleveraging ideas from mainstream computer structure. We introduce five key\\ncomponents of datar by reviewing the cur- rent status of BDMS. Datar features\\nwith configuration chain of pluggable engines, automatic dataflow on job\\npipelines, intelligent self-driving system management and interactive user\\ninterfaces. Moreover, we present biggy as an implementation of datar with\\nmanipulation details demonstrated by four running examples. Evaluations on\\nefficiency and scalability are carried out to show the performance. Our work\\nargues that the envisioned datar is a feasible solution to the unified\\nframework of BDMS, which can manage big data pluggablly, automatically and\\nintelligently with specific functionalities, where specific functionalities\\nrefer to input, storage, computation, control and output of big data.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.09378v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.09378v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.09399v1', updated=datetime.datetime(2018, 10, 22, 16, 47, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 22, 16, 47, 46, tzinfo=datetime.timezone.utc), title='Towards a context-dependent numerical data quality evaluation framework', authors=[arxiv.Result.Author('Milen S. Marev'), arxiv.Result.Author('Ernesto Compatangelo'), arxiv.Result.Author('Wamberto Vasconcelos')], summary='This paper focuses on numeric data, with emphasis on distinct characteristics\\nlike varying significance, unstructured format, mass volume and real-time\\nprocessing. We propose a novel, context-dependent valuation framework\\nspecifically devised to assess quality in numeric datasets. Our framework uses\\neight relevant data quality dimensions, and provide a simple metric to evaluate\\ndataset quality along each dimension. We argue that the proposed set of\\ndimensions and corresponding metrics adequately captures the unique quality\\nantipatterns that are typically associated with numerical data. The\\nintroduction of our framework is part of a wider research effort that aims at\\ndeveloping an articulated numerical data quality improvement approach for Oil\\nand Gas exploration and production workflows that is based on artificial\\nintelligence techniques.', comment='Keywords: Data Quality; Numerical Data; Evaluation framework. 12\\n  pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.09399v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.09399v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.09524v1', updated=datetime.datetime(2018, 10, 22, 20, 4, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 22, 20, 4, 24, tzinfo=datetime.timezone.utc), title='Selection of BJI configuration: Approach based on minimal transversals', authors=[arxiv.Result.Author('Issam Ghabry')], summary='Decision systems deal with a large volume of data stored in new databases\\ncalled data warehouses. Data warehouses are typically modeled by a star schema\\nthat conventionally presents a central fact table and a set of dimension\\ntables. The corresponding queries for this type of model are therefore very\\ncomplex. In order to reduce the cost of executing complex queries, which\\ncontain very expensive joins, the solution envisaged would be to guarantee a\\ngood physical design of the data warehouses. Binary join indexes are very\\nsuitable to reduce the cost of executing these joins. In this work, we proposed\\na binary join index selection approach based on the notion of minimal\\ntransversal. The final configuration obtained is composed of several indexes,\\nwhich make it possible to optimize the execution cost of the query set.', comment='Masters thesis (2017) supervised by Sadok Ben Yahia and Mohamed\\n  Nidhal Jelassi, in French. arXiv admin note: text overlap with\\n  arXiv:1902.00911 by other authors', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.09524v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.09524v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.11832v3', updated=datetime.datetime(2018, 12, 11, 15, 42, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 28, 16, 41, 22, tzinfo=datetime.timezone.utc), title='VDMS: Efficient Big-Visual-Data Access for Machine Learning Workloads', authors=[arxiv.Result.Author('Luis Remis'), arxiv.Result.Author('Vishakha Gupta-Cledat'), arxiv.Result.Author('Christina Strong'), arxiv.Result.Author('Ragaad Altarawneh')], summary='We introduce the Visual Data Management System (VDMS), which enables faster\\naccess to big-visual-data and adds support to visual analytics. This is\\nachieved by searching for relevant visual data via metadata stored as a graph,\\nand enabling faster access to visual data through new machine-friendly storage\\nformats. VDMS differs from existing large scale photo serving, video streaming,\\nand textual big-data management systems due to its primary focus on supporting\\nmachine learning and data analytics pipelines that use visual data (images,\\nvideos, and feature vectors), treating these as first class entities. We\\ndescribe how to use VDMS via its user friendly interface and how it enables\\nrich and efficient vision analytics through a machine learning pipeline for\\nprocessing medical images. We show the improved performance of 2x in complex\\nqueries over a comparable set-up.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.11832v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.11832v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1810.12059v1', updated=datetime.datetime(2018, 10, 29, 11, 26, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 10, 29, 11, 26, 17, tzinfo=datetime.timezone.utc), title='Studio e confronto delle strutture di Apache Spark', authors=[arxiv.Result.Author('Massimiliano Morrelli')], summary=\"English. This document is designed to study the data structures that can be\\nused in the Apache Spark framework and to evaluate the best performing ones to\\nimplement solutions, in particular we will evaluate advantages / disadvantages\\nderiving from the use of Dataset for job creation. The observation of the\\nresults provides further support in evaluating the use of Dataset as an\\nalternative to RDD, in order to understand its strengths and weaknesses. The\\nexamination of the results is possible thanks to specifically designed and\\nimplemented in Java 1.8 language. The execution of the jobs, entrusted to a\\nsuitable distributed environment, will end with the comparison between\\nexecution times and results obtained.\\n  Italiano. Il presente documento nasce allo scopo di studiare le strutture\\ndati utilizzabili nel framework Apache Spark e valutare quelle pi\\\\`u\\nperformanti per implementare soluzioni; valuteremo in articolare i vantaggi /\\nsvantaggi derivanti dall'utilizzo dei Dataset nella progettazione dei job.\\nL'osservazione dei risultati fornisce ulteriore supporto nel valutare\\nl'utilizzo dei Dataset in alternativa a RDD, al fine di comprederne i punti di\\nforza e di debolezza. L'esame dei risultati \\\\`e possibile in virt\\\\`u di due\\ncasi appositamente pensati e implementati in linguaggio Java 1.8. L'esecuzione\\ndei job, affidata a un adeguato ambiente distribuito, si concluder\\\\`a con il\\nconfronto tra tempi di esecuzione e risultati ottenuti.\", comment='in Italian', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1810.12059v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1810.12059v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.00132v1', updated=datetime.datetime(2019, 1, 31, 23, 43, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 31, 23, 43, 35, tzinfo=datetime.timezone.utc), title='Plan-Structured Deep Neural Network Models for Query Performance Prediction', authors=[arxiv.Result.Author('Ryan Marcus'), arxiv.Result.Author('Olga Papaemmanouil')], summary='Query performance prediction, the task of predicting the latency of a query,\\nis one of the most challenging problem in database management systems. Existing\\napproaches rely on features and performance models engineered by human experts,\\nbut often fail to capture the complex interactions between query operators and\\ninput relations, and generally do not adapt naturally to workload\\ncharacteristics and patterns in query execution plans. In this paper, we argue\\nthat deep learning can be applied to the query performance prediction problem,\\nand we introduce a novel neural network architecture for the task: a\\nplan-structured neural network. Our approach eliminates the need for\\nhuman-crafted feature selection and automatically discovers complex performance\\nmodels both at the operator and query plan level. Our novel neural network\\narchitecture can match the structure of any optimizer-selected query execution\\nplan and predict its latency with high accuracy. We also propose a number of\\noptimizations that reduce training overhead without sacrificing effectiveness.\\nWe evaluated our techniques on various workloads and we demonstrate that our\\nplan-structured neural network can outperform the state-of-the-art in query\\nperformance prediction.', comment=None, journal_ref=None, doi='10.14778/3342263.3342646', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3342263.3342646', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1902.00132v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.00132v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.00585v1', updated=datetime.datetime(2019, 2, 1, 23, 10, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 1, 23, 10, 4, tzinfo=datetime.timezone.utc), title='Incremental Techniques for Large-Scale Dynamic Query Processing', authors=[arxiv.Result.Author('Iman Elghandour'), arxiv.Result.Author('Ahmet Kara'), arxiv.Result.Author('Dan Olteanu'), arxiv.Result.Author('Stijn Vansummeren')], summary='Many applications from various disciplines are now required to analyze fast\\nevolving big data in real time. Various approaches for incremental processing\\nof queries have been proposed over the years. Traditional approaches rely on\\nupdating the results of a query when updates are streamed rather than\\nre-computing these queries, and therefore, higher execution performance is\\nexpected. However, they do not perform well for large databases that are\\nupdated at high frequencies. Therefore, new algorithms and approaches have been\\nproposed in the literature to address these challenges by, for instance,\\nreducing the complexity of processing updates. Moreover, many of these\\nalgorithms are now leveraging distributed streaming platforms such as Spark\\nStreaming and Flink. In this tutorial, we briefly discuss legacy approaches for\\nincremental query processing, and then give an overview of the new challenges\\nintroduced due to processing big data streams. We then discuss in detail the\\nrecently proposed algorithms that address some of these challenges. We\\nemphasize the characteristics and algorithmic analysis of various proposed\\napproaches and conclude by discussing future research directions.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1902.00585v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.00585v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.00609v1', updated=datetime.datetime(2019, 2, 2, 1, 18, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 2, 1, 18, 24, tzinfo=datetime.timezone.utc), title='Transparent Concurrency Control: Decoupling Concurrency Control from DBMS', authors=[arxiv.Result.Author('Ningnan Zhou'), arxiv.Result.Author('Xuan Zhou'), arxiv.Result.Author('Kian-lee Tan'), arxiv.Result.Author('Shan Wang')], summary='For performance reasons, conventional DBMSes adopt monolithic architectures.\\nA monolithic design cripples the adaptability of a DBMS, making it difficult to\\ncustomize, to meet particular requirements of different applications. In this\\npaper, we propose to completely separate the code of concurrency control (CC)\\nfrom a monolithic DBMS. This allows us to add / remove functionalities or data\\nstructures to / from a DBMS easily, without concerning the issues of data\\nconsistency. As the separation deprives the concurrency controller of the\\nknowledge about data organization and processing, it may incur severe\\nperformance issues. To minimize the performance loss, we devised a two-level CC\\nmechanism. At the operational level, we propose a robust scheduler that\\nguarantees to complete any data operation at a manageable cost. At the\\ntransactional level, the scheduler can utilize data semantics to achieve\\nenhanced performance. Extensive experiments were conducted to demonstrate the\\nfeasibility and effectiveness of our approach.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1902.00609v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.00609v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.00624v1', updated=datetime.datetime(2019, 2, 2, 2, 24, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 2, 2, 24, 8, tzinfo=datetime.timezone.utc), title='A Question Answering System Using Graph-Pattern Association Rules (QAGPAR) On YAGO Knowledge Base', authors=[arxiv.Result.Author('Wahyudi'), arxiv.Result.Author('Masayu Leylia Khodra'), arxiv.Result.Author('Ary Setijadi Prihatmanto'), arxiv.Result.Author('Carmadi Machbub')], summary='A question answering system (QA System) was developed that uses graph-pattern\\nassociation rules on the YAGO knowledge base. The answer as output of the\\nsystem is provided based on a user question as input. If the answer is missing\\nor unavailable in the database, then graph-pattern association rules are used\\nto get the answer. The architecture of this question answering system is as\\nfollows: question classification, graph component generation, query generation,\\nand query processing. The question answering system uses association graph\\npatterns in a waterfall model. In this paper, the architecture of the system is\\ndescribed, specifically discussing its reasoning and performance capabilities.\\nThe results of this research is that rules with high confidence and correct\\nlogic produce correct answers, and vice versa', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1902.00624v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.00624v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.00952v4', updated=datetime.datetime(2020, 4, 14, 23, 59, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 3, 18, 43, 25, tzinfo=datetime.timezone.utc), title='Automating Software Citation using GitCite', authors=[arxiv.Result.Author('Leshang Chen'), arxiv.Result.Author('Susan Davidson')], summary='The ability to cite software and give credit to its authors and contributors\\nis increasingly important. While the number of online open-source software\\nrepositories has grown rapidly over the past few years, few are being properly\\ncited when used due to the difficulty of creating appropriate citations and the\\nlack of automated techniques. This paper presents GitCite, a model for software\\ncitation with version control which enables citations to be inferred for any\\nproject component based on a small number of explicit citations attached to\\nsubdirectories/files, and an implementation that integrates with Git and\\nGitHub. The implementation includes a browser extension and a local executable\\ntool, which enable citations to be added/modified/deleted to software project\\nrepositories and managed through functions such as fork/merge/copy.', comment=None, journal_ref='2020 IEEE 36th International Conference on Data Engineering\\n  (ICDE), Dallas, TX, USA, 2020, pp. 1754-1757', doi='10.1109/ICDE48307.2020.00162', primary_category='cs.DB', categories=['cs.DB', 'cs.DL'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/ICDE48307.2020.00162', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1902.00952v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.00952v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.02140v1', updated=datetime.datetime(2019, 2, 6, 12, 33, 7, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 6, 12, 33, 7, tzinfo=datetime.timezone.utc), title='Close-reading of Linked Data: a case study in regards to the quality of online authority files', authors=[arxiv.Result.Author('Ettore Rizza'), arxiv.Result.Author('Anne Chardonnens'), arxiv.Result.Author('Seth van Hooland')], summary=\"More and more cultural institutions use Linked Data principles to share and\\nconnect their collection metadata. In the archival field, initiatives emerge to\\nexploit data contained in archival descriptions and adapt encoding standards to\\nthe semantic web. In this context, online authority files can be used to enrich\\nmetadata. However, relying on a decentralized network of knowledge bases such\\nas Wikidata, DBpedia or even Viaf has its own difficulties. This paper aims to\\noffer a critical view of these linked authority files by adopting a\\nclose-reading approach. Through a practical case study, we intend to identify\\nand illustrate the possibilities and limits of RDF triples compared to\\ninstitutions' less structured metadata.\", comment='Workshop \"Dariah \"Trust and Understanding: the value of metadata in a\\n  digitally joined-up world\" (14/05/2018, Brussels), preprint of the submission\\n  to the journal \"Archives et Biblioth\\\\`eques de Belgique\"', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1902.02140v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.02140v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.02698v3', updated=datetime.datetime(2021, 10, 31, 19, 45, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 7, 15, 46, 35, tzinfo=datetime.timezone.utc), title='Ranked Enumeration of Conjunctive Query Results', authors=[arxiv.Result.Author('Shaleen Deep'), arxiv.Result.Author('Paraschos Koutris')], summary='We investigate the enumeration of top-k answers for conjunctive queries\\nagainst relational databases according to a given ranking function. The task is\\nto design data structures and algorithms that allow for efficient enumeration\\nafter a preprocessing phase. Our main contribution is a novel priority queue\\nbased algorithm with near-optimal delay and non-trivial space guarantees that\\nare output sensitive and depend on structure of the query. In particular, we\\nexploit certain desirable properties of ranking functions that frequently occur\\nin practice and degree information in the database instance, allowing for\\nefficient enumeration. We introduce the notion of {\\\\em decomposable} and {\\\\em\\ncompatible} ranking functions in conjunction with query decomposition, a\\nproperty that allows for partial aggregation of tuple scores in order to\\nefficiently enumerate the ranked output. We complement the algorithmic results\\nwith lower bounds justifying why certain assumptions about properties of\\nranking functions are necessary and discuss popular conjectures providing\\nevidence for optimality of enumeration delay guarantees. Our results extend and\\nimprove upon a long line of work that has studied ranked enumeration from both\\ntheoretical and practical perspective.', comment='LMCS journal submission', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1902.02698v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.02698v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.03296v1', updated=datetime.datetime(2019, 2, 5, 21, 45, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 5, 21, 45, 45, tzinfo=datetime.timezone.utc), title='Efficient Power Theft Detection for Residential Consumers Using Mean Shift Data Mining Knowledge Discovery Process', authors=[arxiv.Result.Author('Konstantinos Blazakis'), arxiv.Result.Author('Georgios Stavrakakis')], summary='Energy theft constitutes an issue of great importance for electricity\\noperators. The attempt to detect and reduce non-technical losses is a\\nchallenging task due to insufficient inspection methods. With the evolution of\\nadvanced metering infrastructure (AMI) in smart grids, a more complicated\\nstatus quo in energy theft has emerged and many new technologies are being\\nadopted to solve the problem. In order to identify illegal residential\\nconsumers, a computational method of analyzing and identifying electricity\\nconsumption patterns of consumers based on data mining techniques has been\\npresented. Combining principal component analysis (PCA) with mean shift\\nalgorithm for different power theft scenarios, we can now cope with the power\\ntheft detection problem sufficiently. The overall research has shown\\nencouraging results in residential consumers power theft detection that will\\nhelp utilities to improve the reliability, security and operation of power\\nnetwork.', comment='17 pages, 6 figures', journal_ref=None, doi='10.5121/ijaia.2019.10106', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.5121/ijaia.2019.10106', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1902.03296v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.03296v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.03297v1', updated=datetime.datetime(2019, 2, 7, 18, 51, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 7, 18, 51, 59, tzinfo=datetime.timezone.utc), title='Probably the Best Itemsets', authors=[arxiv.Result.Author('Nikolaj Tatti')], summary=\"One of the main current challenges in itemset mining is to discover a small\\nset of high-quality itemsets. In this paper we propose a new and general\\napproach for measuring the quality of itemsets. The method is solidly founded\\nin Bayesian statistics and decreases monotonically, allowing for efficient\\ndiscovery of all interesting itemsets. The measure is defined by connecting\\nstatistical models and collections of itemsets. This allows us to score\\nindividual itemsets with the probability of them occuring in random models\\nbuilt on the data.\\n  As a concrete example of this framework we use exponential models. This class\\nof models possesses many desirable properties. Most importantly, Occam's razor\\nin Bayesian model selection provides a defence for the pattern explosion. As\\ngeneral exponential models are infeasible in practice, we use decomposable\\nmodels; a large sub-class for which the measure is solvable. For the actual\\ncomputation of the score we sample models from the posterior distribution using\\nan MCMC approach.\\n  Experimentation on our method demonstrates the measure works in practice and\\nresults in interpretable and insightful itemsets for both synthetic and\\nreal-world data.\", comment=None, journal_ref=None, doi='10.1145/1835804.1835843', primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/1835804.1835843', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1902.03297v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.03297v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.03576v1', updated=datetime.datetime(2019, 2, 10, 11, 25, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 10, 11, 25, 32, tzinfo=datetime.timezone.utc), title='Antidote SQL: Relaxed When Possible, Strict When Necessary', authors=[arxiv.Result.Author('Pedro Lopes'), arxiv.Result.Author('João Sousa'), arxiv.Result.Author('Valter Balegas'), arxiv.Result.Author('Carla Ferreira'), arxiv.Result.Author('Ségio Duarte'), arxiv.Result.Author('Annette Bieniusa'), arxiv.Result.Author('Rodrigo Rodrigues'), arxiv.Result.Author('Nuno Preguiça')], summary='Geo-replication poses an inherent trade-off between low latency, high\\navailability and strong consistency. While NoSQL databases favor low latency\\nand high availability, relaxing consistency, more recent cloud databases favor\\nstrong consistency and ease of programming, while still providing high\\nscalability. In this paper, we present Antidote SQL, a database system that\\nallows application developers to relax SQL consistency when possible. Unlike\\nNoSQL databases, our approach enforces primary key, foreign key and check SQL\\nconstraints even under relaxed consistency, which is sufficient for\\nguaranteeing the correctness of many applications. To this end, we defined\\nconcurrency semantics for SQL constraints under relaxed consistency and show\\nhow to implement such semantics efficiently. For applications that require\\nstrict SQL consistency, Antidote SQL provides support for such semantics at the\\ncost of requiring coordination among replicas.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1902.03576v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.03576v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.03975v1', updated=datetime.datetime(2019, 2, 11, 16, 34, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 11, 16, 34, 44, tzinfo=datetime.timezone.utc), title='A Blockchain Framework for Managing and Monitoring Data in Multi-Site Clinical Trials', authors=[arxiv.Result.Author('Olivia Choudhury'), arxiv.Result.Author('Noor Fairoza'), arxiv.Result.Author('Issa Sylla'), arxiv.Result.Author('Amar Das')], summary='The cost of conducting multi-site clinical trials has significantly increased\\nover time, with site monitoring, data management, and amendments being key\\ndrivers. Clinical trial data management approaches typically rely on a central\\ndatabase, and require manual efforts to encode and maintain data capture and\\nreporting requirements. To reduce the administrative burden, time, and effort\\nof ensuring data integrity and privacy in multi-site trials, we propose a novel\\ndata management framework based on permissioned blockchain technology. We\\ndemonstrate how our framework, which uses smart contracts and private channels,\\nenables confidential data communication, protocol enforcement, and and an\\nautomated audit trail. We compare this framework with the traditional data\\nmanagement approach and evaluate its effectiveness in satisfying the major\\nrequirements of multi-site clinical trials. We show that our framework ensures\\nenforcement of IRB-related regulatory requirements across multiple sites and\\nstakeholders.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/1902.03975v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.03975v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.04379v1', updated=datetime.datetime(2019, 2, 12, 13, 38, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 12, 13, 38, 26, tzinfo=datetime.timezone.utc), title='Generalized Lineage-Aware Temporal Windows: Supporting Outer and Anti Joins in Temporal-Probabilistic Databases', authors=[arxiv.Result.Author('Katerina Papaioannou'), arxiv.Result.Author('Martin Theobald'), arxiv.Result.Author('Michael Böhlen')], summary='The result of a temporal-probabilistic (TP) join with negation includes, at\\neach time point, the probability with which a tuple of a positive relation\\n${\\\\bf p}$ matches none of the tuples in a negative relation ${\\\\bf n}$, for a\\ngiven join condition $\\\\theta$. TP outer and anti joins thus resemble the\\ncharacteristics of relational outer and anti joins also in the case when there\\nexist time points at which input tuples from ${\\\\bf p}$ have non-zero\\nprobabilities to be $true$ and input tuples from ${\\\\bf n}$ have non-zero\\nprobabilities to be $false$, respectively. For the computation of TP joins with\\nnegation, we introduce generalized lineage-aware temporal windows, a mechanism\\nthat binds an output interval to the lineages of all the matching valid tuples\\nof each input relation. We group the windows of two TP relations into three\\ndisjoint sets based on the way attributes, lineage expressions and intervals\\nare produced. We compute all windows in an incremental manner, and we show that\\npipelined computations allow for the direct integration of our approach into\\nPostgreSQL. We thereby alleviate the prevalent redundancies in the interval\\ncomputations of existing approaches, which is proven by an extensive\\nexperimental evaluation with real-world datasets.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1902.04379v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.04379v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.04790v1', updated=datetime.datetime(2019, 2, 13, 8, 53, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 13, 8, 53, 59, tzinfo=datetime.timezone.utc), title='SaGe: Web Preemption for Public SPARQL Query Services', authors=[arxiv.Result.Author('Thomas Minier'), arxiv.Result.Author('Hala Skaf-Molli'), arxiv.Result.Author('Pascal Molli')], summary='To provide stable and responsive public SPARQL query services, data providers\\nenforce quotas on server usage. Queries which exceed these quotas are\\ninterrupted and deliver partial results. Such interruption is not an issue if\\nit is possible to resume queries execution afterward. Unfortunately, there is\\nno preemption model for the Web that allows for suspending and resuming SPARQL\\nqueries. In this paper, we propose SaGe: a SPARQL query engine based on Web\\npreemption. SaGe allows SPARQL queries to be suspended by the Web server after\\na fixed time quantum and resumed upon client request. Web preemption is\\ntractable only if its cost in time is negligible compared to the time quantum.\\nThe challenge is to support the full SPARQL query language while keeping the\\ncost of preemption negligible. Experimental results demonstrate that SaGe\\noutperforms existing SPARQL query processing approaches by several orders of\\nmagnitude in term of the average total query execution time and the time for\\nfirst results.', comment=\"11 pages, to be published in Proceedings of the 2019 World Wide Web\\n  Conference (WWW'19)\", journal_ref=\"Proceedings of the 2019 World Wide Web Conference (WWW'19)\", doi='10.1145/3308558.3313652', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3308558.3313652', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1902.04790v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.04790v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.05170v2', updated=datetime.datetime(2019, 5, 19, 19, 11, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 14, 0, 7, 26, tzinfo=datetime.timezone.utc), title='GrapAL: Connecting the Dots in Scientific Literature', authors=[arxiv.Result.Author('Christine Betts'), arxiv.Result.Author('Joanna Power'), arxiv.Result.Author('Waleed Ammar')], summary='We introduce GrapAL (Graph database of Academic Literature), a versatile tool\\nfor exploring and investigating a knowledge base of scientific literature, that\\nwas semi-automatically constructed using NLP methods. GrapAL satisfies a\\nvariety of use cases and information needs requested by researchers. At the\\ncore of GrapAL is a Neo4j graph database with an intuitive schema and a simple\\nquery language. In this paper, we describe the basic elements of GrapAL, how to\\nuse it, and several use cases such as finding experts on a given topic for peer\\nreviewing, discovering indirect connections between biomedical entities and\\ncomputing citation-based metrics. We open source the demo code to help other\\nresearchers develop applications that build on GrapAL.', comment='To appear at ACL 2019 (Demonstration Track)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DL'], links=[arxiv.Result.Link('http://arxiv.org/abs/1902.05170v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.05170v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.06427v1', updated=datetime.datetime(2019, 2, 18, 7, 17, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 18, 7, 17, 26, tzinfo=datetime.timezone.utc), title='Schema Validation and Evolution for Graph Databases', authors=[arxiv.Result.Author('Angela Bonifati'), arxiv.Result.Author('Peter Furniss'), arxiv.Result.Author('Alastair Green'), arxiv.Result.Author('Russ Harmer'), arxiv.Result.Author('Eugenia Oshurko'), arxiv.Result.Author('Hannes Voigt')], summary='Despite the maturity of commercial graph databases, little consensus has been\\nreached so far on the standardization of data definition languages (DDLs) for\\nproperty graphs (PG). The discussion on the characteristics of PG schemas is\\nongoing in many standardization and community groups. Although some basic\\naspects of a schema are already present in Neo4j 3.5, like in most commercial\\ngraph databases, full support is missing allowing to constraint property graphs\\nwith more or less flexibility. In this paper, we focus on two different\\nperspectives from which a PG schema should be considered, as being descriptive\\nor prescriptive, and we show how it would be possible to switch from one to\\nanother as the application under development gains more stability. Apart from\\nproposing concise schema DDL inspired by Cypher syntax, we show how schema\\nvalidation can be enforced through homomorphisms between PG schemas and PG\\ninstances; and how schema evolution can be described through the use of graph\\nrewriting operations. Our prototypical implementation demonstrates feasibility\\nand shows the need of offering high-level query primitives to accommodate\\nflexible graph schema requirements as showcased in our work.', comment='36 pages, 9 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1902.06427v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.06427v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.07688v1', updated=datetime.datetime(2019, 2, 20, 18, 23, 37, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 20, 18, 23, 37, tzinfo=datetime.timezone.utc), title='Towards Semantic Big Graph Analytics for Cross-Domain Knowledge Discovery', authors=[arxiv.Result.Author('Feichen Shen')], summary=\"In recent years, the size of big linked data has grown rapidly and this\\nnumber is still rising. Big linked data and knowledge bases come from different\\ndomains such as life sciences, publications, media, social web, and so on.\\nHowever, with the rapid increasing of data, it is very challenging for people\\nto acquire a comprehensive collection of cross domain knowledge to meet their\\nneeds. Under this circumstance, it is extremely difficult for people without\\nexpertise to extract knowledge from various domains. Therefore, nowadays human\\nlimited knowledge can't feed the high requirement for discovering large amount\\nof cross domain knowledge. In this research, we present a big graph analytics\\nframework aims at addressing this issue by providing semantic methods to\\nfacilitate the management of big graph data from close domains in order to\\ndiscover cross domain knowledge in a more accurate and efficient way.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1902.07688v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.07688v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.08271v5', updated=datetime.datetime(2020, 8, 15, 21, 22, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 21, 21, 23, 5, tzinfo=datetime.timezone.utc), title='An IDEA: An Ingestion Framework for Data Enrichment in AsterixDB', authors=[arxiv.Result.Author('Xikui Wang'), arxiv.Result.Author('Michael J. Carey')], summary='Big Data today is being generated at an unprecedented rate from various\\nsources such as sensors, applications, and devices, and it often needs to be\\nenriched based on other reference information to support complex analytical\\nqueries. Depending on the use case, the enrichment operations can be compiled\\ncode, declarative queries, or machine learning models with different\\ncomplexities. For enrichments that will be frequently used in the future, it\\ncan be advantageous to push their computation into the ingestion pipeline so\\nthat they can be stored (and queried) together with the data. In some cases,\\nthe referenced information may change over time, so the ingestion pipeline\\nshould be able to adapt to such changes to guarantee the currency and/or\\ncorrectness of the enrichment results.\\n  In this paper, we present a new data ingestion framework that supports data\\ningestion at scale, enrichments requiring complex operations, and adaptiveness\\nto reference data changes. We explain how this framework has been built on top\\nof Apache AsterixDB and investigate its performance at scale under various\\nworkloads.', comment='21 pages, 40 Figures, accepted in VLDB 2019', journal_ref=None, doi='10.14778/3342263.3342628', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3342263.3342628', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1902.08271v5', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.08271v5', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.08283v5', updated=datetime.datetime(2019, 10, 1, 19, 37, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 21, 22, 13, 29, tzinfo=datetime.timezone.utc), title='Capuchin: Causal Database Repair for Algorithmic Fairness', authors=[arxiv.Result.Author('Babak Salimi'), arxiv.Result.Author('Luke Rodriguez'), arxiv.Result.Author('Bill Howe'), arxiv.Result.Author('Dan Suciu')], summary=\"Fairness is increasingly recognized as a critical component of machine\\nlearning systems. However, it is the underlying data on which these systems are\\ntrained that often reflect discrimination, suggesting a database repair\\nproblem. Existing treatments of fairness rely on statistical correlations that\\ncan be fooled by statistical anomalies, such as Simpson's paradox. Proposals\\nfor causality-based definitions of fairness can correctly model some of these\\nsituations, but they require specification of the underlying causal models. In\\nthis paper, we formalize the situation as a database repair problem, proving\\nsufficient conditions for fair classifiers in terms of admissible variables as\\nopposed to a complete causal model. We show that these conditions correctly\\ncapture subtle fairness violations. We then use these conditions as the basis\\nfor database repair algorithms that provide provable fairness guarantees about\\nclassifiers trained on their training labels. We evaluate our algorithms on\\nreal data, demonstrating improvement over the state of the art on multiple\\nfairness metrics proposed in the literature while retaining high utility.\", comment=None, journal_ref='Proceedings of the 2019 International Conference on Management of\\n  Data. ACM, 2019', doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1902.08283v5', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.08283v5', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.08291v2', updated=datetime.datetime(2019, 3, 19, 8, 51, 21, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 21, 22, 33, 52, tzinfo=datetime.timezone.utc), title='How I Learned to Stop Worrying and Love Re-optimization', authors=[arxiv.Result.Author('Matthew Perron'), arxiv.Result.Author('Zeyuan Shang'), arxiv.Result.Author('Tim Kraska'), arxiv.Result.Author('Michael Stonebraker')], summary='Cost-based query optimizers remain one of the most important components of\\ndatabase management systems for analytic workloads. Though modern optimizers\\nselect plans close to optimal performance in the common case, a small number of\\nqueries are an order of magnitude slower than they could be. In this paper we\\ninvestigate why this is still the case, despite decades of improvements to cost\\nmodels, plan enumeration, and cardinality estimation. We demonstrate why we\\nbelieve that a re-optimization mechanism is likely the most cost-effective way\\nto improve end-to-end query performance. We find that even a simple\\nre-optimization scheme can improve the latency of many poorly performing\\nqueries. We demonstrate that re-optimization improves the end-to-end latency of\\nthe top 20 longest running queries in the Join Order Benchmark by 27%,\\nrealizing most of the benefit of perfect cardinality estimation.', comment='Short version appearing in ICDE 2019', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1902.08291v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.08291v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.08318v6', updated=datetime.datetime(2020, 1, 2, 14, 56, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 22, 0, 24, 1, tzinfo=datetime.timezone.utc), title='Parsing Gigabytes of JSON per Second', authors=[arxiv.Result.Author('Geoff Langdale'), arxiv.Result.Author('Daniel Lemire')], summary='JavaScript Object Notation or JSON is a ubiquitous data exchange format on\\nthe Web. Ingesting JSON documents can become a performance bottleneck due to\\nthe sheer volume of data. We are thus motivated to make JSON parsing as fast as\\npossible.\\n  Despite the maturity of the problem of JSON parsing, we show that substantial\\nspeedups are possible. We present the first standard-compliant JSON parser to\\nprocess gigabytes of data per second on a single core, using commodity\\nprocessors. We can use a quarter or fewer instructions than a state-of-the-art\\nreference parser like RapidJSON. Unlike other validating parsers, our software\\n(simdjson) makes extensive use of Single Instruction, Multiple Data (SIMD)\\ninstructions. To ensure reproducibility, simdjson is freely available as\\nopen-source software under a liberal license.', comment='software: https://github.com/lemire/simdjson', journal_ref='The VLDB Journal, 28(6), 2019', doi='10.1007/s00778-019-00578-5', primary_category='cs.DB', categories=['cs.DB', 'cs.PF'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/s00778-019-00578-5', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1902.08318v6', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.08318v6', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.09586v1', updated=datetime.datetime(2019, 2, 25, 19, 43, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 25, 19, 43, 26, tzinfo=datetime.timezone.utc), title='Utility-driven Data Analytics on Uncertain Data', authors=[arxiv.Result.Author('Wensheng Gan'), arxiv.Result.Author('Jerry Chun-Wei Lin'), arxiv.Result.Author('Han-Chieh Chao'), arxiv.Result.Author('Athanasios V. Vasilakos'), arxiv.Result.Author('Philip S. Yu')], summary='Modern Internet of Things (IoT) applications generate massive amounts of\\ndata, much of it in the form of objects/items of readings, events, and log\\nentries. Specifically, most of the objects in these IoT data contain rich\\nembedded information (e.g., frequency and uncertainty) and different level of\\nimportance (e.g., unit utility of items, interestingness, cost, risk, or\\nweight). Many existing approaches in data mining and analytics have limitations\\nsuch as only the binary attribute is considered within a transaction, as well\\nas all the objects/items having equal weights or importance. To solve these\\ndrawbacks, a novel utility-driven data analytics algorithm named HUPNU is\\npresented, to extract High-Utility patterns by considering both Positive and\\nNegative unit utilities from Uncertain data. The qualified high-utility\\npatterns can be effectively discovered for risk prediction, manufacturing\\nmanagement, decision-making, among others. By using the developed vertical\\nProbability-Utility list with the Positive-and-Negative utilities structure, as\\nwell as several effective pruning strategies. Experiments showed that the\\ndeveloped HUPNU approach performed great in mining the qualified patterns\\nefficiently and effectively.', comment='Under review in IEEE Internet of Things Journal since 2018, 11 pages', journal_ref='IEEE Systems Journal, 2020', doi='10.1109/JSYST.2020.2979279', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/JSYST.2020.2979279', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1902.09586v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.09586v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.09711v1', updated=datetime.datetime(2019, 2, 26, 2, 44, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 26, 2, 44, 20, tzinfo=datetime.timezone.utc), title='Detecting Data Errors with Statistical Constraints', authors=[arxiv.Result.Author('Jing Nathan Yan'), arxiv.Result.Author('Oliver Schulte'), arxiv.Result.Author('Jiannan Wang'), arxiv.Result.Author('Reynold Cheng')], summary=\"A powerful approach to detecting erroneous data is to check which potentially\\ndirty data records are incompatible with a user's domain knowledge. Previous\\napproaches allow the user to specify domain knowledge in the form of logical\\nconstraints (e.g., functional dependency and denial constraints). We extend the\\nconstraint-based approach by introducing a novel class of statistical\\nconstraints (SCs). An SC treats each column as a random variable, and enforces\\nan independence or dependence relationship between two (or a few) random\\nvariables. Statistical constraints are expressive, allowing the user to specify\\na wide range of domain knowledge, beyond traditional integrity constraints.\\nFurthermore, they work harmoniously with downstream statistical modeling. We\\ndevelop CODED, an SC-Oriented Data Error Detection system that supports three\\nkey tasks: (1) Checking whether an SC is violated or not on a given dataset,\\n(2) Identify the top-k records that contribute the most to the violation of an\\nSC, and (3) Checking whether a set of input SCs have conflicts or not. We\\npresent effective solutions for each task. Experiments on synthetic and\\nreal-world data illustrate how SCs apply to error detection, and provide\\nevidence that CODED performs better than state-of-the-art approaches.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1902.09711v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.09711v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.10703v1', updated=datetime.datetime(2019, 2, 27, 11, 58, 7, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 27, 11, 58, 7, tzinfo=datetime.timezone.utc), title='Decentralized Evolution and Consolidation of RDF Graphs', authors=[arxiv.Result.Author('Natanael Arndt'), arxiv.Result.Author('Michael Martin')], summary='The World Wide Web and the Semantic Web are designed as a network of\\ndistributed services and datasets. In this network and its genesis,\\ncollaboration played and still plays a crucial role. But currently we only have\\ncentral collaboration solutions for RDF data, such as SPARQL endpoints and wiki\\nsystems, while decentralized solutions can enable applications for many more\\nuse-cases. Inspired by a successful distributed source code management\\nmethodology in software engineering a framework to support distributed\\nevolution is proposed. The system is based on Git and provides distributed\\ncollaboration on RDF graphs. This paper covers the formal expression of the\\nevolution and consolidation of distributed datasets, the synchronization, as\\nwell as other supporting operations.', comment='ICWE 2017. arXiv admin note: substantial text overlap with\\n  arXiv:1805.03721', journal_ref=None, doi='10.1007/978-3-319-60131-1_2', primary_category='cs.DB', categories=['cs.DB', '68M14'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-319-60131-1_2', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1902.10703v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.10703v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1902.10999v1', updated=datetime.datetime(2019, 2, 28, 10, 36, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 2, 28, 10, 36, 51, tzinfo=datetime.timezone.utc), title='Evaluation of Frequent Itemset Mining Platforms using Apriori and FP-Growth Algorithm', authors=[arxiv.Result.Author('Ravi Ranjan'), arxiv.Result.Author('Aditi Sharma')], summary='With the overwhelming amount of complex and heterogeneous data pouring from\\nany-where, any-time, and any-device, there is undeniably an era of Big Data.\\nThe emergence of the Big Data as a disruptive technology for next generation of\\nintelligent systems, has brought many issues of how to extract and make use of\\nthe knowledge obtained from the data within short times, limited budget and\\nunder high rates of data generation. Companies are recognizing that big data\\ncan be used to make more accurate predictions, and can be used to enhance the\\nbusiness with the help of appropriate association rule mining algorithm. To\\nhelp these organizations, with which software and algorithm is more appropriate\\nfor them depending on their dataset, we compared the most famous three\\nMapReduce based software Hadoop, Spark, Flink on two widely used algorithms\\nApriori and Fp-Growth on different scales of dataset.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1902.10999v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1902.10999v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1909.00271v1', updated=datetime.datetime(2019, 8, 31, 19, 43, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 31, 19, 43, 5, tzinfo=datetime.timezone.utc), title='Exploring Reproducibility and FAIR Principles in Data Science Using Ecological Niche Modeling as a Case Study', authors=[arxiv.Result.Author('Maria Luiza Mondelli'), arxiv.Result.Author('A. Townsend Peterson'), arxiv.Result.Author('Luiz M. R. Gadelha Jr')], summary='Reproducibility is a fundamental requirement of the scientific process since\\nit enables outcomes to be replicated and verified. Computational scientific\\nexperiments can benefit from improved reproducibility for many reasons,\\nincluding validation of results and reuse by other scientists. However,\\ndesigning reproducible experiments remains a challenge and hence the need for\\ndeveloping methodologies and tools that can support this process. Here, we\\npropose a conceptual model for reproducibility to specify its main attributes\\nand properties, along with a framework that allows for computational\\nexperiments to be findable, accessible, interoperable, and reusable. We present\\na case study in ecological niche modeling to demonstrate and evaluate the\\nimplementation of this framework.', comment='10 pages, 4 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1909.00271v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1909.00271v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1909.00607v1', updated=datetime.datetime(2019, 9, 2, 8, 59, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 9, 2, 8, 59, 17, tzinfo=datetime.timezone.utc), title='DeepDB: Learn from Data, not from Queries!', authors=[arxiv.Result.Author('Benjamin Hilprecht'), arxiv.Result.Author('Andreas Schmidt'), arxiv.Result.Author('Moritz Kulessa'), arxiv.Result.Author('Alejandro Molina'), arxiv.Result.Author('Kristian Kersting'), arxiv.Result.Author('Carsten Binnig')], summary='The typical approach for learned DBMS components is to capture the behavior\\nby running a representative set of queries and use the observations to train a\\nmachine learning model. This workload-driven approach, however, has two major\\ndownsides. First, collecting the training data can be very expensive, since all\\nqueries need to be executed on potentially large databases. Second, training\\ndata has to be recollected when the workload and the data changes. To overcome\\nthese limitations, we take a different route: we propose to learn a pure\\ndata-driven model that can be used for different tasks such as query answering\\nor cardinality estimation. This data-driven model also supports ad-hoc queries\\nand updates of the data without the need of full retraining when the workload\\nor data changes. Indeed, one may now expect that this comes at a price of lower\\naccuracy since workload-driven models can make use of more information.\\nHowever, this is not the case. The results of our empirical evaluation\\ndemonstrate that our data-driven approach not only provides better accuracy\\nthan state-of-the-art learned components but also generalizes better to unseen\\nqueries.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1909.00607v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1909.00607v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1909.00743v2', updated=datetime.datetime(2019, 9, 4, 12, 19, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 9, 2, 14, 45, 6, tzinfo=datetime.timezone.utc), title='Blended Integrated Open Data: dados abertos públicos integrados', authors=[arxiv.Result.Author('Fabiola Santore'), arxiv.Result.Author('Lucas F. Oliveira'), arxiv.Result.Author('Rafael de Paulo Dias'), arxiv.Result.Author('Henrique V. Ehrenfried'), arxiv.Result.Author('Alessandro Elias'), arxiv.Result.Author('Diego Pasqualin'), arxiv.Result.Author('Luis C. E. de Bona'), arxiv.Result.Author('Marcos Didonet Del Fabro'), arxiv.Result.Author('Marcos Sunye')], summary='While several public institutions provide its data openly, the effort\\nrequired to access, integrate and query this data is too high, reducing the\\namount of possible dataset users. The Blended Integrated Open Data (BIOD)\\nproject has as objective to ease the access to public Open Data. It integrates\\nand makes available more than 300Gb of data, containing billions of records\\nfrom different Open Data Sets, allowing to query over them, and thus to\\nretrieve related information from originally disconnected data sets. This paper\\npresents the set of open data available, how to access it and how produce new\\ncompatible data to improve the existing data set.', comment='8 pages. in Portuguese', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.SE'], links=[arxiv.Result.Link('http://arxiv.org/abs/1909.00743v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1909.00743v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1909.00841v4', updated=datetime.datetime(2020, 5, 5, 15, 25, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 9, 2, 19, 45, 55, tzinfo=datetime.timezone.utc), title='Approximate Query Service on Autonomous IoT Cameras', authors=[arxiv.Result.Author('Mengwei Xu'), arxiv.Result.Author('Xiwen Zhang'), arxiv.Result.Author('Yunxin Liu'), arxiv.Result.Author('Gang Huang'), arxiv.Result.Author('Xuanzhe Liu'), arxiv.Result.Author('Felix Xiaozhu Lin')], summary=\"Elf is a runtime for an energy-constrained camera to continuously summarize\\nvideo scenes as approximate object counts. Elf's novelty centers on planning\\nthe camera's count actions under energy constraint. (1) Elf explores the rich\\naction space spanned by the number of sample image frames and the choice of\\nper-frame object counters; it unifies errors from both sources into one single\\nbounded error. (2) To decide count actions at run time, Elf employs a\\nlearning-based planner, jointly optimizing for past and future videos without\\ndelaying result materialization. Tested with more than 1,000 hours of videos\\nand under realistic energy constraints, Elf continuously generates object\\ncounts within only 11% of the true counts on average. Alongside the counts, Elf\\npresents narrow errors shown to be bounded and up to 3.4x smaller than\\ncompetitive baselines. At a higher level, Elf makes a case for advancing the\\ngeographic frontier of video analytics.\", comment=None, journal_ref=None, doi='10.1145/3386901.3388948', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3386901.3388948', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1909.00841v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1909.00841v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1909.00955v1', updated=datetime.datetime(2019, 9, 3, 5, 1, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 9, 3, 5, 1, 44, tzinfo=datetime.timezone.utc), title='Los Angeles Metro Bus Data Analysis Using GPS Trajectory and Schedule Data (Demo Paper)', authors=[arxiv.Result.Author('Kien Nguyen'), arxiv.Result.Author('Jingyun Yang'), arxiv.Result.Author('Yijun Lin'), arxiv.Result.Author('Jianfa Lin'), arxiv.Result.Author('Yao-Yi Chiang'), arxiv.Result.Author('Cyrus Shahabi')], summary='With the widespread installation of location-enabled devices on public\\ntransportation, public vehicles are generating massive amounts of trajectory\\ndata in real time. However, using these trajectory data for meaningful analysis\\nrequires careful considerations in storing, managing, processing, and\\nvisualizing the data. Using the location data of the Los Angeles Metro bus\\nsystem, along with publicly available bus schedule data, we conduct a data\\nprocessing and analyses study to measure the performance of the public\\ntransportation system in Los Angeles utilizing a number of metrics including\\ntravel-time reliability, on-time performance, bus bunching, and travel-time\\nestimation. We demonstrate the visualization of the data analysis results\\nthrough an interactive web-based application. The developed algorithms and\\nsystem provide powerful tools to detect issues and improve the efficiency of\\npublic transportation systems.', comment=\"SIGSPATIAL'18, demo paper, 4 pages\", journal_ref=\"26th ACM SIGSPATIAL International Conference on Advances in\\n  Geographic Information Systems (SIGSPATIAL '18), 2018\", doi='10.1145/3274895.3274911', primary_category='cs.DB', categories=['cs.DB', 'cs.CY'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3274895.3274911', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1909.00955v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1909.00955v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1909.00985v1', updated=datetime.datetime(2019, 9, 3, 7, 28, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 9, 3, 7, 28, 23, tzinfo=datetime.timezone.utc), title='Finding Maximal Non-Redundant Association Rules in Tennis Data', authors=[arxiv.Result.Author('Daniel Weidner'), arxiv.Result.Author('Martin Atzmueller'), arxiv.Result.Author('Dietmar Seipel')], summary='The concept of association rules is well--known in data mining. But often\\nredundancy and subsumption are not considered, and standard approaches produce\\nthousands or even millions of resulting association rules. Without further\\ninformation or post--mining approaches, this huge number of rules is typically\\nuseless for the domain specialist -- which is an instance of the infamous\\npattern explosion problem. In this work, we present a new definition of\\nredundancy and subsumption based on the confidence and the support of the rules\\nand propose post-- mining to prune a set of association rules. In a case study,\\nwe apply our method to association rules mined from spatio--temporal data. The\\ndata represent the trajectories of the ball in tennis matches -- more\\nprecisely, the points/times the tennis ball hits the ground. The goal is to\\nanalyze the strategies of the players and to try to improve their performance\\nby looking at the resulting association rules. The proposed approach is\\ngeneral, and can also be applied to other spatio--temporal data with a similar\\nstructure.', comment='Part of DECLARE 19 proceedings', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1909.00985v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1909.00985v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1909.04881v3', updated=datetime.datetime(2022, 7, 20, 18, 43, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 9, 11, 7, 29, 28, tzinfo=datetime.timezone.utc), title='Algebraic Property Graphs', authors=[arxiv.Result.Author('Joshua Shinavier'), arxiv.Result.Author('Ryan Wisnesky'), arxiv.Result.Author('Joshua G. Meyers')], summary='We present a case study in applied category theory written from the point of\\nview of an applied domain: the formalization of the widely-used property graphs\\ndata model in an enterprise setting using elementary constructions from type\\ntheory and category theory, including limit and co-limit sketches. Observing\\nthat algebraic data types are a common foundation of most of the enterprise\\nschema languages we deal with in practice, for graph data or otherwise, we\\nintroduce a type theory for algebraic property graphs wherein the types denote\\nboth algebraic data types in the sense of functional programming and join-union\\nE/R diagrams in the sense of database theory. We also provide theoretical\\nfoundations for graph transformation along schema mappings with by-construction\\nguarantees of semantic consistency. Our data model originated as a\\nformalization of a data integration toolkit developed at Uber which carries\\ndata and schemas along composable mappings between data interchange languages\\nsuch as Apache Avro, Apache Thrift, and Protocol Buffers, and graph languages\\nincluding RDF with OWL or SHACL-based schemas.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1909.04881v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1909.04881v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1909.05380v1', updated=datetime.datetime(2019, 9, 11, 21, 25, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 9, 11, 21, 25, 40, tzinfo=datetime.timezone.utc), title='Selecting Data to Clean for Fact Checking: Minimizing Uncertainty vs. Maximizing Surprise', authors=[arxiv.Result.Author('Stavros Sintos'), arxiv.Result.Author('Pankaj K. Agarwal'), arxiv.Result.Author('Jun Yang')], summary='We study the optimization problem of selecting numerical quantities to clean\\nin order to fact-check claims based on such data. Oftentimes, such claims are\\ntechnically correct, but they can still mislead for two reasons. First, data\\nmay contain uncertainty and errors. Second, data can be \"fished\" to advance\\nparticular positions. In practice, fact-checkers cannot afford to clean all\\ndata and must choose to clean what \"matters the most\" to checking a claim. We\\nexplore alternative definitions of what \"matters the most\": one is to ascertain\\nclaim qualities (by minimizing uncertainty in these measures), while an\\nalternative is just to counter the claim (by maximizing the probability of\\nfinding a counterargument). We show whether the two objectives align with each\\nother, with important implications on when fact-checkers should exercise care\\nin selective data cleaning, to avoid potential bias introduced by their desire\\nto counter claims. We develop efficient algorithms for solving the various\\nvariants of the optimization problem, showing significant improvements over\\nnaive solutions. The problem is particularly challenging because the objectives\\nin the fact-checking context are complex, non-linear functions over data. We\\nobtain results that generalize to a large class of functions, with potential\\napplications beyond fact-checking.', comment=None, journal_ref=None, doi='10.14778/3358701.3358708', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3358701.3358708', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1909.05380v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1909.05380v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1909.06182v1', updated=datetime.datetime(2019, 9, 11, 22, 1, 58, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 9, 11, 22, 1, 58, tzinfo=datetime.timezone.utc), title='DBPal: Weak Supervision for Learning a Natural Language Interface to Databases', authors=[arxiv.Result.Author('Nathaniel Weir'), arxiv.Result.Author('Andrew Crotty'), arxiv.Result.Author('Alex Galakatos'), arxiv.Result.Author('Amir Ilkhechi'), arxiv.Result.Author('Shekar Ramaswamy'), arxiv.Result.Author('Rohin Bhushan'), arxiv.Result.Author('Ugur Cetintemel'), arxiv.Result.Author('Prasetya Utama'), arxiv.Result.Author('Nadja Geisler'), arxiv.Result.Author('Benjamin Hättasch'), arxiv.Result.Author('Steffen Eger'), arxiv.Result.Author('Carsten Binnig')], summary='This paper describes DBPal, a new system to translate natural language\\nutterances into SQL statements using a neural machine translation model. While\\nother recent approaches use neural machine translation to implement a Natural\\nLanguage Interface to Databases (NLIDB), existing techniques rely on supervised\\nlearning with manually curated training data, which results in substantial\\noverhead for supporting each new database schema. In order to avoid this issue,\\nDBPal implements a novel training pipeline based on weak supervision that\\nsynthesizes all training data from a given database schema. In our evaluation,\\nwe show that DBPal can outperform existing rule-based NLIDBs while achieving\\ncomparable performance to other NLIDBs that leverage deep neural network models\\nwithout relying on manually curated training data for every new database\\nschema.', comment='arXiv admin note: text overlap with arXiv:1804.00401', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1909.06182v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1909.06182v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1909.06610v4', updated=datetime.datetime(2020, 7, 14, 15, 41, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 9, 14, 15, 19, 23, tzinfo=datetime.timezone.utc), title='Harmonise and integrate heterogeneous areal data with the R package arealDB', authors=[arxiv.Result.Author('Steffen Ehrmann'), arxiv.Result.Author('Ralf Seppelt'), arxiv.Result.Author('Carsten Meyer')], summary='Many relevant applications in the environmental and socioeconomic sciences\\nuse areal data, such as biodiversity checklists, agricultural statistics, or\\nsocioeconomic surveys. For applications that surpass the spatial, temporal or\\nthematic scope of any single data source, data must be integrated from several\\nheterogeneous sources. Inconsistent concepts, definitions, or messy data tables\\nmake this a tedious and error-prone process. To date, a dedicated tool to\\naddress these challenges is still lacking. Here, we introduce the R package\\narealDB that integrates heterogeneous areal data and associated geometries into\\na consistent database, in an easy-to-use workflow. It is useful for harmonising\\nlanguage and semantics of variables, relating data to geometries, and\\ndocumenting metadata and provenance. We illustrate the functionality by\\nintegrating two disparate datasets (Brazil, USA) on the harvested area of\\nsoybean. arealDB promises quality-improvements to downstream scientific,\\nmonitoring, and management applications but also substantial time-savings to\\ndatabase collation efforts.', comment='14 pages, 3 supplements, 6 figures, R-package', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'stat.AP'], links=[arxiv.Result.Link('http://arxiv.org/abs/1909.06610v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1909.06610v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1909.06997v4', updated=datetime.datetime(2022, 4, 17, 15, 18, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 9, 16, 5, 39, 49, tzinfo=datetime.timezone.utc), title='MVDLite: a Fast Validation Algorithm for Model View Definition Rules', authors=[arxiv.Result.Author('Han Liu'), arxiv.Result.Author('Ge Gao'), arxiv.Result.Author('Hehua Zhang'), arxiv.Result.Author('Yu-Shen Liu'), arxiv.Result.Author('Yan Song'), arxiv.Result.Author('Ming Gu')], summary='Model View Definition (MVD) is the standard methodology to define the data\\nexchange requirements and rule constraints for Building Information Models\\n(BIMs). In this paper, the MVDLite algorithm is proposed for the fast\\nvalidation of MVD rules. A \"rule chain\" structure is introduced to combine the\\ndata templates, constraint statements, and logical interconnections in an input\\nmvdXML ruleset, which leads to fast filtering of data nodes through the rule\\nchain. By establishing the correspondence of each prefix of the rule chain with\\na string, the deep-caching strategy further improves efficiency. The\\noutperforming experimental results show that our algorithm significantly\\nreduces the running time of MVD validation on large real-world BIMs.', comment='Preprint submitted to 29th International Workshop on Intelligent\\n  Computing in Engineering (EG-ICE)', journal_ref=None, doi='10.7146/aul.455.c192', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.7146/aul.455.c192', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1909.06997v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1909.06997v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1909.08006v1', updated=datetime.datetime(2019, 9, 17, 18, 10, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 9, 17, 18, 10, 4, tzinfo=datetime.timezone.utc), title='Leyenda: An Adaptive, Hybrid Sorting Algorithm for Large Scale Data with Limited Memory', authors=[arxiv.Result.Author('Yuanjing Shi'), arxiv.Result.Author('Zhaoxing Li')], summary=\"Sorting is the one of the fundamental tasks of modern data management\\nsystems. With Disk I/O being the most-accused performance bottleneck and more\\ncomputation-intensive workloads, it has come to our attention that in\\nheterogeneous environment, performance bottleneck may vary among different\\ninfrastructure. As a result, sort kernels need to be adaptive to changing\\nhardware conditions. In this paper, we propose Leyenda, a hybrid, parallel and\\nefficient Radix Most-Significant-Bit (MSB) MergeSort algorithm, with\\nutilization of local thread-level CPU cache and efficient disk/memory I/O.\\nLeyenda is capable of performing either internal or external sort efficiently,\\nbased on different I/O and processing conditions. We benchmarked Leyenda with\\nthree different workloads from Sort Benchmark, targeting three unique use\\ncases, including internal, partially in-memory and external sort, and we found\\nLeyenda to outperform GNU's parallel in-memory quick/merge sort implementations\\nby up to three times. Leyenda is also ranked the second best external sort\\nalgorithm on ACM 2019 SIGMOD programming contest and forth overall.\", comment='5 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC', 'cs.DS'], links=[arxiv.Result.Link('http://arxiv.org/abs/1909.08006v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1909.08006v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1909.09377v1', updated=datetime.datetime(2019, 9, 20, 9, 6, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 9, 20, 9, 6, 28, tzinfo=datetime.timezone.utc), title='Metadata Systems for Data Lakes: Models and Features', authors=[arxiv.Result.Author('Pegdwendé Sawadogo'), arxiv.Result.Author('Etienne Scholly'), arxiv.Result.Author('Cécile Favre'), arxiv.Result.Author('Eric Ferey'), arxiv.Result.Author('Sabine Loudcher'), arxiv.Result.Author('Jérôme Darmont')], summary='Over the past decade, the data lake concept has emerged as an alternative to\\ndata warehouses for storing and analyzing big data. A data lake allows storing\\ndata without any predefined schema. Therefore, data querying and analysis\\ndepend on a metadata system that must be efficient and comprehensive. However,\\nmetadata management in data lakes remains a current issue and the criteria for\\nevaluating its effectiveness are more or less nonexistent.In this paper, we\\nintroduce MEDAL, a generic, graph-based model for metadata management in data\\nlakes. We also propose evaluation criteria for data lake metadata systems\\nthrough a list of expected features. Eventually, we show that our approach is\\nmore comprehensive than existing metadata systems.', comment=None, journal_ref='1st International Workshop on BI and Big Data Applications\\n  (BBIGAP@ADBIS 2019), Sep 2019, Bled, Slovenia. pp.440-451', doi='10.1007/978-3-030-30278-8', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-030-30278-8', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1909.09377v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1909.09377v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1909.11057v1', updated=datetime.datetime(2019, 9, 24, 17, 16, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 9, 24, 17, 16, 40, tzinfo=datetime.timezone.utc), title='A Rule-Based Relational XML Access Control Model in the Presence of Authorization Conflicts', authors=[arxiv.Result.Author('Ali Alwehaibi'), arxiv.Result.Author('Mustafa Atay')], summary='There is considerable amount of sensitive XML data stored in relational\\ndatabases. It is a challenge to enforce node level fine-grained authorization\\npolicies for XML data stored in relational databases which typically support\\ntable and column level access control. Moreover, it is common to have\\nconflicting authorization policies over the hierarchical nested structure of\\nXML data. There are a couple of XML access control models for relational XML\\ndatabases proposed in the literature. However, to our best knowledge, none of\\nthem discussed handling authorization conflicts with conditions in the domain\\nof relational XML databases. Therefore, we believe that there is a need to\\ndefine and incorporate effective fine-grained XML authorization models with\\nconflict handling mechanisms in the presence of conditions into relational XML\\ndatabases. We address this issue in this study.', comment='14th International Conference on Information Technology - New\\n  Generations, Las Vegas, NV, April 10-12, 2017, Published by Springer, Cham; 6\\n  pages, 5 figures, 2 tables', journal_ref='Advances in Intelligent Systems and Computing Information\\n  Technology - New Generations, 311-319 (2017)', doi='10.1007/978-3-319-54978-1_43', primary_category='cs.DB', categories=['cs.DB', 'cs.IR', 'K.6.5; H.3.2; H.3.3; H.2.3'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-319-54978-1_43', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1909.11057v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1909.11057v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1909.11224v2', updated=datetime.datetime(2019, 9, 26, 1, 23, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 9, 24, 23, 17, 29, tzinfo=datetime.timezone.utc), title='Skyline Queries Over Incomplete Data Streams (Technical Report)', authors=[arxiv.Result.Author('Weilong Ren'), arxiv.Result.Author('Xiang Lian'), arxiv.Result.Author('Kambiz Ghazinour')], summary='Nowadays, efficient and effective processing over massive stream data has\\nattracted much attention from the database community, which are useful in many\\nreal applications such as sensor data monitoring, network intrusion detection,\\nand so on. In practice, due to the malfunction of sensing devices or imperfect\\ndata collection techniques, real-world stream data may often contain missing or\\nincomplete data attributes. In this paper, we will formalize and tackle a novel\\nand important problem, named skyline query over incomplete data stream\\n(Sky-iDS), which retrieves skyline objects (in the presence of missing\\nattributes) with high confidences from incomplete data stream. In order to\\ntackle the Sky-iDS problem, we will design efficient approaches to impute\\nmissing attributes of objects from incomplete data stream via differential\\ndependency (DD) rules. We will propose effective pruning strategies to reduce\\nthe search space of the Sky-iDS problem, devise cost-model-based index\\nstructures to facilitate the data imputation and skyline computation at the\\nsame time, and integrate our proposed techniques into an efficient Sky-iDS\\nquery answering algorithm. Extensive experiments have been conducted to confirm\\nthe efficiency and effectiveness of our Sky-iDS processing approach over both\\nreal and synthetic data sets.', comment='26 pages, 20 figures, VLDB Journal', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1909.11224v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1909.11224v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1909.11567v5', updated=datetime.datetime(2022, 4, 8, 8, 53, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 9, 20, 8, 42, 53, tzinfo=datetime.timezone.utc), title='Discovering Process Models from Uncertain Event Data', authors=[arxiv.Result.Author('Marco Pegoraro'), arxiv.Result.Author('Merih Seran Uysal'), arxiv.Result.Author('Wil M. P. van der Aalst')], summary='Modern information systems are able to collect event data in the form of\\nevent logs. Process mining techniques allow to discover a model from event\\ndata, to check the conformance of an event log against a reference model, and\\nto perform further process-centric analyses. In this paper, we consider\\nuncertain event logs, where data is recorded together with explicit uncertainty\\ninformation. We describe a technique to discover a directly-follows graph from\\nsuch event data which retains information about the uncertainty in the process.\\nWe then present experimental results of performing inductive mining over the\\ndirectly-follows graph to obtain models representing the certain and uncertain\\npart of the process.', comment='12 pages, 7 figures, 1 table, 9 references', journal_ref='Business Process Management Workshops (2019) 238-249', doi='10.1007/978-3-030-37453-2_20', primary_category='cs.DB', categories=['cs.DB', 'cs.SE'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-030-37453-2_20', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1909.11567v5', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1909.11567v5', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1806.00227v1', updated=datetime.datetime(2018, 6, 1, 7, 56, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 6, 1, 7, 56, 22, tzinfo=datetime.timezone.utc), title='SaGe: Preemptive Query Execution for High Data Availability on the Web', authors=[arxiv.Result.Author('Thomas Minier'), arxiv.Result.Author('Hala Skaf-Molli'), arxiv.Result.Author('Pascal Molli')], summary='Semantic Web applications require querying available RDF Data with high\\nperformance and reliability. However, ensuring both data availability and\\nperformant SPARQL query execution in the context of public SPARQL servers are\\nchallenging problems. Queries could have arbitrary execution time and unknown\\narrival rates. In this paper, we propose SaGe, a preemptive server-side SPARQL\\nquery engine. SaGe relies on a preemptable physical query execution plan and\\npreemptable physical operators. SaGe stops query execution after a given slice\\nof time, saves the state of the plan and sends the saved plan back to the\\nclient with retrieved results. Later, the client can continue the query\\nexecution by resubmitting the saved plan to the server. By ensuring a fair\\nquery execution, SaGe maintains server availability and provides high query\\nthroughput. Experimental results demonstrate that SaGe outperforms the state of\\nthe art SPARQL query engines in terms of query throughput, query timeout and\\nanswer completeness.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1806.00227v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1806.00227v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1806.00637v1', updated=datetime.datetime(2018, 6, 2, 14, 1, 49, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 6, 2, 14, 1, 49, tzinfo=datetime.timezone.utc), title='Quality-Assured Synchronized Task Assignment in Crowdsourcing', authors=[arxiv.Result.Author('Jiayang Tu'), arxiv.Result.Author('Peng Cheng'), arxiv.Result.Author('Lei Chen')], summary='With the rapid development of crowdsourcing platforms that aggregate the\\nintelligence of Internet workers, crowdsourcing has been widely utilized to\\naddress problems that require human cognitive abilities. Considering great\\ndynamics of worker arrival and departure, it is of vital importance to design a\\ntask assignment scheme to adaptively select the most beneficial tasks for the\\navailable workers. In this paper, in order to make the most efficient\\nutilization of the worker labor and balance the accuracy of answers and the\\noverall latency, we a) develop a parameter estimation model that assists in\\nestimating worker expertise, question easiness and answer confidence; b)\\npropose a \\\\textit{quality-assured synchronized task assignment scheme} that\\nexecutes in batches and maximizes the number of potentially completed questions\\n(MCQ) within each batch. We prove that MCQ problem is NP-hard and present two\\ngreedy approximation solutions to address the problem. The effectiveness and\\nefficiency of the approximation solutions are further evaluated through\\nextensive experiments on synthetic and real datasets. The experimental results\\nshow that the accuracy and the overall latency of the MCQ approaches outperform\\nthe existing online task assignment algorithms in the synchronized task\\nassignment scenario.', comment='12 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1806.00637v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1806.00637v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1806.01657v1', updated=datetime.datetime(2018, 6, 5, 12, 42, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 6, 5, 12, 42, 45, tzinfo=datetime.timezone.utc), title='Native Directly Follows Operator', authors=[arxiv.Result.Author('Alifah Syamsiyah'), arxiv.Result.Author('Boudewijn F. van Dongen'), arxiv.Result.Author('Remco M. Dijkman')], summary='Typical legacy information systems store data in relational databases.\\nProcess mining is a research discipline that analyzes this data to obtain\\ninsights into processes. Many different process mining techniques can be\\napplied to data. In current techniques, an XES event log serves as a basis for\\nanalysis. However, because of the static characteristic of an XES event log, we\\nneed to create one XES file for each process mining question, which leads to\\noverhead and inflexibility. As an alternative, people attempt to perform\\nprocess mining directly on the data source using so-called intermediate\\nstructures. In previous work, we investigated methods to build intermediate\\nstructures on source data by executing a basic SQL query on the database.\\nHowever, the nested form in the SQL query can cause performance issues on the\\ndatabase side. Therefore, in this paper, we propose a native SQL operator for\\ndirect process discovery on relational databases. We define a native operator\\nfor the simplest form of the intermediate structure, called the \"directly\\nfollows relation\". This approach has been evaluated with big event data and the\\nexperimental results show that it performs faster than the state-of-the-art of\\ndatabase approaches.', comment='12 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1806.01657v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1806.01657v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1806.02227v1', updated=datetime.datetime(2018, 6, 6, 14, 45, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 6, 6, 14, 45, 46, tzinfo=datetime.timezone.utc), title='Curator: Provenance Management for Modern Distributed Systems', authors=[arxiv.Result.Author('Warren Smith'), arxiv.Result.Author('Thomas Moyer'), arxiv.Result.Author('Charles Munson')], summary='Data provenance is a valuable tool for protecting and troubleshooting\\ndistributed systems. Careful design of the provenance components reduces the\\nimpact on the design, implementation, and operation of the distributed system.\\nIn this paper, we present Curator, a provenance management toolkit that can be\\neasily integrated with microservice-based systems and other modern distributed\\nsystems. This paper describes the design of Curator and discusses how we have\\nused Curator to add provenance to distributed systems. We find that our\\napproach results in no changes to the design of these distributed systems and\\nminimal additional code and dependencies to manage. In addition, Curator uses\\nthe same scalable infrastructure as the distributed system and can therefore\\nscale with the distributed system.', comment='Published at TaPP 2018, 6 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1806.02227v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1806.02227v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1806.04004v1', updated=datetime.datetime(2018, 6, 11, 14, 10, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 6, 11, 14, 10, 1, tzinfo=datetime.timezone.utc), title='PubMed Labs: An experimental platform for improving biomedical literature search', authors=[arxiv.Result.Author('Nicolas Fiorini'), arxiv.Result.Author('Kathi Canese'), arxiv.Result.Author('Rostyslav Bryzgunov'), arxiv.Result.Author('Ievgeniia Radetska'), arxiv.Result.Author('Asta Gindulyte'), arxiv.Result.Author('Martin Latterner'), arxiv.Result.Author('Vadim Miller'), arxiv.Result.Author('Maxim Osipov'), arxiv.Result.Author('Michael Kholodov'), arxiv.Result.Author('Grisha Starchenko'), arxiv.Result.Author('Evgeny Kireev'), arxiv.Result.Author('Zhiyong Lu')], summary='PubMed is a freely accessible system for searching the biomedical literature,\\nwith approximately 2.5 million users worldwide on an average workday. We have\\nrecently developed PubMed Labs (www.pubmed.gov/labs), an experimental platform\\nfor users to test new features/tools and provide feedback, which enables us to\\nmake more informed decisions about potential changes to improve the search\\nquality and overall usability of PubMed. In doing so, we hope to better meet\\nour user needs in an era of information overload. Another novel aspect of\\nPubMed Labs lies in its mobile-first and responsive layout, which offers better\\nsupport for accessing PubMed on the increasingly popular use of mobile and\\nsmall-screen devices. Currently, PubMed Labs only includes a core subset of\\nPubMed functionalities, e.g. search, facets. We encourage users to test PubMed\\nLabs and share their experience with us, based on which we expect to\\ncontinuously improve PubMed Labs with more advanced features and better user\\nexperience.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DL', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1806.04004v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1806.04004v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1806.04761v1', updated=datetime.datetime(2018, 6, 12, 20, 38, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 6, 12, 20, 38, 56, tzinfo=datetime.timezone.utc), title='Performance evaluation for CRUD operations in asynchronously replicated document oriented database', authors=[arxiv.Result.Author('Ciprian-Octavian Truică'), arxiv.Result.Author('Florin Rădulescu'), arxiv.Result.Author('Alexandru Boicea'), arxiv.Result.Author('Ion Bucur')], summary='NoSQL databases are becoming increasingly popular as more developers seek new\\nways for storing information. The popularity of these databases has risen due\\nto their flexibility and scalability needed in domains like Big Data and Cloud\\nComputing. This paper examines asynchronous replication, one of the key\\nfeatures for a scalable and flexible system. Three of the most popular\\nDocument-Oriented Databases, MongoDB, CouchDB, and Couchbase, are examined. For\\ntesting, the execution time for CRUD operations for a single database instance\\nand for a distributed environment with two nodes is taken into account and the\\nresults are compared with tests outcomes obtained for three relational database\\nmanagement systems: Microsoft SQL Server, MySQL, and PostgreSQL.', comment=None, journal_ref=None, doi='10.1109/CSCS.2015.32', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/CSCS.2015.32', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1806.04761v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1806.04761v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1806.04952v1', updated=datetime.datetime(2018, 6, 13, 11, 19, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 6, 13, 11, 19, 33, tzinfo=datetime.timezone.utc), title='Towards Semantically Enhanced Data Understanding', authors=[arxiv.Result.Author('Markus Schröder'), arxiv.Result.Author('Christian Jilek'), arxiv.Result.Author('Jörn Hees'), arxiv.Result.Author('Andreas Dengel')], summary='In the field of machine learning, data understanding is the practice of\\ngetting initial insights in unknown datasets. Such knowledge-intensive tasks\\nrequire a lot of documentation, which is necessary for data scientists to grasp\\nthe meaning of the data. Usually, documentation is separate from the data in\\nvarious external documents, diagrams, spreadsheets and tools which causes\\nconsiderable look up overhead. Moreover, other supporting applications are not\\nable to consume and utilize such unstructured data. That is why we propose a\\nmethodology that uses a single semantic model that interlinks data with its\\ndocumentation. Hence, data scientists are able to directly look up the\\nconnected information about the data by simply following links. Equally, they\\ncan browse the documentation which always refers to the data. Furthermore, the\\nmodel can be used by other approaches providing additional support, like\\nsearching, comparing, integrating or visualizing data. To showcase our approach\\nwe also demonstrate an early prototype.', comment='4 pages, 3 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1806.04952v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1806.04952v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1806.04968v2', updated=datetime.datetime(2018, 10, 19, 2, 31, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 6, 13, 12, 4, 20, tzinfo=datetime.timezone.utc), title='Crowd-Powered Data Mining', authors=[arxiv.Result.Author('Chengliang Chai'), arxiv.Result.Author('Ju Fan'), arxiv.Result.Author('Guoliang Li'), arxiv.Result.Author('Jiannan Wang'), arxiv.Result.Author('Yudian Zheng')], summary='Many data mining tasks cannot be completely addressed by auto- mated\\nprocesses, such as sentiment analysis and image classification. Crowdsourcing\\nis an effective way to harness the human cognitive ability to process these\\nmachine-hard tasks. Thanks to public crowdsourcing platforms, e.g., Amazon\\nMechanical Turk and Crowd- Flower, we can easily involve hundreds of thousands\\nof ordinary workers (i.e., the crowd) to address these machine-hard tasks. In\\nthis tutorial, we will survey and synthesize a wide spectrum of existing\\nstudies on crowd-powered data mining. We first give an overview of\\ncrowdsourcing, and then summarize the fundamental techniques, including quality\\ncontrol, cost control, and latency control, which must be considered in\\ncrowdsourced data mining. Next we review crowd-powered data mining operations,\\nincluding classification, clustering, pattern mining, machine learning using\\nthe crowd (including deep learning, transfer learning and semi-supervised\\nlearning) and knowledge discovery. Finally, we provide the emerging challenges\\nin crowdsourced data mining.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1806.04968v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1806.04968v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1806.05918v2', updated=datetime.datetime(2018, 6, 18, 6, 16, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 6, 15, 11, 48, 39, tzinfo=datetime.timezone.utc), title='Efficient Handling of SPARQL OPTIONAL for OBDA (Extended Version)', authors=[arxiv.Result.Author('Guohui Xiao'), arxiv.Result.Author('Roman Kontchakov'), arxiv.Result.Author('Benjamin Cogrel'), arxiv.Result.Author('Diego Calvanese'), arxiv.Result.Author('Elena Botoeva')], summary='OPTIONAL is a key feature in SPARQL for dealing with missing information.\\nWhile this operator is used extensively, it is also known for its complexity,\\nwhich can make efficient evaluation of queries with OPTIONAL challenging. We\\ntackle this problem in the Ontology-Based Data Access (OBDA) setting, where the\\ndata is stored in a SQL relational database and exposed as a virtual RDF graph\\nby means of an R2RML mapping. We start with a succinct translation of a SPARQL\\nfragment into SQL. It fully respects bag semantics and three-valued logic and\\nrelies on the extensive use of the LEFT JOIN operator and COALESCE function. We\\nthen propose optimisation techniques for reducing the size and improving the\\nstructure of generated SQL queries. Our optimisations capture interactions\\nbetween JOIN, LEFT JOIN, COALESCE and integrity constraints such as attribute\\nnullability, uniqueness and foreign key constraints. Finally, we empirically\\nverify effectiveness of our techniques on the BSBM OBDA benchmark.', comment='technical report for ISWC 2018 paper', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1806.05918v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1806.05918v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1806.06205v1', updated=datetime.datetime(2018, 6, 16, 8, 17, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 6, 16, 8, 17, 8, tzinfo=datetime.timezone.utc), title='TrQuery: An Embedding-based Framework for Recommanding SPARQL Queries', authors=[arxiv.Result.Author('Lijing Zhang'), arxiv.Result.Author('Xiaowang Zhang'), arxiv.Result.Author('Zhiyong Feng')], summary='In this paper, we present an embedding-based framework (TrQuery) for\\nrecommending solutions of a SPARQL query, including approximate solutions when\\nexact querying solutions are not available due to incompleteness or\\ninconsistencies of real-world RDF data. Within this framework, embedding is\\napplied to score solutions together with edit distance so that we could obtain\\nmore fine-grained recommendations than those recommendations via edit distance.\\nFor instance, graphs of two querying solutions with a similar structure can be\\ndistinguished in our proposed framework while the edit distance depending on\\nstructural difference becomes unable. To this end, we propose a novel score\\nmodel built on vector space generated in embedding system to compute the\\nsimilarity between an approximate subgraph matching and a whole graph matching.\\nFinally, we evaluate our approach on large RDF datasets DBpedia and YAGO, and\\nexperimental results show that TrQuery exhibits an excellent behavior in terms\\nof both effectiveness and efficiency.', comment='17 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', '68T05'], links=[arxiv.Result.Link('http://arxiv.org/abs/1806.06205v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1806.06205v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1806.07084v1', updated=datetime.datetime(2018, 6, 19, 7, 50, 48, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 6, 19, 7, 50, 48, tzinfo=datetime.timezone.utc), title='Itemsets of interest for negative association rules', authors=[arxiv.Result.Author('Hyeok Kong'), arxiv.Result.Author('Dokjun An'), arxiv.Result.Author('Jihyang Ri')], summary='So far, most of association rule minings have considered about positive\\nassociation rules based on frequent itemsets in databases[2,5-7], but they have\\nnot considered the problem of mining negative association rules correlated with\\nfrequent and infrequent itemsets. Negative association rule mining is much more\\ndifficult than positive association rule mining because it needs infrequent\\nitemsets, and only the rare association rule mining which is a kind of negative\\nassociation rule minings has been studied. This paper presents a mathematical\\nmodel to mine positive and negative association rules precisely, for which in a\\npoint of view that negation of a frequent itemset is an infrequent itemset, we\\nmake clear the importance of the problem of mining negative association rules\\nbased on certain infrequent itemsets and study on what conditions infrequent\\nitemsets of interest should satisfy for negative association rules.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1806.07084v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1806.07084v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1806.07524v1', updated=datetime.datetime(2018, 6, 20, 2, 14, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 6, 20, 2, 14, 9, tzinfo=datetime.timezone.utc), title='Developing a Temporal Bibliographic Data Set for Entity Resolution', authors=[arxiv.Result.Author('Yichen Hu'), arxiv.Result.Author('Qing Wang'), arxiv.Result.Author('Peter Christen')], summary='Entity resolution is the process of identifying groups of records within or\\nacross data sets where each group represents a real-world entity. Novel\\ntechniques that consider temporal features to improve the quality of entity\\nresolution have recently attracted significant attention. However, there are\\ncurrently no large data sets available that contain both temporal information\\nas well as ground truth information to evaluate the quality of temporal entity\\nresolution approaches. In this paper, we describe the preparation of a temporal\\ndata set based on author profiles extracted from the Digital Bibliography and\\nLibrary Project (DBLP). We completed missing links between publications and\\nauthor profiles in the DBLP data set using the DBLP public API. We then used\\nthe Microsoft Academic Graph (MAG) to link temporal affiliation information for\\nDBLP authors. We selected around 80K (1%) of author profiles that cover 2\\nmillion (50%) publications using information in DBLP such as alternative author\\nnames and personal web profile to improve the reliability of the resulting\\nground truth, while at the same time keeping the data set challenging for\\ntemporal entity resolution research.', comment='7 pages, 6 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DL'], links=[arxiv.Result.Link('http://arxiv.org/abs/1806.07524v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1806.07524v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1806.07691v1', updated=datetime.datetime(2018, 6, 20, 12, 24, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 6, 20, 12, 24, 20, tzinfo=datetime.timezone.utc), title='Searching of interesting itemsets for negative association rules', authors=[arxiv.Result.Author('Hyeok Kong'), arxiv.Result.Author('Dokjun An'), arxiv.Result.Author('Douk Han')], summary='In this paper, we propose an algorithm of searching for both positive and\\nnegative itemsets of interest which should be given at the first stage for\\npositive and negative association rules mining. Traditional association rule\\nmining algorithms extract positive association rules based on frequent\\nitemsets, for which the frequent itemsets, i.e. only positive itemsets of\\ninterest are searched. Further, there are useful itemsets among the frequent\\nitemsets pruned from the traditional algorithms to reduce the search space, for\\nmining of negative association rules. Therefore, the traditional algorithms\\nhave not come true to find negative itemsets needed in mining of negative\\nassociation rules. Our new algorithm to search for both positive and negative\\nitemsets of interest prepares preconditions for mining of all positive and\\nnegative association rules.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1806.07691v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1806.07691v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1806.08384v1', updated=datetime.datetime(2018, 6, 21, 18, 34, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 6, 21, 18, 34, 3, tzinfo=datetime.timezone.utc), title='Novel Selectivity Estimation Strategy for Modern DBMS', authors=[arxiv.Result.Author('Jun Hyung Shin')], summary='Selectivity estimation is important in query optimization, however accurate\\nestimation is difficult when predicates are complex. Instead of existing\\ndatabase synopses and statistics not helpful for such cases, we introduce a new\\napproach to compute the exact selectivity by running an aggregate query during\\nthe optimization phase. Exact selectivity can be achieved without significant\\noverhead for in-memory and GPU-accelerated databases by adding extra query\\nexecution calls. We implement a selection push-down extension based on the\\nnovel selectivity estimation strategy in the MapD database system. Our approach\\nrecords constant and less than 30 millisecond overheads in any circumstances\\nwhile running on GPU. The novel strategy successfully generates better query\\nexecution plans which result in performance improvement up to 4.8 times from\\nTPC-H benchmark SF-50 queries and 7.3 times from star schema benchmark SF-80\\nqueries.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1806.08384v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1806.08384v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1806.09967v1', updated=datetime.datetime(2018, 6, 26, 13, 33, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 6, 26, 13, 33, 20, tzinfo=datetime.timezone.utc), title='A Tensor Based Data Model for Polystore: An Application to Social Networks Data', authors=[arxiv.Result.Author('Eric Leclercq'), arxiv.Result.Author('Marinette Savonnet')], summary='In this article, we show how the mathematical object tensor can be used to\\nbuild a multi-paradigm model for the storage of social data in data warehouses.\\nFrom an architectural point of view, our approach allows to link different\\nstorage systems (polystore) and limits the impact of ETL tools performing model\\ntransformations required to feed different analysis algorithms. Therefore,\\nsystems can take advantage of multiple data models both in terms of query\\nexecution performance and the semantic expressiveness of data representation.\\nThe proposed model allows to reach the logical independence between data and\\nprograms implementing analysis algorithms. With a concrete case study on\\nmessage virality on Twitter during the French presidential election of 2017, we\\nhighlight some of the contributions of our model.', comment=None, journal_ref='IDEAS 2018 22nd International Database Engineering \\\\& Applications\\n  Symposium}{June 18--20, 2018}{Villa San Giovanni, Italy', doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1806.09967v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1806.09967v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1812.01663v1', updated=datetime.datetime(2018, 12, 4, 20, 15, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 12, 4, 20, 15, 39, tzinfo=datetime.timezone.utc), title='Skyline Diagram: Efficient Space Partitioning for Skyline Queries', authors=[arxiv.Result.Author('Jinfei Liu'), arxiv.Result.Author('Juncheng Yang'), arxiv.Result.Author('Li Xiong'), arxiv.Result.Author('Jian Pei'), arxiv.Result.Author('Jun Luo'), arxiv.Result.Author('Yuzhang Guo'), arxiv.Result.Author('Shuaicheng Ma'), arxiv.Result.Author('Chenglin Fan')], summary='Skyline queries are important in many application domains. In this paper, we\\npropose a novel structure Skyline Diagram, which given a set of points,\\npartitions the plane into a set of regions, referred to as skyline polyominos.\\nAll query points in the same skyline polyomino have the same skyline query\\nresults. Similar to $k^{th}$-order Voronoi diagram commonly used to facilitate\\n$k$ nearest neighbor ($k$NN) queries, skyline diagram can be used to facilitate\\nskyline queries and many other applications. However, it may be computationally\\nexpensive to build the skyline diagram. By exploiting some interesting\\nproperties of skyline, we present several efficient algorithms for building the\\ndiagram with respect to three kinds of skyline queries, quadrant, global, and\\ndynamic skylines. In addition, we propose an approximate skyline diagram which\\ncan significantly reduce the space cost. Experimental results on both real and\\nsynthetic datasets show that our algorithms are efficient and scalable.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1812.01663v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1812.01663v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1812.01801v1', updated=datetime.datetime(2018, 12, 5, 3, 22, 37, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 12, 5, 3, 22, 37, tzinfo=datetime.timezone.utc), title='Mapping RDF Graphs to Property Graphs', authors=[arxiv.Result.Author('Shota Matsumoto'), arxiv.Result.Author('Ryota Yamanaka'), arxiv.Result.Author('Hirokazu Chiba')], summary='Increasing amounts of scientific and social data are published in the\\nResource Description Framework (RDF). Although the RDF data can be queried\\nusing the SPARQL language, even the SPARQL-based operation has a limitation in\\nimplementing traversal or analytical algorithms. Recently, a variety of graph\\ndatabase implementations dedicated to analyses on the property graph model have\\nemerged. However, the RDF model and the property graph model are not\\ninteroperable. Here, we developed a framework based on the Graph to Graph\\nMapping Language (G2GML) for mapping RDF graphs to property graphs to make the\\nmost of accumulated RDF data. Using this framework, graph data described in the\\nRDF model can be converted to the property graph model and can be loaded to\\nseveral graph database engines for further analysis. Future works include\\nimplementing and utilizing graph algorithms to make the most of the accumulated\\ndata in various analytical engines.', comment='4 pages, 4 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1812.01801v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1812.01801v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1812.02386v1', updated=datetime.datetime(2018, 12, 6, 7, 33, 37, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 12, 6, 7, 33, 37, tzinfo=datetime.timezone.utc), title='vChain: Enabling Verifiable Boolean Range Queries over Blockchain Databases', authors=[arxiv.Result.Author('Cheng Xu'), arxiv.Result.Author('Ce Zhang'), arxiv.Result.Author('Jianliang Xu')], summary=\"Blockchains have recently been under the spotlight due to the boom of\\ncryptocurrencies and decentralized applications. There is an increasing demand\\nfor querying the data stored in a blockchain database. To ensure query\\nintegrity, the user can maintain the entire blockchain database and query the\\ndata locally. However, this approach is not economic, if not infeasible,\\nbecause of the blockchain's huge data size and considerable maintenance costs.\\nIn this paper, we take the first step toward investigating the problem of\\nverifiable query processing over blockchain databases. We propose a novel\\nframework, called vChain, that alleviates the storage and computing costs of\\nthe user and employs verifiable queries to guarantee the results' integrity. To\\nsupport verifiable Boolean range queries, we propose an accumulator-based\\nauthenticated data structure that enables dynamic aggregation over arbitrary\\nquery attributes. Two new indexes are further developed to aggregate\\nintra-block and inter-block data records for efficient query verification. We\\nalso propose an inverted prefix tree structure to accelerate the processing of\\na large number of subscription queries simultaneously. Security analysis and\\nempirical study validate the robustness and practicality of the proposed\\ntechniques.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1812.02386v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1812.02386v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1812.03831v5', updated=datetime.datetime(2021, 5, 6, 13, 8, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 12, 10, 14, 39, 35, tzinfo=datetime.timezone.utc), title='On the Enumeration Complexity of Unions of Conjunctive Queries', authors=[arxiv.Result.Author('Nofar Carmeli'), arxiv.Result.Author('Markus Kröll')], summary='We study the enumeration complexity of Unions of Conjunctive Queries(UCQs).\\nWe aim to identify the UCQs that are tractable in the sense that the answer\\ntuples can be enumerated with a linear preprocessing phase and a constant delay\\nbetween every successive tuples. It has been established that, in the absence\\nof self-joins and under conventional complexity assumptions, the CQs that admit\\nsuch an evaluation are precisely the free-connex ones. A union of tractable CQs\\nis always tractable. We generalize the notion of free-connexity from CQs to\\nUCQs, thus showing that some unions containing intractable CQs are, in fact,\\ntractable. Interestingly, some unions consisting of only intractable CQs are\\ntractable too. We show how to use the techniques presented in this article also\\nin settings where the database contains cardinality dependencies (including\\nfunctional dependencies and key constraints) or when the UCQs contain\\ndisequalities. The question of finding a full characterization of the\\ntractability of UCQs remains open. Nevertheless, we prove that for several\\nclasses of queries, free-connexity fully captures the tractable UCQs.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1812.03831v5', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1812.03831v5', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1812.04329v2', updated=datetime.datetime(2018, 12, 13, 13, 37, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 12, 11, 11, 1, 42, tzinfo=datetime.timezone.utc), title='Semantic Width of Conjunctive Queries and Constraint Satisfaction Problems', authors=[arxiv.Result.Author('Georg Gottlob'), arxiv.Result.Author('Matthias Lanzinger'), arxiv.Result.Author('Reinhard Pichler')], summary=\"Answering Conjunctive Queries (CQs) and solving Constraint Satisfaction\\nProblems (CSPs) are arguably among the most fundamental tasks in Computer\\nScience. They are classical NP-complete problems. Consequently, the search for\\ntractable fragments of these problems has received a lot of research interest\\nover the decades. This research has traditionally progressed along three\\northogonal threads. a) Reformulating queries into simpler, equivalent, queries\\n(semantic optimization) b) Bounding answer sizes based on structural properties\\nof the query c) Decomposing the query in such a way that global consistency\\nfollows from local consistency. Much progress has been made by various works\\nthat connect two of these threads. Bounded answer sizes and decompositions have\\nbeen shown to be tightly connected through the important notions of fractional\\nhypertree width and, more recently, submodular width. recent papers by\\nBarcel\\\\'o et al. study decompositions up to generalized hypertree width under\\nsemantic optimization. In this work, we connect all three of these threads by\\nintroducing a general notion of semantic width and investigating semantic\\nversions of fractional hypertree width, adaptive width, submodular width and\\nthe fractional cover number.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2.4'], links=[arxiv.Result.Link('http://arxiv.org/abs/1812.04329v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1812.04329v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1812.04386v1', updated=datetime.datetime(2018, 12, 11, 13, 25, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 12, 11, 13, 25, 19, tzinfo=datetime.timezone.utc), title='The Empusa code generator: bridging the gap between the intended and the actual content of RDF resources', authors=[arxiv.Result.Author('Jesse C. J. van Dam'), arxiv.Result.Author('Jasper J. Koehorst'), arxiv.Result.Author('Peter J. Schaap'), arxiv.Result.Author('Maria Suarez-Diez')], summary='The RDF data model facilitates integration of diverse data available in\\nstructured and semi-structured formats. To obtain an RDF graph with a low\\namount of errors and internal redundancy, the chosen ontology must be\\nconsistently applied. However, with each addition of new diverse data the\\nontology must evolve thereby increasing its complexity, which could lead to\\naccumulation of unintended erroneous composites. Thus, there is a need for a\\ngatekeeping system that compares the intended content described in the ontology\\nwith the actual content of the resource.\\n  Here we present Empusa, a tool that has been developed to facilitate the\\ncreation of composite RDF resources from disparate sources. Empusa can be used\\nto convert a schema into an associated application programming interface (API)\\nthat can be used to perform data consistency checks and generates Markdown\\ndocumentation to make persistent URLs resolvable. In this way, the use of\\nEmpusa ensures consistency within and between the ontology (OWL), the Shape\\nExpressions (ShEx) describing the graph structure, and the content of the\\nresource.', comment=None, journal_ref=None, doi='10.1038/s41597-019-0263-7', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1038/s41597-019-0263-7', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1812.04386v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1812.04386v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1812.05762v1', updated=datetime.datetime(2018, 12, 14, 2, 32, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 12, 14, 2, 32, 45, tzinfo=datetime.timezone.utc), title='Helix: Holistic Optimization for Accelerating Iterative Machine Learning', authors=[arxiv.Result.Author('Doris Xin'), arxiv.Result.Author('Stephen Macke'), arxiv.Result.Author('Litian Ma'), arxiv.Result.Author('Jialin Liu'), arxiv.Result.Author('Shuchen Song'), arxiv.Result.Author('Aditya Parameswaran')], summary='Machine learning workflow development is a process of trial-and-error:\\ndevelopers iterate on workflows by testing out small modifications until the\\ndesired accuracy is achieved. Unfortunately, existing machine learning systems\\nfocus narrowly on model training---a small fraction of the overall development\\ntime---and neglect to address iterative development. We propose Helix, a\\nmachine learning system that optimizes the execution across\\niterations---intelligently caching and reusing, or recomputing intermediates as\\nappropriate. Helix captures a wide variety of application needs within its\\nScala DSL, with succinct syntax defining unified processes for data\\npreprocessing, model specification, and learning. We demonstrate that the reuse\\nproblem can be cast as a Max-Flow problem, while the caching problem is\\nNP-Hard. We develop effective lightweight heuristics for the latter. Empirical\\nevaluation shows that Helix is not only able to handle a wide variety of use\\ncases in one unified workflow but also much faster, providing run time\\nreductions of up to 19x over state-of-the-art systems, such as DeepDive or\\nKeystoneML, on four real-world applications in natural language processing,\\ncomputer vision, social and natural sciences.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1812.05762v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1812.05762v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1812.05804v1', updated=datetime.datetime(2018, 12, 14, 7, 41, 25, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 12, 14, 7, 41, 25, tzinfo=datetime.timezone.utc), title='Data Provenance for Sport', authors=[arxiv.Result.Author('Andrew J. Simmons'), arxiv.Result.Author('Scott Barnett'), arxiv.Result.Author('Simon Vajda'), arxiv.Result.Author('Rajesh Vasa')], summary='Data analysts often discover irregularities in their underlying dataset,\\nwhich need to be traced back to the original source and corrected. Standards\\nfor representing data provenance (i.e. the origins of the data), such as the\\nW3C PROV standard, can assist with this process, however require a mapping\\nbetween abstract provenance concepts and the domain of use in order to apply\\nthem effectively. We propose a custom notation for expressing provenance of\\ninformation in the sport performance analysis domain, and map our notation to\\nconcepts in the W3C PROV standard where possible. We evaluate the functionality\\nof W3C PROV (without specialisations) and the VisTrails workflow manager\\n(without extensions), and find that as is, neither are able to fully capture\\nsport performance analysis workflows, notably due to limitations surrounding\\ncapture of automated and manual activities respectively. Furthermore, their\\nnotations suffer from ineffective use of visual design space, and present\\npotential usability issues as their terminology is unlikely to match that of\\nsport practitioners. Our findings suggest that one-size-fits-all provenance and\\nworkflow systems are a poor fit in practice, and that their notation and\\nfunctionality need to be optimised for the domain of use.', comment='12 pages, 6 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1812.05804v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1812.05804v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1812.07527v3', updated=datetime.datetime(2019, 7, 19, 16, 16, 49, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 12, 18, 17, 48, tzinfo=datetime.timezone.utc), title='LSM-based Storage Techniques: A Survey', authors=[arxiv.Result.Author('Chen Luo'), arxiv.Result.Author('Michael J. Carey')], summary='Recently, the Log-Structured Merge-tree (LSM-tree) has been widely adopted\\nfor use in the storage layer of modern NoSQL systems. Because of this, there\\nhave been a large number of research efforts, from both the database community\\nand the operating systems community, that try to improve various aspects of\\nLSM-trees. In this paper, we provide a survey of recent research efforts on\\nLSM-trees so that readers can learn the state-of-the-art in LSM-based storage\\ntechniques. We provide a general taxonomy to classify the literature of\\nLSM-trees, survey the efforts in detail, and discuss their strengths and\\ntrade-offs. We further survey several representative LSM-based open-source\\nNoSQL systems and discuss some potential future research directions resulting\\nfrom the survey.', comment='This is a pre-print of an article published in VLDB Journal. The\\n  final authenticated version is available online at:\\n  https://doi.org/10.1007/s00778-019-00555-y', journal_ref='VLDB Journal, 2019', doi='10.1007/s00778-019-00555-y', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/s00778-019-00555-y', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1812.07527v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1812.07527v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1812.07607v1', updated=datetime.datetime(2018, 12, 18, 19, 25, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 12, 18, 19, 25, 26, tzinfo=datetime.timezone.utc), title='DeepLens: Towards a Visual Data Management System', authors=[arxiv.Result.Author('Sanjay Krishnan'), arxiv.Result.Author('Adam Dziedzic'), arxiv.Result.Author('Aaron J. Elmore')], summary='Advances in deep learning have greatly widened the scope of automatic\\ncomputer vision algorithms and enable users to ask questions directly about the\\ncontent in images and video. This paper explores the necessary steps towards a\\nfuture Visual Data Management System (VDMS), where the predictions of such deep\\nlearning models are stored, managed, queried, and indexed. We propose a query\\nand data model that disentangles the neural network models used, the query\\nworkload, and the data source semantics from the query processing layer. Our\\nsystem, DeepLens, is based on dataflow query processing systems and this\\nresearch prototype presents initial experiments to elicit important open\\nresearch questions in visual analytics systems. One of our main conclusions is\\nthat any future \"declarative\" VDMS will have to revisit query optimization and\\nautomated physical design from a unified perspective of performance and\\naccuracy tradeoffs. Physical design and query optimization choices can not only\\nchange performance by orders of magnitude, they can potentially affect the\\naccuracy of results.', comment='In Proceeds of CIDR 2019', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1812.07607v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1812.07607v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1812.07658v1', updated=datetime.datetime(2018, 12, 18, 21, 58, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 12, 18, 21, 58, 19, tzinfo=datetime.timezone.utc), title='Demonstration of a Multiresolution Schema Mapping System', authors=[arxiv.Result.Author('Zhongjun Jin'), arxiv.Result.Author('Christopher Baik'), arxiv.Result.Author('Michael Cafarella'), arxiv.Result.Author('H. V. Jagadish'), arxiv.Result.Author('Yuze Lou')], summary='Enterprise databases usually contain large and complex schemas. Authoring\\ncomplete schema mapping queries in this case requires deep knowledge about the\\nsource and target schemas and is thereby very challenging to programmers.\\nSample-driven schema mapping allows the user to describe the schema mapping\\nusing data records. However, real data records are still harder to specify than\\nother useful insights about the desired schema mapping the user might have. In\\nthis project, we develop a schema mapping system, PRISM, that enables\\nmultiresolution schema mapping. The end user is not limited to providing\\nhigh-resolution constraints like exact data records but may also provide\\nconstraints of various resolutions, like incomplete data records, value ranges,\\nand data types. This new interaction paradigm gives the user more flexibility\\nin describing the desired schema mapping. This demonstration showcases how to\\nuse PRISM for schema mapping in a real database.', comment='4 pages, 5 figures, CIDR 2019', journal_ref='9th Biennial Conference on Innovative Data Systems Research (CIDR\\n  2019)', doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1812.07658v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1812.07658v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1812.09141v1', updated=datetime.datetime(2018, 12, 21, 14, 24, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 12, 21, 14, 24, 56, tzinfo=datetime.timezone.utc), title='Speeding-up the Verification Phase of Set Similarity Joins in the GPGPU paradigm', authors=[arxiv.Result.Author('Christos Bellas'), arxiv.Result.Author('Anastasios Gounaris')], summary='We investigate the problem of exact set similarity joins using a co-process\\nCPU-GPU scheme. The state-of-the-art CPU solutions split the wok in two main\\nphases. First, filtering and index building takes place to reduce the candidate\\nsets to be compared as much as possible; then the pairs are compared to verify\\nwhether they should become part of the result. We investigate in-depth\\nsolutions for transferring the second, so-called verification phase, to the GPU\\naddressing several challenges regarding the data serialization and layout, the\\nthread management and the techniques to compare sets of tokens. Using real\\ndatasets, we provide concrete experimental proofs that our solutions have\\nreached their maximum potential, since they totally overlap verification with\\nCPU tasks, and manage to yield significant speed-ups, up to 2.6X in our cases.', comment='13 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1812.09141v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1812.09141v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1812.09233v1', updated=datetime.datetime(2018, 12, 20, 3, 6, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 12, 20, 3, 6, 17, tzinfo=datetime.timezone.utc), title='Partitioned Data Security on Outsourced Sensitive and Non-sensitive Data', authors=[arxiv.Result.Author('Sharad Mehrotra'), arxiv.Result.Author('Shantanu Sharma'), arxiv.Result.Author('Jeffrey D. Ullman'), arxiv.Result.Author('Anurag Mishra')], summary='Despite extensive research on cryptography, secure and efficient query\\nprocessing over outsourced data remains an open challenge. This paper continues\\nalong the emerging trend in secure data processing that recognizes that the\\nentire dataset may not be sensitive, and hence, non-sensitivity of data can be\\nexploited to overcome limitations of existing encryption-based approaches. We\\npropose a new secure approach, entitled query binning (QB) that allows\\nnon-sensitive parts of the data to be outsourced in clear-text while\\nguaranteeing that no information is leaked by the joint processing of\\nnon-sensitive data (in clear-text) and sensitive data (in encrypted form). QB\\nmaps a query to a set of queries over the sensitive and non-sensitive data in a\\nway that no leakage will occur due to the joint processing over sensitive and\\nnon-sensitive data. Interestingly, in addition to improve performance, we show\\nthat QB actually strengthens the security of the underlying cryptographic\\ntechnique by preventing size, frequency-count, and workload-skew attacks.', comment='Accepted in IEEE International Conference on Data Engineering (ICDE),\\n  2019. arXiv admin note: text overlap with arXiv:1812.01741', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CR', 'cs.DC', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1812.09233v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1812.09233v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1812.09551v1', updated=datetime.datetime(2018, 12, 22, 16, 11, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 12, 22, 16, 11, 17, tzinfo=datetime.timezone.utc), title='TaxoGen: Unsupervised Topic Taxonomy Construction by Adaptive Term Embedding and Clustering', authors=[arxiv.Result.Author('Chao Zhang'), arxiv.Result.Author('Fangbo Tao'), arxiv.Result.Author('Xiusi Chen'), arxiv.Result.Author('Jiaming Shen'), arxiv.Result.Author('Meng Jiang'), arxiv.Result.Author('Brian Sadler'), arxiv.Result.Author('Michelle Vanni'), arxiv.Result.Author('Jiawei Han')], summary='Taxonomy construction is not only a fundamental task for semantic analysis of\\ntext corpora, but also an important step for applications such as information\\nfiltering, recommendation, and Web search. Existing pattern-based methods\\nextract hypernym-hyponym term pairs and then organize these pairs into a\\ntaxonomy. However, by considering each term as an independent concept node,\\nthey overlook the topical proximity and the semantic correlations among terms.\\nIn this paper, we propose a method for constructing topic taxonomies, wherein\\nevery node represents a conceptual topic and is defined as a cluster of\\nsemantically coherent concept terms. Our method, TaxoGen, uses term embeddings\\nand hierarchical clustering to construct a topic taxonomy in a recursive\\nfashion. To ensure the quality of the recursive process, it consists of: (1) an\\nadaptive spherical clustering module for allocating terms to proper levels when\\nsplitting a coarse topic into fine-grained ones; (2) a local embedding module\\nfor learning term embeddings that maintain strong discriminative power at\\ndifferent levels of the taxonomy. Our experiments on two real datasets\\ndemonstrate the effectiveness of TaxoGen compared with baseline methods.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1812.09551v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1812.09551v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1812.11346v2', updated=datetime.datetime(2020, 3, 12, 17, 4, 57, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 12, 29, 11, 43, 32, tzinfo=datetime.timezone.utc), title='Explaining Aggregates for Exploratory Analytics', authors=[arxiv.Result.Author('Fotis Savva'), arxiv.Result.Author('Christos Anagnostopoulos'), arxiv.Result.Author('Peter Triantafillou')], summary=\"Analysts wishing to explore multivariate data spaces, typically pose queries\\ninvolving selection operators, i.e., range or radius queries, which define data\\nsubspaces of possible interest and then use aggregation functions, the results\\nof which determine their exploratory analytics interests. However, such\\naggregate query (AQ) results are simple scalars and as such, convey limited\\ninformation about the queried subspaces for exploratory analysis. We address\\nthis shortcoming aiding analysts to explore and understand data subspaces by\\ncontributing a novel explanation mechanism coined XAXA: eXplaining Aggregates\\nfor eXploratory Analytics. XAXA's novel AQ explanations are represented using\\nfunctions obtained by a three-fold joint optimization problem. Explanations\\nassume the form of a set of parametric piecewise-linear functions acquired\\nthrough a statistical learning model. A key feature of the proposed solution is\\nthat model training is performed by only monitoring AQs and their answers\\non-line. In XAXA, explanations for future AQs can be computed without any\\ndatabase (DB) access and can be used to further explore the queried data\\nsubspaces, without issuing any more queries to the DB. We evaluate the\\nexplanation accuracy and efficiency of XAXA through theoretically grounded\\nmetrics over real-world and synthetic datasets and query workloads.\", comment='13 pages', journal_ref=None, doi='10.1109/BigData.2018.8621953', primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/BigData.2018.8621953', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1812.11346v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1812.11346v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.00140v1', updated=datetime.datetime(2019, 6, 1, 3, 31, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 1, 3, 31, 6, tzinfo=datetime.timezone.utc), title='Fast Algorithm for K-Truss Discovery on Public-Private Graphs', authors=[arxiv.Result.Author('Soroush Ebadian'), arxiv.Result.Author('Xin Huang')], summary='In public-private graphs, users share one public graph and have their own\\nprivate graphs. A private graph consists of personal private contacts that only\\ncan be visible to its owner, e.g., hidden friend lists on Facebook and secret\\nfollowing on Sina Weibo. However, existing public-private analytic algorithms\\nhave not yet investigated the dense subgraph discovery of k-truss, where each\\nedge is contained in at least k-2 triangles. This paper aims at finding k-truss\\nefficiently in public-private graphs. The core of our solution is a novel\\nalgorithm to update k-truss with node insertions. We develop a\\nclassification-based hybrid strategy of node insertions and edge insertions to\\nincrementally compute k-truss in public-private graphs. Extensive experiments\\nvalidate the superiority of our proposed algorithms against state-of-the-art\\nmethods on real-world datasets.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.DS'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.00140v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.00140v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.00179v1', updated=datetime.datetime(2019, 6, 1, 8, 15, 48, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 1, 8, 15, 48, tzinfo=datetime.timezone.utc), title='Enriching Ontology-based Data Access with Provenance (Extended Version)', authors=[arxiv.Result.Author('Diego Calvanese'), arxiv.Result.Author('Davide Lanti'), arxiv.Result.Author('Ana Ozaki'), arxiv.Result.Author('Rafael Penaloza'), arxiv.Result.Author('Guohui Xiao')], summary='Ontology-based data access (OBDA) is a popular paradigm for querying\\nheterogeneous data sources by connecting them through mappings to an ontology.\\nIn OBDA, it is often difficult to reconstruct why a tuple occurs in the answer\\nof a query. We address this challenge by enriching OBDA with provenance\\nsemirings, taking inspiration from database theory. In particular, we\\ninvestigate the problems of (i) deciding whether a provenance annotated OBDA\\ninstance entails a provenance annotated conjunctive query, and (ii) computing a\\npolynomial representing the provenance of a query entailed by a provenance\\nannotated OBDA instance. Differently from pure databases, in our case these\\npolynomials may be infinite. To regain finiteness, we consider idempotent\\nsemirings, and study the complexity in the case of DL-Lite ontologies. We\\nimplement Task (ii) in a state-of-the-art OBDA system and show the practical\\nfeasibility of the approach through an extensive evaluation against two popular\\nbenchmarks.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.00179v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.00179v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.00341v2', updated=datetime.datetime(2019, 8, 7, 11, 43, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 2, 4, 28, 6, tzinfo=datetime.timezone.utc), title='Efficient Algorithms for Densest Subgraph Discovery', authors=[arxiv.Result.Author('Yixiang Fang'), arxiv.Result.Author('Kaiqiang Yu'), arxiv.Result.Author('Reynold Cheng'), arxiv.Result.Author('Laks V. S. Lakshmanan'), arxiv.Result.Author('Xuemin Lin')], summary='Densest subgraph discovery (DSD) is a fundamental problem in graph mining. It\\nhas been studied for decades, and is widely used in various areas, including\\nnetwork science, biological analysis, and graph databases. Given a graph G, DSD\\naims to find a subgraph D of G with the highest density (e.g., the number of\\nedges over the number of vertices in D). Because DSD is difficult to solve, we\\npropose a new solution paradigm in this paper. Our main observation is that a\\ndensest subgraph can be accurately found through a k-core (a kind of dense\\nsubgraph of G), with theoretical guarantees. Based on this intuition, we\\ndevelop efficient exact and approximation solutions for DSD. Moreover, our\\nsolutions are able to find the densest subgraphs for a wide range of graph\\ndensity definitions, including clique-based and general pattern-based density.\\nWe have performed extensive experimental evaluation on eleven real datasets.\\nOur results show that our algorithms are up to four orders of magnitude faster\\nthan existing approaches.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.SI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.00341v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.00341v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.01933v1', updated=datetime.datetime(2019, 6, 5, 10, 51, 25, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 5, 10, 51, 25, tzinfo=datetime.timezone.utc), title='Evaluating Geospatial RDF stores Using the Benchmark Geographica 2', authors=[arxiv.Result.Author('Theofilos Ioannidis'), arxiv.Result.Author('George Garbis'), arxiv.Result.Author('Kostis Kyzirakos'), arxiv.Result.Author('Konstantina Bereta'), arxiv.Result.Author('Manolis Koubarakis')], summary='Since 2007, geospatial extensions of SPARQL, like GeoSPARQL and stSPARQL,\\nhave been defined and corresponding geospatial RDF stores have been\\nimplemented. In addition, some work on developing benchmarks for evaluating\\ngeospatial RDF stores has been carried out. In this paper, we revisit the\\nGeographica benchmark defined by our group in 2013 which uses both real world\\nand synthetic data to test the performance and functionality of geospatial RDF\\nstores. We present Geographica 2, a new version of the benchmark which extends\\nGeographica by adding one more workload, extending our existing workloads and\\nevaluating 5 more RDF stores. Using three different real workloads, Geographica\\n2 tests the efficiency of primitive spatial functions in RDF stores and the\\nperformance of the RDF stores in real use case scenarios, a more detailed\\nevaluation is performed using a synthetic workload and the scalability of the\\nRDF stores is stressed with the scalability workload. In total eight systems\\nare evaluated out of which six adequately support GeoSPARQL and two offer\\nlimited spatial support.', comment='36 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.01933v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.01933v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.02074v1', updated=datetime.datetime(2019, 6, 5, 15, 32, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 5, 15, 32, 22, tzinfo=datetime.timezone.utc), title='An Effective Algorithm for Learning Single Occurrence Regular Expressions with Interleaving', authors=[arxiv.Result.Author('Yeting Li'), arxiv.Result.Author('Haiming Chen'), arxiv.Result.Author('Xiaolan Zhang'), arxiv.Result.Author('Lingqi Zhang')], summary='The advantages offered by the presence of a schema are numerous. However,\\nmany XML documents in practice are not accompanied by a (valid) schema, making\\nschema inference an attractive research problem. The fundamental task in XML\\nschema learning is inferring restricted subclasses of regular expressions. Most\\nprevious work either lacks support for interleaving or only has limited support\\nfor interleaving. In this paper, we first propose a new subclass Single\\nOccurrence Regular Expressions with Interleaving (SOIRE), which has\\nunrestricted support for interleaving. Then, based on single occurrence\\nautomaton and maximum independent set, we propose an algorithm iSOIRE to infer\\nSOIREs. Finally, we further conduct a series of experiments on real datasets to\\nevaluate the effectiveness of our work, comparing with both ongoing learning\\nalgorithms in academia and industrial tools in real-world. The results reveal\\nthe practicability of SOIRE and the effectiveness of iSOIRE, showing the high\\npreciseness and conciseness of our work.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.02074v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.02074v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.03345v1', updated=datetime.datetime(2019, 6, 7, 22, 0, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 7, 22, 0, 8, tzinfo=datetime.timezone.utc), title='Increasing Transparent and Accountable Use of Data by Quantifying the Actual Privacy Risk in Interactive Record Linkage', authors=[arxiv.Result.Author('Qinbo Li'), arxiv.Result.Author(\"Adam G. D'Souza\"), arxiv.Result.Author('Cason Schmit'), arxiv.Result.Author('Hye-Chung Kum')], summary='Record linkage refers to the task of integrating data from two or more\\ndatabases without a common identifier. MINDFIRL (MInimum Necessary Disclosure\\nFor Interactive Record Linkage) is a software system that demonstrates the\\ntradeoff between utility and privacy in interactive record linkage. Due to the\\nneed to access personally identifiable information (PII) to accurately assess\\nwhether different records refer to the same person in heterogeneous databases,\\nprivacy is a major concern in interactive record linkage. MINDFIRL supports\\ninteractive record linkage while minimizing the privacy risk by (1) using\\npseudonyms to separate the identifying information from the sensitive\\ninformation, (2) dynamically disclosing only the minimum necessary information\\nincrementally, as needed on-demand at the point of decision, and (3) quantifies\\nthe risk due to the needed information disclosure to support transparency, the\\nreasoning, communication, and decisions on the privacy and utility trade off.\\nIn this paper we present an overview of the MINDFIRL system and the\\nk-Anonymized Privacy Risk (KAPR) score used to measure the privacy risk based\\non the disclosed information. We prove that KAPR score is a norm meeting all\\nthe desirable properties for a risk score for interactive record linkage.', comment='7 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.03345v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.03345v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.05409v1', updated=datetime.datetime(2019, 6, 12, 22, 26, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 12, 22, 26, 6, tzinfo=datetime.timezone.utc), title='A Countrywide Traffic Accident Dataset', authors=[arxiv.Result.Author('Sobhan Moosavi'), arxiv.Result.Author('Mohammad Hossein Samavatian'), arxiv.Result.Author('Srinivasan Parthasarathy'), arxiv.Result.Author('Rajiv Ramnath')], summary='Reducing traffic accidents is an important public safety challenge. However,\\nthe majority of studies on traffic accident analysis and prediction have used\\nsmall-scale datasets with limited coverage, which limits their impact and\\napplicability; and existing large-scale datasets are either private, old, or do\\nnot include important contextual information such as environmental stimuli\\n(weather, points-of-interest, etc.). In order to help the research community\\naddress these shortcomings we have - through a comprehensive process of data\\ncollection, integration, and augmentation - created a large-scale publicly\\navailable database of accident information named US-Accidents. US-Accidents\\ncurrently contains data about $2.25$ million instances of traffic accidents\\nthat took place within the contiguous United States, and over the last three\\nyears. Each accident record consists of a variety of intrinsic and contextual\\nattributes such as location, time, natural language description, weather,\\nperiod-of-day, and points-of-interest. We present this dataset in this paper,\\nalong with a wide range of insights gleaned from this dataset with respect to\\nthe spatiotemporal characteristics of accidents. The dataset is publicly\\navailable at https://smoosavi.org/datasets/us_accidents.', comment='New preprint, 6 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.05409v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.05409v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.05505v4', updated=datetime.datetime(2019, 8, 30, 18, 0, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 13, 6, 50, 2, tzinfo=datetime.timezone.utc), title='Scalable Community Detection over Geo-Social Network', authors=[arxiv.Result.Author('Xiuwen Zheng'), arxiv.Result.Author('Qiyu Liu'), arxiv.Result.Author('Amarnath Gupta')], summary='We consider a community finding problem called Co-located Community Detection\\n(CCD) over geo-social networks, which retrieves communities that satisfy both\\nhigh structural tightness and spatial closeness constraints. To provide a\\nsolution that benefits from existing studies on community detection, we\\ndecouple the spatial constraint from graph structural constraint and propose a\\nuniform CCD framework which gives users the freedom to choose customized\\nmeasurements for social cohesiveness (e.g., $k$-core or $k$-truss). For the\\nspatial closeness constraint, we apply the bounded radius spatial constraint\\nand develop an exact algorithm together with effective pruning rules. To\\nfurther improve the efficiency and make our framework scale to a very large\\nscale of data, we propose a near-linear time approximation algorithm with a\\nconstant approximation ratio ($\\\\sqrt{2}$). We conduct extensive experiments on\\nboth synthetic and real-world datasets to demonstrate the efficiency and\\neffectiveness of our algorithms.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.SI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.05505v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.05505v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.06542v1', updated=datetime.datetime(2019, 6, 15, 12, 32, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 15, 12, 32, 17, tzinfo=datetime.timezone.utc), title='A Books Recommendation Approach Based on Online Bookstore Data', authors=[arxiv.Result.Author('Xinyu Wei'), arxiv.Result.Author('Jiahui Chen'), arxiv.Result.Author('Jing Chen'), arxiv.Result.Author('Bernie Liu')], summary='In the era of information explosion, facing complex information, it is\\ndifficult for users to choose the information of interest, and businesses also\\nneed detailed information on ways to let the ad stand out. By this time, it is\\nrecommended that a good way. We firstly by using random interviews,\\nsimulations, asking experts, summarizes methods outlined the main factors\\naffecting the scores of books that users drew. In order to further illustrate\\nthe impact of these factors, we also by combining the AHP consistency test,\\nthen fuzzy evaluation method, empowered each factor, influencing factors and\\nthe degree of influence come. For the second question, predict user evaluation\\nof the listed books from the predict annex. First, given the books Annex\\nlabels, user data extraction scorebooks and mathematical analysis of data\\nobtained from SPSS user preferences and then use software to nearest neighbor\\nanalysis to result in predicted value.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.06542v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.06542v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.06956v2', updated=datetime.datetime(2019, 6, 18, 7, 32, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 17, 11, 15, 21, tzinfo=datetime.timezone.utc), title='Scalable Distributed Subtrajectory Clustering', authors=[arxiv.Result.Author('Panagiotis Tampakis'), arxiv.Result.Author('Nikos Pelekis'), arxiv.Result.Author('Christos Doulkeridis'), arxiv.Result.Author('Yannis Theodoridis')], summary='Trajectory clustering is an important operation of knowledge discovery from\\nmobility data. Especially nowadays, the need for performing advanced analytic\\noperations over massively produced data, such as mobility traces, in efficient\\nand scalable ways is imperative. However, discovering clusters of complete\\ntrajectories can overlook significant patterns that exist only for a small\\nportion of their lifespan. In this paper, we address the problem of Distributed\\nSubtrajectory Clustering in an efficient and highly scalable way. The problem\\nis challenging because the subtrajectories to be clustered are not known in\\nadvance, but they need to be discovered dynamically based on adjacent\\nsubtrajectories in space and time. Towards this objective, we split the\\noriginal problem to three sub-problems, namely Subtrajectory Join, Trajectory\\nSegmentation and Clustering and Outlier Detection, and deal with each one in a\\ndistributed fashion by utilizing the MapReduce programming model. The\\nefficiency and the effectiveness of our solution is demonstrated experimentally\\nover a synthetic and two large real datasets from the maritime and urban\\ndomains and through comparison with two state of the art subtrajectory\\nclustering algorithms.', comment=None, journal_ref='2019 IEEE International Conference on Big Data (Big Data)', doi='10.1109/BigData47090.2019.9005563', primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/BigData47090.2019.9005563', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1906.06956v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.06956v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.08042v1', updated=datetime.datetime(2019, 6, 17, 20, 33, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 17, 20, 33, 24, tzinfo=datetime.timezone.utc), title='Low-resource Deep Entity Resolution with Transfer and Active Learning', authors=[arxiv.Result.Author('Jungo Kasai'), arxiv.Result.Author('Kun Qian'), arxiv.Result.Author('Sairam Gurajada'), arxiv.Result.Author('Yunyao Li'), arxiv.Result.Author('Lucian Popa')], summary='Entity resolution (ER) is the task of identifying different representations\\nof the same real-world entities across databases. It is a key step for\\nknowledge base creation and text mining. Recent adaptation of deep learning\\nmethods for ER mitigates the need for dataset-specific feature engineering by\\nconstructing distributed representations of entity records. While these methods\\nachieve state-of-the-art performance over benchmark data, they require large\\namounts of labeled data, which are typically unavailable in realistic ER\\napplications. In this paper, we develop a deep learning-based method that\\ntargets low-resource settings for ER through a novel combination of transfer\\nlearning and active learning. We design an architecture that allows us to learn\\na transferable model from a high-resource setting to a low-resource one. To\\nfurther adapt to the target dataset, we incorporate active learning that\\ncarefully selects a few informative examples to fine-tune the transferred\\nmodel. Empirical evaluation demonstrates that our method achieves comparable,\\nif not better, performance compared to state-of-the-art learning-based methods\\nwhile using an order of magnitude fewer labels.', comment='This paper is accepted by ACL 2019', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CL', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.08042v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.08042v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.08097v3', updated=datetime.datetime(2019, 7, 18, 16, 48, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 19, 13, 48, 8, tzinfo=datetime.timezone.utc), title='Observing LOD using Equivalent Set Graphs: it is mostly flat and sparsely linked', authors=[arxiv.Result.Author('Luigi Asprino'), arxiv.Result.Author('Wouter Beek'), arxiv.Result.Author('Paolo Ciancarini'), arxiv.Result.Author('Frank van Harmelen'), arxiv.Result.Author('Valentina Presutti')], summary='This paper presents an empirical study aiming at understanding the modeling\\nstyle and the overall semantic structure of Linked Open Data. We observe how\\nclasses, properties and individuals are used in practice. We also investigate\\nhow hierarchies of concepts are structured, and how much they are linked. In\\naddition to discussing the results, this paper contributes (i) a conceptual\\nframework, including a set of metrics, which generalises over the observable\\nconstructs; (ii) an open source implementation that facilitates its application\\nto other Linked Data knowledge graphs.', comment='18 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.08097v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.08097v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.08149v1', updated=datetime.datetime(2019, 6, 19, 15, 23, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 19, 15, 23, 41, tzinfo=datetime.timezone.utc), title='Efficient privacy preservation of big data for accurate data mining', authors=[arxiv.Result.Author('M. A. P. Chamikara'), arxiv.Result.Author('P. Bertok'), arxiv.Result.Author('D. Liu'), arxiv.Result.Author('S. Camtepe'), arxiv.Result.Author('I. Khalil')], summary='Computing technologies pervade physical spaces and human lives, and produce a\\nvast amount of data that is available for analysis. However, there is a growing\\nconcern that potentially sensitive data may become public if the collected data\\nare not appropriately sanitized before being released for investigation.\\nAlthough there are more than a few privacy-preserving methods available, they\\nare not efficient, scalable or have problems with data utility, and/or privacy.\\nThis paper addresses these issues by proposing an efficient and scalable\\nnonreversible perturbation algorithm, PABIDOT, for privacy preservation of big\\ndata via optimal geometric transformations. PABIDOT was tested for efficiency,\\nscalability, resistance, and accuracy using nine datasets and five\\nclassification algorithms. Experiments show that PABIDOT excels in execution\\nspeed, scalability, attack resistance and accuracy in large-scale\\nprivacy-preserving data classification when compared with two other, related\\nprivacy-preserving algorithms.', comment='Information Sciences', journal_ref=None, doi='10.1016/j.ins.2019.05.053', primary_category='cs.DB', categories=['cs.DB', 'cs.CR'], links=[arxiv.Result.Link('http://dx.doi.org/10.1016/j.ins.2019.05.053', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1906.08149v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.08149v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.08687v1', updated=datetime.datetime(2019, 6, 20, 15, 20, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 20, 15, 20, 20, tzinfo=datetime.timezone.utc), title='A Layered Aggregate Engine for Analytics Workloads', authors=[arxiv.Result.Author('Maximilian Schleich'), arxiv.Result.Author('Dan Olteanu'), arxiv.Result.Author('Mahmoud Abo Khamis'), arxiv.Result.Author('Hung Q. Ngo'), arxiv.Result.Author('XuanLong Nguyen')], summary='This paper introduces LMFAO (Layered Multiple Functional Aggregate\\nOptimization), an in-memory optimization and execution engine for batches of\\naggregates over the input database. The primary motivation for this work stems\\nfrom the observation that for a variety of analytics over databases, their\\ndata-intensive tasks can be decomposed into group-by aggregates over the join\\nof the input database relations. We exemplify the versatility and\\ncompetitiveness of LMFAO for a handful of widely used analytics: learning ridge\\nlinear regression, classification trees, regression trees, and the structure of\\nBayesian networks using Chow-Liu trees; and data cubes used for exploration in\\ndata warehousing.\\n  LMFAO consists of several layers of logical and code optimizations that\\nsystematically exploit sharing of computation, parallelism, and code\\nspecialization.\\n  We conducted two types of performance benchmarks. In experiments with four\\ndatasets, LMFAO outperforms by several orders of magnitude on one hand, a\\ncommercial database system and MonetDB for computing batches of aggregates, and\\non the other hand, TensorFlow, Scikit, R, and AC/DC for learning a variety of\\nmodels over databases.', comment='18 pages, 7 figures, 4 tables', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2.4; I.2.6'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.08687v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.08687v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.08986v2', updated=datetime.datetime(2020, 1, 19, 3, 34, 25, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 21, 7, 26, 31, tzinfo=datetime.timezone.utc), title='Database Meets Deep Learning: Challenges and Opportunities', authors=[arxiv.Result.Author('Wei Wang'), arxiv.Result.Author('Meihui Zhang'), arxiv.Result.Author('Gang Chen'), arxiv.Result.Author('H. V. Jagadish'), arxiv.Result.Author('Beng Chin Ooi'), arxiv.Result.Author('Kian-Lee Tan')], summary='Deep learning has recently become very popular on account of its incredible\\nsuccess in many complex data-driven applications, such as image classification\\nand speech recognition. The database community has worked on data-driven\\napplications for many years, and therefore should be playing a lead role in\\nsupporting this new wave. However, databases and deep learning are different in\\nterms of both techniques and applications. In this paper, we discuss research\\nproblems at the intersection of the two fields. In particular, we discuss\\npossible improvements for deep learning systems from a database perspective,\\nand analyze database applications that may benefit from deep learning\\ntechniques.', comment='The first version of this paper has appeared in SIGMOD Record. In\\n  this (third) version, we extend it to include the recent developments in this\\n  field and references to recent work (especially for section 3.2 and section\\n  4.2)', journal_ref='ACM SIGMOD Record, Volume 45 Issue 2, June 2016, Pages 17-22', doi='10.1145/3003665.3003669', primary_category='cs.DB', categories=['cs.DB', 'cs.DC', 'cs.LG'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3003665.3003669', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1906.08986v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.08986v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.08990v1', updated=datetime.datetime(2019, 6, 21, 7, 49, 36, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 21, 7, 49, 36, tzinfo=datetime.timezone.utc), title='A Comparative Survey of Recent Natural Language Interfaces for Databases', authors=[arxiv.Result.Author('Katrin Affolter'), arxiv.Result.Author('Kurt Stockinger'), arxiv.Result.Author('Abraham Bernstein')], summary='Over the last few years natural language interfaces (NLI) for databases have\\ngained significant traction both in academia and industry. These systems use\\nvery different approaches as described in recent survey papers. However, these\\nsystems have not been systematically compared against a set of benchmark\\nquestions in order to rigorously evaluate their functionalities and expressive\\npower.\\n  In this paper, we give an overview over 24 recently developed NLIs for\\ndatabases. Each of the systems is evaluated using a curated list of ten sample\\nquestions to show their strengths and weaknesses. We categorize the NLIs into\\nfour groups based on the methodology they are using: keyword-, pattern-,\\nparsing-, and grammar-based NLI. Overall, we learned that keyword-based systems\\nare enough to answer simple questions. To solve more complex questions\\ninvolving subqueries, the system needs to apply some sort of parsing to\\nidentify structural dependencies. Grammar-based systems are overall the most\\npowerful ones, but are highly dependent on their manually designed rules. In\\naddition to providing a systematic analysis of the major systems, we derive\\nlessons learned that are vital for designing NLIs that can answer a wide range\\nof user questions.', comment=None, journal_ref='VLDB Journal 2019', doi='10.1007/s00778-019-00567-8', primary_category='cs.DB', categories=['cs.DB', 'cs.CL', 'cs.LG'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/s00778-019-00567-8', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1906.08990v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.08990v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.09198v1', updated=datetime.datetime(2019, 6, 21, 15, 35, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 21, 15, 35, 3, tzinfo=datetime.timezone.utc), title='Explainable Fact Checking with Probabilistic Answer Set Programming', authors=[arxiv.Result.Author('Naser Ahmadi'), arxiv.Result.Author('Joohyung Lee'), arxiv.Result.Author('Paolo Papotti'), arxiv.Result.Author('Mohammed Saeed')], summary='One challenge in fact checking is the ability to improve the transparency of\\nthe decision. We present a fact checking method that uses reference information\\nin knowledge graphs (KGs) to assess claims and explain its decisions. KGs\\ncontain a formal representation of knowledge with semantic descriptions of\\nentities and their relationships. We exploit such rich semantics to produce\\ninterpretable explanations for the fact checking output. As information in a KG\\nis inevitably incomplete, we rely on logical rule discovery and on Web text\\nmining to gather the evidence to assess a given claim. Uncertain rules and\\nfacts are turned into logical programs and the checking task is modeled as an\\ninference problem in a probabilistic extension of answer set programs.\\nExperiments show that the probabilistic inference enables the efficient\\nlabeling of claims with interpretable explanations, and the quality of the\\nresults is higher than state of the art baselines.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LO'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.09198v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.09198v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.09667v6', updated=datetime.datetime(2020, 4, 11, 5, 36, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 23, 22, 57, 55, tzinfo=datetime.timezone.utc), title='On Performance Stability in LSM-based Storage Systems (Extended Version)', authors=[arxiv.Result.Author('Chen Luo'), arxiv.Result.Author('Michael J. Carey')], summary='The Log-Structured Merge-Tree (LSM-tree) has been widely adopted for use in\\nmodern NoSQL systems for its superior write performance. Despite the popularity\\nof LSM-trees, they have been criticized for suffering from write stalls and\\nlarge performance variances due to the inherent mismatch between their fast\\nin-memory writes and slow background I/O operations. In this paper, we use a\\nsimple yet effective two-phase experimental approach to evaluate write stalls\\nfor various LSM-tree designs. We further explore the design choices of LSM\\nmerge schedulers to minimize write stalls given an I/O bandwidth budget. We\\nhave conducted extensive experiments in the context of the Apache AsterixDB\\nsystem and we present the results here.', comment='This is the extended version of a paper published at VLDB 2020. The\\n  published version is available at https://doi.org/10.14778/3372716.3372719', journal_ref=None, doi='10.14778/3372716.3372719', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3372716.3372719', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1906.09667v6', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.09667v6', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.09727v3', updated=datetime.datetime(2021, 7, 5, 4, 24, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 24, 5, 20, 24, tzinfo=datetime.timezone.utc), title='Bag Query Containment and Information Theory', authors=[arxiv.Result.Author('Mahmoud Abo Khamis'), arxiv.Result.Author('Phokion G. Kolaitis'), arxiv.Result.Author('Hung Q. Ngo'), arxiv.Result.Author('Dan Suciu')], summary='The query containment problem is a fundamental algorithmic problem in data\\nmanagement. While this problem is well understood under set semantics, it is by\\nfar less understood under bag semantics. In particular, it is a long-standing\\nopen question whether or not the conjunctive query containment problem under\\nbag semantics is decidable. We unveil tight connections between information\\ntheory and the conjunctive query containment under bag semantics. These\\nconnections are established using information inequalities, which are\\nconsidered to be the laws of information theory. Our first main result asserts\\nthat deciding the validity of maxima of information inequalities is many-one\\nequivalent to the restricted case of conjunctive query containment in which the\\ncontaining query is acyclic; thus, either both these problems are decidable or\\nboth are undecidable. Our second main result identifies a new decidable case of\\nthe conjunctive query containment problem under bag semantics. Specifically, we\\ngive an exponential time algorithm for conjunctive query containment under bag\\nsemantics, provided the containing query is chordal and admits a simple\\njunction tree.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IT', 'math.IT'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.09727v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.09727v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.10261v1', updated=datetime.datetime(2019, 6, 24, 22, 52, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 24, 22, 52, 9, tzinfo=datetime.timezone.utc), title='Datalog Materialisation in Distributed RDF Stores with Dynamic Data Exchange', authors=[arxiv.Result.Author('Temitope Ajileye'), arxiv.Result.Author('Boris Motik'), arxiv.Result.Author('Ian Horrocks')], summary='Several centralised RDF systems support datalog reasoning by precomputing and\\nstoring all logically implied triples using the wellknown seminaive algorithm.\\nLarge RDF datasets often exceed the capacity of centralised RDF systems, and a\\ncommon solution is to distribute the datasets in a cluster of shared-nothing\\nservers. While numerous distributed query answering techniques are known,\\ndistributed seminaive evaluation of arbitrary datalog rules is less understood.\\nIn fact, most distributed RDF stores either support no reasoning or can handle\\nonly limited datalog fragments. In this paper we extend the dynamic data\\nexchange approach for distributed query answering by Potter et al. [12] to a\\nreasoning algorithm that can handle arbitrary rules while preserving important\\nproperties such as nonrepetition of inferences. We also show empirically that\\nour algorithm scales well to very large RDF datasets', comment='16 pages, ISWC conference', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC', 'cs.LO'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.10261v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.10261v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.11518v1', updated=datetime.datetime(2019, 6, 27, 9, 38, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 27, 9, 38, 46, tzinfo=datetime.timezone.utc), title='A Survey and Experimental Analysis of Distributed Subgraph Matching', authors=[arxiv.Result.Author('Longbin Lai'), arxiv.Result.Author('Zhu Qing'), arxiv.Result.Author('Zhengyi Yang'), arxiv.Result.Author('Xin Jin'), arxiv.Result.Author('Zhengmin Lai'), arxiv.Result.Author('Ran Wang'), arxiv.Result.Author('Kongzhang Hao'), arxiv.Result.Author('Xuemin Lin'), arxiv.Result.Author('Lu Qin'), arxiv.Result.Author('Wenjie Zhang'), arxiv.Result.Author('Ying Zhang'), arxiv.Result.Author('Zhengping Qian'), arxiv.Result.Author('Jingren Zhou')], summary='Recently there emerge many distributed algorithms that aim at solving\\nsubgraph matching at scale. Existing algorithm-level comparisons failed to\\nprovide a systematic view to the pros and cons of each algorithm mainly due to\\nthe intertwining of strategy and optimization. In this paper, we identify four\\nstrategies and three general-purpose optimizations from representative\\nstate-of-the-art works. We implement the four strategies with the optimizations\\nbased on the common Timely dataflow system for systematic strategy-level\\ncomparison. Our implementation covers all representation algorithms. We conduct\\nextensive experiments for both unlabelled matching and labelled matching to\\nanalyze the performance of distributed subgraph matching under various\\nsettings, which is finally summarized as a practical guide.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.11518v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.11518v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1906.12018v1', updated=datetime.datetime(2019, 6, 28, 2, 19, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 28, 2, 19, 19, tzinfo=datetime.timezone.utc), title='Pruned Landmark Labeling Meets Vertex Centric Computation: A Surprisingly Happy Marriage!', authors=[arxiv.Result.Author('Ruoming Jin'), arxiv.Result.Author('Zhen Peng'), arxiv.Result.Author('Wendell Wu'), arxiv.Result.Author('Feodor Dragan'), arxiv.Result.Author('Gagan Agrawal'), arxiv.Result.Author('Bin Ren')], summary='In this paper, we study how the Pruned Landmark Labeling (PPL) algorithm can\\nbe parallelized in a scalable fashion, producing the same results as the\\nsequential algorithm. More specifically, we parallelize using a Vertex-Centric\\n(VC) computational model on a modern SIMD powered multicore architecture. We\\ndesign a new VC-PLL algorithm that resolves the apparent mismatch between the\\ninherent sequential dependence of the PLL algorithm and the Vertex- Centric\\n(VC) computing model. Furthermore, we introduce a novel batch execution model\\nfor VC computation and the BVC-PLL algorithm to reduce the computational\\ninefficiency in VC-PLL. Quite surprisingly, the theoretical analysis reveals\\nthat under a reasonable assumption, BVC-PLL has lower computational and memory\\naccess costs than PLL and indicates it may run faster than PLL as a sequential\\nalgorithm. We also demonstrate how BVC-PLL algorithm can be extended to handle\\ndirected graphs and weighted graphs and how it can utilize the hierarchical\\nparallelism on a modern parallel computing architecture. Extensive experiments\\non real-world graphs not only show the sequential BVC-PLL can run more than two\\ntimes faster than the original PLL, but also demonstrates its parallel\\nefficiency and scalability.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1906.12018v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1906.12018v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.00783v3', updated=datetime.datetime(2018, 1, 27, 2, 55, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 2, 1, 22, 48, tzinfo=datetime.timezone.utc), title='A Semantic-Rich Similarity Measure in Heterogeneous Information Networks', authors=[arxiv.Result.Author('Yu Zhou'), arxiv.Result.Author('Jianbin Huang'), arxiv.Result.Author('Heli Sun')], summary='Measuring the similarities between objects in information networks has\\nfundamental importance in recommendation systems, clustering and web search.\\nThe existing metrics depend on the meta path or meta structure specified by\\nusers. In this paper, we propose a stratified meta structure based similarity\\n$SMSS$ in heterogeneous information networks. The stratified meta structure can\\nbe constructed automatically and capture rich semantics. Then, we define the\\ncommuting matrix of the stratified meta structure by virtue of the commuting\\nmatrices of meta paths and meta structures. As a result, $SMSS$ is defined by\\nvirtue of these commuting matrices. Experimental evaluations show that the\\nproposed $SMSS$ on the whole outperforms the state-of-the-art metrics in terms\\nof ranking and clustering.', comment='arXiv admin note: text overlap with arXiv:1712.09008', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1801.00783v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.00783v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.01012v1', updated=datetime.datetime(2018, 1, 3, 14, 24, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 3, 14, 24, 8, tzinfo=datetime.timezone.utc), title='Graph Pattern Matching for Dynamic Team Formation', authors=[arxiv.Result.Author('Shuai Ma'), arxiv.Result.Author('Jia Li'), arxiv.Result.Author('Chunming Hu'), arxiv.Result.Author('Xudong Liu'), arxiv.Result.Author('Jinpeng Huai')], summary='Finding a list of k teams of experts, referred to as top-k team formation,\\nwith the required skills and high collaboration compatibility has been\\nextensively studied. However, existing methods have not considered the specific\\ncollaboration relationships among different team members, i.e., structural\\nconstraints, which are typically needed in practice. In this study, we first\\npropose a novel graph pattern matching approach for top-k team formation, which\\nincorporates both structural constraints and capacity bounds. Second, we\\nformulate and study the dynamic top-k team formation problem due to the growing\\nneed of a dynamic environment. Third, we develop an unified incremental\\napproach, together with an optimization technique, to handle continuous pattern\\nand data updates, separately and simultaneously, which has not been explored\\nbefore. Finally, using real-life and synthetic data, we conduct an extensive\\nexperimental study to show the effectiveness and efficiency of our graph\\npattern matching approach for (dynamic) top-k team formation.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1801.01012v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.01012v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.03233v1', updated=datetime.datetime(2018, 1, 10, 3, 55, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 10, 3, 55, 9, tzinfo=datetime.timezone.utc), title='Eliciting Worker Preference for Task Completion', authors=[arxiv.Result.Author('Mohammadreza Esfandiari'), arxiv.Result.Author('Senjuti Basu Roy'), arxiv.Result.Author('Sihem Amer-Yahia')], summary='Current crowdsourcing platforms provide little support for worker feedback.\\nWorkers are sometimes invited to post free text describing their experience and\\npreferences in completing tasks. They can also use forums such as Turker\\nNation1 to exchange preferences on tasks and requesters. In fact, crowdsourcing\\nplatforms rely heavily on observing workers and inferring their preferences\\nimplicitly. In this work, we believe that asking workers to indicate their\\npreferences explicitly improve their experience in task completion and hence,\\nthe quality of their contributions. Explicit elicitation can indeed help to\\nbuild more accurate worker models for task completion that captures the\\nevolving nature of worker preferences. We design a worker model whose accuracy\\nis improved iteratively by requesting preferences for task factors such as\\nrequired skills, task payment, and task relevance. We propose a generic\\nframework, develop efficient solutions in realistic scenarios, and run\\nextensive experiments that show the benefit of explicit preference elicitation\\nover implicit ones with statistical significance.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1801.03233v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.03233v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.03493v1', updated=datetime.datetime(2018, 1, 10, 18, 52, 25, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 10, 18, 52, 25, tzinfo=datetime.timezone.utc), title='Focus: Querying Large Video Datasets with Low Latency and Low Cost', authors=[arxiv.Result.Author('Kevin Hsieh'), arxiv.Result.Author('Ganesh Ananthanarayanan'), arxiv.Result.Author('Peter Bodik'), arxiv.Result.Author('Paramvir Bahl'), arxiv.Result.Author('Matthai Philipose'), arxiv.Result.Author('Phillip B. Gibbons'), arxiv.Result.Author('Onur Mutlu')], summary='Large volumes of videos are continuously recorded from cameras deployed for\\ntraffic control and surveillance with the goal of answering \"after the fact\"\\nqueries: identify video frames with objects of certain classes (cars, bags)\\nfrom many days of recorded video. While advancements in convolutional neural\\nnetworks (CNNs) have enabled answering such queries with high accuracy, they\\nare too expensive and slow. We build Focus, a system for low-latency and\\nlow-cost querying on large video datasets. Focus uses cheap ingestion\\ntechniques to index the videos by the objects occurring in them. At\\ningest-time, it uses compression and video-specific specialization of CNNs.\\nFocus handles the lower accuracy of the cheap CNNs by judiciously leveraging\\nexpensive CNNs at query-time. To reduce query time latency, we cluster similar\\nobjects and hence avoid redundant processing. Using experiments on video\\nstreams from traffic, surveillance and news channels, we see that Focus uses\\n58X fewer GPU cycles than running expensive ingest processors and is 37X faster\\nthan processing all the video at query time.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CV', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1801.03493v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.03493v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.04891v3', updated=datetime.datetime(2018, 2, 26, 11, 26, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 15, 17, 58, 18, tzinfo=datetime.timezone.utc), title='Cobra: A Framework for Cost Based Rewriting of Database Applications', authors=[arxiv.Result.Author('K. Venkatesh Emani'), arxiv.Result.Author('S. Sudarshan')], summary='Database applications are typically written using a mixture of imperative\\nlanguages and declarative frameworks for data processing. Application logic\\ngets distributed across the declarative and imperative parts of a program.\\nOften, there is more than one way to implement the same program, whose\\nefficiency may depend on a number of parameters. In this paper, we propose a\\nframework that automatically generates all equivalent alternatives of a given\\nprogram using a given set of program transformations, and chooses the least\\ncost alternative. We use the concept of program regions as an algebraic\\nabstraction of a program and extend the Volcano/Cascades framework for\\noptimization of algebraic expressions, to optimize programs. We illustrate the\\nuse of our framework for optimizing database applications. We show through\\nexperimental results, that our framework has wide applicability in real world\\napplications and provides significant performance benefits.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1801.04891v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.04891v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.05161v1', updated=datetime.datetime(2018, 1, 16, 8, 55, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 16, 8, 55, 41, tzinfo=datetime.timezone.utc), title='An Integration-Oriented Ontology to Govern Evolution in Big Data Ecosystems', authors=[arxiv.Result.Author('Sergi Nadal'), arxiv.Result.Author('Oscar Romero'), arxiv.Result.Author('Alberto Abelló'), arxiv.Result.Author('Panos Vassiliadis'), arxiv.Result.Author('Stijn Vansummeren')], summary='Big Data architectures allow to flexibly store and process heterogeneous\\ndata, from multiple sources, in their original format. The structure of those\\ndata, commonly supplied by means of REST APIs, is continuously evolving. Thus\\ndata analysts need to adapt their analytical processes after each API release.\\nThis gets more challenging when performing an integrated or historical\\nanalysis. To cope with such complexity, in this paper, we present the Big Data\\nIntegration ontology, the core construct to govern the data integration process\\nunder schema evolution by systematically annotating it with information\\nregarding the schema of the sources. We present a query rewriting algorithm\\nthat, using the annotated ontology, converts queries posed over the ontology to\\nqueries over the sources. To cope with syntactic evolution in the sources, we\\npresent an algorithm that semi-automatically adapts the ontology upon new\\nreleases. This guarantees ontology-mediated queries to correctly retrieve data\\nfrom the most recent schema version as well as correctness in historical\\nqueries. A functional and performance evaluation on real-world APIs is\\nperformed to validate our approach.', comment='Preprint submitted to Information Systems. 35 pages', journal_ref=None, doi='10.1016/j.is.2018.01.006', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1016/j.is.2018.01.006', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1801.05161v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.05161v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.05360v1', updated=datetime.datetime(2018, 1, 12, 9, 36, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 12, 9, 36, 9, tzinfo=datetime.timezone.utc), title='One-Pass Trajectory Simplification Using the Synchronous Euclidean Distance', authors=[arxiv.Result.Author('Xuelian Lin'), arxiv.Result.Author('Jiahao Jiang'), arxiv.Result.Author('Shuai Ma'), arxiv.Result.Author('Yimeng Zuo'), arxiv.Result.Author('Chunming Hu')], summary='Various mobile devices have been used to collect, store and transmit\\ntremendous trajectory data, and it is known that raw trajectory data seriously\\nwastes the storage, network band and computing resource. To attack this issue,\\none-pass line simplification (LS) algorithms have are been developed, by\\ncompressing data points in a trajectory to a set of continuous line segments.\\nHowever, these algorithms adopt the perpendicular Euclidean distance, and none\\nof them uses the synchronous Euclidean distance (SED), and cannot support\\nspatio-temporal queries. To do this, we develop two one-pass error bounded\\ntrajectory simplification algorithms (CISED-S and CISED-W) using SED, based on\\na novel spatio-temporal cone intersection technique. Using four real-life\\ntrajectory datasets, we experimentally show that our approaches are both\\nefficient and effective. In terms of running time, algorithms CISED-S and\\nCISED-W are on average 3 times faster than SQUISH-E (the most efficient\\nexisting LS algorithm using SED). In terms of compression ratios, algorithms\\nCISED-S and CISED-W are comparable with and 19.6% better than DPSED (the most\\neffective existing LS algorithm using SED) on average, respectively, and are\\n21.1% and 42.4% better than SQUISH-E on average, respectively.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://arxiv.org/abs/1801.05360v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.05360v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.05613v2', updated=datetime.datetime(2018, 2, 2, 22, 7, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 17, 10, 21, 49, tzinfo=datetime.timezone.utc), title='Query2Vec: An Evaluation of NLP Techniques for Generalized Workload Analytics', authors=[arxiv.Result.Author('Shrainik Jain'), arxiv.Result.Author('Bill Howe'), arxiv.Result.Author('Jiaqi Yan'), arxiv.Result.Author('Thierry Cruanes')], summary='We consider methods for learning vector representations of SQL queries to\\nsupport generalized workload analytics tasks, including workload summarization\\nfor index selection and predicting queries that will trigger memory errors. We\\nconsider vector representations of both raw SQL text and optimized query plans,\\nand evaluate these methods on synthetic and real SQL workloads. We find that\\ngeneral algorithms based on vector representations can outperform existing\\napproaches that rely on specialized features. For index recommendation, we\\ncluster the vector representations to compress large workloads with no loss in\\nperformance from the recommended index. For error prediction, we train a\\nclassifier over learned vectors that can automatically relate subtle syntactic\\npatterns with specific errors raised during query execution. Surprisingly, we\\nalso find that these methods enable transfer learning, where a model trained on\\none SQL corpus can be applied to an unrelated corpus and still enable good\\nperformance. We find that these general approaches, when trained on a large\\ncorpus of SQL queries, provides a robust foundation for a variety of workload\\nanalysis tasks and database features, without requiring application-specific\\nfeature engineering.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/1801.05613v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.05613v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.06396v2', updated=datetime.datetime(2019, 5, 29, 15, 10, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 19, 13, 20, 56, tzinfo=datetime.timezone.utc), title='Computing Possible and Certain Answers over Order-Incomplete Data', authors=[arxiv.Result.Author('Antoine Amarilli'), arxiv.Result.Author('Mouhamadou Lamine Ba'), arxiv.Result.Author('Daniel Deutch'), arxiv.Result.Author('Pierre Senellart')], summary='This paper studies the complexity of query evaluation for databases whose\\nrelations are partially ordered; the problem commonly arises when combining or\\ntransforming ordered data from multiple sources. We focus on queries in a\\nuseful fragment of SQL, namely positive relational algebra with aggregates,\\nwhose bag semantics we extend to the partially ordered setting. Our semantics\\nleads to the study of two main computational problems: the possibility and\\ncertainty of query answers. We show that these problems are respectively\\nNP-complete and coNP-complete, but identify tractable cases depending on the\\nquery operators or input partial orders. We further introduce a duplicate\\nelimination operator and study its effect on the complexity results.', comment='55 pages, 56 references. Extended journal version of\\n  arXiv:1707.07222. Up to the stylesheet, page/environment numbering, and\\n  possible minor publisher-induced changes, this is the exact content of the\\n  journal paper that will appear in Theoretical Computer Science', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1801.06396v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.06396v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.06402v3', updated=datetime.datetime(2021, 5, 13, 14, 46, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 19, 13, 50, 47, tzinfo=datetime.timezone.utc), title='RAQ: Relationship-Aware Graph Querying in Large Networks', authors=[arxiv.Result.Author('Jithin Vachery'), arxiv.Result.Author('Akhil Arora'), arxiv.Result.Author('Sayan Ranu'), arxiv.Result.Author('Arnab Bhattacharya')], summary='The phenomenal growth of graph data from a wide variety of real-world\\napplications has rendered graph querying to be a problem of paramount\\nimportance. Traditional techniques use structural as well as node similarities\\nto find matches of a given query graph in a (large) target graph. However,\\nalmost all existing techniques have tacitly ignored the presence of\\nrelationships in graphs, which are usually encoded through interactions between\\nnode and edge labels. In this paper, we propose RAQ -- Relationship-Aware Graph\\nQuerying, to mitigate this gap. Given a query graph, RAQ identifies the $k$\\nbest matching subgraphs of the target graph that encode similar relationships\\nas in the query graph. To assess the utility of RAQ as a graph querying\\nparadigm for knowledge discovery and exploration tasks, we perform a user\\nsurvey on the Internet Movie Database (IMDb), where an overwhelming 86% of the\\n170 surveyed users preferred the relationship-aware match over traditional\\ngraph querying. The need to perform subgraph isomorphism renders RAQ NP-hard.\\nThe querying is made practical through beam stack search. Extensive experiments\\non multiple real-world graph datasets demonstrate RAQ to be effective,\\nefficient, and scalable.', comment=None, journal_ref='WWW 2019', doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1801.06402v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.06402v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.06408v1', updated=datetime.datetime(2018, 1, 19, 14, 11, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 19, 14, 11, 45, tzinfo=datetime.timezone.utc), title='PRESTO: Probabilistic Cardinality Estimation for RDF Queries Based on Subgraph Overlapping', authors=[arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Eugene Siow'), arxiv.Result.Author('Aastha Madaan'), arxiv.Result.Author('Thanassis Tiropanis')], summary='In query optimisation accurate cardinality estimation is essential for\\nfinding optimal query plans. It is especially challenging for RDF due to the\\nlack of explicit schema and the excessive occurrence of joins in RDF queries.\\nExisting approaches typically collect statistics based on the counts of triples\\nand estimate the cardinality of a query as the product of its join components,\\nwhere errors can accumulate even when the estimation of each component is\\naccurate. As opposed to existing methods, we propose PRESTO, a cardinality\\nestimation method that is based on the counts of subgraphs instead of triples\\nand uses a probabilistic method to estimate cardinalities of RDF queries as a\\nwhole. PRESTO avoids some major issues of existing approaches and is able to\\naccurately estimate arbitrary queries under a bound memory constraint. We\\nevaluate PRESTO with YAGO and show that PRESTO is more accurate for both simple\\nand complex queries.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1801.06408v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.06408v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.06965v1', updated=datetime.datetime(2018, 1, 22, 6, 35, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 22, 6, 35, 17, tzinfo=datetime.timezone.utc), title='An Efficient Density-based Clustering Algorithm for Higher-Dimensional Data', authors=[arxiv.Result.Author('Thapana Boonchoo'), arxiv.Result.Author('Xiang Ao'), arxiv.Result.Author('Qing He')], summary='DBSCAN is a typically used clustering algorithm due to its clustering ability\\nfor arbitrarily-shaped clusters and its robustness to outliers. Generally, the\\ncomplexity of DBSCAN is O(n^2) in the worst case, and it practically becomes\\nmore severe in higher dimension. Grid-based DBSCAN is one of the recent\\nimproved algorithms aiming at facilitating efficiency. However, the performance\\nof grid-based DBSCAN still suffers from two problems: neighbour explosion and\\nredundancies in merging, which make the algorithms infeasible in\\nhigh-dimensional space. In this paper, we propose a novel algorithm named GDPAM\\nattempting to extend Grid-based DBSCAN to higher data dimension. In GDPAM, a\\nbitmap indexing is utilized to manage non-empty grids so that the neighbour\\ngrid queries can be performed efficiently. Furthermore, we adopt an efficient\\nunion-find algorithm to maintain the clustering information in order to reduce\\nredundancies in the merging. The experimental results on both real-world and\\nsynthetic datasets demonstrate that the proposed algorithm outperforms the\\nstate-of-the-art exact/approximate DBSCAN and suggests a good scalability.', comment='8 pages, 7 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1801.06965v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.06965v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.07005v1', updated=datetime.datetime(2018, 1, 22, 9, 23, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 22, 9, 23, 42, tzinfo=datetime.timezone.utc), title='ACGreGate: A Framework for Practical Access Control for Applications using Weakly Consistent Databases', authors=[arxiv.Result.Author('Mathias Weber'), arxiv.Result.Author('Annette Bieniusa')], summary='Scalable and highly available systems often require data stores that offer\\nweaker consistency guarantees than traditional relational databases systems.\\nThe correctness of these applications highly depends on the resilience of the\\napplication model against data inconsistencies. In particular regarding\\napplication security, it is difficult to determine which inconsistencies can be\\ntolerated and which might lead to security breaches.\\n  In this paper, we discuss the problem of how to develop an access control\\nlayer for applications using weakly consistent data stores without loosing the\\nperformance benefits gained by using weaker consistency models. We present\\nACGreGate, a Java framework for implementing correct access control layers for\\napplications using weakly consistent data stores. Under certain requirements on\\nthe data store, ACGreGate ensures that the access control layer operates\\ncorrectly with respect to dynamically adaptable security policies. We used\\nACGreGate to implement the access control layer of a student management system.\\nThis case study shows that practically useful security policies can be\\nimplemented with the framework incurring little overhead. A comparison with a\\nsetup using a centralized server shows the benefits of using ACGreGate for\\nscalability of the service to geo-scale.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1801.07005v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.07005v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.07947v1', updated=datetime.datetime(2018, 1, 24, 12, 10, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 24, 12, 10, 46, tzinfo=datetime.timezone.utc), title='TritanDB: Time-series Rapid Internet of Things Analytics', authors=[arxiv.Result.Author('Eugene Siow'), arxiv.Result.Author('Thanassis Tiropanis'), arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Wendy Hall')], summary='The efficient management of data is an important prerequisite for realising\\nthe potential of the Internet of Things (IoT). Two issues given the large\\nvolume of structured time-series IoT data are, addressing the difficulties of\\ndata integration between heterogeneous Things and improving ingestion and query\\nperformance across databases on both resource-constrained Things and in the\\ncloud. In this paper, we examine the structure of public IoT data and discover\\nthat the majority exhibit unique flat, wide and numerical characteristics with\\na mix of evenly and unevenly-spaced time-series. We investigate the advances in\\ntime-series databases for telemetry data and combine these findings with\\nmicrobenchmarks to determine the best compression techniques and storage data\\nstructures to inform the design of a novel solution optimised for IoT data. A\\nquery translation method with low overhead even on resource-constrained Things\\nallows us to utilise rich data models like the Resource Description Framework\\n(RDF) for interoperability and data integration on top of the optimised\\nstorage. Our solution, TritanDB, shows an order of magnitude performance\\nimprovement across both Things and cloud hardware on many state-of-the-art\\ndatabases within IoT scenarios. Finally, we describe how TritanDB supports\\nvarious analyses of IoT time-series data like forecasting.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1801.07947v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.07947v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.09240v3', updated=datetime.datetime(2018, 9, 4, 3, 9, 37, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 28, 14, 43, 4, tzinfo=datetime.timezone.utc), title='Time Constrained Continuous Subgraph Search over Streaming Graphs', authors=[arxiv.Result.Author('Youhuan Li'), arxiv.Result.Author('Lei Zou'), arxiv.Result.Author('M. Tamer Ozsu'), arxiv.Result.Author('Dongyan Zhao')], summary='The growing popularity of dynamic applications such as social networks\\nprovides a promising way to detect valuable information in real time. Efficient\\nanalysis over high-speed data from dynamic applications is of great\\nsignificance. Data from these dynamic applications can be easily modeled as\\nstreaming graph. In this paper, we study the subgraph (isomorphism) search over\\nstreaming graph data that obeys timing order constraints over the occurrence of\\nedges in the stream. We propose a data structure and algorithm to efficiently\\nanswer subgraph search and introduce optimizations to greatly reduce the space\\ncost, and propose concurrency management to improve system throughput.\\nExtensive experiments on real network traffic data and synthetic social\\nstreaming data confirms the efficiency and effectiveness of our solution.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1801.09240v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.09240v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.09556v1', updated=datetime.datetime(2018, 1, 25, 23, 15, 36, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 25, 23, 15, 36, tzinfo=datetime.timezone.utc), title='Killing Two Birds with One Stone -- Querying Property Graphs using SPARQL via GREMLINATOR', authors=[arxiv.Result.Author('Harsh Thakkar'), arxiv.Result.Author('Dharmen Punjani'), arxiv.Result.Author('Jens Lehmann'), arxiv.Result.Author('Sören Auer')], summary='Knowledge graphs have become popular over the past decade and frequently rely\\non the Resource Description Framework (RDF) or Property Graph (PG) databases as\\ndata models. However, the query languages for these two data models -- SPARQL\\nfor RDF and the PG traversal language Gremlin -- are lacking interoperability.\\nWe present Gremlinator, the first translator from SPARQL -- the W3C\\nstandardized language for RDF -- and Gremlin -- a popular property graph\\ntraversal language. Gremlinator translates SPARQL queries to Gremlin path\\ntraversals for executing graph pattern matching queries over graph databases.\\nThis allows a user, who is well versed in SPARQL, to access and query a wide\\nvariety of Graph Data Management Systems (DMSs) avoiding the steep learning\\ncurve for adapting to a new Graph Query Language (GQL). Gremlin is a graph\\ncomputing system-agnostic traversal language (covering both OLTP graph database\\nor OLAP graph processors), making it a desirable choice for supporting\\ninteroperability for querying Graph DMSs. Gremlinator currently supports the\\ntranslation of a subset of SPARQL 1.0, specifically the SPARQL SELECT queries.', comment='4 pages, 8 figures, DEMO paper submission. arXiv admin note: text\\n  overlap with arXiv:1801.02911', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1801.09556v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.09556v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.09619v2', updated=datetime.datetime(2018, 2, 16, 11, 52, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 29, 16, 48, 33, tzinfo=datetime.timezone.utc), title='Estimating the Cardinality of Conjunctive Queries over RDF Data Using Graph Summarisation', authors=[arxiv.Result.Author('Giorgio Stefanoni'), arxiv.Result.Author('Boris Motik'), arxiv.Result.Author('Egor V. Kostylev')], summary='Estimating the cardinality (i.e., the number of answers) of conjunctive\\nqueries is particularly difficult in RDF systems: queries over RDF data are\\nnavigational and thus tend to involve many joins. We present a new, principled\\ncardinality estimation technique based on graph summarisation. We interpret a\\nsummary of an RDF graph using a possible world semantics and formalise the\\nestimation problem as computing the expected cardinality over all RDF graphs\\nrepresented by the summary, and we present a closed-form formula for computing\\nthe expectation of arbitrary queries. We also discuss approaches to RDF graph\\nsummarisation. Finally, we show empirically that our cardinality technique is\\nmore accurate and more consistent, often by orders of magnitude, than the state\\nof the art.', comment=None, journal_ref=None, doi='10.1145/3178876.3186003', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3178876.3186003', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1801.09619v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.09619v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1801.09802v2', updated=datetime.datetime(2018, 6, 19, 5, 1, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 1, 30, 0, 2, 57, tzinfo=datetime.timezone.utc), title='Automatically Leveraging MapReduce Frameworks for Data-Intensive Applications', authors=[arxiv.Result.Author('Maaz Bin Safeer Ahmad'), arxiv.Result.Author('Alvin Cheung')], summary='MapReduce is a popular programming paradigm for developing large-scale,\\ndata-intensive computation. Many frameworks that implement this paradigm have\\nrecently been developed. To leverage these frameworks, however, developers must\\nbecome familiar with their APIs and rewrite existing code. Casper is a new tool\\nthat automatically translates sequential Java programs into the MapReduce\\nparadigm. Casper identifies potential code fragments to rewrite and translates\\nthem in two steps: (1) Casper uses program synthesis to search for a program\\nsummary (i.e., a functional specification) of each code fragment. The summary\\nis expressed using a high-level intermediate language resembling the MapReduce\\nparadigm and verified to be semantically equivalent to the original using a\\ntheorem prover. (2) Casper generates executable code from the summary, using\\neither the Hadoop, Spark, or Flink API. We evaluated Casper by automatically\\nconverting real-world, sequential Java benchmarks to MapReduce. The resulting\\nbenchmarks perform up to 48.2x faster compared to the original.', comment='12 pages, additional 4 pages of references and appendix', journal_ref=\"SIGMOD '18 Proceedings of the 2018 International Conference on\\n  Management of Data, Pages 1205-1220\", doi='10.1145/3183713.3196891', primary_category='cs.DB', categories=['cs.DB', 'cs.DC', 'cs.PL'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3183713.3196891', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1801.09802v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1801.09802v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1802.00259v1', updated=datetime.datetime(2018, 2, 1, 12, 17, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 2, 1, 12, 17, 54, tzinfo=datetime.timezone.utc), title='Anomaly Detection in Log Data using Graph Databases and Machine Learning to Defend Advanced Persistent Threats', authors=[arxiv.Result.Author('Timo Schindler')], summary='Advanced Persistent Threats (APTs) are a main impendence in cyber security of\\ncomputer networks. In 2015, a successful breach remains undetected 146 days on\\naverage, reported by [Fi16].With our work we demonstrate a feasible and fast\\nway to analyse real world log data to detect breaches or breach attempts. By\\nadapting well-known kill chain mechanisms and a combine of a time series\\ndatabase and an abstracted graph approach, it is possible to create flexible\\nattack profiles. Using this approach, it can be demonstrated that the graph\\nanalysis successfully detects simulated attacks by analysing the log data of a\\nsimulated computer network. Considering another source for log data, the\\nframework is capable to deliver sufficient performance for analysing real-world\\ndata in short time. By using the computing power of the graph database it is\\npossible to identify the attacker and furthermore it is feasible to detect\\nother affected system components. We believe to significantly reduce the\\ndetection time of breaches with this approach and react fast to new attack\\nvectors.', comment='Lecture Notes in Informatics (LNI), Gesellschaft f\\\\\"ur Informatik,\\n  Bonn 2017 2371', journal_ref=None, doi='10.18420/in2017_241', primary_category='cs.DB', categories=['cs.DB', 'cs.CR'], links=[arxiv.Result.Link('http://dx.doi.org/10.18420/in2017_241', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1802.00259v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1802.00259v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1802.00304v1', updated=datetime.datetime(2018, 2, 1, 14, 41, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 2, 1, 14, 41, 33, tzinfo=datetime.timezone.utc), title='Distributed Clustering Algorithm for Spatial Data Mining', authors=[arxiv.Result.Author('Malika Bendechache'), arxiv.Result.Author('M-Tahar Kechadi')], summary='Distributed data mining techniques and mainly distributed clustering are\\nwidely used in the last decade because they deal with very large and\\nheterogeneous datasets which cannot be gathered centrally. Current distributed\\nclustering approaches are normally generating global models by aggregating\\nlocal results that are obtained on each site. While this approach mines the\\ndatasets on their locations the aggregation phase is complex, which may produce\\nincorrect and ambiguous global clusters and therefore incorrect knowledge. In\\nthis paper we propose a new clustering approach for very large spatial datasets\\nthat are heterogeneous and distributed. The approach is based on K-means\\nAlgorithm but it generates the number of global clusters dynamically. Moreover,\\nthis approach uses an elaborated aggregation phase. The aggregation phase is\\ndesigned in such a way that the overall process is efficient in time and memory\\nallocation. Preliminary results show that the proposed approach produces high\\nquality results and scales up well. We also compared it to two popular\\nclustering algorithms and show that this approach is much more efficient.', comment='6 pages. arXiv admin note: text overlap with arXiv:1704.03421', journal_ref='Spatial Data Mining and Geographical Knowledge Services (ICSDM),\\n  2015 2nd IEEE International Conference on, pages 60--65, 2015', doi='10.1109/ICSDM.2015.7298026', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/ICSDM.2015.7298026', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1802.00304v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1802.00304v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1802.00688v1', updated=datetime.datetime(2018, 2, 1, 12, 50, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 2, 1, 12, 50, 59, tzinfo=datetime.timezone.utc), title='Hierarchical Aggregation Approach for Distributed clustering of spatial datasets', authors=[arxiv.Result.Author('Malika Bendechache'), arxiv.Result.Author('Nhien-An Le-Khac'), arxiv.Result.Author('M-Tahar Kechadi')], summary='In this paper, we present a new approach of distributed clustering for\\nspatial datasets, based on an innovative and efficient aggregation technique.\\nThis distributed approach consists of two phases: 1) local clustering phase,\\nwhere each node performs a clustering on its local data, 2) aggregation phase,\\nwhere the local clusters are aggregated to produce global clusters. This\\napproach is characterised by the fact that the local clusters are represented\\nin a simple and efficient way. And The aggregation phase is designed in such a\\nway that the final clusters are compact and accurate while the overall process\\nis efficient in both response time and memory allocation. We evaluated the\\napproach with different datasets and compared it to well-known clustering\\ntechniques. The experimental results show that our approach is very promising\\nand outperforms all those algorithms', comment='6 pages. arXiv admin note: substantial text overlap with\\n  arXiv:1704.03421', journal_ref=None, doi='10.1109/ICDMW.2016.0158', primary_category='cs.DB', categories=['cs.DB', 'cs.DC', 'cs.LG'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/ICDMW.2016.0158', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1802.00688v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1802.00688v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1802.02229v3', updated=datetime.datetime(2018, 5, 24, 1, 20, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 2, 6, 21, 40, 50, tzinfo=datetime.timezone.utc), title='Axiomatic Foundations and Algorithms for Deciding Semantic Equivalences of SQL Queries', authors=[arxiv.Result.Author('Shumo Chu'), arxiv.Result.Author('Brendan Murphy'), arxiv.Result.Author('Jared Roesch'), arxiv.Result.Author('Alvin Cheung'), arxiv.Result.Author('Dan Suciu')], summary=\"Deciding the equivalence of SQL queries is a fundamental problem in data\\nmanagement. As prior work has mainly focused on studying the theoretical\\nlimitations of the problem, very few implementations for checking such\\nequivalences exist. In this paper, we present a new formalism and\\nimplementation for reasoning about the equivalences of SQL queries. Our\\nformalism, U-semiring, extends SQL's semiring semantics with unbounded\\nsummation and duplicate elimination. U-semiring is defined using only very few\\naxioms and can thus be easily implemented using proof assistants such as Coq\\nfor automated query reasoning. Yet, they are sufficient enough to enable us\\nreason about sophisticated SQL queries that are evaluated over bags and sets,\\nalong with various integrity constraints. To evaluate the effectiveness of\\nU-semiring, we have used it to formally verify 39 query rewrite rules from both\\nclassical data management research papers and real-world SQL engines, where\\nmany of them have never been proven correct before.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.PL'], links=[arxiv.Result.Link('http://arxiv.org/abs/1802.02229v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1802.02229v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1802.03638v2', updated=datetime.datetime(2018, 2, 13, 13, 48, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 2, 10, 18, 46, 54, tzinfo=datetime.timezone.utc), title='Beyond Markov Logic: Efficient Mining of Prediction Rules in Large Graphs', authors=[arxiv.Result.Author('Tommaso Soru'), arxiv.Result.Author('André Valdestilhas'), arxiv.Result.Author('Edgard Marx'), arxiv.Result.Author('Axel-Cyrille Ngonga Ngomo')], summary='Graph representations of large knowledge bases may comprise billions of\\nedges. Usually built upon human-generated ontologies, several knowledge bases\\ndo not feature declared ontological rules and are far from being complete.\\nCurrent rule mining approaches rely on schemata or store the graph in-memory,\\nwhich can be unfeasible for large graphs. In this paper, we introduce\\nHornConcerto, an algorithm to discover Horn clauses in large graphs without the\\nneed of a schema. Using a standard fact-based confidence score, we can mine\\nclose Horn rules having an arbitrary body size. We show that our method can\\noutperform existing approaches in terms of runtime and memory consumption and\\nmine high-quality rules for the link prediction task, achieving\\nstate-of-the-art results on a widely-used benchmark. Moreover, we find that\\nrules alone can perform inference significantly faster than embedding-based\\nmethods and achieve accuracies on link prediction comparable to\\nresource-demanding approaches such as Markov Logic Networks.', comment='13 pages, 4 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'G.3.8; E.1.3'], links=[arxiv.Result.Link('http://arxiv.org/abs/1802.03638v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1802.03638v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1802.04060v1', updated=datetime.datetime(2018, 2, 12, 14, 24, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 2, 12, 14, 24, 55, tzinfo=datetime.timezone.utc), title='Notable Characteristics Search through Knowledge Graphs', authors=[arxiv.Result.Author('Davide Mottin'), arxiv.Result.Author('Bastian Grasnick'), arxiv.Result.Author('Axel Kroschk'), arxiv.Result.Author('Patrick Siegler'), arxiv.Result.Author('Emmanuel Mueller')], summary='Query answering routinely employs knowledge graphs to assist the user in the\\nsearch process. Given a knowledge graph that represents entities and\\nrelationships among them, one aims at complementing the search with intuitive\\nbut effective mechanisms. In particular, we focus on the comparison of two or\\nmore entities and the detection of unexpected, surprising properties, called\\nnotable characteristics. Such characteristics provide intuitive explanations of\\nthe peculiarities of the selected entities with respect to similar entities. We\\npropose a solid probabilistic approach that first retrieves entity nodes\\nsimilar to the query nodes provided by the user, and then exploits\\ndistributional properties to understand whether a certain attribute is\\ninteresting or not. Our preliminary experiments demonstrate the solidity of our\\napproach and show that we are able to discover notable characteristics that are\\nindeed interesting and relevant for the user.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1802.04060v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1802.04060v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1802.04613v4', updated=datetime.datetime(2020, 2, 24, 9, 35, 57, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 2, 13, 13, 29, 50, tzinfo=datetime.timezone.utc), title='First-order queries on classes of structures with bounded expansion', authors=[arxiv.Result.Author('Wojtek Kazana'), arxiv.Result.Author('Luc Segoufin')], summary='We consider the evaluation of first-order queries over classes of databases\\nwith bounded expansion. The notion of bounded expansion is fairly broad and\\ngeneralizes bounded degree, bounded treewidth and exclusion of at least one\\nminor. It was known that over a class of databases with bounded expansion,\\nfirst-order sentences could be evaluated in time linear in the size of the\\ndatabase. We give a different proof of this result. Moreover, we show that\\nanswers to first-order queries can be enumerated with constant delay after a\\nlinear time preprocessing. We also show that counting the number of answers to\\na query can be done in time linear in the size of the database.', comment=None, journal_ref='Logical Methods in Computer Science, Volume 16, Issue 1 (February\\n  25, 2020) lmcs:6156', doi='10.23638/LMCS-16(1:25)2020', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.23638/LMCS-16(1:25)2020', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1802.04613v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1802.04613v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1802.05898v1', updated=datetime.datetime(2018, 2, 16, 11, 25, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 2, 16, 11, 25, 15, tzinfo=datetime.timezone.utc), title='PRoST: Distributed Execution of SPARQL Queries Using Mixed Partitioning Strategies', authors=[arxiv.Result.Author('Matteo Cossu'), arxiv.Result.Author('Michael Färber'), arxiv.Result.Author('Georg Lausen')], summary='The rapidly growing size of RDF graphs in recent years necessitates\\ndistributed storage and parallel processing strategies. To obtain efficient\\nquery processing using computer clusters a wide variety of different approaches\\nhave been proposed. Related to the approach presented in the current paper are\\nsystems built on top of Hadoop HDFS, for example using Apache Accumulo or using\\nApache Spark. We present a new RDF store called PRoST (Partitioned RDF on Spark\\nTables) based on Apache Spark. PRoST introduces an innovative strategy that\\ncombines the Vertical Partitioning approach with the Property Table, two\\npreexisting models for storing RDF datasets. We demonstrate that our proposal\\noutperforms state-of-the-art systems w.r.t. the runtime for a wide range of\\nquery types and without any extensive precomputing phase.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1802.05898v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1802.05898v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1802.08052v1', updated=datetime.datetime(2018, 2, 22, 14, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 2, 22, 14, 24, tzinfo=datetime.timezone.utc), title='Data Consistency Simulation Tool for NoSQL Database Systems', authors=[arxiv.Result.Author('Nazim Faour')], summary='Various data consistency levels have an important part in the integrity of\\ndata and also affect performance especially the data that is replicated many\\ntimes across or over the cluster. Based on BASE and the theorem of CAP\\ntradeoffs, most systems of NoSQL have more relaxed consistency guarantees than\\nanother kind of databases which implement ACID. Most systems of NoSQL gave\\ndifferent methods to adjust a required level of consistency to ensure the\\nminimal numbering of the replicas accepted in each operation. Simulations are\\nalways depending on a simplified model and ignore many details and facts about\\nthe real system. Therefore, a simulation can only work as an estimation or an\\nexplanation vehicle for observed behavior. So to create simulation tool, I have\\nto characterize a model, identify influence factors and simply implement that\\ndepending on a (modeled) workload. In this paper, I have a model of simulation\\nto measure the consistency of the data and to detect the data consistency\\nviolations in simulated network partition settings. So workloads are needed\\nwith the set of users who make requests and then put the results for analysis.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1802.08052v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1802.08052v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1802.09180v1', updated=datetime.datetime(2018, 2, 26, 6, 50, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 2, 26, 6, 50, 43, tzinfo=datetime.timezone.utc), title='Cuttlefish: A Lightweight Primitive for Adaptive Query Processing', authors=[arxiv.Result.Author('Tomer Kaftan'), arxiv.Result.Author('Magdalena Balazinska'), arxiv.Result.Author('Alvin Cheung'), arxiv.Result.Author('Johannes Gehrke')], summary=\"Modern data processing applications execute increasingly sophisticated\\nanalysis that requires operations beyond traditional relational algebra. As a\\nresult, operators in query plans grow in diversity and complexity. Designing\\nquery optimizer rules and cost models to choose physical operators for all of\\nthese novel logical operators is impractical. To address this challenge, we\\ndevelop Cuttlefish, a new primitive for adaptively processing online query\\nplans that explores candidate physical operator instances during query\\nexecution and exploits the fastest ones using multi-armed bandit reinforcement\\nlearning techniques. We prototype Cuttlefish in Apache Spark and adaptively\\nchoose operators for image convolution, regular expression matching, and\\nrelational joins. Our experiments show Cuttlefish-based adaptive convolution\\nand regular expression operators can reach 72-99% of the throughput of an\\nall-knowing oracle that always selects the optimal algorithm, even when\\nindividual physical operators are up to 105x slower than the optimal.\\nAdditionally, Cuttlefish achieves join throughput improvements of up to 7.5x\\ncompared with Spark SQL's query optimizer.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1802.09180v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1802.09180v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1802.09488v1', updated=datetime.datetime(2018, 2, 26, 18, 11, 36, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 2, 26, 18, 11, 36, tzinfo=datetime.timezone.utc), title='Adaptive Geospatial Joins for Modern Hardware', authors=[arxiv.Result.Author('Andreas Kipf'), arxiv.Result.Author('Harald Lang'), arxiv.Result.Author('Varun Pandey'), arxiv.Result.Author('Raul Alexandru Persa'), arxiv.Result.Author('Peter Boncz'), arxiv.Result.Author('Thomas Neumann'), arxiv.Result.Author('Alfons Kemper')], summary=\"Geospatial joins are a core building block of connected mobility\\napplications. An especially challenging problem are joins between streaming\\npoints and static polygons. Since points are not known beforehand, they cannot\\nbe indexed. Nevertheless, points need to be mapped to polygons with low\\nlatencies to enable real-time feedback.\\n  We present an adaptive geospatial join that uses true hit filtering to avoid\\nexpensive geometric computations in most cases. Our technique uses a\\nquadtree-based hierarchical grid to approximate polygons and stores these\\napproximations in a specialized radix tree. We emphasize on an approximate\\nversion of our algorithm that guarantees a user-defined precision. The exact\\nversion of our algorithm can adapt to the expected point distribution by\\nrefining the index. We optimized our implementation for modern hardware\\narchitectures with wide SIMD vector processing units, including Intel's brand\\nnew Knights Landing. Overall, our approach can perform up to two orders of\\nmagnitude faster than existing techniques.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1802.09488v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1802.09488v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1802.09594v1', updated=datetime.datetime(2018, 2, 26, 20, 32, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 2, 26, 20, 32, 5, tzinfo=datetime.timezone.utc), title='All nearest neighbor calculation based on Delaunay graphs', authors=[arxiv.Result.Author('Nasrin Mazaheri Soudani'), arxiv.Result.Author('Ali Karami')], summary='When we have two data sets and want to find the nearest neighbour of each\\npoint in the first dataset among points in the second one, we need the all\\nnearest neighbour operator. This is an operator in spatial databases that has\\nmany application in different fields such as GIS and VLSI circuit design.\\nExisting algorithms for calculating this operator assume that there is no pre\\ncomputation on these data sets. These algorithms has o(n*m*d) time complexity\\nwhere n and m are the number of points in two data sets and d is the dimension\\nof data points. With assumption of some pre computation on data sets algorithms\\nwith lower time complexity can be obtained. One of the most common pre\\ncomputation on spatial data is Delaunay graphs. In the Delaunay graph of a data\\nset each point is linked to its nearest neighbours. In this paper, we introduce\\nan algorithm for computing the all nearest neighbour operator on spatial data\\nsets based on their Delaunay graphs. The performance of this algorithm is\\ncompared with one of the best existing algorithms for computing ANN operator in\\nterms of CPU time and the number of IOs. The experimental results show that\\nthis algorithm has better performance than the other.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1802.09594v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1802.09594v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1802.10233v1', updated=datetime.datetime(2018, 2, 28, 2, 10, 36, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 2, 28, 2, 10, 36, tzinfo=datetime.timezone.utc), title='Apache Calcite: A Foundational Framework for Optimized Query Processing Over Heterogeneous Data Sources', authors=[arxiv.Result.Author('Edmon Begoli'), arxiv.Result.Author('Jesús Camacho Rodríguez'), arxiv.Result.Author('Julian Hyde'), arxiv.Result.Author('Michael J. Mior'), arxiv.Result.Author('Daniel Lemire')], summary=\"Apache Calcite is a foundational software framework that provides query\\nprocessing, optimization, and query language support to many popular\\nopen-source data processing systems such as Apache Hive, Apache Storm, Apache\\nFlink, Druid, and MapD. Calcite's architecture consists of a modular and\\nextensible query optimizer with hundreds of built-in optimization rules, a\\nquery processor capable of processing a variety of query languages, an adapter\\narchitecture designed for extensibility, and support for heterogeneous data\\nmodels and stores (relational, semi-structured, streaming, and geospatial).\\nThis flexible, embeddable, and extensible architecture is what makes Calcite an\\nattractive choice for adoption in big-data frameworks. It is an active project\\nthat continues to introduce support for the new types of data sources, query\\nlanguages, and approaches to query processing and optimization.\", comment=\"SIGMOD'18\", journal_ref=None, doi='10.1145/3183713.3190662', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3183713.3190662', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1802.10233v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1802.10233v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1811.00602v1', updated=datetime.datetime(2018, 11, 1, 19, 35, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 11, 1, 19, 35, 11, tzinfo=datetime.timezone.utc), title='VizRec: A framework for secure data exploration via visual representation', authors=[arxiv.Result.Author('Lorenzo De Stefani'), arxiv.Result.Author('Leonhard F. Spiegelberg'), arxiv.Result.Author('Tim Kraska'), arxiv.Result.Author('Eli Upfal')], summary='Visual representations of data (visualizations) are tools of great importance\\nand widespread use in data analytics as they provide users visual insight to\\npatterns in the observed data in a simple and effective way. However, since\\nvisualizations tools are applied to sample data, there is a a risk of\\nvisualizing random fluctuations in the sample rather than a true pattern in the\\ndata. This problem is even more significant when visualization is used to\\nidentify interesting patterns among many possible possibilities, or to identify\\nan interesting deviation in a pair of observations among many possible pairs,\\nas commonly done in visual recommendation systems.\\n  We present VizRec, a framework for improving the performance of visual\\nrecommendation systems by quantifying the statistical significance of\\nrecommended visualizations. The proposed methodology allows to control the\\nprobability of misleading visual recommendations using both classical\\nstatistical testing procedures and a novel application of the Vapnik\\nChervonenkis (VC) dimension method which is a fundamental concept in\\nstatistical learning theory.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1811.00602v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1811.00602v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1811.01660v1', updated=datetime.datetime(2018, 11, 5, 13, 16, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 11, 5, 13, 16, 54, tzinfo=datetime.timezone.utc), title='Data Integration for Supporting Biomedical Knowledge Graph Creation at Large-Scale', authors=[arxiv.Result.Author('Samaneh Jozashoori'), arxiv.Result.Author('Tatiana Novikova'), arxiv.Result.Author('Maria-Esther Vidal')], summary='In recent years, following FAIR and open data principles, the number of\\navailable big data including biomedical data has been increased exponentially.\\nIn order to extract knowledge, these data should be curated, integrated, and\\nsemantically described. Accordingly, several semantic integration techniques\\nhave been developed; albeit effective, they may suffer from scalability in\\nterms of different properties of big data. Even scaled-up approaches may be\\nhighly costly because tasks of semantification, curation and integration are\\nperformed independently. In order to overcome these issues, we devise ConMap, a\\nsemantic integration approach which exploits knowledge encoded in ontology in\\norder to describe mapping rules to perform these tasks at the same time.\\nExperimental results performed on different data sets suggest that ConMap can\\nsignificantly reduce the time required for knowledge graph creation by up to\\n70\\\\% of the time that is consumed following a traditional approach.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1811.01660v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1811.01660v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1811.02059v3', updated=datetime.datetime(2019, 7, 21, 13, 9, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 11, 5, 22, 20, 25, tzinfo=datetime.timezone.utc), title='STAR: Scaling Transactions through Asymmetric Replication', authors=[arxiv.Result.Author('Yi Lu'), arxiv.Result.Author('Xiangyao Yu'), arxiv.Result.Author('Samuel Madden')], summary='In this paper, we present STAR, a new distributed in-memory database with\\nasymmetric replication. By employing a single-node non-partitioned architecture\\nfor some replicas and a partitioned architecture for other replicas, STAR is\\nable to efficiently run both highly partitionable workloads and workloads that\\ninvolve cross-partition transactions. The key idea is a new phase-switching\\nalgorithm where the execution of single-partition and cross-partition\\ntransactions is separated. In the partitioned phase, single-partition\\ntransactions are run on multiple machines in parallel to exploit more\\nconcurrency. In the single-master phase, mastership for the entire database is\\nswitched to a single designated master node, which can execute these\\ntransactions without the use of expensive coordination protocols like two-phase\\ncommit. Because the master node has a full copy of the database, this\\nphase-switching can be done at negligible cost. Our experiments on two popular\\nbenchmarks (YCSB and TPC-C) show that high availability via replication can\\ncoexist with fast serializable transaction execution in distributed in-memory\\ndatabases, with STAR outperforming systems that employ conventional concurrency\\ncontrol and replication algorithms by up to one order of magnitude.', comment=None, journal_ref='PVLDB, 12(11): 1316- 1329, 2019', doi='10.14778/3342263.3342270', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3342263.3342270', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1811.02059v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1811.02059v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1811.04160v1', updated=datetime.datetime(2018, 11, 9, 23, 55, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 11, 9, 23, 55, 14, tzinfo=datetime.timezone.utc), title='Meet Cyrus - The Query by Voice Mobile Assistant for the Tutoring and Formative Assessment of SQL Learners', authors=[arxiv.Result.Author('Josue Espinosa Godinez'), arxiv.Result.Author('Hasan M. Jamil')], summary=\"Being declarative, SQL stands a better chance at being the programming\\nlanguage for conceptual computing next to natural language programming. We\\nexamine the possibility of using SQL as a back-end for natural language\\ndatabase programming. Distinctly from keyword based SQL querying, keyword\\ndependence and SQL's table structure constraints are significantly less\\npronounced in our approach. We present a mobile device voice query interface,\\ncalled Cyrus, to arbitrary relational databases. Cyrus supports a large type of\\nquery classes, sufficient for an entry level database class. Cyrus is also\\napplication independent, allows test database adaptation, and not limited to\\nspecific sets of keywords or natural language sentence structures. It's\\ncooperative error reporting is more intuitive, and iOS based mobile platform is\\nalso more accessible compared to most contemporary mobile and voice enabled\\nsystems.\", comment='6 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1811.04160v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1811.04160v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1811.04967v1', updated=datetime.datetime(2018, 11, 12, 19, 15, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 11, 12, 19, 15, 27, tzinfo=datetime.timezone.utc), title='The Impact of Timestamp Granularity in Optimistic Concurrency Control', authors=[arxiv.Result.Author('Yihe Huang'), arxiv.Result.Author('Hao Bai'), arxiv.Result.Author('Eddie Kohler'), arxiv.Result.Author('Barbara Liskov'), arxiv.Result.Author('Liuba Shrira')], summary=\"Optimistic concurrency control (OCC) can exploit the strengths of parallel\\nhardware to provide excellent performance for uncontended transactions, and is\\npopular in high-performance in-memory databases and transactional systems. But\\nat high contention levels, OCC is susceptible to frequent aborts, leading to\\nwasted work and degraded performance. Contention managers, mixed\\noptimistic/pessimistic concurrency control algorithms, and novel\\noptimistic-inspired concurrency control algorithms, such as TicToc, aim to\\naddress this problem, but these mechanisms introduce sometimes-high overheads\\nof their own. We show that in real-world benchmarks, traditional OCC can\\noutperform these alternative mechanisms by simply adding fine-grained version\\ntimestamps (using different timestamps for disjoint components of each record).\\nWith fine-grained timestamps, OCC gets 1.14x TicToc's throughput in TPC-C at\\n128 cores (previous work reported TicToc having 1.8x higher throughput than OCC\\nat 80 hyperthreads). Our study shows that timestamp granularity has a greater\\nimpact than previously thought on the performance of transaction processing\\nsystems, and should not be overlooked in the push for faster concurrency\\ncontrol schemes.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1811.04967v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1811.04967v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1811.05065v1', updated=datetime.datetime(2018, 11, 13, 2, 1, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 11, 13, 2, 1, 32, tzinfo=datetime.timezone.utc), title='PanJoin: A Partition-based Adaptive Stream Join', authors=[arxiv.Result.Author('Fei Pan'), arxiv.Result.Author('Hans-Arno Jacobsen')], summary='In stream processing, stream join is one of the critical sources of\\nperformance bottlenecks. The sliding-window-based stream join provides a\\nprecise result but consumes considerable computational resources. The current\\nsolutions lack support for the join predicates on large windows. These\\nalgorithms and their hardware accelerators are either limited to equi-join or\\nuse a nested loop join to process all the requests.\\n  In this paper, we present a new algorithm called PanJoin which has high\\nthroughput on large windows and supports both equi-join and non-equi-join.\\nPanJoin implements three new data structures to reduce computations during the\\nprobing phase of stream join. We also implement the most hardware-friendly data\\nstructure, called BI-Sort, on FPGA. Our evaluation shows that PanJoin\\noutperforms several recently proposed stream join methods by more than 1000x,\\nand it also adapts well to highly skewed data.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1811.05065v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1811.05065v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1811.05361v1', updated=datetime.datetime(2018, 11, 13, 15, 26, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 11, 13, 15, 26, 30, tzinfo=datetime.timezone.utc), title='Personal Names Popularity Estimation and its Application to Record Linkage', authors=[arxiv.Result.Author('Ksenia Zhagorina'), arxiv.Result.Author('Pavel Braslavski'), arxiv.Result.Author('Vladimir Gusev')], summary='This study deals with a fairly simply formulated problem -- how to estimate\\nthe number of people bearing the same full name in a large population.\\nEstimation of name popularity can leverage personal name matching in databases\\nand be of interest for many other domains. A distinctive feature of large\\ncollections of names is that they contain a large number of unique items, which\\nis challenging for statistical modeling. We investigate a number of statistical\\ntechniques and also propose a simple yet effective method aimed at obtaining\\nmore accurate count estimates. In our experiments we use a dataset containing\\nabout 20 million name occurrences that correspond to about 13 million\\nreal-world persons. We perform a thorough evaluation of the name count\\nestimation methods and a record linkage experiment guided by name popularity\\nestimates. Obtained results suggest that theoretically informed approaches\\noutperform simple heuristics and can be useful in a variety of applications.', comment='This is an extended version of a short paper presented at ADBIS2018', journal_ref=None, doi='10.1007/978-3-030-00063-9_9', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-030-00063-9_9', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1811.05361v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1811.05361v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1811.07389v1', updated=datetime.datetime(2018, 11, 18, 19, 58, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 11, 18, 19, 58, 26, tzinfo=datetime.timezone.utc), title='Privacy Preserving Utility Mining: A Survey', authors=[arxiv.Result.Author('Wensheng Gan'), arxiv.Result.Author('Jerry Chun-Wei Lin'), arxiv.Result.Author('Han-Chieh Chao'), arxiv.Result.Author('Shyue-Liang Wang'), arxiv.Result.Author('Philip S. Yu')], summary='In big data era, the collected data usually contains rich information and\\nhidden knowledge. Utility-oriented pattern mining and analytics have shown a\\npowerful ability to explore these ubiquitous data, which may be collected from\\nvarious fields and applications, such as market basket analysis, retail,\\nclick-stream analysis, medical analysis, and bioinformatics. However, analysis\\nof these data with sensitive private information raises privacy concerns. To\\nachieve better trade-off between utility maximizing and privacy preserving,\\nPrivacy-Preserving Utility Mining (PPUM) has become a critical issue in recent\\nyears. In this paper, we provide a comprehensive overview of PPUM. We first\\npresent the background of utility mining, privacy-preserving data mining and\\nPPUM, then introduce the related preliminaries and problem formulation of PPUM,\\nas well as some key evaluation criteria for PPUM. In particular, we present and\\ndiscuss the current state-of-the-art PPUM algorithms, as well as their\\nadvantages and deficiencies in detail. Finally, we highlight and discuss some\\ntechnical challenges and open directions for future research on PPUM.', comment='2018 IEEE International Conference on Big Data, 10 pages', journal_ref='IEEE International Conference on Big Data (Big Data), 2018', doi='10.1109/BigData.2018.8622405', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/BigData.2018.8622405', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1811.07389v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1811.07389v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1811.08143v1', updated=datetime.datetime(2018, 11, 20, 9, 32, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 11, 20, 9, 32, 11, tzinfo=datetime.timezone.utc), title='StarStar Models: Process Analysis on top of Databases', authors=[arxiv.Result.Author('Alessandro Berti'), arxiv.Result.Author('Wil van der Aalst')], summary='Much time in process mining projects is spent on finding and understanding\\ndata sources and extracting the event data needed. As a result, only a fraction\\nof time is spent actually applying techniques to discover, control and predict\\nthe business process. Moreover, there is a lack of techniques to display\\nrelationships on top of databases without the need to express a complex query\\nto get the required information. In this paper, a novel modeling technique that\\nworks on top of databases is presented. This technique is able to show a\\nmultigraph representing activities inferred from database events, connected\\nwith edges that are annotated with frequency and performance information. The\\nrepresentation may be the entry point to apply advanced process mining\\ntechniques that work on classic event logs, as the model provides a simple way\\nto retrieve a classic event log from a specified piece of model. Comparison\\nwith similar techniques and an empirical evaluation are provided.', comment='15 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1811.08143v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1811.08143v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1811.08181v1', updated=datetime.datetime(2018, 11, 20, 11, 18, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 11, 20, 11, 18, 22, tzinfo=datetime.timezone.utc), title='HyperBench: A Benchmark and Tool for Hypergraphs and Empirical Findings', authors=[arxiv.Result.Author('Wolfgang Fischl'), arxiv.Result.Author('Georg Gottlob'), arxiv.Result.Author('Davide M. Longo'), arxiv.Result.Author('Reinhard Pichler')], summary='To cope with the intractability of answering Conjunctive Queries (CQs) and\\nsolving Constraint Satisfaction Problems (CSPs), several notions of hypergraph\\ndecompositions have been proposed -- giving rise to different notions of width,\\nnoticeably, plain, generalized, and fractional hypertree width (hw, ghw, and\\nfhw). Given the increasing interest in using such decomposition methods in\\npractice, a publicly accessible repository of decomposition software, as well\\nas a large set of benchmarks, and a web-accessible workbench for inserting,\\nanalysing, and retrieving hypergraphs are called for.\\n  We address this need by providing (i) concrete implementations of hypergraph\\ndecompositions (including new practical algorithms), (ii) a new, comprehensive\\nbenchmark of hypergraphs stemming from disparate CQ and CSP collections, and\\n(iii) HyperBench, our new web-inter\\\\-face for accessing the benchmark and the\\nresults of our analyses. In addition, we describe a number of actual\\nexperiments we carried out with this new infrastructure.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1811.08181v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1811.08181v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1811.09248v1', updated=datetime.datetime(2018, 11, 22, 17, 34, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 11, 22, 17, 34, 35, tzinfo=datetime.timezone.utc), title='Data Context Informed Data Wrangling', authors=[arxiv.Result.Author('Martin Koehler'), arxiv.Result.Author('Alex Bogatu'), arxiv.Result.Author('Cristina Civili'), arxiv.Result.Author('Nikolaos Konstantinou'), arxiv.Result.Author('Edward Abel'), arxiv.Result.Author('Alvaro A. A. Fernandes'), arxiv.Result.Author('John Keane'), arxiv.Result.Author('Leonid Libkin'), arxiv.Result.Author('Norman W. Paton')], summary='The process of preparing potentially large and complex data sets for further\\nanalysis or manual examination is often called data wrangling. In classical\\nwarehousing environments, the steps in such a process have been carried out\\nusing Extract-Transform-Load platforms, with significant manual involvement in\\nspecifying, configuring or tuning many of them. Cost-effective data wrangling\\nprocesses need to ensure that data wrangling steps benefit from automation\\nwherever possible. In this paper, we define a methodology to fully automate an\\nend-to-end data wrangling process incorporating data context, which associates\\nportions of a target schema with potentially spurious extensional data of types\\nthat are commonly available. Instance-based evidence together with data\\nprofiling paves the way to inform automation in several steps within the\\nwrangling process, specifically, matching, mapping validation, value format\\ntransformation, and data repair. The approach is evaluated with real estate\\ndata showing substantial improvements in the results of automated wrangling.', comment=None, journal_ref='2017 IEEE International Conference on Big Data (Big Data), pp.\\n  956-963, Boston, MA, 11-14 December, 2017', doi='10.1109/BigData.2017.8258015', primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/BigData.2017.8258015', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1811.09248v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1811.09248v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1811.10000v2', updated=datetime.datetime(2019, 6, 17, 8, 21, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 11, 25, 12, 52, 26, tzinfo=datetime.timezone.utc), title='Enabling Efficient Updates in KV Storage via Hashing: Design and Performance Evaluation', authors=[arxiv.Result.Author('Yongkun Li'), arxiv.Result.Author('Helen H. W. Chan'), arxiv.Result.Author('Patrick P. C. Lee'), arxiv.Result.Author('Yinlong Xu')], summary='Persistent key-value (KV) stores mostly build on the Log-Structured Merge\\n(LSM) tree for high write performance, yet the LSM-tree suffers from the\\ninherently high I/O amplification. KV separation mitigates I/O amplification by\\nstoring only keys in the LSM-tree and values in separate storage. However, the\\ncurrent KV separation design remains inefficient under update-intensive\\nworkloads due to its high garbage collection (GC) overhead in value storage. We\\npropose HashKV, which aims for high update performance atop KV separation under\\nupdate-intensive workloads. HashKV uses hash-based data grouping, which\\ndeterministically maps values to storage space so as to make both updates and\\nGC efficient. We further relax the restriction of such deterministic mappings\\nvia simple but useful design extensions. We extensively evaluate various design\\naspects of HashKV. We show that HashKV achieves 4.6x update throughput and\\n53.4% less write traffic compared to the current KV separation design. In\\naddition, we demonstrate that we can integrate the design of HashKV with\\nstate-of-the-art KV stores and improve their respective performance.', comment='28 pages. Accepted by ACM Transactions on Storage', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1811.10000v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1811.10000v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1811.10786v2', updated=datetime.datetime(2019, 1, 7, 6, 22, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 11, 27, 3, 5, 26, tzinfo=datetime.timezone.utc), title='Adaptive Wavelet Clustering for Highly Noisy Data', authors=[arxiv.Result.Author('Zengjian Chen'), arxiv.Result.Author('Jiayi Liu'), arxiv.Result.Author('Yihe Deng'), arxiv.Result.Author('Kun He'), arxiv.Result.Author('John E. Hopcroft')], summary='In this paper we make progress on the unsupervised task of mining arbitrarily\\nshaped clusters in highly noisy datasets, which is a task present in many\\nreal-world applications. Based on the fundamental work that first applies a\\nwavelet transform to data clustering, we propose an adaptive clustering\\nalgorithm, denoted as AdaWave, which exhibits favorable characteristics for\\nclustering. By a self-adaptive thresholding technique, AdaWave is parameter\\nfree and can handle data in various situations. It is deterministic, fast in\\nlinear time, order-insensitive, shape-insensitive, robust to highly noisy data,\\nand requires no pre-knowledge on data models. Moreover, AdaWave inherits the\\nability from the wavelet transform to cluster data in different resolutions. We\\nadopt the \"grid labeling\" data structure to drastically reduce the memory\\nconsumption of the wavelet transform so that AdaWave can be used for relatively\\nhigh dimensional data. Experiments on synthetic as well as natural datasets\\ndemonstrate the effectiveness and efficiency of our proposed method.', comment='11 pages,13 figures,ICDE', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1811.10786v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1811.10786v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1811.10835v1', updated=datetime.datetime(2018, 11, 27, 6, 34, 57, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 11, 27, 6, 34, 57, tzinfo=datetime.timezone.utc), title='A Frequency Scaling based Performance Indicator Framework for Big Data Systems', authors=[arxiv.Result.Author('Chen Yang'), arxiv.Result.Author('Zhihui Du'), arxiv.Result.Author('Xiaofeng Meng'), arxiv.Result.Author('Yongjie Du'), arxiv.Result.Author('Zhiqiang Duan')], summary='It is important for big data systems to identify their performance\\nbottleneck. However, the popular indicators such as resource utilizations, are\\noften misleading and incomparable with each other. In this paper, a novel\\nindicator framework which can directly compare the impact of different\\nindicators with each other is proposed to identify and analyze the performance\\nbottleneck efficiently. A methodology which can construct the indicator from\\nthe performance change with the CPU frequency scaling is described. Spark is\\nused as an example of a big data system and two typical SQL benchmarks are used\\nas the workloads to evaluate the proposed method. Experimental results show\\nthat the proposed method is accurate compared with the resource utilization\\nmethod and easy to implement compared with some white-box method. Meanwhile,\\nthe analysis with our indicators lead to some interesting findings and valuable\\nperformance optimization suggestions for big data systems.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1811.10835v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1811.10835v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1811.10955v2', updated=datetime.datetime(2019, 1, 26, 11, 11, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 11, 27, 13, 11, 16, tzinfo=datetime.timezone.utc), title='Efficiently Charting RDF', authors=[arxiv.Result.Author('Oren Kalinsky'), arxiv.Result.Author('Oren Mishali'), arxiv.Result.Author('Aidan Hogan'), arxiv.Result.Author('Yoav Etsion'), arxiv.Result.Author('Benny Kimelfeld')], summary='We propose a visual query language for interactively exploring large-scale\\nknowledge graphs. Starting from an overview, the user explores bar charts\\nthrough three interactions: class expansion, property expansion, and\\nsubject/object expansion. A major challenge faced is performance: a\\nstate-of-the-art SPARQL engine may require tens of minutes to compute the\\nmultiway join, grouping and counting required to render a bar chart. A\\npromising alternative is to apply approximation through online aggregation,\\ntrading precision for performance. However, state-of-the-art online aggregation\\nalgorithms such as Wander Join have two limitations for our exploration\\nscenario: (1) a high number of rejected paths slows the convergence of the\\ncount estimations, and (2) no unbiased estimator exists for counts under the\\ndistinct operator. We thus devise a specialized algorithm for online\\naggregation that augments Wander Join with exact partial computations to reduce\\nthe number of rejected paths encountered, as well as a novel estimator that we\\nprove to be unbiased in the case of the distinct operator. In an experimental\\nstudy with random interactions exploring two large-scale knowledge graphs, our\\nalgorithm shows a clear reduction in error with respect to computation time\\nversus Wander Join.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1811.10955v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1811.10955v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1811.11561v1', updated=datetime.datetime(2018, 11, 28, 13, 51, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 11, 28, 13, 51, 39, tzinfo=datetime.timezone.utc), title='Approximate Evaluation of Label-Constrained Reachability Queries', authors=[arxiv.Result.Author('Stefania Dumbrava'), arxiv.Result.Author('Angela Bonifati'), arxiv.Result.Author('Amaia Nazabal Ruiz Diaz'), arxiv.Result.Author('Romain Vuillemot')], summary='The current surge of interest in graph-based data models mirrors the usage of\\nincreasingly complex reachability queries, as witnessed by recent analytical\\nstudies on real-world graph query logs. Despite the maturity of graph DBMS\\ncapabilities, complex label-constrained reachability queries, along with their\\ncorresponding aggregate versions, remain difficult to evaluate. In this paper,\\nwe focus on the approximate evaluation of counting label-constrained\\nreachability queries. We offer a human-explainable solution to graph\\nApproximate Query Processing (AQP). This consists of a summarization algorithm\\n(GRASP), as well as of a custom visualization plug-in, which allows users to\\nexplore the obtained summaries. We prove that the problem of node group\\nminimization, associated to the creation of GRASP summaries, is NP-complete.\\nNonetheless, our GRASP summaries are reasonably small in practice, even for\\nlarge graph instances, and guarantee approximate graph query answering, paired\\nwith controllable error estimates. We experimentally gauge the scalability and\\nefficiency of our GRASP algorithm, and verify the accuracy and error estimation\\nof the graph AQP module. To the best of our knowledge, ours is the first system\\ncapable of handling visualization-driven approximate graph analytics for\\ncomplex label-constrained reachability queries.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1811.11561v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1811.11561v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1811.12204v2', updated=datetime.datetime(2020, 4, 16, 13, 28, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 11, 29, 14, 45, 48, tzinfo=datetime.timezone.utc), title='Chiller: Contention-centric Transaction Execution and Data Partitioning for Modern Networks', authors=[arxiv.Result.Author('Erfan Zamanian'), arxiv.Result.Author('Julian Shun'), arxiv.Result.Author('Carsten Binnig'), arxiv.Result.Author('Tim Kraska')], summary='Distributed transactions on high-overhead TCP/IP-based networks were\\nconventionally considered to be prohibitively expensive and thus were avoided\\nat all costs. To that end, the primary goal of almost any existing partitioning\\nscheme is to minimize the number of cross-partition transactions. However, with\\nthe new generation of fast RDMA-enabled networks, this assumption is no longer\\nvalid. In fact, recent work has shown that distributed databases can scale even\\nwhen the majority of transactions are cross-partition. In this paper, we first\\nmake the case that the new bottleneck which hinders truly scalable transaction\\nprocessing in modern RDMA-enabled databases is data contention, and that\\noptimizing for data contention leads to different partitioning layouts than\\noptimizing for the number of distributed transactions. We then present Chiller,\\na new approach to data partitioning and transaction execution, which aims to\\nminimize data contention for both local and distributed transactions. Finally,\\nwe evaluate Chiller using various workloads, and show that our partitioning and\\nexecution strategy outperforms traditional partitioning techniques which try to\\navoid distributed transactions, by up to a factor of 2.', comment=None, journal_ref=None, doi='10.1145/3318464.3389724', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3318464.3389724', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1811.12204v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1811.12204v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.00228v1', updated=datetime.datetime(2019, 1, 2, 1, 2, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 2, 1, 2, 56, tzinfo=datetime.timezone.utc), title='Verity: Blockchains to Detect Insider Attacks in DBMS', authors=[arxiv.Result.Author('Shubham S. Srivastava'), arxiv.Result.Author('Medha Atre'), arxiv.Result.Author('Shubham Sharma'), arxiv.Result.Author('Rahul Gupta'), arxiv.Result.Author('Sandeep K. Shukla')], summary=\"Integrity and security of the data in database systems are typically\\nmaintained with access control policies and firewalls. However, insider attacks\\n-- where someone with an intimate knowledge of the system and administrative\\nprivileges tampers with the data -- pose a unique challenge. Measures like\\nappend only logging prove to be insufficient because an attacker with\\nadministrative privileges can alter logs and login records to eliminate the\\ntrace of attack, thus making insider attacks hard to detect.\\n  In this paper, we propose Verity -- first of a kind system to the best of our\\nknowledge. Verity serves as a dataless framework by which any blockchain\\nnetwork can be used to store fixed-length metadata about tuples from any SQL\\ndatabase, without complete migration of the database. Verity uses a formalism\\nfor parsing SQL queries and query results to check the respective tuples'\\nintegrity using blockchains to detect insider attacks. We have implemented our\\ntechnique using Hyperledger Fabric, Composer REST API, and SQLite database.\\nUsing TPC-H data and SQL queries of varying complexity and types, our\\nexperiments demonstrate that any overhead of integrity checking remains\\nconstant per tuple in a query's results, and scales linearly.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC', 'H.2.7'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.00228v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00228v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.00232v1', updated=datetime.datetime(2019, 1, 2, 1, 38, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 2, 1, 38, 30, tzinfo=datetime.timezone.utc), title='Approximate Computation for Big Data Analytics', authors=[arxiv.Result.Author('Shuai Ma'), arxiv.Result.Author('Jinpeng Huai')], summary=\"Over the past a few years, research and development has made significant\\nprogresses on big data analytics. A fundamental issue for big data analytics is\\nthe efficiency. If the optimal solution is unable to attain or not required or\\nhas a price to high to pay, it is reasonable to sacrifice optimality with a\\n`good' feasible solution that can be computed efficiently. Existing\\napproximation techniques can be in general classified into approximation\\nalgorithms, approximate query processing for aggregate SQL queries and\\napproximation computing for multiple layers of the system stack. In this\\narticle, we systematically introduce approximate computation, i.e., query\\napproximation and data approximation, for efficiency and effectiveness big data\\nanalytics. We first explain the idea and rationale of query approximation, and\\nshow efficiency can be obtained with high effectiveness in practice with three\\nanalytic tasks: graph pattern matching, trajectory compression and dense\\nsubgraph computation. We then explain the idea and rationale of data\\napproximation, and show efficiency can be obtained even without sacrificing for\\neffectiveness in practice with three analytic tasks: shortest paths/distances,\\nnetwork anomaly detection and link prediction.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.00232v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00232v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.00671v1', updated=datetime.datetime(2019, 1, 3, 10, 30, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 3, 10, 30, 14, tzinfo=datetime.timezone.utc), title='Une nouvelle approche de complétion des valeurs manquantes dans les bases de données', authors=[arxiv.Result.Author('Leila Ben Othman')], summary=\"When tackling real-life datasets, it is common to face the existence of\\nscrambled missing values within data. Considered as 'dirty data', usually it is\\nremoved during a pre-processing step. Starting from the fact that 'making up\\nthis missing data is better than throwing out it away', we present a new\\napproach trying to complete missing data. The main singularity of the\\nintroduced approach is that it sheds light on a fruitful synergy between\\ngeneric basis of association rules and the topic of missing values handling. In\\nfact, beyond interesting compactness rate, such generic association rules make\\nit possible to get a considerable reduction of conflicts during the completion\\nstep. A new metric called 'Robustness' is also introduced, and aims to select\\nthe robust association rule for the completion of a missing value whenever a\\nconflict appears. Carried out experiments on benchmark datasets confirm the\\nsoundness of our approach. Thus, it reduces conflict during the completion step\\nwhile offering a high percentage of correct completion accuracy.\", comment='in French', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.00671v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00671v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.00716v2', updated=datetime.datetime(2019, 1, 8, 1, 36, 58, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 2, 18, 48, 34, tzinfo=datetime.timezone.utc), title='Improving Suppression to Reduce Disclosure Risk and Enhance Data Utility', authors=[arxiv.Result.Author('Marmar Orooji'), arxiv.Result.Author('Gerald M. Knapp')], summary='In Privacy Preserving Data Publishing, various privacy models have been\\ndeveloped for employing anonymization operations on sensitive individual level\\ndatasets, in order to publish the data for public access while preserving the\\nprivacy of individuals in the dataset. However, there is always a trade-off\\nbetween preserving privacy and data utility; the more changes we make on the\\nconfidential dataset to reduce disclosure risk, the more information the data\\nloses and the less data utility it preserves. The optimum privacy technique is\\nthe one that results in a dataset with minimum disclosure risk and maximum data\\nutility. In this paper, we propose an improved suppression method, which\\nreduces the disclosure risk and enhances the data utility by targeting the\\nhighest risk records and keeping other records intact. We have shown the\\neffectiveness of our approach through an experiment on a real-world\\nconfidential dataset.', comment='6 pages, conference', journal_ref='Institute of Industrial and Systems Engineers (2018)', doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.00716v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00716v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.00735v1', updated=datetime.datetime(2019, 1, 3, 14, 6, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 3, 14, 6, 13, tzinfo=datetime.timezone.utc), title='Dataset search: a survey', authors=[arxiv.Result.Author('Adriane Chapman'), arxiv.Result.Author('Elena Simperl'), arxiv.Result.Author('Laura Koesten'), arxiv.Result.Author('George Konstantinidis'), arxiv.Result.Author('Luis-Daniel Ibáñez-Gonzalez'), arxiv.Result.Author('Emilia Kacprzak'), arxiv.Result.Author('Paul Groth')], summary='Generating value from data requires the ability to find, access and make\\nsense of datasets. There are many efforts underway to encourage data sharing\\nand reuse, from scientific publishers asking authors to submit data alongside\\nmanuscripts to data marketplaces, open data portals and data communities.\\nGoogle recently beta released a search service for datasets, which allows users\\nto discover data stored in various online repositories via keyword queries.\\nThese developments foreshadow an emerging research field around dataset search\\nor retrieval that broadly encompasses frameworks, methods and tools that help\\nmatch a user data need against a collection of datasets. Here, we survey the\\nstate of the art of research and commercial systems in dataset retrieval. We\\nidentify what makes dataset search a research field in its own right, with\\nunique challenges and methods and highlight open problems. We look at\\napproaches and implementations from related areas dataset search is drawing\\nupon, including information retrieval, databases, entity-centric and tabular\\nsearch in order to identify possible paths to resolve these open problems as\\nwell as immediate next steps that will take the field forward.', comment='20 pages, 153 references', journal_ref=None, doi='10.1007/s00778-019-00564-x', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/s00778-019-00564-x', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1901.00735v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.00735v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.01488v1', updated=datetime.datetime(2019, 1, 6, 2, 20, 57, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 6, 2, 20, 57, tzinfo=datetime.timezone.utc), title='Exact Selectivity Computation for Modern In-Memory Database Query Optimization', authors=[arxiv.Result.Author('Jun Hyung Shin'), arxiv.Result.Author('Florin Rusu'), arxiv.Result.Author('Alex Suhan')], summary='Selectivity estimation remains a critical task in query optimization even\\nafter decades of research and industrial development. Optimizers rely on\\naccurate selectivities when generating execution plans. They maintain a large\\nrange of statistical synopses for efficiently estimating selectivities.\\nNonetheless, small errors -- propagated exponentially -- can lead to severely\\nsub-optimal plans---especially, for complex predicates. Database systems for\\nmodern computing architectures rely on extensive in-memory processing supported\\nby massive multithread parallelism and vectorized instructions. However, they\\nmaintain the same synopses approach to query optimization as traditional\\ndisk-based databases. We introduce a novel query optimization paradigm for\\nin-memory and GPU-accelerated databases based on \\\\textit{exact selectivity\\ncomputation (ESC)}. The central idea in ESC is to compute selectivities exactly\\nthrough queries during query optimization. In order to make the process\\nefficient, we propose several optimizations targeting the selection and\\nmaterialization of tables and predicates to which ESC is applied. We implement\\nESC in the MapD open-source database system. Experiments on the TPC-H and SSB\\nbenchmarks show that ESC records constant and less than 30 milliseconds\\noverhead when running on GPU and generates improved query execution plans that\\nare as much as 32X faster.', comment='Long version of the CIDR 2019 lightning talk', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.01488v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.01488v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.01973v1', updated=datetime.datetime(2019, 1, 7, 18, 59, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 7, 18, 59, 51, tzinfo=datetime.timezone.utc), title='Looking Back at Postgres', authors=[arxiv.Result.Author('Joseph M. Hellerstein')], summary='This is a recollection of the UC Berkeley Postgres project, which was led by\\nMike Stonebraker from the mid-1980\\'s to the mid-1990\\'s. The article was\\nsolicited for Stonebraker\\'s Turing Award book, as one of many\\npersonal/historical recollections. As a result it focuses on Stonebraker\\'s\\ndesign ideas and leadership. But Stonebraker was never a coder, and he stayed\\nout of the way of his development team. The Postgres codebase was the work of a\\nteam of brilliant students and the occasional university \"staff programmers\"\\nwho had little more experience (and only slightly more compensation) than the\\nstudents. I was lucky to join that team as a student during the latter years of\\nthe project. I got helpful input on this writeup from some of the more senior\\nstudents on the project, but any errors or omissions are mine. If you spot any\\nsuch, please contact me and I will try to fix them.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.01973v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.01973v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.03179v2', updated=datetime.datetime(2019, 3, 8, 22, 52, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 7, 2, 53, 49, tzinfo=datetime.timezone.utc), title='Popular SQL Server Database Encryption Choices', authors=[arxiv.Result.Author('Sourav Mukherjee')], summary=\"This article gives an overview of different database encryption choices in\\nSQL Server. Which one works best in which situation. In today's world Data is\\nmore crucial than the expensive hardware cost. No one wants their personal data\\nto be comprised. Same for business houses as well and they also do not want\\ntheir data to be inappropriately handled to go out of the business. To help\\nprotect the public rights and safety, recently this year, the European Union\\nhad come up with strict rules and regulation of GDPR (General Data Protection\\nRegulation).\", comment='6 pages, 3 figures, Published with International Journal of Computer\\n  Trends and Technology (IJCTT)', journal_ref='International Journal of Computer Trends and Technology (IJCTT).\\n  Volume 66 Number 1. December 2018. Published by Seventh Sense Research Group', doi='10.14445/22312803/IJCTT-V66P103', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.14445/22312803/IJCTT-V66P103', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1901.03179v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.03179v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.03633v2', updated=datetime.datetime(2019, 7, 19, 14, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 11, 16, 22, 26, tzinfo=datetime.timezone.utc), title='Solving linear programs on factorized databases', authors=[arxiv.Result.Author('Florent Capelli'), arxiv.Result.Author('Nicolas Crosetti'), arxiv.Result.Author('Joachim Niehren'), arxiv.Result.Author('Jan Ramon')], summary='A typical workflow for solving a linear programming problem is to first write\\na linear program parametrized by the data in a language such as Math GNU Prog\\nor AMPL then call the solver on this program while providing the data. When the\\ndata is extracted using a query on a database, this approach ignores the\\nunderlying structure of the answer set which may result in a blow-up of the\\nsize of the linear program if the answer set is big. In this paper, we study\\nthe problem of solving linear programming problems whose variables are the\\nanswers to a conjunctive query. We show that one can exploit the structure of\\nthe query to rewrite the linear program so that its size depends only on the\\nsize of the database and not on the size of the answer set. More precisely, we\\ngive a generic way of rewriting a linear program whose variables are the tuples\\nin Q(D) for a conjunctive query Q and a database D into a linear program having\\na number of variables that only depends on the size of a factorized\\nrepresentation of Q(D), which can be much smaller when the fractional hypertree\\nwidth of Q is bounded.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.03633v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.03633v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.03897v2', updated=datetime.datetime(2019, 7, 5, 18, 6, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 12, 20, 51, 58, tzinfo=datetime.timezone.utc), title='All-Instances Restricted Chase Termination', authors=[arxiv.Result.Author('Tomasz Gogacz'), arxiv.Result.Author('Jerzy Marcinkowski'), arxiv.Result.Author('Andreas Pieris')], summary='The chase procedure is a fundamental algorithmic tool in database theory with\\na variety of applications. A key problem concerning the chase procedure is\\nall-instances termination: for a given set of tuple-generating dependencies\\n(TGDs), is it the case that the chase terminates for every input database? In\\nview of the fact that this problem is undecidable, it is natural to ask whether\\nknown well-behaved classes of TGDs ensure decidability. We consider here the\\nmain paradigms that led to robust TGD-based formalisms, that is, guardedness\\nand stickiness. Although all-instances termination is well-understood for the\\noblivious version of the chase, the more subtle case of the restricted (a.k.a.\\nthe standard) chase is rather unexplored. We show that all-instances restricted\\nchase termination for guarded and sticky single-head TGDs is decidable.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LO'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.03897v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.03897v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.05451v1', updated=datetime.datetime(2019, 1, 16, 13, 38, 12, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 16, 13, 38, 12, tzinfo=datetime.timezone.utc), title='Exploring Communities in Large Profiled Graphs', authors=[arxiv.Result.Author('Yankai Chen'), arxiv.Result.Author('Yixiang Fang'), arxiv.Result.Author('Reynold Cheng'), arxiv.Result.Author('Yun Li'), arxiv.Result.Author('Xiaojun Chen'), arxiv.Result.Author('Jie Zhang')], summary='Given a graph $G$ and a vertex $q\\\\in G$, the community search (CS) problem\\naims to efficiently find a subgraph of $G$ whose vertices are closely related\\nto $q$. Communities are prevalent in social and biological networks, and can be\\nused in product advertisement and social event recommendation. In this paper,\\nwe study profiled community search (PCS), where CS is performed on a profiled\\ngraph. This is a graph in which each vertex has labels arranged in a\\nhierarchical manner. Extensive experiments show that PCS can identify\\ncommunities with themes that are common to their vertices, and is more\\neffective than existing CS approaches. As a naive solution for PCS is highly\\nexpensive, we have also developed a tree index, which facilitate efficient and\\nonline solutions for PCS.', comment=None, journal_ref='IEEE Transactions on Knowledge and Data Engineering 2018', doi='10.1109/TKDE.2018.2882837', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/TKDE.2018.2882837', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1901.05451v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.05451v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.06208v1', updated=datetime.datetime(2019, 1, 18, 12, 59, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 18, 12, 59, 59, tzinfo=datetime.timezone.utc), title='Data Quality Measures and Data Cleansing for Research Information Systems', authors=[arxiv.Result.Author('Otmane Azeroual'), arxiv.Result.Author('Gunter Saake'), arxiv.Result.Author('Mohammad Abuosba')], summary='The collection, transfer and integration of research information into\\ndifferent research Information systems can result in different data errors that\\ncan have a variety of negative effects on data quality. In order to detect\\nerrors at an early stage and treat them efficiently, it is necessary to\\ndetermine the clean-up measures and the new techniques of data cleansing for\\nquality improvement in research institutions. Thereby an adequate and reliable\\nbasis for decision-making using an RIS is provided, and confidence in a given\\ndataset increased. In this paper, possible measures and the new techniques of\\ndata cleansing for improving and increasing the data quality in research\\ninformation systems will be presented and how these are to be applied to the\\nResearch information.', comment='16(1), pp.12-21', journal_ref='Journal of Digital Information Management, Digital Information\\n  Research Foundation, 2018', doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.06208v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.06208v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.06862v1', updated=datetime.datetime(2019, 1, 21, 10, 35, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 21, 10, 35, 53, tzinfo=datetime.timezone.utc), title='An Experimental Study of the Treewidth of Real-World Graph Data (Extended Version)', authors=[arxiv.Result.Author('Silviu Maniu'), arxiv.Result.Author('Pierre Senellart'), arxiv.Result.Author('Suraj Jog')], summary='Treewidth is a parameter that measures how tree-like a relational instance\\nis, and whether it can reasonably be decomposed into a tree. Many computation\\ntasks are known to be tractable on databases of small treewidth, but computing\\nthe treewidth of a given instance is intractable. This article is the first\\nlarge-scale experimental study of treewidth and tree decompositions of\\nreal-world database instances (25 datasets from 8 different domains, with sizes\\nranging from a few thousand to a few million vertices). The goal is to\\ndetermine which data, if any, can benefit of the wealth of algorithms for\\ndatabases of small treewidth. For each dataset, we obtain upper and lower bound\\nestimations of their treewidth, and study the properties of their tree\\ndecompositions. We show in particular that, even when treewidth is high, using\\npartial tree decompositions can result in data structures that can assist\\nalgorithms.', comment='Extended version of an article published in the proceedings of ICDT\\n  2019', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.06862v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.06862v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.07064v1', updated=datetime.datetime(2019, 1, 21, 20, 16, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 21, 20, 16, 47, tzinfo=datetime.timezone.utc), title='Predictive Indexing', authors=[arxiv.Result.Author('Joy Arulraj'), arxiv.Result.Author('Ran Xian'), arxiv.Result.Author('Lin Ma'), arxiv.Result.Author('Andrew Pavlo')], summary=\"There has been considerable research on automated index tuning in database\\nmanagement systems (DBMSs). But the majority of these solutions tune the index\\nconfiguration by retrospectively making computationally expensive physical\\ndesign changes all at once. Such changes degrade the DBMS's performance during\\nthe process, and have reduced utility during subsequent query processing due to\\nthe delay between a workload shift and the associated change. A better approach\\nis to generate small changes that tune the physical design over time, forecast\\nthe utility of these changes, and apply them ahead of time to maximize their\\nimpact.\\n  This paper presents predictive indexing that continuously improves a\\ndatabase's physical design using lightweight physical design changes. It uses a\\nmachine learning model to forecast the utility of these changes, and\\ncontinuously refines the index configuration of the database to handle evolving\\nworkloads. We introduce a lightweight hybrid scan operator with which a DBMS\\ncan make use of partially-built indexes for query processing. Our evaluation\\nshows that predictive indexing improves the throughput of a DBMS by 3.5--5.2x\\ncompared to other state-of-the-art indexing approaches. We demonstrate that\\npredictive indexing works seamlessly with other lightweight automated physical\\ndesign tuning methods.\", comment='12 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2.2; H.2.4'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.07064v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.07064v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.07627v1', updated=datetime.datetime(2019, 1, 22, 22, 19, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 22, 22, 19, 17, tzinfo=datetime.timezone.utc), title='Just-in-Time Index Compilation', authors=[arxiv.Result.Author('Darshana Balakrishnan'), arxiv.Result.Author('Lukasz Ziarek'), arxiv.Result.Author('Oliver Kennedy')], summary='Creating or modifying a primary index is a time-consuming process, as the\\nindex typically needs to be rebuilt from scratch. In this paper, we explore a\\nmore graceful \"just-in-time\" approach to index reorganization, where small\\nchanges are dynamically applied in the background. To enable this type of\\nreorganization, we formalize a composable organizational grammar, expressive\\nenough to capture instances of not only existing index structures, but\\narbitrary hybrids as well. We introduce an algebra of rewrite rules for such\\nstructures, and a framework for defining and optimizing policies for\\njust-in-time rewriting. Our experimental analysis shows that the resulting\\nindex structure is flexible enough to adapt to a variety of performance goals,\\nwhile also remaining competitive with existing structures like the C++ standard\\ntemplate library map.', comment='Work Supported by NSF Award #IIS-1617586', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.07627v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.07627v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.07655v3', updated=datetime.datetime(2019, 5, 3, 1, 47, 58, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 23, 0, 21, 48, tzinfo=datetime.timezone.utc), title='A Concentration of Measure Approach to Database De-anonymization', authors=[arxiv.Result.Author('Farhad Shirani'), arxiv.Result.Author('Siddharth Garg'), arxiv.Result.Author('Elza Erkip')], summary='In this paper, matching of correlated high-dimensional databases is\\ninvestigated. A stochastic database model is considered where the correlation\\namong the database entries is governed by an arbitrary joint distribution.\\nConcentration of measure theorems such as typicality and laws of large numbers\\nare used to develop a database matching scheme and derive necessary conditions\\nfor successful matching. Furthermore, it is shown that these conditions are\\ntight through a converse result which characterizes a set of distributions on\\nthe database entries for which reliable matching is not possible. The necessary\\nand sufficient conditions for reliable matching are evaluated in the cases when\\nthe database entries are independent and identically distributed as well as\\nunder Markovian database models.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.07655v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.07655v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.07747v1', updated=datetime.datetime(2019, 1, 23, 6, 49, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 23, 6, 49, 2, tzinfo=datetime.timezone.utc), title='Fast and Robust Distributed Subgraph Enumeration', authors=[arxiv.Result.Author('Xuguang Ren'), arxiv.Result.Author('Junhu Wang'), arxiv.Result.Author('Wook-Shin Han'), arxiv.Result.Author('Jeffrey Xu Yu')], summary='We study the classic subgraph enumeration problem under distributed settings.\\nExisting solutions either suffer from severe memory crisis or rely on large\\nindexes, which makes them impractical for very large graphs. Most of them\\nfollow a synchronous model where the performance is often bottlenecked by the\\nmachine with the worst performance. Motivated by this, in this paper, we\\npropose RADS, a Robust Asynchronous Distributed Subgraph enumeration system.\\nRADS first identifies results that can be found using single-machine\\nalgorithms. This strategy not only improves the overall performance but also\\nreduces network communication and memory cost. Moreover, RADS employs a novel\\nregion-grouped multi-round expand verify & filter framework which does not need\\nto shuffle and exchange the intermediate results, nor does it need to replicate\\na large part of the data graph in each machine. This feature not only reduces\\nnetwork communication cost and memory usage, but also allows us to adopt simple\\nstrategies for memory control and load balancing, making it more robust.\\nSeveral heuristics are also used in RADS to further improve the performance.\\nOur experiments verified the superiority of RADS to state-of-the-art subgraph\\nenumeration approaches.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.07747v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.07747v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.08304v3', updated=datetime.datetime(2019, 4, 19, 2, 53, 38, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 24, 9, 40, 28, tzinfo=datetime.timezone.utc), title='Benchmarking Time Series Databases with IoTDB-Benchmark for IoT Scenarios', authors=[arxiv.Result.Author('Rui Liu'), arxiv.Result.Author('Jun Yuan')], summary='With the wide application of time series databases (TSDBs) in big data fields\\nlike cluster monitoring and industrial IoT, there have been developed a number\\nof TSDBs for time series data management. Different TSDBs have test reports\\ncomparing themselves with other databases to show their advantages, but the\\ncomparisons are typically based on their own tools without using a common\\nwell-recognized test framework. To the best of our knowledge, there is no\\nmature TSDB benchmark either. With the goal of establishing a standard of\\nevaluating TSDB systems, we present the IoTDB-Benchmark framework, specifically\\ndesigned for TSDB and IoT application scenarios. We pay close attention to some\\nspecial data ingestion scenarios and summarize 10 basic queries types. We use\\nthis benchmark to compare four TSDB systems: InfluxDB, OpenTSDB, KairosDB and\\nTimescaleDB. Our benchmark framework/tool not only measures performance metrics\\nbut also takes system resource consumption into consideration.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.08304v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.08304v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.08666v1', updated=datetime.datetime(2019, 1, 24, 22, 20, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 24, 22, 20, 46, tzinfo=datetime.timezone.utc), title='HRDBMS: Combining the Best of Modern and Traditional Relational Databases', authors=[arxiv.Result.Author('Jason Arnold'), arxiv.Result.Author('Boris Glavic'), arxiv.Result.Author('Ioan Raicu')], summary='HRDBMS is a novel distributed relational database that uses a hybrid model\\ncombining the best of traditional distributed relational databases and Big Data\\nanalytics platforms such as Hive. This allows HRDBMS to leverage years worth of\\nresearch regarding query optimization, while also taking advantage of the\\nscalability of Big Data platforms. The system uses an execution framework that\\nis tailored for relational processing, thus addressing some of the performance\\nchallenges of running SQL on top of platforms such as MapReduce and Spark.\\nThese include excessive materialization of intermediate results, lack of a\\nglobal cost-based optimization, unnecessary sorting, lack of index support, no\\nstatistics, no support for DML and ACID, and excessive communication caused by\\nthe rigid communication patterns enforced by these platforms.', comment='Oral Ph.D. Qualifier Report', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.08666v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.08666v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.09090v2', updated=datetime.datetime(2019, 1, 31, 23, 30, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 25, 21, 33, 50, tzinfo=datetime.timezone.utc), title='Flexible Operator Embeddings via Deep Learning', authors=[arxiv.Result.Author('Ryan Marcus'), arxiv.Result.Author('Olga Papaemmanouil')], summary=\"Integrating machine learning into the internals of database management\\nsystems requires significant feature engineering, a human effort-intensive\\nprocess to determine the best way to represent the pieces of information that\\nare relevant to a task. In addition to being labor intensive, the process of\\nhand-engineering features must generally be repeated for each data management\\ntask, and may make assumptions about the underlying database that are not\\nuniversally true. We introduce flexible operator embeddings, a deep learning\\ntechnique for automatically transforming query operators into feature vectors\\nthat are useful for a multiple data management tasks and is custom-tailored to\\nthe underlying database. Our approach works by taking advantage of an\\noperator's context, resulting in a neural network that quickly transforms\\nsparse representations of query operators into dense, information-rich feature\\nvectors. Experimentally, we show that our flexible operator embeddings perform\\nwell across a number of data management tasks, using both synthetic and\\nreal-world datasets.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.09090v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.09090v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1901.11376v1', updated=datetime.datetime(2019, 1, 12, 3, 36, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 1, 12, 3, 36, 1, tzinfo=datetime.timezone.utc), title='Learning of High Dengue Incidence with Clustering and FP-Growth Algorithm using WHO Historical Data', authors=[arxiv.Result.Author('Franz Stewart V. Dizon'), arxiv.Result.Author('Stephen Kyle R. Farinas'), arxiv.Result.Author('Reynaldo John Tristan H. Mahinay Jr.'), arxiv.Result.Author('Harry S. Pardo'), arxiv.Result.Author('Cecil Jose A. Delfinado')], summary='This paper applies FP-Growth algorithm in mining fuzzy association rules for\\na prediction system of dengue. The system mines its rules through input of\\nhistoric predictor variables for dengue. The rules will be used to build a\\nrule-based classifier to predict the dengue incidence for the next month for\\nthe years 2001-2006 in the Philippines. The FP-Growth Algorithm was compared to\\nApriori Algorithm by Sensitivity, Specificity, PPV, NPV, execution time and\\nmemory usage. The results showed that FP-Growth Algorithm is significantly\\nbetter in execution time, numerically better in memory and comparable in\\nSensitivity, Specificity, PPV and NPV to Apriori Algorithm.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1901.11376v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.11376v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.00424v1', updated=datetime.datetime(2019, 3, 1, 17, 31, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 1, 17, 31, 13, tzinfo=datetime.timezone.utc), title='SCAR: Strong Consistency using Asynchronous Replication with Minimal Coordination', authors=[arxiv.Result.Author('Yi Lu'), arxiv.Result.Author('Xiangyao Yu'), arxiv.Result.Author('Samuel Madden')], summary=\"Data replication is crucial in modern distributed systems as a means to\\nprovide high availability. Many techniques have been proposed to utilize\\nreplicas to improve a system's performance, often requiring expensive\\ncoordination or sacrificing consistency. In this paper, we present SCAR, a new\\ndistributed and replicated in-memory database that allows serializable\\ntransactions to read from backup replicas with minimal coordination. SCAR works\\nby assigning logical timestamps to database records so that a transaction can\\nsafely read from a backup replica without coordinating with the primary\\nreplica, because the records cannot be changed up to a certain logical time. In\\naddition, we propose two optimization techniques, timestamp synchronization and\\nparallel locking and validation, to further reduce coordination. We show that\\nSCAR outperforms systems with conventional concurrency control algorithms and\\nreplication strategies by up to a factor of 2 on three popular benchmarks. We\\nalso demonstrate that SCAR achieves higher throughput by running under reduced\\nisolation levels and detects concurrency anomalies in real time.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1903.00424v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.00424v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.00452v1', updated=datetime.datetime(2019, 3, 1, 18, 25, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 1, 18, 25, 4, tzinfo=datetime.timezone.utc), title='Parallel Index-based Stream Join on a Multicore CPU', authors=[arxiv.Result.Author('Amirhesam Shahvarani'), arxiv.Result.Author('Hans-Arno Jacobsen')], summary='There is increasing interest in using multicore processors to accelerate\\nstream processing. For example, indexing sliding window content to enhance the\\nperformance of streaming queries is greatly improved by utilizing the\\ncomputational capabilities of a multicore processor. However, designing an\\neffective concurrency control mechanism that addresses the problem of\\nconcurrent indexing in highly dynamic settings remains a challenge. In this\\npaper, we introduce an index data structure, called the Partitioned In-memory\\nMerge-Tree, to address the challenges that arise when indexing highly dynamic\\ndata, which are common in streaming settings. To complement the index, we\\ndesign an algorithm to realize a parallel index-based stream join that exploits\\nthe computational power of multicore processors. Our experiments using an\\nocta-core processor show that our parallel stream join achieves up to 5.5 times\\nhigher throughput than a single-threaded approach.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1903.00452v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.00452v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.00731v1', updated=datetime.datetime(2019, 3, 2, 16, 32, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 2, 16, 32, 33, tzinfo=datetime.timezone.utc), title='HISTEX (HISTory EXerciser) : A tool for testing the implementation of Isolation Levels of Relational Database Management Systems', authors=[arxiv.Result.Author('Dimitrios Liarokapis'), arxiv.Result.Author('Elizabeth ONeil'), arxiv.Result.Author('Patrick ONeil')], summary='We present a multi-process application called HISTEX (HISTory EXerciser),\\nwhich executes input histories in a generic transactional notation on\\ncommercial DBMS platforms. HISTEX could be used to discover potential errors in\\nthe implementation of Isolation Levels by Relational Database Management\\nSystems or cases where a system behaves over restrictively. It can also be used\\nfor performance measurements related to database workloads executing on real\\ndatabase systems instead of simulated environments. HISTEX has been implemented\\nin C by utilizing Embedded SQL. However, many of its ideas could be\\nreincarnated in new implementations that could rely on other database\\nconnectivity paradigms such as JDBC, JPA etc. We expect that by presenting some\\nof the ideas behind its development we could re-invigorate some fresh interest\\nand involvement in the research community regarding such tools.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1903.00731v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.00731v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.01363v1', updated=datetime.datetime(2019, 3, 4, 16, 54, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 4, 16, 54, 56, tzinfo=datetime.timezone.utc), title='Opportunistic View Materialization with Deep Reinforcement Learning', authors=[arxiv.Result.Author('Xi Liang'), arxiv.Result.Author('Aaron J. Elmore'), arxiv.Result.Author('Sanjay Krishnan')], summary='Carefully selected materialized views can greatly improve the performance of\\nOLAP workloads. We study using deep reinforcement learning to learn adaptive\\nview materialization and eviction policies. Our insight is that such selection\\npolicies can be effectively trained with an asynchronous RL algorithm, that\\nruns paired counter-factual experiments during system idle times to evaluate\\nthe incremental value of persisting certain views. Such a strategy obviates the\\nneed for accurate cardinality estimation or hand-designed scoring heuristics.\\nWe focus on inner-join views and modeling effects in a main-memory, OLAP\\nsystem. Our research prototype system, called DQM, is implemented in SparkSQL\\nand we experiment on several workloads including the Join Order Benchmark and\\nthe TPC-DS workload. Results suggest that: (1) DQM can outperform heuristic\\nwhen their assumptions are not satisfied by the workload or there are temporal\\neffects like period maintenance, (2) even with the cost of learning, DQM is\\nmore adaptive to changes in the workload, and (3) DQM is broadly applicable to\\ndifferent workloads and skews.', comment='14 Pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1903.01363v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.01363v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.01498v1', updated=datetime.datetime(2019, 3, 4, 19, 25, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 4, 19, 25, 15, tzinfo=datetime.timezone.utc), title='Voyageur: An Experiential Travel Search Engine', authors=[arxiv.Result.Author('Sara Evensen'), arxiv.Result.Author('Aaron Feng'), arxiv.Result.Author('Alon Halevy'), arxiv.Result.Author('Jinfeng Li'), arxiv.Result.Author('Vivian Li'), arxiv.Result.Author('Yuliang Li'), arxiv.Result.Author('Huining Liu'), arxiv.Result.Author('George Mihaila'), arxiv.Result.Author('John Morales'), arxiv.Result.Author('Natalie Nuno'), arxiv.Result.Author('Ekaterina Pavlovic'), arxiv.Result.Author('Wang-Chiew Tan'), arxiv.Result.Author('Xiaolan Wang')], summary='We describe Voyageur, which is an application of experiential search to the\\ndomain of travel. Unlike traditional search engines for online services,\\nexperiential search focuses on the experiential aspects of the service under\\nconsideration. In particular, Voyageur needs to handle queries for subjective\\naspects of the service (e.g., quiet hotel, friendly staff) and combine these\\nwith objective attributes, such as price and location. Voyageur also highlights\\ninteresting facts and tips about the services the user is considering to\\nprovide them with further insights into their choices.', comment='Demo paper accepted to the Web Conference', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1903.01498v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.01498v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.02076v2', updated=datetime.datetime(2019, 6, 2, 16, 40, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 5, 21, 57, 11, tzinfo=datetime.timezone.utc), title='Optimizing Subgraph Queries by Combining Binary and Worst-Case Optimal Joins', authors=[arxiv.Result.Author('Amine Mhedhbi'), arxiv.Result.Author('Semih Salihoglu')], summary='We study the problem of optimizing subgraph queries using the new worst-case\\noptimal join plans. Worst-case optimal plans evaluate queries by matching one\\nquery vertex at a time using multiway intersections. The core problem in\\noptimizing worst-case optimal plans is to pick an ordering of the query\\nvertices to match. We design a cost-based optimizer that (i) picks efficient\\nquery vertex orderings for worst-case optimal plans; and (ii) generates hybrid\\nplans that mix traditional binary joins with worst-case optimal style multiway\\nintersections. Our cost metric combines the cost of binary joins with a new\\ncost metric called intersection-cost. The plan space of our optimizer contains\\nplans that are not in the plan spaces based on tree decompositions from prior\\nwork. In addition to our optimizer, we describe an adaptive technique that\\nchanges the orderings of the worst-case optimal sub-plans during query\\nexecution. We demonstrate the effectiveness of the plans our optimizer picks\\nand adaptive technique through extensive experiments. Our optimizer is\\nintegrated into the Graphflow DBMS.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1903.02076v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.02076v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.02990v2', updated=datetime.datetime(2019, 5, 29, 12, 36, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 7, 15, 27, 30, tzinfo=datetime.timezone.utc), title='Scheduling OLTP Transactions via Machine Learning', authors=[arxiv.Result.Author('Yangjun Sheng'), arxiv.Result.Author('Anthony Tomasic'), arxiv.Result.Author('Tieying Zhang'), arxiv.Result.Author('Andrew Pavlo')], summary='Current main memory database system architectures are still challenged by\\nhigh contention workloads and this challenge will continue to grow as the\\nnumber of cores in processors continues to increase. These systems schedule\\ntransactions randomly across cores to maximize concurrency and to produce a\\nuniform load across cores. Scheduling never considers potential conflicts.\\nPerformance could be improved if scheduling balanced between concurrency to\\nmaximize throughput and scheduling transactions linearly to avoid conflicts. In\\nthis paper, we present the design of several intelligent transaction scheduling\\nalgorithms that consider both potential transaction conflicts and concurrency.\\nTo incorporate reasoning about transaction conflicts, we develop a supervised\\nmachine learning model that estimates the probability of conflict. This model\\nis incorporated into several scheduling algorithms. In addition, we integrate\\nan unsupervised machine learning algorithm into an intelligent scheduling\\nalgorithm. We then empirically measure the performance impact of different\\nscheduling algorithms on OLTP and social networking workloads. Our results show\\nthat, with appropriate settings, intelligent scheduling can increase throughput\\nby 54% and reduce abort rate by 80% on a 20-core machine, relative to random\\nscheduling. In summary, the paper provides preliminary evidence that\\nintelligent scheduling significantly improves DBMS performance.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1903.02990v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.02990v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.03676v2', updated=datetime.datetime(2022, 12, 5, 3, 27, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 8, 21, 47, 28, tzinfo=datetime.timezone.utc), title='Automated data validation: an industrial experience report', authors=[arxiv.Result.Author('Lei Zhang'), arxiv.Result.Author('Sean Howard'), arxiv.Result.Author('Tom Montpool'), arxiv.Result.Author('Jessica Moore'), arxiv.Result.Author('Krittika Mahajan'), arxiv.Result.Author('Andriy Miranskyy')], summary='There has been a massive explosion of data generated by customers and\\nretained by companies in the last decade. However, there is a significant\\nmismatch between the increasing volume of data and the lack of automation\\nmethods and tools. The lack of best practices in data science programming may\\nlead to software quality degradation, release schedule slippage, and budget\\noverruns. To mitigate these concerns, we would like to bring software\\nengineering best practices into data science. Specifically, we focus on\\nautomated data validation in the data preparation phase of the software\\ndevelopment life cycle.\\n  This paper studies a real-world industrial case and applies software\\nengineering best practices to develop an automated test harness called RESTORE.\\nWe release RESTORE as an open-source R package. Our experience report, done on\\nthe geodemographic data, shows that RESTORE enables efficient and effective\\ndetection of errors injected during the data preparation phase. RESTORE also\\nsignificantly reduced the cost of testing. We hope that the community benefits\\nfrom the open-source project and the practical advice based on our experience.', comment='39 pages, 3 figures, accepted by the Journal of Systems and Software,\\n  Elsevier', journal_ref=None, doi='10.1016/j.jss.2022.111573', primary_category='cs.DB', categories=['cs.DB', 'cs.SE', 'stat.AP'], links=[arxiv.Result.Link('http://dx.doi.org/10.1016/j.jss.2022.111573', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1903.03676v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.03676v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.03683v1', updated=datetime.datetime(2019, 3, 8, 22, 12, 12, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 8, 22, 12, 12, tzinfo=datetime.timezone.utc), title='Transparency, Fairness, Data Protection, Neutrality: Data Management Challenges in the Face of New Regulation', authors=[arxiv.Result.Author('Serge Abiteboul'), arxiv.Result.Author('Julia Stoyanovich')], summary=\"The data revolution continues to transform every sector of science, industry\\nand government. Due to the incredible impact of data-driven technology on\\nsociety, we are becoming increasingly aware of the imperative to use data and\\nalgorithms responsibly -- in accordance with laws and ethical norms. In this\\narticle we discuss three recent regulatory frameworks: the European Union's\\nGeneral Data Protection Regulation (GDPR), the New York City Automated\\nDecisions Systems (ADS) Law, and the Net Neutrality principle, that aim to\\nprotect the rights of individuals who are impacted by data collection and\\nanalysis. These frameworks are prominent examples of a global trend:\\nGovernments are starting to recognize the need to regulate data-driven\\nalgorithmic technology.\\n  Our goal in this paper is to bring these regulatory frameworks to the\\nattention of the data management community, and to underscore the technical\\nchallenges they raise and which we, as a community, are well-equipped to\\naddress. The main take-away of this article is that legal and ethical norms\\ncannot be incorporated into data-driven systems as an afterthought. Rather, we\\nmust think in terms of responsibility by design, viewing it as a systems\\nrequirement.\", comment='To appear in the ACM Journal of Data and Information Quality (JDIQ)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/1903.03683v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.03683v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.03761v2', updated=datetime.datetime(2019, 3, 14, 7, 25, 34, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 9, 8, 18, 55, tzinfo=datetime.timezone.utc), title='RadegastXDB - Prototype of Native XML Database Management System: Technical Report', authors=[arxiv.Result.Author('Petr Lukáš'), arxiv.Result.Author('Radim Bača'), arxiv.Result.Author('Michal Krátký')], summary='A lot of advances in the processing of XML data have been proposed in last\\ntwo decades. There were many approaches focused on the efficient processing of\\ntwig pattern queries (TPQ). However, including the TPQ into an XQuery compiler\\nis not a straightforward task and current XML DBMSs process XQueries without\\nany TPQ detection. In this paper, we demonstrate our prototype of a native XML\\nDBMS called RadegastXDB that uses a TPQ detection to accelerate structural\\nXQueries. Such a detection allows us to utilize state-of-the-art TPQ processing\\nalgorithms. Our experiments show that, for the structural queries, these\\nalgorithms and state-of-the-art XML indexing techniques make our prototype\\nfaster than all of the current XML DBMSs, especially for large data\\ncollections. We also show that using the same techniques is also efficient for\\nthe processing of queries with value predicates.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1903.03761v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.03761v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.05826v1', updated=datetime.datetime(2019, 3, 14, 6, 8, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 14, 6, 8, 46, tzinfo=datetime.timezone.utc), title='A Hybrid Data Cleaning Framework using Markov Logic Networks', authors=[arxiv.Result.Author('Yunjun Gao'), arxiv.Result.Author('Congcong Ge'), arxiv.Result.Author('Xiaoye Miao'), arxiv.Result.Author('Haobo Wang'), arxiv.Result.Author('Bin Yao'), arxiv.Result.Author('Qing Li')], summary='With the increase of dirty data, data cleaning turns into a crux of data\\nanalysis. Most of the existing algorithms rely on either qualitative techniques\\n(e.g., data rules) or quantitative ones (e.g., statistical methods). In this\\npaper, we present a novel hybrid data cleaning framework on top of Markov logic\\nnetworks (MLNs), termed as MLNClean, which is capable of cleaning both\\nschema-level and instance-level errors. MLNClean mainly consists of two\\ncleaning stages, namely, first cleaning multiple data versions separately (each\\nof which corresponds to one data rule), and then deriving the final clean data\\nbased on multiple data versions. Moreover, we propose a series of\\ntechniques/concepts, e.g., the MLN index, the concepts of reliability score and\\nfusion score, to facilitate the cleaning process. Extensive experimental\\nresults on both real and synthetic datasets demonstrate the superiority of\\nMLNClean to the state-of-the-art approach in terms of both accuracy and\\nefficiency.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1903.05826v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.05826v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.06453v2', updated=datetime.datetime(2019, 4, 29, 13, 22, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 15, 10, 46, 53, tzinfo=datetime.timezone.utc), title='Adding Value by Combining Business and Sensor Data: An Industry 4.0 Use Case', authors=[arxiv.Result.Author('Guenter Hesse'), arxiv.Result.Author('Christoph Matthies'), arxiv.Result.Author('Werner Sinzig'), arxiv.Result.Author('Matthias Uflacker')], summary=\"Industry 4.0 and the Internet of Things are recent developments that have\\nlead to the creation of new kinds of manufacturing data. Linking this new kind\\nof sensor data to traditional business information is crucial for enterprises\\nto take advantage of the data's full potential. In this paper, we present a\\ndemo which allows experiencing this data integration, both vertically between\\ntechnical and business contexts and horizontally along the value chain. The\\ntool simulates a manufacturing company, continuously producing both business\\nand sensor data, and supports issuing ad-hoc queries that answer specific\\nquestions related to the business. In order to adapt to different environments,\\nusers can configure sensor characteristics to their needs.\", comment='Accepted at International Conference on Database Systems for Advanced\\n  Applications (DASFAA 2019)', journal_ref=None, doi='10.1007/978-3-030-18590-9_80', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-030-18590-9_80', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1903.06453v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.06453v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.08132v2', updated=datetime.datetime(2019, 3, 22, 17, 47, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 19, 17, 42, 5, tzinfo=datetime.timezone.utc), title='ExplainIt! -- A declarative root-cause analysis engine for time series data (extended version)', authors=[arxiv.Result.Author('Vimalkumar Jeyakumar'), arxiv.Result.Author('Omid Madani'), arxiv.Result.Author('Ali Parandeh'), arxiv.Result.Author('Ashutosh Kulshreshtha'), arxiv.Result.Author('Weifei Zeng'), arxiv.Result.Author('Navindra Yadav')], summary='We present ExplainIt!, a declarative, unsupervised root-cause analysis engine\\nthat uses time series monitoring data from large complex systems such as data\\ncentres. ExplainIt! empowers operators to succinctly specify a large number of\\ncausal hypotheses to search for causes of interesting events. ExplainIt! then\\nranks these hypotheses, reducing the number of causal dependencies from\\nhundreds of thousands to a handful for human understanding. We show how a\\ndeclarative language, such as SQL, can be effective in declaratively\\nenumerating hypotheses that probe the structure of an unknown probabilistic\\ngraphical causal model of the underlying system. Our thesis is that databases\\nare in a unique position to enable users to rapidly explore the possible causal\\nmechanisms in data collected from diverse sources. We empirically demonstrate\\nhow ExplainIt! had helped us resolve over 30 performance issues in a commercial\\nproduct since late 2014, of which we discuss a few cases in detail.', comment='SIGMOD Industry Track 2019', journal_ref=None, doi='10.1145/3299869.3314048', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3299869.3314048', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1903.08132v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.08132v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.08334v1', updated=datetime.datetime(2019, 3, 20, 3, 55, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 20, 3, 55, 23, tzinfo=datetime.timezone.utc), title='Indexes in Microsoft SQL Server', authors=[arxiv.Result.Author('Sourav Mukherjee')], summary='Indexes are the best apposite choice for quickly retrieving the records. This\\nis nothing but cutting down the number of Disk IO. Instead of scanning the\\ncomplete table for the results, we can decrease the number of IO\\'s or page\\nfetches using index structures such as B-Trees or Hash Indexes to retrieve the\\ndata faster. The most convenient way to consider an index is to think like a\\ndictionary. It has words and its corresponding definitions against those words.\\nThe dictionary will have an index on \"word\" because when we open a dictionary\\nand we want to fetch its corresponding word quickly, then find its definition.\\nThe dictionary generally contains just a single index - an index ordered by\\nword. When we modify any record and change the corresponding value of an\\nindexed column in a clustered index, the database might require moving the\\nentire row into a separately new position to maintain the rows in the sorted\\norder. This action is essentially turned into an update query into a DELETE\\nfollowed by an INSERT, and it decreases the performance of the query. The\\nclustered index in the table can often be available on the primary key or a\\nforeign key column because key values usually do not modify once a record is\\ninjected into the database.', comment='16 pages, 3 figures', journal_ref=None, doi='10.6084/m9.figshare.7866176', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.6084/m9.figshare.7866176', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1903.08334v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.08334v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.08587v4', updated=datetime.datetime(2020, 5, 25, 12, 48, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 20, 16, 15, 24, tzinfo=datetime.timezone.utc), title='Reliability Maximization in Uncertain Graphs', authors=[arxiv.Result.Author('Xiangyu Ke'), arxiv.Result.Author('Arijit Khan'), arxiv.Result.Author('Mohammad Al Hasan'), arxiv.Result.Author('Rojin Rezvansangsari')], summary='Network reliability measures the probability that a target node is reachable\\nfrom a source node in an uncertain graph, i.e., a graph where every edge is\\nassociated with a probability of existence. In this paper, we investigate the\\nnovel and fundamental problem of adding a small number of edges in the\\nuncertain network for maximizing the reliability between a given pair of nodes.\\nWe study the NP-hardness and the approximation hardness of our problem, and\\ndesign effective, scalable solutions. Furthermore, we consider extended\\nversions of our problem (e.g., multiple source and target nodes can be provided\\nas input) to support and demonstrate a wider family of queries and\\napplications, including sensor network reliability maximization and social\\ninfluence maximization. Experimental results validate the effectiveness and\\nefficiency of the proposed algorithms.', comment=None, journal_ref='IEEE Transaction on Knowledge and Data Engineering, 2020', doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1903.08587v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.08587v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.09242v1', updated=datetime.datetime(2019, 3, 21, 21, 28, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 21, 21, 28, 28, tzinfo=datetime.timezone.utc), title='Repairing mappings under policy views', authors=[arxiv.Result.Author('Angela Bonifati'), arxiv.Result.Author('Ugo Comignani'), arxiv.Result.Author('Efthymia Tsamoura')], summary='The problem of data exchange involves a source schema, a target schema and a\\nset of mappings from transforming the data between the two schemas. We study\\nthe problem of data exchange in the presence of privacy restrictions on the\\nsource. The privacy restrictions are expressed as a set of policy views\\nrepresenting the information that is safe to expose over all instances of the\\nsource. We propose a protocol that provides formal privacy guarantees and is\\ndata-independent, i.e., if certain criteria are met, then the protocol\\nguarantees that the mappings leak no sensitive information independently of the\\ndata that lies in the source. We also propose an algorithm for repairing an\\ninput mapping w.r.t. a set of policy views, in cases where the input mapping\\nleaks sensitive information. The empirical evaluation of our work shows that\\nthe proposed algorithm is quite efficient, repairing sets of 300 s-t tgds in an\\naverage time of 5s on a commodity machine. To the best of our knowledge, our\\nwork is the first one that studies the problems of exchanging data and\\nrepairing mappings under such privacy restrictions. Furthermore, our work is\\nthe first to provide practical algorithms for a logical privacy-preservation\\nparadigm, described as an open research challenge in previous work on this\\narea.', comment='12 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1903.09242v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.09242v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.10000v3', updated=datetime.datetime(2019, 11, 18, 20, 5, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 24, 15, 21, 19, tzinfo=datetime.timezone.utc), title='Approximate Query Processing using Deep Generative Models', authors=[arxiv.Result.Author('Saravanan Thirumuruganathan'), arxiv.Result.Author('Shohedul Hasan'), arxiv.Result.Author('Nick Koudas'), arxiv.Result.Author('Gautam Das')], summary='Data is generated at an unprecedented rate surpassing our ability to analyze\\nthem. The database community has pioneered many novel techniques for\\nApproximate Query Processing (AQP) that could give approximate results in a\\nfraction of time needed for computing exact results. In this work, we explore\\nthe usage of deep learning (DL) for answering aggregate queries specifically\\nfor interactive applications such as data exploration and visualization. We use\\ndeep generative models, an unsupervised learning based approach, to learn the\\ndata distribution faithfully such that aggregate queries could be answered\\napproximately by generating samples from the learned model. The model is often\\ncompact - few hundred KBs - so that arbitrary AQP queries could be answered on\\nthe client side without contacting the database server. Our other contributions\\ninclude identifying model bias and minimizing it through a rejection sampling\\nbased approach and an algorithm to build model ensembles for AQP for improved\\naccuracy. Our extensive experiments show that our proposed approach can provide\\nanswers with high accuracy and low latency.', comment='Accepted to ICDE 2020 as \"Approximate Query Processing for Data\\n  Exploration using Deep Generative Models\"', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1903.10000v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.10000v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1903.10970v1', updated=datetime.datetime(2019, 3, 26, 15, 53, 49, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 26, 15, 53, 49, tzinfo=datetime.timezone.utc), title='Apache Hive: From MapReduce to Enterprise-grade Big Data Warehousing', authors=[arxiv.Result.Author('Jesús Camacho-Rodríguez'), arxiv.Result.Author('Ashutosh Chauhan'), arxiv.Result.Author('Alan Gates'), arxiv.Result.Author('Eugene Koifman'), arxiv.Result.Author(\"Owen O'Malley\"), arxiv.Result.Author('Vineet Garg'), arxiv.Result.Author('Zoltan Haindrich'), arxiv.Result.Author('Sergey Shelukhin'), arxiv.Result.Author('Prasanth Jayachandran'), arxiv.Result.Author('Siddharth Seth'), arxiv.Result.Author('Deepak Jaiswal'), arxiv.Result.Author('Slim Bouguerra'), arxiv.Result.Author('Nishant Bangarwa'), arxiv.Result.Author('Sankar Hariappan'), arxiv.Result.Author('Anishek Agarwal'), arxiv.Result.Author('Jason Dere'), arxiv.Result.Author('Daniel Dai'), arxiv.Result.Author('Thejas Nair'), arxiv.Result.Author('Nita Dembla'), arxiv.Result.Author('Gopal Vijayaraghavan'), arxiv.Result.Author('Günther Hagleitner')], summary=\"Apache Hive is an open-source relational database system for analytic\\nbig-data workloads. In this paper we describe the key innovations on the\\njourney from batch tool to fully fledged enterprise data warehousing system. We\\npresent a hybrid architecture that combines traditional MPP techniques with\\nmore recent big data and cloud concepts to achieve the scale and performance\\nrequired by today's analytic applications. We explore the system by detailing\\nenhancements along four main axis: Transactions, optimizer, runtime, and\\nfederation. We then provide experimental results to demonstrate the performance\\nof the system for typical workloads and conclude with a look at the community\\nroadmap.\", comment=\"SIGMOD'19, 14 pages\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1903.10970v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1903.10970v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.00474v1', updated=datetime.datetime(2019, 10, 1, 15, 17, 21, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 1, 15, 17, 21, tzinfo=datetime.timezone.utc), title='Lineage-Aware Temporal Windows: Supporting Set Operations in Temporal-Probabilistic Databases', authors=[arxiv.Result.Author('Katerina Papaioannou'), arxiv.Result.Author('Martin Theobald'), arxiv.Result.Author('Michael H. Böhlen')], summary='In temporal-probabilistic (TP) databases, the combination of the temporal and\\nthe probabilistic dimension adds significant overhead to the computation of set\\noperations. Although set queries are guaranteed to yield linearly sized output\\nrelations, existing solutions exhibit quadratic runtime complexity. They suffer\\nfrom redundant interval comparisons and additional joins for the formation of\\nlineage expressions. In this paper, we formally define the semantics of set\\noperations in TP databases and study their properties. For their efficient\\ncomputation, we introduce the lineage-aware temporal window, a mechanism that\\ndirectly binds intervals with lineage expressions. We suggest the lineage-aware\\nwindow advancer (LAWA) for producing the windows of two TP relations in\\nlinearithmic time, and we implement all TP set operations based on LAWA. By\\nexploiting the flexibility of lineage-aware temporal windows, we perform direct\\nfiltering of irrelevant intervals and finalization of output lineage\\nexpressions and thus guarantee that no additional computational cost or buffer\\nspace is needed. A series of experiments over both synthetic and real-world\\ndatasets show that (a) our approach has predictable performance, depending only\\non the input size and not on the number of time intervals per fact or their\\noverlap, and that (b) it outperforms state-of-the-art approaches in both\\ntemporal and probabilistic databases.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.00474v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.00474v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.00765v2', updated=datetime.datetime(2020, 2, 16, 23, 11, 37, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 2, 3, 51, 37, tzinfo=datetime.timezone.utc), title='SharPer: Sharding Permissioned Blockchains Over Network Clusters', authors=[arxiv.Result.Author('Mohammad Javad Amiri'), arxiv.Result.Author('Divyakant Agrawal'), arxiv.Result.Author('Amr El Abbadi')], summary='Scalability is one of the main roadblocks to business adoption of blockchain\\nsystems. Despite recent intensive research on using sharding techniques to\\nenhance the scalability of blockchain systems, existing solutions do not\\nefficiently address cross-shard transactions. In this paper, we introduce\\nSharPer, a permissioned blockchain system that improves scalability by\\nclustering (partitioning) the nodes and assigning different data shards to\\ndifferent clusters where each data shard is replicated on the nodes of a\\ncluster. SharPer supports both intra-shard and cross-shard transactions and\\nprocesses intra-shard transactions of different clusters as well as cross-shard\\ntransactions with non-overlapping clusters simultaneously. In SharPer, the\\nblockchain ledger is formed as a directed acyclic graph where each cluster\\nmaintains only a view of the ledger. SharPer also incorporates a flattened\\nprotocol to establish consensus among clusters on the order of cross-shard\\ntransactions. The experimental results reveal the efficiency of SharPer in\\nterms of performance and scalability especially in workloads with a low\\npercentage of cross-shard transactions.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.00765v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.00765v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.00985v3', updated=datetime.datetime(2019, 10, 22, 12, 46, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 2, 14, 44, 47, tzinfo=datetime.timezone.utc), title='A Blueprint for Interoperable Blockchains', authors=[arxiv.Result.Author('Tien Tuan Anh Dinh'), arxiv.Result.Author('Anwitaman Datta'), arxiv.Result.Author('Beng Chin Ooi')], summary='Research in blockchain systems has mainly focused on improving security and\\nbridging the performance gaps between blockchains and databases. Despite many\\npromising results, we observe a worrying trend that the blockchain landscape is\\nfragmented in which many systems exist in silos. Apart from a handful of\\ngeneral-purpose blockchains, such as Ethereum or Hyperledger Fabric, there are\\nhundreds of others designed for specific applications and typically do not talk\\nto each other. In this paper, we describe our vision of interoperable\\nblockchains. We argue that supporting interaction among different blockchains\\nrequires overcoming challenges that go beyond data standardization. The\\nunderlying problem is to allow smart contracts running in different blockchains\\nto communicate. We discuss three open problems: access control, general\\ncross-chain transactions, and cross-chain communication. We describe partial\\nsolutions to some of these problems in the literature. Finally, we propose a\\nnovel design to overcome these challenges.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.00985v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.00985v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.04640v1', updated=datetime.datetime(2019, 10, 10, 15, 19, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 10, 15, 19, 19, tzinfo=datetime.timezone.utc), title='E2FM: an encrypted and compressed full-text index for collections of genomic sequences', authors=[arxiv.Result.Author('Ferdinando Montecuollo'), arxiv.Result.Author('Giovannni Schmid'), arxiv.Result.Author('Roberto Tagliaferri')], summary='Next Generation Sequencing (NGS) platforms and, more generally,\\nhigh-throughput technologies are giving rise to an exponential growth in the\\nsize of nucleotide sequence databases. Moreover, many emerging applications of\\nnucleotide datasets -- as those related to personalized medicine -- require the\\ncompliance with regulations about the storage and processing of sensitive data.\\nWe have designed and carefully engineered E2FM-index, a new full-text index in\\nminute space which was optimized for compressing and encrypting nucleotide\\nsequence collections in FASTA format and for performing fast pattern-search\\nqueries. E2FM-index allows to build self-indexes which occupy till to 1/20 of\\nthe storage required by the input FASTA file, thus permitting to save about 95%\\nof storage when indexing collections of highly similar sequences; moreover, it\\ncan exactly search the built indexes for patterns in times ranging from few\\nmilliseconds to a few hundreds milliseconds, depending on pattern length.\\nSupplementary material and supporting datasets are available through\\nBioinformatics Online and https://figshare.com/s/6246ee9c1bd730a8bf6e.', comment='23 pages with pseudo-code and experimental results', journal_ref='Bioinformatics, 33(18), 2017, 2808-2817', doi='10.1093/bioinformatics/btx313', primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://dx.doi.org/10.1093/bioinformatics/btx313', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1910.04640v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.04640v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.04728v1', updated=datetime.datetime(2019, 10, 10, 17, 41, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 10, 17, 41, 53, tzinfo=datetime.timezone.utc), title='LISA: Towards Learned DNA Sequence Search', authors=[arxiv.Result.Author('Darryl Ho'), arxiv.Result.Author('Jialin Ding'), arxiv.Result.Author('Sanchit Misra'), arxiv.Result.Author('Nesime Tatbul'), arxiv.Result.Author('Vikram Nathan'), arxiv.Result.Author('Vasimuddin Md'), arxiv.Result.Author('Tim Kraska')], summary='Next-generation sequencing (NGS) technologies have enabled affordable\\nsequencing of billions of short DNA fragments at high throughput, paving the\\nway for population-scale genomics. Genomics data analytics at this scale\\nrequires overcoming performance bottlenecks, such as searching for short DNA\\nsequences over long reference sequences. In this paper, we introduce LISA\\n(Learned Indexes for Sequence Analysis), a novel learning-based approach to DNA\\nsequence search. As a first proof of concept, we focus on accelerating one of\\nthe most essential flavors of the problem, called exact search. LISA builds on\\nand extends FM-index, which is the state-of-the-art technique widely deployed\\nin genomics tool-chains. Initial experiments with human genome datasets\\nindicate that LISA achieves up to a factor of 4X performance speedup against\\nits traditional counterpart.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.04728v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.04728v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.05773v2', updated=datetime.datetime(2020, 8, 29, 15, 30, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 13, 15, 57, 13, tzinfo=datetime.timezone.utc), title='LiveGraph: A Transactional Graph Storage System with Purely Sequential Adjacency List Scans', authors=[arxiv.Result.Author('Xiaowei Zhu'), arxiv.Result.Author('Guanyu Feng'), arxiv.Result.Author('Marco Serafini'), arxiv.Result.Author('Xiaosong Ma'), arxiv.Result.Author('Jiping Yu'), arxiv.Result.Author('Lei Xie'), arxiv.Result.Author('Ashraf Aboulnaga'), arxiv.Result.Author('Wenguang Chen')], summary=\"The specific characteristics of graph workloads make it hard to design a\\none-size-fits-all graph storage system. Systems that support transactional\\nupdates use data structures with poor data locality, which limits the\\nefficiency of analytical workloads or even simple edge scans. Other systems run\\ngraph analytics workloads efficiently, but cannot properly support\\ntransactions.\\n  This paper presents LiveGraph, a graph storage system that outperforms both\\nthe best graph transactional systems and the best systems for real-time graph\\nanalytics on fresh data. LiveGraph does that by ensuring that adjacency list\\nscans, a key operation in graph workloads, are purely sequential: they never\\nrequire random accesses even in presence of concurrent transactions. This is\\nachieved by combining a novel graph-aware data structure, the Transactional\\nEdge Log (TEL), together with a concurrency control mechanism that leverages\\nTEL's data layout. Our evaluation shows that LiveGraph significantly\\noutperforms state-of-the-art (graph) database solutions on both transactional\\nand real-time analytical workloads.\", comment=None, journal_ref=None, doi='10.14778/3384345.3384351', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3384345.3384351', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1910.05773v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.05773v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.06584v2', updated=datetime.datetime(2020, 1, 17, 14, 3, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 15, 8, 18, tzinfo=datetime.timezone.utc), title='Semantic Guided and Response Times Bounded Top-k Similarity Search over Knowledge Graphs', authors=[arxiv.Result.Author('Yuxiang Wang'), arxiv.Result.Author('Arijit Khan'), arxiv.Result.Author('Tianxing Wu'), arxiv.Result.Author('Jiahui Jin'), arxiv.Result.Author('Haijiang Yan')], summary=\"Recently, graph query is widely adopted for querying knowledge graphs. Given\\na query graph $G_Q$, the graph query finds subgraphs in a knowledge graph $G$\\nthat exactly or approximately match $G_Q$. We face two challenges on graph\\nquery: (1) the structural gap between $G_Q$ and the predefined schema in $G$\\ncauses mismatch with query graph, (2) users cannot view the answers until the\\ngraph query terminates, leading to a longer system response time (SRT). In this\\npaper, we propose a semantic-guided and response-time-bounded graph query to\\nreturn the top-k answers effectively and efficiently. We leverage a knowledge\\ngraph embedding model to build the semantic graph $SG_Q$, and we define the\\npath semantic similarity ($pss$) over $SG_Q$ as the metric to evaluate the\\nanswer's quality. Then, we propose an A* semantic search on $SG_Q$ to find the\\ntop-k answers with the greatest $pss$ via a heuristic $pss$ estimation.\\nFurthermore, we make an approximate optimization on A* semantic search to allow\\nusers to trade off the effectiveness for SRT within a user-specific time bound.\\nExtensive experiments over real datasets confirm the effectiveness and\\nefficiency of our solution.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.06584v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.06584v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.07519v2', updated=datetime.datetime(2020, 3, 16, 16, 55, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 16, 8, 21, 57, tzinfo=datetime.timezone.utc), title='On foundational aspects of RDF and SPARQL', authors=[arxiv.Result.Author('Dominique Duval'), arxiv.Result.Author('Rachid Echahed'), arxiv.Result.Author('Frederic Prost')], summary='We consider the recommendations of the World Wide Web Consortium (W3C) about\\nthe Resource Description Framework (RDF) and the associated query language\\nSPARQL. We propose a new formal framework based on category theory which\\nprovides clear and concise formal definitions of the main basic features of RDF\\nand SPARQL. We propose to define the notions of RDF graphs as well as SPARQL\\nbasic graph patterns as objects of some nested categories. This allows one to\\nclarify, in particular, the role of blank nodes. Furthermore, we consider basic\\nSPARQL CONSTRUCT and SELECT queries and formalize their operational semantics\\nfollowing a novel algebraic graph transformation approach called POIM.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.FL', 'cs.PL'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.07519v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.07519v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.07786v1', updated=datetime.datetime(2019, 10, 17, 9, 29, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 17, 9, 29, 24, tzinfo=datetime.timezone.utc), title='Service Wrapper: a system for converting web data into web services', authors=[arxiv.Result.Author('Naibo Wang'), arxiv.Result.Author('Zhiling Luo'), arxiv.Result.Author('Xiya Lyu'), arxiv.Result.Author('Zitong Yang'), arxiv.Result.Author('Jianwei Yin')], summary='Web services are widely used in many areas via callable APIs, however, data\\nare not always available in this way. We always need to get some data from web\\npages whose structure is not in order. Many developers use web data extraction\\nmethods to generate wrappers to get useful contents from websites and convert\\nthem into well-structured files. These methods, however, are designed\\nspecifically for professional wrapper program developers and not friendly to\\nusers without expertise in this domain. In this work, we construct a service\\nwrapper system to convert available data in web pages into web services.\\nAdditionally, a set of algorithms are introduced to solve problems in the whole\\nconversion process. People can use our system to convert web data into web\\nservices with fool-style operations and invoke these services by one simple\\nstep, which greatly expands the use of web data. Our cases show the ease of\\nuse, high availability, and stability of our system.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.07786v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.07786v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.08023v1', updated=datetime.datetime(2019, 10, 17, 16, 51, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 17, 16, 51, 44, tzinfo=datetime.timezone.utc), title='MV-PBT: Multi-Version Index for Large Datasets and HTAP Workloads', authors=[arxiv.Result.Author('Christian Riegger'), arxiv.Result.Author('Tobias Vincon'), arxiv.Result.Author('Robert Gottstein'), arxiv.Result.Author('Ilia Petrov')], summary=\"Modern mixed (HTAP) workloads execute fast update-transactions and\\nlong-running analytical queries on the same dataset and system. In\\nmulti-version (MVCC) systems, such workloads result in many short-lived\\nversions and long version-chains as well as in increased and frequent\\nmaintenance overhead. Consequently, the index pressure increases significantly.\\nFirstly, the frequent modifications cause frequent creation of new versions,\\nyielding a surge in index maintenance overhead. Secondly and more importantly,\\nindex-scans incur extra I/O overhead to determine, which of the resulting\\ntuple-versions are visible to the executing transaction (visibility-check) as\\ncurrent designs only store version/timestamp information in the base table --\\nnot in the index. Such index-only visibility-check is critical for HTAP\\nworkloads on large datasets. In this paper we propose the Multi-Version\\nPartitioned B-Tree (MV-PBT) as a version-aware index structure, supporting\\nindex-only visibility checks and flash-friendly I/O patterns. The experimental\\nevaluation indicates a 2x improvement for analytical queries and 15% higher\\ntransactional throughput under HTAP workloads (CH-Benchmark). MV-PBT offers 40%\\nhigher transactional throughput compared to WiredTiger's LSM-Tree\\nimplementation under YCSB.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.08023v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.08023v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.08185v2', updated=datetime.datetime(2020, 5, 11, 5, 23, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 17, 22, 13, 40, tzinfo=datetime.timezone.utc), title='An LSM-based Tuple Compaction Framework for Apache AsterixDB (Extended Version)', authors=[arxiv.Result.Author('Wail Y. Alkowaileet'), arxiv.Result.Author('Sattam Alsubaiee'), arxiv.Result.Author('Michael J. Carey')], summary='Document database systems store self-describing semi-structured records, such\\nas JSON, \"as-is\" without requiring the users to pre-define a schema. This\\nprovides users with the flexibility to change the structure of incoming records\\nwithout worrying about taking the system offline or hindering the performance\\nof currently running queries. However, the flexibility of such systems does not\\nfree. The large amount of redundancy in the records can introduce an\\nunnecessary storage overhead and impact query performance.\\n  Our focus in this paper is to address the storage overhead issue by\\nintroducing a tuple compactor framework that infers and extracts the schema\\nfrom self-describing semi-structured records during the data ingestion. As many\\nprominent document stores, such as MongoDB and Couchbase, adopt Log Structured\\nMerge (LSM) trees in their storage engines, our framework exploits LSM\\nlifecycle events to piggyback the schema inference and extraction operations.\\nWe have implemented and empirically evaluated our approach to measure its\\nimpact on storage, data ingestion, and query performance in the context of\\nApache AsterixDB.', comment='18 pages, 28 figures, to appear in VLDB 2020', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.08185v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.08185v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.08678v2', updated=datetime.datetime(2020, 4, 8, 23, 1, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 19, 2, 3, 15, tzinfo=datetime.timezone.utc), title='Effective Discovery of Meaningful Outlier Relationships', authors=[arxiv.Result.Author('Aline Bessa'), arxiv.Result.Author('Juliana Freire'), arxiv.Result.Author('Divesh Srivastava'), arxiv.Result.Author('Tamraparni Dasu')], summary='We propose PODS (Predictable Outliers in Data-trendS), a method that, given a\\ncollection of temporal data sets, derives data-driven explanations for outliers\\nby identifying meaningful relationships between them. First, we formalize the\\nnotion of meaningfulness, which so far has been informally framed in terms of\\nexplainability. Next, since outliers are rare and it is difficult to determine\\nwhether their relationships are meaningful, we develop a new criterion that\\ndoes so by checking if these relationships could have been predicted from\\nnon-outliers, i.e., if we could see the outlier relationships coming. Finally,\\nsearching for meaningful outlier relationships between every pair of data sets\\nin a large data collection is computationally infeasible. To address that, we\\npropose an indexing strategy that prunes irrelevant comparisons across data\\nsets, making the approach scalable. We present the results of an experimental\\nevaluation using real data sets and different baselines, which demonstrates the\\neffectiveness, robustness, and scalability of our approach.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.08678v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.08678v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.10063v1', updated=datetime.datetime(2019, 10, 22, 16, 5, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 22, 16, 5, 2, tzinfo=datetime.timezone.utc), title='Exploiting Data Skew for Improved Query Performance', authors=[arxiv.Result.Author('Wangda Zhang'), arxiv.Result.Author('Kenneth A. Ross')], summary='Analytic queries enable sophisticated large-scale data analysis within many\\ncommercial, scientific and medical domains today. Data skew is a ubiquitous\\nfeature of these real-world domains. In a retail database, some products are\\ntypically much more popular than others. In a text database, word frequencies\\nfollow a Zipf distribution with a small number of very common words, and a long\\ntail of infrequent words. In a geographic database, some regions have much\\nhigher populations (and data measurements) than others. Current systems do not\\nmake the most of caches for exploiting skew. In particular, a whole cache line\\nmay remain cache resident even though only a small part of the cache line\\ncorresponds to a popular data item. In this paper, we propose a novel index\\nstructure for repositioning data items to concentrate popular items into the\\nsame cache lines. The net result is better spatial locality, and better\\nutilization of limited cache resources. We develop a theoretical model for\\nanalyzing the cache behavior, and implement database operators that are\\nefficient in the presence of skew. Our experiments on real and synthetic data\\nshow that exploiting skew can significantly improve in-memory query\\nperformance. In some cases, our techniques can speed up queries by over an\\norder of magnitude.', comment=None, journal_ref=None, doi='10.1109/TKDE.2020.3006446', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/TKDE.2020.3006446', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1910.10063v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.10063v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.10263v1', updated=datetime.datetime(2019, 10, 22, 22, 40, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 22, 22, 40, 24, tzinfo=datetime.timezone.utc), title='Integrating Information About Entities Progressively', authors=[arxiv.Result.Author('Ben McCamish'), arxiv.Result.Author('Christopher Buss'), arxiv.Result.Author('Arash Termehchy'), arxiv.Result.Author('David Maier')], summary='Users often have to integrate information about entities from multiple data\\nsources. This task is challenging as each data source may represent information\\nabout the same entity in a distinct form, e.g., each data source may use a\\ndifferent name for the same person. Currently, data from different\\nrepresentations are translated into a unified one via lengthy and costly expert\\nattention and tuning. Such methods cannot scale to the rapidly increasing\\nnumber and variety of available data sources. We demonstrate ProgMap, a\\nentity-matching framework in which data sources learn to collaborate and\\nintegrate information about entities on-demand and with minimal expert\\nintervention. The data sources leverage user feedback to improve the accuracy\\nof their collaboration and results. ProgMap also has techniques to reduce the\\namount of required user feedback to achieve effective matchings.', comment='demonstration', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.10263v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.10263v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.10959v2', updated=datetime.datetime(2019, 10, 30, 23, 36, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 24, 7, 50, 56, tzinfo=datetime.timezone.utc), title='Toward Co-existing Database Schemas based on Bidirectional Transformation', authors=[arxiv.Result.Author('Jumpei Tanaka'), arxiv.Result.Author('Van-Dang Tran'), arxiv.Result.Author('Hiroyuki kato'), arxiv.Result.Author('Zhenjiang Hu')], summary='According to strong demands for rapid and reliable software delivery,\\nco-existing database schema versions with multiple application versions are\\nreality to contribute them. Current database management systems do not support\\nco-existing schema versions in one database. Although a design of co-existing\\nschema based on updatable view tables was previously proposed, its flexibility\\nis limited due to pre-defined several restrictions to achieve data\\nsynchronization among schemas and handling independent unsynchronized data in\\neach schema. In this preliminary report, we present a new approach for\\nco-existing schemas based on bidirectional transformation. We explain the\\nrequired properties to realize co-existing schemas, bidirectionality and\\ntotality. We show that the co-existing schemas can be implemented\\nsystematically by applying putback-based bidirectional transformation to\\nsatisfy both the bidirectionality and the totality. While the bidirectionality\\ncan be satisfied by applying bidirectional transformation, to satisfy the\\ntotality, extra functions need to be introduced. How to derive these extra\\nfunctions is presented.', comment='Proceedings of the Third Workshop on Software Foundations for Data\\n  Interoperability (SFDI2019+), October 28, 2019, Fukuoka, Japan', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.10959v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.10959v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.11040v1', updated=datetime.datetime(2019, 10, 24, 11, 52, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 24, 11, 52, 13, tzinfo=datetime.timezone.utc), title='Toward a view-based data cleaning architecture', authors=[arxiv.Result.Author('Toshiyuki Shimizu'), arxiv.Result.Author('Hiroki Omori'), arxiv.Result.Author('Masatoshi Yoshikawa')], summary='Big data analysis has become an active area of study with the growth of\\nmachine learning techniques. To properly analyze data, it is important to\\nmaintain high-quality data. Thus, research on data cleaning is also important.\\nIt is difficult to automatically detect and correct inconsistent values for\\ndata requiring expert knowledge or data created by many contributors, such as\\nintegrated data from heterogeneous data sources. An example of such data is\\nmetadata for scientific datasets, which should be confirmed by data managers\\nwhile handling the data. To support the efficient cleaning of data by data\\nmanagers, we propose a data cleaning architecture in which data managers\\ninteractively browse and correct portions of data through views. In this paper,\\nwe explain our view-based data cleaning architecture and discuss some remaining\\nissues.', comment='Proceedings of the Third Workshop on Software Foundations for Data\\n  Interoperability (SFDI2019+), October 28, 2019, Fukuoka, Japan', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.11040v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.11040v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.12261v1', updated=datetime.datetime(2019, 10, 27, 13, 28, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 27, 13, 28, 13, tzinfo=datetime.timezone.utc), title='Typical Snapshots Selection for Shortest Path Query in Dynamic Road Networks', authors=[arxiv.Result.Author('Mengxuan Zhang'), arxiv.Result.Author('Lei Li'), arxiv.Result.Author('Wen Hua'), arxiv.Result.Author('Xiaofang Zhou')], summary='Finding the shortest paths in road network is an important query in our life\\nnowadays, and various index structures are constructed to speed up the query\\nanswering. However, these indexes can hardly work in real-life scenario because\\nthe traffic condition changes dynamically, which makes the pathfinding slower\\nthan in the static environment. In order to speed up path query answering in\\nthe dynamic road network, we propose a framework to support these indexes.\\nFirstly, we view the dynamic graph as a series of static snapshots. After that,\\nwe propose two kinds of methods to select the typical snapshots. The first kind\\nis time-based and it only considers the temporal information. The second\\ncategory is the graph representation-based, which considers more insights:\\nedge-based that captures the road continuity, and vertex-based that reflects\\nthe region traffic fluctuation. Finally, we propose the snapshot matching to\\nfind the most similar typical snapshot for the current traffic condition and\\nuse its index to answer the query directly. Extensive experiments on real-life\\nroad network and traffic conditions validate the effectiveness of our approach.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.12261v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.12261v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.13065v1', updated=datetime.datetime(2019, 10, 29, 3, 26, 36, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 29, 3, 26, 36, tzinfo=datetime.timezone.utc), title='A Survey on Map-Matching Algorithms', authors=[arxiv.Result.Author('Pingfu Chao'), arxiv.Result.Author('Yehong Xu'), arxiv.Result.Author('Wen Hua'), arxiv.Result.Author('Xiaofang Zhou')], summary='The map-matching is an essential preprocessing step for most of the\\ntrajectory-based applications. Although it has been an active topic for more\\nthan two decades and, driven by the emerging applications, is still under\\ndevelopment. There is a lack of categorisation of existing solutions recently\\nand analysis for future research directions. In this paper, we review the\\ncurrent status of the map-matching problem and survey the existing algorithms.\\nWe propose a new categorisation of the solutions according to their\\nmap-matching models and working scenarios. In addition, we experimentally\\ncompare three representative methods from different categories to reveal how\\nmatching model affects the performance. Besides, the experiments are conducted\\non multiple real datasets with different settings to demonstrate the influence\\nof other factors in map-matching problem, like the trajectory quality, data\\ncompression and matching latency.', comment='12 pages, 5 figures, submitted to ADC 2020', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.13065v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.13065v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.00338v1', updated=datetime.datetime(2020, 1, 2, 6, 26, 36, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 2, 6, 26, 36, tzinfo=datetime.timezone.utc), title='Informal Data Transformation Considered Harmful', authors=[arxiv.Result.Author('Eric Daimler'), arxiv.Result.Author('Ryan Wisnesky')], summary='In this paper we take the common position that AI systems are limited more by\\nthe integrity of the data they are learning from than the sophistication of\\ntheir algorithms, and we take the uncommon position that the solution to\\nachieving better data integrity in the enterprise is not to clean and validate\\ndata ex-post-facto whenever needed (the so-called data lake approach to data\\nmanagement, which can lead to data scientists spending 80% of their time\\ncleaning data), but rather to formally and automatically guarantee that data\\nintegrity is preserved as it transformed (migrated, integrated, composed,\\nqueried, viewed, etc) throughout the enterprise, so that data and programs that\\ndepend on that data need not constantly be re-validated for every particular\\nuse.', comment='Proceedings paper for AAAI FSS-19: HAI19', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.00338v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00338v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.00432v1', updated=datetime.datetime(2020, 1, 2, 13, 44, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 2, 13, 44, 19, tzinfo=datetime.timezone.utc), title='RDF 1.1: Knowledge Representation and Data Integration Language for the Web', authors=[arxiv.Result.Author('Dominik Tomaszuk'), arxiv.Result.Author('David Hyland-Wood')], summary=\"Resource Description Framework (RDF) can seen as a solution in today's\\nlandscape of knowledge representation research. An RDF language has symmetrical\\nfeatures because subjects and objects in triples can be interchangeably used.\\nMoreover, the regularity and symmetry of the RDF language allow knowledge\\nrepresentation that is easily processed by machines, and because its structure\\nis similar to natural languages, it is reasonably readable for people. RDF\\nprovides some useful features for generalized knowledge representation. Its\\ndistributed nature, due to its identifier grounding in IRIs, naturally scales\\nto the size of the Web. However, its use is often hidden from view and is,\\ntherefore, one of the less well-known of the knowledge representation\\nframeworks. Therefore, we summarise RDF v1.0 and v1.1 to broaden its audience\\nwithin the knowledge representation community. This article reviews current\\napproaches, tools, and applications for mapping from relational databases to\\nRDF and from XML to RDF. We discuss RDF serializations, including formats with\\nsupport for multiple graphs and we analyze RDF compression proposals. Finally,\\nwe present a summarized formal definition of RDF 1.1 that provides additional\\ninsights into the modeling of reification, blank nodes, and entailments.\", comment='26 pages, 4 figures', journal_ref='Symmetry 2020, 12, 84', doi='10.3390/sym12010084', primary_category='cs.DB', categories=['cs.DB', 'H.2.0'], links=[arxiv.Result.Link('http://dx.doi.org/10.3390/sym12010084', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2001.00432v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00432v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.00688v2', updated=datetime.datetime(2020, 11, 18, 2, 59, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 3, 2, 5, 8, tzinfo=datetime.timezone.utc), title='Statistical Detection of Collective Data Fraud', authors=[arxiv.Result.Author('Ruoyu Wang'), arxiv.Result.Author('Xiaobo Hu'), arxiv.Result.Author('Daniel Sun'), arxiv.Result.Author('Guoqiang Li'), arxiv.Result.Author('Raymond Wong'), arxiv.Result.Author('Shiping Chen'), arxiv.Result.Author('Jianquan Liu')], summary='Statistical divergence is widely applied in multimedia processing, basically\\ndue to regularity and interpretable features displayed in data. However, in a\\nbroader range of data realm, these advantages may no longer be feasible, and\\ntherefore a more general approach is required. In data detection, statistical\\ndivergence can be used as a similarity measurement based on collective\\nfeatures. In this paper, we present a collective detection technique based on\\nstatistical divergence. The technique extracts distribution similarities among\\ndata collections, and then uses the statistical divergence to detect collective\\nanomalies. Evaluation shows that it is applicable in the real world.', comment='6 pages, 6 figures and tables, submitted to ICME 2020', journal_ref='2020 IEEE International Conference on Multimedia and Expo (ICME),\\n  London, United Kingdom, 2020, pp. 1-6', doi='10.1109/ICME46284.2020.9102889', primary_category='cs.DB', categories=['cs.DB', 'E.0; H.2'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/ICME46284.2020.9102889', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2001.00688v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00688v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.01174v1', updated=datetime.datetime(2020, 1, 5, 5, 58, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 5, 5, 58, 41, tzinfo=datetime.timezone.utc), title='Distributed Nonblocking Commit Protocols for Many-Party Cross-Blockchain Transactions', authors=[arxiv.Result.Author('Xinying Wang'), arxiv.Result.Author('Olamide Timothy Tawose'), arxiv.Result.Author('Feng Yan'), arxiv.Result.Author('Dongfang Zhao')], summary=\"The interoperability across multiple blockchains would play a critical role\\nin future blockchain-based data management paradigm. Existing techniques either\\nwork only for two blockchains or requires a centralized component to govern the\\ncross-blockchain transaction execution, neither of which would meet the\\nscalability requirement. This paper proposes a new distributed commit protocol,\\nnamely \\\\textit{cross-blockchain transaction} (CBT), for conducting transactions\\nacross an arbitrary number of blockchains without any centralized component.\\nThe key idea of CBT is to extend the two-phase commit protocol with a heartbeat\\nmechanism to ensure the liveness of CBT without introducing additional nodes or\\nblockchains. We have implemented CBT and compared it to the state-of-the-art\\nprotocols, demonstrating CBT's low overhead (3.6\\\\% between two blockchains,\\nless than $1\\\\%$ among 32 or more blockchains) and high scalability (linear\\nscalability on up to 64-blockchain transactions). In addition, we developed a\\ngraphic user interface for users to virtually monitor the status of the\\ncross-blockchain transactions.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.01174v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.01174v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.01491v1', updated=datetime.datetime(2020, 1, 6, 11, 31, 12, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 6, 11, 31, 12, tzinfo=datetime.timezone.utc), title='Clustering based Privacy Preserving of Big Data using Fuzzification and Anonymization Operation', authors=[arxiv.Result.Author('Saira Khan'), arxiv.Result.Author('Khalid Iqbal'), arxiv.Result.Author('Safi Faizullah'), arxiv.Result.Author('Muhammad Fahad'), arxiv.Result.Author('Jawad Ali'), arxiv.Result.Author('Waqas Ahmed')], summary='Big Data is used by data miner for analysis purpose which may contain\\nsensitive information. During the procedures it raises certain privacy\\nchallenges for researchers. The existing privacy preserving methods use\\ndifferent algorithms that results into limitation of data reconstruction while\\nsecuring the sensitive data. This paper presents a clustering based privacy\\npreservation probabilistic model of big data to secure sensitive\\ninformation..model to attain minimum perturbation and maximum privacy. In our\\nmodel, sensitive information is secured after identifying the sensitive data\\nfrom data clusters to modify or generalize it.The resulting dataset is analysed\\nto calculate the accuracy level of our model in terms of hidden data, lossed\\ndata as result of reconstruction. Extensive experiements are carried out in\\norder to demonstrate the results of our proposed model. Clustering based\\nPrivacy preservation of individual data in big data with minimum perturbation\\nand successful reconstruction highlights the significance of our model in\\naddition to the use of standard performance evaluation measures.', comment='08 Page, 07 figures', journal_ref='International Journal of Advanced Computer Science and\\n  Applications, Volume 10 Issue 12, 2019', doi='10.14569/IJACSA.2019.0101239', primary_category='cs.DB', categories=['cs.DB', 'cs.CR'], links=[arxiv.Result.Link('http://dx.doi.org/10.14569/IJACSA.2019.0101239', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2001.01491v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.01491v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.01902v2', updated=datetime.datetime(2020, 3, 4, 3, 18, 57, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 7, 6, 1, 30, tzinfo=datetime.timezone.utc), title='Monte Carlo Tree Search for Generating Interactive Data Analysis Interfaces', authors=[arxiv.Result.Author('Yiru Chen'), arxiv.Result.Author('Eugene Wu')], summary='Interactive tools like user interfaces help democratize data access for\\nend-users by hiding underlying programming details and exposing the necessary\\nwidget interface to users. Since customized interfaces are costly to build,\\nautomated interface generation is desirable. SQL is the dominant way to analyze\\ndata and there already exists logs to analyze data. Previous work proposed a\\nsyntactic approach to analyze structural changes in SQL query logs and\\nautomatically generates a set of widgets to express the changes. However, they\\ndo not consider layout usability and the sequential order of queries in the\\nlog. We propose to adopt Monte Carlo Tree Search(MCTS) to search for the\\noptimal interface that accounts for hierarchical layout as well as the\\nusability in terms of how easy to express the query log.', comment='4 pages, 6 figures, The AAAI-20 Workshop on Intelligent Process\\n  Automation (IPA-20)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.01902v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.01902v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.02172v2', updated=datetime.datetime(2020, 6, 12, 13, 51, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 7, 16, 56, 23, tzinfo=datetime.timezone.utc), title='Data Structure Primitives on Persistent Memory: An Evaluation', authors=[arxiv.Result.Author('Philipp Götze'), arxiv.Result.Author('Arun Kumar Tharanatha'), arxiv.Result.Author('Kai-Uwe Sattler')], summary='Persistent Memory (PMem), as already available, e.g., with Intel Optane DC\\nPersistent Memory, represents a very promising, next-generation memory solution\\nwith a significant impact on database architectures. Several data structures\\nfor this new technology and its properties have already been proposed. However,\\nprimarily only complete structures are presented and evaluated. Thus, the\\nimplications of the individual ideas and PMem features are concealed.\\nTherefore, in this paper, we disassemble the structures presented so far,\\nidentify their underlying design primitives, and assign them to appropriate\\ndesign goals regarding PMem. As a result of our comprehensive experiments on\\nreal PM hardware, we can reveal the trade-offs of the primitives for various\\naccess patterns. This allowed us to pinpoint their best use cases as well as\\nvulnerabilities. Besides our general insights regarding PMem-based data\\nstructure design, we also discovered new combinations not examined in the\\nliterature so far.', comment='13 pages, 14 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS', 'cs.ET'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.02172v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.02172v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.02562v1', updated=datetime.datetime(2020, 1, 8, 15, 15, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 8, 15, 15, 32, tzinfo=datetime.timezone.utc), title='Extracting Multiple Viewpoint Models from Relational Databases', authors=[arxiv.Result.Author('Alessandro Berti'), arxiv.Result.Author('Wil van der Aalst')], summary='Much time in process mining projects is spent on finding and understanding\\ndata sources and extracting the event data needed. As a result, only a fraction\\nof time is spent actually applying techniques to discover, control and predict\\nthe business process. Moreover, current process mining techniques assume a\\nsingle case notion. However, in reallife processes often different case notions\\nare intertwined. For example, events of the same order handling process may\\nrefer to customers, orders, order lines, deliveries, and payments. Therefore,\\nwe propose to use Multiple Viewpoint (MVP) models that relate events through\\nobjects and that relate activities through classes. The required event data are\\nmuch closer to existing relational databases. MVP models provide a holistic\\nview on the process, but also allow for the extraction of classical event logs\\nusing different viewpoints. This way existing process mining techniques can be\\nused for each viewpoint without the need for new data extractions and\\ntransformations. We provide a toolchain allowing for the discovery of MVP\\nmodels (annotated with performance and frequency information) from relational\\ndatabases. Moreover, we demonstrate that classical process mining techniques\\ncan be applied to any selected viewpoint.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.02562v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.02562v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.03284v1', updated=datetime.datetime(2020, 1, 10, 2, 25, 58, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 10, 2, 25, 58, tzinfo=datetime.timezone.utc), title='GeoCMS : Towards a Geo-Tagged Media Management System', authors=[arxiv.Result.Author('Jang You Park'), arxiv.Result.Author('YongHee Jung'), arxiv.Result.Author('Wei Ding'), arxiv.Result.Author('Kwang Woo Nam')], summary=\"In this paper, we propose the design and implementation of the new geotagged\\nmedia management system. A large amount of daily geo-tagged media data\\ngenerated by user's smart phone, mobile device, dash cam and camera. Geotagged\\nmedia, such as geovideos and geophotos, can be captured with spatial temporal\\ninformation such as time, location, visible area, camera direction, moving\\ndirection and visible distance information. Due to the increase in geo-tagged\\nmultimedia data, the researches for efficient managing and mining geo-tagged\\nmultimedia are newly expected to be a new area in database and data mining.\\nThis paper proposes a geo-tagged media management system, so called Open\\nGeoCMS(Geotagged media Contents Management System). Open GeoCMS is a new\\nframework to manage geotagged media data on the web. Our framework supports\\nvarious types which are for moving point, moving photo - a sequence of photos\\nby a drone, moving double and moving video. Also, GeoCMS has the label viewer\\nand editor system for photos and videos. The Open GeoCMS have been developed as\\nan open source system.\", comment=None, journal_ref='Proceedings of FOSS4G 2019 Conference, Bucharest', doi='10.5194/isprs-archives-XLII-4-W14-185-2019', primary_category='cs.DB', categories=['cs.DB', 'cs.SI'], links=[arxiv.Result.Link('http://dx.doi.org/10.5194/isprs-archives-XLII-4-W14-185-2019', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2001.03284v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.03284v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.04910v1', updated=datetime.datetime(2020, 1, 14, 17, 9, 34, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 14, 17, 9, 34, tzinfo=datetime.timezone.utc), title='A Case Study on Visualizing Large Spatial Datasets in a Web-based Map Viewer', authors=[arxiv.Result.Author('Alejandro Cortiñas'), arxiv.Result.Author('Miguel R. Luaces'), arxiv.Result.Author('Tirso V. Rodeiro')], summary='Lately, many companies are using Mobile Workforce Management technologies\\ncombined with information collected by sensors from mobile devices in order to\\nimprove their business processes. Even for small companies, the information\\nthat needs to be handled grows at a high rate, and most of the data collected\\nhave a geographic dimension. Being able to visualize this data in real-time\\nwithin a map viewer is a very important deal for these companies. In this paper\\nwe focus on this topic, presenting a case study on visualizing large spatial\\ndatasets. Particularly, since most of the Mobile Workforce Management software\\nis web-based, we propose a solution suitable for this environment.', comment=\"This research has received funding from the European Union's Horizon\\n  2020 research and innovation programme under the Marie Sk{\\\\l}odowska-Curie\\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941\", journal_ref='Proc. of the 18th International Conference On Web Engineering\\n  (ICWE 2018), Springer International Publishing, Caceres (Spain), 2018, pp.\\n  296-303', doi='10.1007/978-3-319-91662-0_23', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-319-91662-0_23', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2001.04910v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.04910v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.05157v1', updated=datetime.datetime(2020, 1, 15, 7, 21, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 15, 7, 21, 11, tzinfo=datetime.timezone.utc), title='An Efficient and Wear-Leveling-Aware Frequent-Pattern Mining on Non-Volatile Memory', authors=[arxiv.Result.Author('Jiaqi Dong'), arxiv.Result.Author('Runyu Zhang'), arxiv.Result.Author('Chaoshu Yang'), arxiv.Result.Author('Yujuan Tan'), arxiv.Result.Author('Duo Liu')], summary='Frequent-pattern mining is a common approach to reveal the valuable hidden\\ntrends behind data. However, existing frequent-pattern mining algorithms are\\ndesigned for DRAM, instead of persistent memories (PMs), which can lead to\\nsevere performance and energy overhead due to the utterly different\\ncharacteristics between DRAM and PMs when they are running on PMs. In this\\npaper, we propose an efficient and Wear-leveling-aware Frequent-Pattern Mining\\nscheme, WFPM, to solve this problem. The proposed WFPM is evaluated by a series\\nof experiments based on realistic datasets from diversified application\\nscenarios, where WFPM achieves 32.0% performance improvement and prolongs the\\nNVM lifetime of header table by 7.4x over the EvFP-Tree.', comment=None, journal_ref=None, doi='10.1088/1742-6596/1570/1/012087', primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://dx.doi.org/10.1088/1742-6596/1570/1/012087', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2001.05157v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.05157v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.05581v1', updated=datetime.datetime(2020, 1, 15, 22, 24, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 15, 22, 24, 40, tzinfo=datetime.timezone.utc), title='Complete and Sufficient Spatial Domination of Multidimensional Rectangles', authors=[arxiv.Result.Author('Tobias Emrich'), arxiv.Result.Author('Hans-Peter Kriegel'), arxiv.Result.Author('Andreas Züfle'), arxiv.Result.Author('Peer Kröger'), arxiv.Result.Author('Matthias Renz')], summary='Rectangles are used to approximate objects, or sets of objects, in a plethora\\nof applications, systems and index structures. Many tasks, such as nearest\\nneighbor search and similarity ranking, require to decide if objects in one\\nrectangle A may, must, or must not be closer to objects in a second rectangle\\nB, than objects in a third rectangle R. To decide this relation of \"Spatial\\nDomination\" it can be shown that using minimum and maximum distances it is\\noften impossible to detect spatial domination. This spatial gem provides a\\nnecessary and sufficient decision criterion for spatial domination that can be\\ncomputed efficiently even in higher dimensional space. In addition, this\\nspatial gem provides an example, pseudocode and an implementation in Python.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.05581v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.05581v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.05722v2', updated=datetime.datetime(2020, 7, 28, 12, 28, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 16, 10, 7, 25, tzinfo=datetime.timezone.utc), title='Query Results over Ongoing Databases that Remain Valid as Time Passes By (Extended Version)', authors=[arxiv.Result.Author('Yvonne Mülle'), arxiv.Result.Author('Michael H. Böhlen')], summary='Ongoing time point now is used to state that a tuple is valid from the start\\npoint onward. For database systems ongoing time points have far-reaching\\nimplications since they change continuously as time passes by. State-of-the-art\\napproaches deal with ongoing time points by instantiating them to the reference\\ntime. The instantiation yields query results that are only valid at the chosen\\ntime and get invalidated as time passes by. We propose a solution that keeps\\nongoing time points uninstantiated during query processing. We do so by\\nevaluating predicates and functions at all possible reference times. This\\nrenders query results independent of a specific reference time and yields\\nresults that remain valid as time passes by. As query results, we propose\\nongoing relations that include a reference time attribute. The value of the\\nreference time attribute is restricted by predicates and functions on ongoing\\nattributes. We describe and evaluate an efficient implementation of ongoing\\ndata types and operations in PostgreSQL.', comment='Extended version of ICDE paper', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.05722v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.05722v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.06358v3', updated=datetime.datetime(2022, 2, 16, 11, 7, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 17, 15, 2, 21, tzinfo=datetime.timezone.utc), title='Generative Datalog with Continuous Distributions', authors=[arxiv.Result.Author('Martin Grohe'), arxiv.Result.Author('Benjamin Lucien Kaminski'), arxiv.Result.Author('Joost-Pieter Katoen'), arxiv.Result.Author('Peter Lindner')], summary='Arguing for the need to combine declarative and probabilistic programming,\\nB\\\\\\'ar\\\\\\'any et al. (TODS 2017) recently introduced a probabilistic extension of\\nDatalog as a \"purely declarative probabilistic programming language.\" We\\nrevisit this language and propose a more principled approach towards defining\\nits semantics based on stochastic kernels and Markov processes - standard\\nnotions from probability theory. This allows us to extend the semantics to\\ncontinuous probability distributions, thereby settling an open problem posed by\\nB\\\\\\'ar\\\\\\'any et al.\\n  We show that our semantics is fairly robust, allowing both parallel execution\\nand arbitrary chase orders when evaluating a program. We cast our semantics in\\nthe framework of infinite probabilistic databases (Grohe and Lindner, ICDT\\n2020), and show that the semantics remains meaningful even when the input of a\\nprobabilistic Datalog program is an arbitrary probabilistic database.', comment='Extended Version', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LO'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.06358v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.06358v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.06543v1', updated=datetime.datetime(2020, 1, 17, 21, 51, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 17, 21, 51, 55, tzinfo=datetime.timezone.utc), title='Siamese Graph Neural Networks for Data Integration', authors=[arxiv.Result.Author('Evgeny Krivosheev'), arxiv.Result.Author('Mattia Atzeni'), arxiv.Result.Author('Katsiaryna Mirylenka'), arxiv.Result.Author('Paolo Scotton'), arxiv.Result.Author('Fabio Casati')], summary='Data integration has been studied extensively for decades and approached from\\ndifferent angles. However, this domain still remains largely rule-driven and\\nlacks universal automation. Recent development in machine learning and in\\nparticular deep learning has opened the way to more general and more efficient\\nsolutions to data integration problems. In this work, we propose a general\\napproach to modeling and integrating entities from structured data, such as\\nrelational databases, as well as unstructured sources, such as free text from\\nnews articles. Our approach is designed to explicitly model and leverage\\nrelations between entities, thereby using all available information and\\npreserving as much context as possible. This is achieved by combining siamese\\nand graph neural networks to propagate information between connected entities\\nand support high scalability. We evaluate our method on the task of integrating\\ndata about business entities, and we demonstrate that it outperforms standard\\nrule-based systems, as well as other deep learning approaches that do not use\\ngraph-based representations.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.06543v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.06543v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.06630v2', updated=datetime.datetime(2020, 1, 22, 6, 56, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 18, 9, 9, 34, tzinfo=datetime.timezone.utc), title='RCELF: A Residual-based Approach for Influence Maximization Problem', authors=[arxiv.Result.Author('Xinxun Zeng'), arxiv.Result.Author('Shiqi Zhang'), arxiv.Result.Author('Bo Tang')], summary='Influence Maximization Problem (IMP) is selecting a seed set of nodes in the\\nsocial network to spread the influence as widely as possible. It has many\\napplications in multiple domains, e.g., viral marketing is frequently used for\\nnew products or activities advertisements. While it is a classic and\\nwell-studied problem in computer science, unfortunately, all those proposed\\ntechniques are compromising among time efficiency, memory consumption, and\\nresult quality. In this paper, we conduct comprehensive experimental studies on\\nthe state-of-the-art IMP approximate approaches to reveal the underlying\\ntrade-off strategies. Interestingly, we find that even the state-of-the-art\\napproaches are impractical when the propagation probability of the network have\\nbeen taken into consideration. With the findings of existing approaches, we\\npropose a novel residual-based approach (i.e., RCELF) for IMP, which i)\\novercomes the deficiencies of existing approximate approaches, and ii) provides\\ntheoretical guaranteed results with high efficiency in both time- and space-\\nperspectives. We demonstrate the superiority of our proposal by extensive\\nexperimental evaluation on real datasets.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS', 'cs.SI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.06630v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.06630v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.06731v1', updated=datetime.datetime(2020, 1, 18, 22, 11, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 18, 22, 11, 23, tzinfo=datetime.timezone.utc), title='AI Data Wrangling with Associative Arrays', authors=[arxiv.Result.Author('Jeremy Kepner'), arxiv.Result.Author('Vijay Gadepally'), arxiv.Result.Author('Hayden Jananthan'), arxiv.Result.Author('Lauren Milechin'), arxiv.Result.Author('Siddharth Samsi')], summary='The AI revolution is data driven. AI \"data wrangling\" is the process by which\\nunusable data is transformed to support AI algorithm development (training) and\\ndeployment (inference). Significant time is devoted to translating diverse data\\nrepresentations supporting the many query and analysis steps found in an AI\\npipeline. Rigorous mathematical representations of these data enables data\\ntranslation and analysis optimization within and across steps. Associative\\narray algebra provides a mathematical foundation that naturally describes the\\ntabular structures and set mathematics that are the basis of databases.\\nLikewise, the matrix operations and corresponding inference/training\\ncalculations used by neural networks are also well described by associative\\narrays. More surprisingly, a general denormalized form of hierarchical formats,\\nsuch as XML and JSON, can be readily constructed. Finally, pivot tables, which\\nare among the most widely used data analysis tools, naturally emerge from\\nassociative array constructors. A common foundation in associative arrays\\nprovides interoperability guarantees, proving that their operations are linear\\nsystems with rigorous mathematical properties, such as, associativity,\\ncommutativity, and distributivity that are critical to reordering\\noptimizations.', comment='3 pages, 2 figures, 23 references, accepted for Northeast Database\\n  day (NEDB) 2020. arXiv admin note: text overlap with arXiv:1907.04217', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.06731v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.06731v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.06933v1', updated=datetime.datetime(2020, 1, 20, 1, 20, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 20, 1, 20, 55, tzinfo=datetime.timezone.utc), title='Fides: Managing Data on Untrusted Infrastructure', authors=[arxiv.Result.Author('Sujaya Maiyya'), arxiv.Result.Author('Danny Hyun Bum Cho'), arxiv.Result.Author('Divyakant Agrawal'), arxiv.Result.Author('Amr El Abbadi')], summary='Significant amounts of data are currently being stored and managed on\\nthird-party servers. It is impractical for many small scale enterprises to own\\ntheir private datacenters, hence renting third-party servers is a viable\\nsolution for such businesses. But the increasing number of malicious attacks,\\nboth internal and external, as well as buggy software on third-party servers is\\ncausing clients to lose their trust in these external infrastructures. While\\nsmall enterprises cannot avoid using external infrastructures, they need the\\nright set of protocols to manage their data on untrusted infrastructures. In\\nthis paper, we propose TFCommit, a novel atomic commitment protocol that\\nexecutes transactions on data stored across multiple untrusted servers. To our\\nknowledge, TFCommit is the first atomic commitment protocol to execute\\ntransactions in an untrusted environment without using expensive Byzantine\\nreplication. Using TFCommit, we propose an auditable data management system,\\nFides, residing completely on untrustworthy infrastructure. As an auditable\\nsystem, Fides guarantees the detection of potentially malicious failures\\noccurring on untrusted servers using tamper-resistant logs with the support of\\ncryptographic techniques. The experimental evaluation demonstrates the\\nscalability and the relatively low overhead of our approach that allows\\nexecuting transactions on untrusted infrastructure.', comment='14pages, 15 figures/graphs', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.06933v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.06933v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.07906v1', updated=datetime.datetime(2020, 1, 22, 8, 11, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 22, 8, 11, 6, tzinfo=datetime.timezone.utc), title='Graph Generators: State of the Art and Open Challenges', authors=[arxiv.Result.Author('Angela Bonifati'), arxiv.Result.Author('Irena Holubová'), arxiv.Result.Author('Arnau Prat-Pérez'), arxiv.Result.Author('Sherif Sakr')], summary='The abundance of interconnected data has fueled the design and implementation\\nof graph generators reproducing real-world linking properties, or gauging the\\neffectiveness of graph algorithms, techniques and applications manipulating\\nthese data. We consider graph generation across multiple subfields, such as\\nSemantic Web, graph databases, social networks, and community detection, along\\nwith general graphs. Despite the disparate requirements of modern graph\\ngenerators throughout these communities, we analyze them under a common\\numbrella, reaching out the functionalities, the practical usage, and their\\nsupported operations. We argue that this classification is serving the need of\\nproviding scientists, researchers and practitioners with the right data\\ngenerator at hand for their work. This survey provides a comprehensive overview\\nof the state-of-the-art graph generators by focusing on those that are\\npertinent and suitable for several data-intensive tasks. Finally, we discuss\\nopen challenges and missing requirements of current graph generators along with\\ntheir future extensions to new emerging fields.', comment='ACM Computing Surveys, 32 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR', 'cs.SI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.07906v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.07906v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.08392v1', updated=datetime.datetime(2020, 1, 23, 7, 33, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 23, 7, 33, 2, tzinfo=datetime.timezone.utc), title='Towards context in large scale biomedical knowledge graphs', authors=[arxiv.Result.Author('Jens Dörpinghaus'), arxiv.Result.Author('Andreas Stefan'), arxiv.Result.Author('Bruce Schultz'), arxiv.Result.Author('Marc Jacobs')], summary='Contextual information is widely considered for NLP and knowledge discovery\\nin life sciences since it highly influences the exact meaning of natural\\nlanguage. The scientific challenge is not only to extract such context data,\\nbut also to store this data for further query and discovery approaches. Here,\\nwe propose a multiple step knowledge graph approach using labeled property\\ngraphs based on polyglot persistence systems to utilize context data for\\ncontext mining, graph queries, knowledge discovery and extraction. We introduce\\nthe graph-theoretic foundation for a general context concept within semantic\\nnetworks and show a proof-of-concept based on biomedical literature and text\\nmining. Our test system contains a knowledge graph derived from the entirety of\\nPubMed and SCAIView data and is enriched with text mining data and domain\\nspecific language data using BEL. Here, context is a more general concept than\\nannotations. This dense graph has more than 71M nodes and 850M relationships.\\nWe discuss the impact of this novel approach with 27 real world use cases\\nrepresented by graph queries.', comment='26 pages, 14 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.08392v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.08392v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.09078v1', updated=datetime.datetime(2020, 1, 24, 16, 33, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 24, 16, 33, 35, tzinfo=datetime.timezone.utc), title='Adaptive Low-level Storage of Very Large Knowledge Graphs', authors=[arxiv.Result.Author('Jacopo Urbani'), arxiv.Result.Author('Ceriel Jacobs')], summary='The increasing availability and usage of Knowledge Graphs (KGs) on the Web\\ncalls for scalable and general-purpose solutions to store this type of data\\nstructures. We propose Trident, a novel storage architecture for very large KGs\\non centralized systems. Trident uses several interlinked data structures to\\nprovide fast access to nodes and edges, with the physical storage changing\\ndepending on the topology of the graph to reduce the memory footprint. In\\ncontrast to single architectures designed for single tasks, our approach offers\\nan interface with few low-level and general-purpose primitives that can be used\\nto implement tasks like SPARQL query answering, reasoning, or graph analytics.\\nOur experiments show that Trident can handle graphs with 10^11 edges using\\ninexpensive hardware, delivering competitive performance on multiple workloads.', comment='Accepted WWW 2020', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.09078v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.09078v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.10301v2', updated=datetime.datetime(2020, 2, 19, 6, 32, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 28, 13, 3, 58, tzinfo=datetime.timezone.utc), title='Estimating Descriptors for Large Graphs', authors=[arxiv.Result.Author('Zohair Raza Hassan'), arxiv.Result.Author('Mudassir Shabbir'), arxiv.Result.Author('Imdadullah Khan'), arxiv.Result.Author('Waseem Abbas')], summary='Embedding networks into a fixed dimensional feature space, while preserving\\nits essential structural properties is a fundamental task in graph analytics.\\nThese feature vectors (graph descriptors) are used to measure the pairwise\\nsimilarity between graphs. This enables applying data mining algorithms (e.g\\nclassification, clustering, or anomaly detection) on graph-structured data\\nwhich have numerous applications in multiple domains. State-of-the-art\\nalgorithms for computing descriptors require the entire graph to be in memory,\\nentailing a huge memory footprint, and thus do not scale well to increasing\\nsizes of real-world networks. In this work, we propose streaming algorithms to\\nefficiently approximate descriptors by estimating counts of sub-graphs of order\\n$k\\\\leq 4$, and thereby devise extensions of two existing graph comparison\\nparadigms: the Graphlet Kernel and NetSimile. Our algorithms require a single\\nscan over the edge stream, have space complexity that is a fraction of the\\ninput size, and approximate embeddings via a simple sampling scheme. Our design\\nexploits the trade-off between available memory and estimation accuracy to\\nprovide a method that works well for limited memory requirements. We perform\\nextensive experiments on real-world networks and demonstrate that our\\nalgorithms scale well to massive graphs.', comment='Accepted to PAKDD 2020', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.10301v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.10301v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.10719v1', updated=datetime.datetime(2020, 1, 29, 8, 23, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 29, 8, 23, 13, tzinfo=datetime.timezone.utc), title='Query-Sequence Optimization on a Reconfigurable Hardware-Accelerated System', authors=[arxiv.Result.Author('Lekshmi B. G.'), arxiv.Result.Author('Andreas Becher'), arxiv.Result.Author('Klaus Meyer-Wegener')], summary='Hardware acceleration of database query processing can be done with the help\\nof FPGAs. In particular, they are partially reconfigurable during runtime,\\nwhich allows for the runtime adaption of the hardware to a variety of queries.\\nReconfiguration itself, however, takes some time. As the affected area of the\\nFPGA is not available for computations during the reconfiguration, avoiding\\nsome of the reconfigurations can improve overall performance. This paper\\npresents optimizations based on query sequences, which reduces the impact of\\nthe reconfigurations. Knowledge of coming queries is used to (I) speculatively\\nstart reconfiguration already when a query is still running and (II) avoid\\noverwriting of reconfigurable regions that will be used again in subsequent\\nqueries. We evaluate our optimizations with a calibrated model and measurements\\nfor various parameter values. Improvements in execution time of up to 21% can\\nbe obtained even with sequences of only two queries', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.10719v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.10719v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.11100v1', updated=datetime.datetime(2020, 1, 29, 21, 30, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 29, 21, 30, 14, tzinfo=datetime.timezone.utc), title='A Scalable Framework for Quality Assessment of RDF Datasets', authors=[arxiv.Result.Author('Gezim Sejdiu'), arxiv.Result.Author('Anisa Rula'), arxiv.Result.Author('Jens Lehmann'), arxiv.Result.Author('Hajira Jabeen')], summary='Over the last years, Linked Data has grown continuously. Today, we count more\\nthan 10,000 datasets being available online following Linked Data standards.\\nThese standards allow data to be machine readable and inter-operable.\\nNevertheless, many applications, such as data integration, search, and\\ninterlinking, cannot take full advantage of Linked Data if it is of low\\nquality. There exist a few approaches for the quality assessment of Linked\\nData, but their performance degrades with the increase in data size and quickly\\ngrows beyond the capabilities of a single machine. In this paper, we present\\nDistQualityAssessment -- an open source implementation of quality assessment of\\nlarge RDF datasets that can scale out to a cluster of machines. This is the\\nfirst distributed, in-memory approach for computing different quality metrics\\nfor large RDF datasets using Apache Spark. We also provide a quality assessment\\npattern that can be used to generate new scalable metrics that can be applied\\nto big data. The work presented here is integrated with the SANSA framework and\\nhas been applied to at least three use cases beyond the SANSA community. The\\nresults show that our approach is more generic, efficient, and scalable as\\ncompared to previously proposed approaches.', comment='International Semantic Web Conference (ISWC 2019)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.PF'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.11100v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.11100v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.02862v1', updated=datetime.datetime(2020, 5, 30, 18, 28, 49, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 30, 18, 28, 49, tzinfo=datetime.timezone.utc), title='A Novel Approach for Generating SPARQL Queries from RDF Graphs', authors=[arxiv.Result.Author('Emna Jabri')], summary=\"This work is done as part of a research master's thesis project. The goal is\\nto generate SPARQL queries based on user-supplied keywords to query RDF graphs.\\nTo do this, we first transformed the input ontology into an RDF graph that\\nreflects the semantics represented in the ontology. Subsequently, we stored\\nthis RDF graph in the Neo4j graphical database to ensure efficient and\\npersistent management of RDF data. At the time of the interrogation, we studied\\nthe different possible and desired interpretations of the request originally\\nmade by the user. We have also proposed to carry out a sort of transformation\\nbetween the two query languages SPARQL and Cypher, which is specific to Neo4j.\\nThis allows us to implement the architecture of our system over a wide variety\\nof BD-RDFs providing their query languages, without changing any of the other\\ncomponents of the system. Finally, we tested and evaluated our tool using\\ndifferent test bases, and it turned out that our tool is comprehensive,\\neffective, and powerful enough.\", comment='in French. arXiv admin note: substantial text overlap with\\n  arXiv:1810.02869 by other authors', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2006.02862v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.02862v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.02958v3', updated=datetime.datetime(2021, 2, 26, 21, 6, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 4, 15, 40, 42, tzinfo=datetime.timezone.utc), title='TASM: A Tile-Based Storage Manager for Video Analytics', authors=[arxiv.Result.Author('Maureen Daum'), arxiv.Result.Author('Brandon Haynes'), arxiv.Result.Author('Dong He'), arxiv.Result.Author('Amrita Mazumdar'), arxiv.Result.Author('Magdalena Balazinska')], summary='Modern video data management systems store videos as a single encoded file,\\nwhich significantly limits possible storage level optimizations. We design,\\nimplement, and evaluate TASM, a new tile-based storage manager for video data.\\nTASM uses a feature in modern video codecs called \"tiles\" that enables spatial\\nrandom access into encoded videos. TASM physically tunes stored videos by\\noptimizing their tile layouts given the video content and a query workload.\\nAdditionally, TASM dynamically tunes that layout in response to changes in the\\nquery workload or if the query workload and video contents are incrementally\\ndiscovered. Finally, TASM also produces efficient initial tile layouts for\\nnewly ingested videos. We demonstrate that TASM can speed up subframe selection\\nqueries by an average of over 50% and up to 94%. TASM can also improve the\\nthroughput of the full scan phase of object detection queries by up to 2X.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2006.02958v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.02958v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.03198v3', updated=datetime.datetime(2022, 2, 22, 8, 3, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 5, 1, 55, 22, tzinfo=datetime.timezone.utc), title='Efficient Semi-External Depth-First Search', authors=[arxiv.Result.Author('Xiaolong Wan'), arxiv.Result.Author('Hongzhi Wang')], summary='As the sizes of graphs grow rapidly, currently many real-world graphs can\\nhardly be loaded in the main memory. It becomes a hot topic to compute\\ndepth-first search (DFS) results, i.e., depth-first order or DFS-Tree, on\\nsemi-external memory model. Semi-external algorithms assume the main memory\\ncould at least hold a spanning tree T of a graph G, and gradually restructure T\\ninto a DFS-Tree, which is non-trivial. In this paper, we present a\\ncomprehensive study of semi-external DFS problem. Based on our theoretical\\nanalysis of its main challenge, we introduce a new semi-external DFS algorithm,\\nnamed EP-DFS, with a lightweight index N+-index. Unlike traditional algorithms,\\nwe focus on addressing such complex problem efficiently not only with less\\nI/Os, but also with simpler CPU calculation (implementation-friendly) and less\\nrandom I/O accesses (key-to-efficiency). Extensive experimental evaluation is\\nconducted on both synthetic and real graphs. The experimental results confirm\\nthat our EP-DFS algorithm significantly outperforms existing algorithms.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://arxiv.org/abs/2006.03198v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.03198v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.04277v1', updated=datetime.datetime(2020, 6, 7, 21, 56, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 7, 21, 56, 29, tzinfo=datetime.timezone.utc), title='J-Logic: a Logic for Querying JSON', authors=[arxiv.Result.Author('Jan Hidders'), arxiv.Result.Author('Jan Paredaens'), arxiv.Result.Author('Jan Van den Bussche')], summary=\"We propose a logical framework, based on Datalog, to study the foundations of\\nquerying JSON data. The main feature of our approach, which we call J-Logic, is\\nthe emphasis on paths. Paths are sequences of keys and are used to access the\\ntree structure of nested JSON objects. J-Logic also features ``packing'' as a\\nmeans to generate a new key from a path or subpath. J-Logic with recursion is\\ncomputationally complete, but many queries can be expressed without recursion,\\nsuch as deep equality. We give a necessary condition for queries to be\\nexpressible without recursion. Most of our results focus on the deterministic\\nnature of JSON objects as partial functions from keys to values. Predicates\\ndefined by J-Logic programs may not properly describe objects, however.\\nNevertheless we show that every object-to-object transformation in J-Logic can\\nbe defined using only objects in intermediate results. Moreover we show that it\\nis decidable whether a positive, nonrecursive J-Logic program always returns an\\nobject when given objects as inputs. Regarding packing, we show that packing is\\nunnecessary if the output does not require new keys. Finally, we show the\\ndecidability of query containment for positive, nonrecursive J-Logic programs.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2006.04277v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.04277v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.07712v2', updated=datetime.datetime(2020, 11, 5, 14, 46, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 13, 20, 32, 7, tzinfo=datetime.timezone.utc), title='An Ontology for the Materials Design Domain', authors=[arxiv.Result.Author('Huanyu Li'), arxiv.Result.Author('Rickard Armiento'), arxiv.Result.Author('Patrick Lambrix')], summary='In the materials design domain, much of the data from materials calculations\\nare stored in different heterogeneous databases. Materials databases usually\\nhave different data models. Therefore, the users have to face the challenges to\\nfind the data from adequate sources and integrate data from multiple sources.\\nOntologies and ontology-based techniques can address such problems as the\\nformal representation of domain knowledge can make data more available and\\ninteroperable among different systems. In this paper, we introduce the\\nMaterials Design Ontology (MDO), which defines concepts and relations to cover\\nknowledge in the field of materials design. MDO is designed using domain\\nknowledge in materials science (especially in solid-state physics), and is\\nguided by the data from several databases in the materials design field. We\\nshow the application of the MDO to materials data retrieved from well-known\\nmaterials databases.', comment='16 pages', journal_ref=None, doi='10.1007/978-3-030-62466-8_14', primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-030-62466-8_14', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2006.07712v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.07712v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.07916v1', updated=datetime.datetime(2020, 6, 14, 14, 48, 37, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 14, 14, 48, 37, tzinfo=datetime.timezone.utc), title='Categorical anomaly detection in heterogeneous data using minimum description length clustering', authors=[arxiv.Result.Author('James Cheney'), arxiv.Result.Author('Xavier Gombau'), arxiv.Result.Author('Ghita Berrada'), arxiv.Result.Author('Sidahmed Benabderrahmane')], summary='Fast and effective unsupervised anomaly detection algorithms have been\\nproposed for categorical data based on the minimum description length (MDL)\\nprinciple. However, they can be ineffective when detecting anomalies in\\nheterogeneous datasets representing a mixture of different sources, such as\\nsecurity scenarios in which system and user processes have distinct behavior\\npatterns. We propose a meta-algorithm for enhancing any MDL-based anomaly\\ndetection model to deal with heterogeneous data by fitting a mixture model to\\nthe data, via a variant of k-means clustering. Our experimental results show\\nthat using a discrete mixture model provides competitive performance relative\\nto two previous anomaly detection algorithms, while mixtures of more\\nsophisticated models yield further gains, on both synthetic datasets and\\nrealistic datasets from a security scenario.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2006.07916v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.07916v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.08109v2', updated=datetime.datetime(2020, 11, 2, 19, 15, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 15, 3, 21, 46, tzinfo=datetime.timezone.utc), title='NeuroCard: One Cardinality Estimator for All Tables', authors=[arxiv.Result.Author('Zongheng Yang'), arxiv.Result.Author('Amog Kamsetty'), arxiv.Result.Author('Sifei Luan'), arxiv.Result.Author('Eric Liang'), arxiv.Result.Author('Yan Duan'), arxiv.Result.Author('Xi Chen'), arxiv.Result.Author('Ion Stoica')], summary='Query optimizers rely on accurate cardinality estimates to produce good\\nexecution plans. Despite decades of research, existing cardinality estimators\\nare inaccurate for complex queries, due to making lossy modeling assumptions\\nand not capturing inter-table correlations. In this work, we show that it is\\npossible to learn the correlations across all tables in a database without any\\nindependence assumptions. We present NeuroCard, a join cardinality estimator\\nthat builds a single neural density estimator over an entire database.\\nLeveraging join sampling and modern deep autoregressive models, NeuroCard makes\\nno inter-table or inter-column independence assumptions in its probabilistic\\nmodeling. NeuroCard achieves orders of magnitude higher accuracy than the best\\nprior methods (a new state-of-the-art result of 8.5$\\\\times$ maximum error on\\nJOB-light), scales to dozens of tables, while being compact in space (several\\nMBs) and efficient to construct or update (seconds to minutes).', comment='VLDB 2021', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2006.08109v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.08109v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.10949v1', updated=datetime.datetime(2020, 6, 19, 4, 8, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 19, 4, 8, 8, tzinfo=datetime.timezone.utc), title='Sorting-based Interactive Regret Minimization', authors=[arxiv.Result.Author('Jiping Zheng'), arxiv.Result.Author('Chen Chen')], summary='As an important tool for multi-criteria decision making in database systems,\\nthe regret minimization query is shown to have the merits of top-k and skyline\\nqueries: it controls the output size while does not need users to provide any\\npreferences. Existing researches verify that the regret ratio can be much\\ndecreased when interaction is available. In this paper, we study how to enhance\\ncurrent interactive regret minimization query by sorting mechanism. Instead of\\nselecting the most favorite point from the displayed points for each\\ninteraction round, users sort the displayed data points and send the results to\\nthe system. By introducing sorting mechanism, for each round of interaction the\\nutility space explored will be shrunk to some extent. Further the candidate\\npoints selection for following rounds of interaction will be narrowed to\\nsmaller data spaces thus the number of interaction rounds will be reduced. We\\npropose two effective sorting-based algorithms namely Sorting-Simplex and\\nSorting-Random to find the maximum utility point based on Simplex method and\\nrandomly selection strategy respectively. Experiments on synthetic and real\\ndatasets verify our Sorting-Simplex and Sorting-Random algorithms outperform\\ncurrent state-of-art ones.', comment='15 pages, accepted for publication of the 4th APWeb-WAIM joint\\n  international conference on Web and Big Data (APWebWAIM 2020)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2'], links=[arxiv.Result.Link('http://arxiv.org/abs/2006.10949v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.10949v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.11284v1', updated=datetime.datetime(2020, 6, 19, 17, 46, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 19, 17, 46, 30, tzinfo=datetime.timezone.utc), title='Improving Locality Sensitive Hashing by Efficiently Finding Projected Nearest Neighbors', authors=[arxiv.Result.Author('Omid Jafari'), arxiv.Result.Author('Parth Nagarkar'), arxiv.Result.Author('Jonathan Montaño')], summary='Similarity search in high-dimensional spaces is an important task for many\\nmultimedia applications. Due to the notorious curse of dimensionality,\\napproximate nearest neighbor techniques are preferred over exact searching\\ntechniques since they can return good enough results at a much better speed.\\nLocality Sensitive Hashing (LSH) is a very popular random hashing technique for\\nfinding approximate nearest neighbors. Existing state-of-the-art Locality\\nSensitive Hashing techniques that focus on improving performance of the overall\\nprocess, mainly focus on minimizing the total number of IOs while sacrificing\\nthe overall processing time. The main time-consuming process in LSH techniques\\nis the process of finding neighboring points in projected spaces. We present a\\nnovel index structure called radius-optimized Locality Sensitive Hashing\\n(roLSH). With the help of sampling techniques and Neural Networks, we present\\ntwo techniques to find neighboring points in projected spaces efficiently,\\nwithout sacrificing the accuracy of the results. Our extensive experimental\\nanalysis on real datasets shows the performance benefit of roLSH over existing\\nstate-of-the-art LSH techniques.', comment='arXiv admin note: text overlap with arXiv:2003.06415', journal_ref='SISAP 2020. Lecture Notes in Computer Science, vol 12440.\\n  Springer, Cham', doi='10.1007/978-3-030-60936-8_25', primary_category='cs.DB', categories=['cs.DB', 'cs.MM', 'H.2.4'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-030-60936-8_25', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2006.11284v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.11284v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.11454v1', updated=datetime.datetime(2020, 6, 20, 1, 4, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 20, 1, 4, 27, tzinfo=datetime.timezone.utc), title='The Lernaean Hydra of Data Series Similarity Search: An Experimental Evaluation of the State of the Art', authors=[arxiv.Result.Author('Karima Echihabi'), arxiv.Result.Author('Kostas Zoumpatianos'), arxiv.Result.Author('Themis Palpanas'), arxiv.Result.Author('Houda Benbrahim')], summary='Increasingly large data series collections are becoming commonplace across\\nmany different domains and applications. A key operation in the analysis of\\ndata series collections is similarity search, which has attracted lots of\\nattention and effort over the past two decades. Even though several relevant\\napproaches have been proposed in the literature, none of the existing studies\\nprovides a detailed evaluation against the available alternatives. The lack of\\ncomparative results is further exacerbated by the non-standard use of\\nterminology, which has led to confusion and misconceptions. In this paper, we\\nprovide definitions for the different flavors of similarity search that have\\nbeen studied in the past, and present the first systematic experimental\\nevaluation of the efficiency of data series similarity search techniques. Based\\non the experimental results, we describe the strengths and weaknesses of each\\napproach and give recommendations for the best approach to use under typical\\nuse cases. Finally, by identifying the shortcomings of each method, our\\nfindings lay the ground for solid further developments in the field.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2006.11454v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.11454v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.11459v1', updated=datetime.datetime(2020, 6, 20, 1, 27, 49, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 20, 1, 27, 49, tzinfo=datetime.timezone.utc), title='Return of the Lernaean Hydra: Experimental Evaluation of Data Series Approximate Similarity Search', authors=[arxiv.Result.Author('Karima Echihabi'), arxiv.Result.Author('Kostas Zoumpatianos'), arxiv.Result.Author('Themis Palpanas'), arxiv.Result.Author('Houda Benbrahim')], summary='Data series are a special type of multidimensional data present in numerous\\ndomains, where similarity search is a key operation that has been extensively\\nstudied in the data series literature. In parallel, the multidimensional\\ncommunity has studied approximate similarity search techniques. We propose a\\ntaxonomy of similarity search techniques that reconciles the terminology used\\nin these two domains, we describe modifications to data series indexing\\ntechniques enabling them to answer approximate similarity queries with quality\\nguarantees, and we conduct a thorough experimental evaluation to compare\\napproximate similarity search techniques under a unified framework, on\\nsynthetic and real datasets in memory and on disk. Although data series differ\\nfrom generic multidimensional vectors (series usually exhibit correlation\\nbetween neighboring values), our results show that data series techniques\\nanswer approximate %similarity queries with strong guarantees and an excellent\\nempirical performance, on data series and vectors alike. These techniques\\noutperform the state-of-the-art approximate techniques for vectors when\\noperating on disk, and remain competitive in memory.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2006.11459v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.11459v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.12804v2', updated=datetime.datetime(2020, 6, 29, 15, 51, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 23, 7, 37, 59, tzinfo=datetime.timezone.utc), title='Benchmarking Learned Indexes', authors=[arxiv.Result.Author('Ryan Marcus'), arxiv.Result.Author('Andreas Kipf'), arxiv.Result.Author('Alexander van Renen'), arxiv.Result.Author('Mihail Stoian'), arxiv.Result.Author('Sanchit Misra'), arxiv.Result.Author('Alfons Kemper'), arxiv.Result.Author('Thomas Neumann'), arxiv.Result.Author('Tim Kraska')], summary='Recent advancements in learned index structures propose replacing existing\\nindex structures, like B-Trees, with approximate learned models. In this work,\\nwe present a unified benchmark that compares well-tuned implementations of\\nthree learned index structures against several state-of-the-art \"traditional\"\\nbaselines. Using four real-world datasets, we demonstrate that learned index\\nstructures can indeed outperform non-learned indexes in read-only in-memory\\nworkloads over a dense array. We also investigate the impact of caching,\\npipelining, dataset size, and key size. We study the performance profile of\\nlearned index structures, and build an explanation for why learned models\\nachieve such good performance. Finally, we investigate other important\\nproperties of learned index structures, such as their performance in\\nmulti-threaded systems and their build times.', comment=None, journal_ref=None, doi='10.14778/3421424.3421425', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3421424.3421425', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2006.12804v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.12804v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.12856v1', updated=datetime.datetime(2020, 6, 23, 9, 28, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 23, 9, 28, 40, tzinfo=datetime.timezone.utc), title='PRIPEL: Privacy-Preserving Event Log Publishing Including Contextual Information', authors=[arxiv.Result.Author('Stephan A. Fahrenkrog-Petersen'), arxiv.Result.Author('Han van der Aa'), arxiv.Result.Author('Matthias Weidlich')], summary=\"Event logs capture the execution of business processes in terms of executed\\nactivities and their execution context. Since logs contain potentially\\nsensitive information about the individuals involved in the process, they\\nshould be pre-processed before being published to preserve the individuals'\\nprivacy. However, existing techniques for such pre-processing are limited to a\\nprocess' control-flow and neglect contextual information, such as attribute\\nvalues and durations. This thus precludes any form of process analysis that\\ninvolves contextual factors. To bridge this gap, we introduce PRIPEL, a\\nframework for privacy-aware event log publishing. Compared to existing work,\\nPRIPEL takes a fundamentally different angle and ensures privacy on the level\\nof individual cases instead of the complete log. This way, contextual\\ninformation as well as the long tail process behaviour are preserved, which\\nenables the application of a rich set of process analysis techniques. We\\ndemonstrate the feasibility of our framework in a case study with a real-world\\nevent log.\", comment=None, journal_ref='International Conference on Business Process Management 2020', doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.SE'], links=[arxiv.Result.Link('http://arxiv.org/abs/2006.12856v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.12856v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.13079v1', updated=datetime.datetime(2020, 6, 20, 2, 25, 34, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 20, 2, 25, 34, tzinfo=datetime.timezone.utc), title='Coconut Palm: Static and Streaming Data Series Exploration Now in your Palm', authors=[arxiv.Result.Author('Haridimos Kondylakis'), arxiv.Result.Author('Niv Dayan'), arxiv.Result.Author('Kostas Zoumpatianos'), arxiv.Result.Author('Themis Palpanas')], summary='Many modern applications produce massive streams of data series and maintain\\nthem in indexes to be able to explore them through nearest neighbor search.\\nExisting data series indexes, however, are expensive to operate as they issue\\nmany random I/Os to storage. To address this problem, we recently proposed\\nCoconut, a new infrastructure that organizes data series based on a new\\nsortable format. In this way, Coconut is able to leverage state-of-the-art\\nindexing techniques that rely on sorting for the first time to build, maintain\\nand query data series indexes using fast sequential I/Os. In this\\ndemonstration, we present Coconut Palm, a new exploration tool that allows to\\ninteractively combine different indexing techniques from within the Coconut\\ninfrastructure and to thereby seamlessly explore data series from across\\nvarious scientific domains. We highlight the rich indexing design choices that\\nCoconut opens up, and we present a new recommender tool that allows users to\\nintelligently navigate them for both static and streaming data exploration\\nscenarios.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2006.13079v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.13079v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.13282v1', updated=datetime.datetime(2020, 6, 23, 19, 25, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 23, 19, 25, 51, tzinfo=datetime.timezone.utc), title='Tsunami: A Learned Multi-dimensional Index for Correlated Data and Skewed Workloads', authors=[arxiv.Result.Author('Jialin Ding'), arxiv.Result.Author('Vikram Nathan'), arxiv.Result.Author('Mohammad Alizadeh'), arxiv.Result.Author('Tim Kraska')], summary='Filtering data based on predicates is one of the most fundamental operations\\nfor any modern data warehouse. Techniques to accelerate the execution of filter\\nexpressions include clustered indexes, specialized sort orders (e.g., Z-order),\\nmulti-dimensional indexes, and, for high selectivity queries, secondary\\nindexes. However, these schemes are hard to tune and their performance is\\ninconsistent. Recent work on learned multi-dimensional indexes has introduced\\nthe idea of automatically optimizing an index for a particular dataset and\\nworkload. However, the performance of that work suffers in the presence of\\ncorrelated data and skewed query workloads, both of which are common in real\\napplications. In this paper, we introduce Tsunami, which addresses these\\nlimitations to achieve up to 6X faster query performance and up to 8X smaller\\nindex size than existing learned multi-dimensional indexes, in addition to up\\nto 11X faster query performance and 170X smaller index size than\\noptimally-tuned traditional indexes.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2006.13282v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.13282v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.14416v1', updated=datetime.datetime(2020, 6, 25, 13, 59, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 25, 13, 59, 26, tzinfo=datetime.timezone.utc), title='SPIDER: Selective Plotting of Interconnected Data and Entity Relations', authors=[arxiv.Result.Author('Pranav Addepalli'), arxiv.Result.Author('Eric Wu'), arxiv.Result.Author('Douglas Bossart'), arxiv.Result.Author('Christina Lin'), arxiv.Result.Author('Allistar Smith')], summary='Intelligence analysts have long struggled with an abundance of data that must\\nbe investigated on a daily basis. In the U.S. Army, this activity involves\\nreconciling information from various sources, a process that has been automated\\nto a certain extent, but which remains highly manual. To promote automation, a\\nsemantic analysis prototype was designed to aid in the intelligence analysis\\nprocess. This tool, called Selective Plotting of Interconnected Data and Entity\\nRelations (SPIDER), extracts entities and their relationships from text in\\norder to streamline investigations. SPIDER is a web application that can be\\nremotely-accessed via a web browser, and has three major components: (1) a Java\\nAPI that reads documents, extracts entities and relationships using Stanford\\nCoreNLP, (2) a Neo4j graph database that stores entities, relationships, and\\nproperties; (3) a JavaScript-based SigmaJS visualization tool for displaying\\nthe graph on the browser. SPIDER can scale document analysis to thousands of\\nfiles for quick visualization, making the intelligence analysis process more\\nefficient, and allowing military leadership quicker insights into a vast array\\nof potentially-hidden knowledge.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.SI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2006.14416v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.14416v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.16411v2', updated=datetime.datetime(2020, 8, 9, 19, 43, 38, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 29, 22, 5, 28, tzinfo=datetime.timezone.utc), title='Hands-off Model Integration in Spatial Index Structures', authors=[arxiv.Result.Author('Ali Hadian'), arxiv.Result.Author('Ankit Kumar'), arxiv.Result.Author('Thomas Heinis')], summary=\"Spatial indexes are crucial for the analysis of the increasing amounts of\\nspatial data, for example generated through IoT applications. The plethora of\\nindexes that has been developed in recent decades has primarily been optimised\\nfor disk. With increasing amounts of memory even on commodity machines,\\nhowever, moving them to main memory is an option. Doing so opens up the\\nopportunity to use additional optimizations that are only amenable to main\\nmemory. In this paper we thus explore the opportunity to use light-weight\\nmachine learning models to accelerate queries on spatial indexes. We do so by\\nexploring the potential of using interpolation and similar techniques on the\\nR-tree, arguably the most broadly used spatial index. As we show in our\\nexperimental analysis, the query execution time can be reduced by up to 60%\\nwhile simultaneously shrinking the index's memory footprint by over 90%\", comment='Proceedings of the 2nd International Workshop on Applied AI for\\n  Database Systems and Applications (2020)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2006.16411v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.16411v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.16529v5', updated=datetime.datetime(2021, 2, 22, 8, 21, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 30, 4, 49, 44, tzinfo=datetime.timezone.utc), title='Lachesis: Automatic Partitioning for UDF-Centric Analytics', authors=[arxiv.Result.Author('Jia Zou'), arxiv.Result.Author('Amitabh Das'), arxiv.Result.Author('Pratik Barhate'), arxiv.Result.Author('Arun Iyengar'), arxiv.Result.Author('Binhang Yuan'), arxiv.Result.Author('Dimitrije Jankov'), arxiv.Result.Author('Chris Jermaine')], summary=\"Persistent partitioning is effective in avoiding expensive shuffling\\noperations. However it remains a significant challenge to automate this process\\nfor Big Data analytics workloads that extensively use user defined functions\\n(UDFs), where sub-computations are hard to be reused for partitionings compared\\nto relational applications. In addition, functional dependency that is widely\\nutilized for partitioning selection is often unavailable in the unstructured\\ndata that is ubiquitous in UDF-centric analytics. We propose the Lachesis\\nsystem, which represents UDF-centric workloads as workflows of analyzable and\\nreusable sub-computations. Lachesis further adopts a deep reinforcement\\nlearning model to infer which sub-computations should be used to partition the\\nunderlying data. This analysis is then applied to automatically optimize the\\nstorage of the data across applications to improve the performance and users'\\nproductivity.\", comment='In submission', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2006.16529v5', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.16529v5', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2006.16551v1', updated=datetime.datetime(2020, 6, 30, 6, 16, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 6, 30, 6, 16, 19, tzinfo=datetime.timezone.utc), title='Hierarchical Graph Matching Network for Graph Similarity Computation', authors=[arxiv.Result.Author('Haibo Xiu'), arxiv.Result.Author('Xiao Yan'), arxiv.Result.Author('Xiaoqiang Wang'), arxiv.Result.Author('James Cheng'), arxiv.Result.Author('Lei Cao')], summary='Graph edit distance / similarity is widely used in many tasks, such as graph\\nsimilarity search, binary function analysis, and graph clustering. However,\\ncomputing the exact graph edit distance (GED) or maximum common subgraph (MCS)\\nbetween two graphs is known to be NP-hard. In this paper, we propose the\\nhierarchical graph matching network (HGMN), which learns to compute graph\\nsimilarity from data. HGMN is motivated by the observation that two similar\\ngraphs should also be similar when they are compressed into more compact\\ngraphs. HGMN utilizes multiple stages of hierarchical clustering to organize a\\ngraph into successively more compact graphs. At each stage, the earth mover\\ndistance (EMD) is adopted to obtain a one-to-one mapping between the nodes in\\ntwo graphs (on which graph similarity is to be computed), and a correlation\\nmatrix is also derived from the embeddings of the nodes in the two graphs. The\\ncorrelation matrices from all stages are used as input for a convolutional\\nneural network (CNN), which is trained to predict graph similarity by\\nminimizing the mean squared error (MSE). Experimental evaluation on 4 datasets\\nin different domains and 4 performance metrics shows that HGMN consistently\\noutperforms existing baselines in the accuracy of graph similarity\\napproximation.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2006.16551v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2006.16551v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.00129v1', updated=datetime.datetime(2020, 8, 1, 0, 15, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 1, 0, 15, 44, tzinfo=datetime.timezone.utc), title='GraphQL Live Querying with DynamoDB', authors=[arxiv.Result.Author('Austin Silveria')], summary='We present a method of implementing GraphQL live queries at the database\\nlevel. Our DynamoDB simulation in Go mimics a distributed key-value store and\\nimplements live queries to expose possible pitfalls. Two key components for\\nimplementing live queries are storing fields selected in a live query and\\ndetermining which object fields have been updated in each database write. A\\nstream(key, fields) request to the system contains fields to include in the\\nlive query stream and on subsequent put(key, object) operations, the database\\nasynchronously determines which fields were updated and pushes a new query view\\nto the stream if those fields overlap with the stream() request. Following a\\ndiscussion of our implementation, we explore motivations for using live queries\\nsuch as simplifying software communication, minimizing data transfer, and\\nenabling real-time data and describe an architecture for building software with\\nGraphQL and live queries.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2008.00129v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.00129v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.00368v1', updated=datetime.datetime(2020, 8, 2, 0, 33, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 2, 0, 33, 43, tzinfo=datetime.timezone.utc), title='Privacy-Aware Data Cleaning-as-a-Service (Extended Version)', authors=[arxiv.Result.Author('Yu Huang'), arxiv.Result.Author('Mostafa Milani'), arxiv.Result.Author('Fei Chiang')], summary='Data cleaning is a pervasive problem for organizations as they try to reap\\nvalue from their data. Recent advances in networking and cloud computing\\ntechnology have fueled a new computing paradigm called Database-as-a-Service,\\nwhere data management tasks are outsourced to large service providers. In this\\npaper, we consider a Data Cleaning-as-a-Service model that allows a client to\\ninteract with a data cleaning provider who hosts curated, and sensitive data.\\nWe present PACAS: a Privacy-Aware data Cleaning-As-a-Service model that\\nfacilitates interaction between the parties with client query requests for\\ndata, and a service provider using a data pricing scheme that computes prices\\naccording to data sensitivity. We propose new extensions to the model to define\\ngeneralized data repairs that obfuscate sensitive data to allow data sharing\\nbetween the client and service provider. We present a new semantic distance\\nmeasure to quantify the utility of such repairs, and we re-define the notion of\\nconsistency in the presence of generalized values. The PACAS model uses\\n(X,Y,L)-anonymity that extends existing data publishing techniques to consider\\nthe semantics in the data while protecting sensitive values. Our evaluation\\nover real data show that PACAS safeguards semantically related sensitive\\nvalues, and provides lower repair errors compared to existing privacy-aware\\ncleaning techniques.', comment=None, journal_ref=None, doi='10.1016/j.is.2020.101608', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1016/j.is.2020.101608', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2008.00368v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.00368v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.01208v1', updated=datetime.datetime(2020, 8, 3, 21, 37, 38, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 3, 21, 37, 38, tzinfo=datetime.timezone.utc), title='Knowledge Translation: Extended Technical Report', authors=[arxiv.Result.Author('Bahar Ghadiri Bashardoost'), arxiv.Result.Author('Renée J. Miller'), arxiv.Result.Author('Kelly Lyons'), arxiv.Result.Author('Fatemeh Nargesian')], summary=\"We introduce Kensho, a tool for generating mapping rules between two\\nKnowledge Bases (KBs). To create the mapping rules, Kensho starts with a set of\\ncorrespondences and enriches them with additional semantic information\\nautomatically identified from the structure and constraints of the KBs. Our\\napproach works in two phases. In the first phase, semantic associations between\\nresources of each KB are captured. In the second phase, mapping rules are\\ngenerated by interpreting the correspondences in a way that respects the\\ndiscovered semantic associations among elements of each KB. Kensho's mapping\\nrules are expressed using SPARQL queries and can be used directly to exchange\\nknowledge from source to target. Kensho is able to automatically rank the\\ngenerated mapping rules using a set of heuristics. We present an experimental\\nevaluation of Kensho and assess our mapping generation and ranking strategies\\nusing more than 50 synthesized and real world settings, chosen to showcase some\\nof the most important applications of knowledge translation. In addition, we\\nuse three existing benchmarks to demonstrate Kensho's ability to deal with\\ndifferent mapping scenarios.\", comment='Extended technical report of \"Knowledge Translation\" paper, accepted\\n  in VLDB 2020', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2008.01208v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.01208v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.03719v1', updated=datetime.datetime(2020, 8, 9, 13, 2, 12, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 9, 13, 2, 12, tzinfo=datetime.timezone.utc), title='A Rule-based Language for Application Integration', authors=[arxiv.Result.Author('Daniel Ritter'), arxiv.Result.Author('Jan Broß')], summary='Although message-based (business) application integration is based on\\norchestrated message flows, current modeling languages exclusively cover (parts\\nof) the control flow, while under-specifying the data flow. Especially for more\\ndata-intensive integration scenarios, this fact adds to the inherent data\\nprocessing weakness in conventional integration systems.\\n  We argue that with a more data-centric integration language and a relational\\nlogic based implementation of integration semantics, optimizations from the\\ndata management domain(e.g., data partitioning, parallelization) can be\\ncombined with common integration processing (e.g., scatter/gather,\\nsplitter/gather). With the Logic Integration Language (LiLa) we redefine\\nintegration logic tailored for data-intensive processing and propose a novel\\napproach to data-centric integration modeling, from which we derive the\\ncontrol-and data flow and apply them to a conventional integration system.', comment='14 pages, work from 2013/14', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LO', 'cs.PL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2008.03719v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.03719v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.04045v1', updated=datetime.datetime(2020, 8, 1, 23, 37, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 1, 23, 37, 31, tzinfo=datetime.timezone.utc), title='Towards Integrated and Open COVID-19 Data', authors=[arxiv.Result.Author('Georgios M. Santipantakis'), arxiv.Result.Author('George A. Vouros'), arxiv.Result.Author('Christos Doulkeridis')], summary='Motivated by the global unrest related to the COVID-19 pandemic, we present a\\nsystem prototype for ontology-based, integration of national data published\\nfrom various countries. COVID-related data is published from different\\nauthorities, in different formats, at varying spatio-temporal granularity, and\\nirregularly. Consequently, this hinders the joint data exploration and\\nexploitation, which could lead scientists to acquire important insights,\\nwithout having to deal with the cumbersome task of data acquisition and\\nintegration. Motivated by this shortcoming, we propose an approach for data\\nacquisition, ontology-based data representation, and data transformation to\\nRDF, which also enables interlinking with other publicly available data\\nsources. Currently, data coming from the following European countries has been\\nsuccessfully integrated: Austria, Belgium, France, Germany, Greece, Italy, and\\nSweden. The knowledge base is automatically being updated, and it is available\\nto the public through a SPARQL endpoint and a direct download link.\\nFurthermore, we showcase how data integration enables spatio-temporal data\\nanalysis and knowledge discovery, by means of meaningful queries that would not\\nbe feasible to process otherwise.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.SI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2008.04045v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.04045v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.05103v3', updated=datetime.datetime(2020, 10, 15, 10, 11, 21, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 12, 4, 37, 48, tzinfo=datetime.timezone.utc), title='Sampling Based Approximate Skyline Calculation on Big Data', authors=[arxiv.Result.Author('Xingxing Xiao'), arxiv.Result.Author('Jianzhong Li')], summary='The existing algorithms for processing skyline queries cannot adapt to big\\ndata. This paper proposes two approximate skyline algorithms based on sampling.\\nThe first algorithm obtains a fixed size sample and computes the approximate\\nskyline on the sample. The error of the first algorithm is relatively small in\\nmost cases, and is almost independent of the input relation size. The second\\nalgorithm returns an $(\\\\epsilon,\\\\delta)$-approximation for the exact skyline.\\nThe size of sample required by the second algorithm can be regarded as a\\nconstant relative to the input relation size, so is the running time.\\nExperiments verify the error analysis of the first algorithm and show that the\\nsecond algorithm is much faster than the existing skyline algorithms.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2008.05103v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.05103v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.05164v1', updated=datetime.datetime(2020, 8, 12, 8, 26, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 12, 8, 26, 42, tzinfo=datetime.timezone.utc), title='The network footprint of replication in popular DBMSs', authors=[arxiv.Result.Author('Muhammad Karam Shehzad'), arxiv.Result.Author('Jam Muhammad Yousif'), arxiv.Result.Author('Muhammad Saqib Ilyas'), arxiv.Result.Author('Adnan Iqbal')], summary='Database replication is an important component of reliable, disaster tolerant\\nand highly available distributed systems. However, data replication also causes\\ncommunication and processing overhead. Quantification of these overheads is\\ncrucial in choosing a suitable DBMS form several available options and capacity\\nplanning. In this paper, we present results from a comparative empirical\\nanalysis of replication activities of three commonly used DBMSs - MySQL,\\nPostgreSQL and Cassandra under text as well as image traffic. In our\\nexperiments, the total traffic with two replicas (which is the norm) was as\\nmuch as $300$\\\\% higher than the total traffic with no replica. Furthermore,\\nactivation of the compression option for replication traffic, built into MySQL,\\nreduced the total network traffic by as much as $20$\\\\%. We also found that\\naverage CPU utilization and memory utilization were not impacted by the number\\nof replicas or the dataset.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2008.05164v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.05164v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.06265v1', updated=datetime.datetime(2020, 8, 14, 9, 41, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 14, 9, 41, 10, tzinfo=datetime.timezone.utc), title='Towards Querying in Decentralized Environments with Privacy-Preserving Aggregation', authors=[arxiv.Result.Author('Ruben Taelman'), arxiv.Result.Author('Simon Steyskal'), arxiv.Result.Author('Sabrina Kirrane')], summary='The Web is a ubiquitous economic, educational, and collaborative space.\\nHowever, it also serves as a haven for personal information harvesting.\\nExisting decentralised Web-based ecosystems, such as Solid, aim to combat\\npersonal data exploitation on the Web by enabling individuals to manage their\\ndata in the personal data store of their choice. Since personal data in these\\ndecentralised ecosystems are distributed across many sources, there is a need\\nfor techniques to support efficient privacy-preserving query execution over\\npersonal data stores. Towards this end, in this position paper we present a\\nframework for efficient privacy preserving federated querying, and highlight\\nopen research challenges and opportunities. The overarching goal being to\\nprovide a means to position future research into privacy-preserving querying\\nwithin decentralised environments.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2008.06265v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.06265v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.06640v1', updated=datetime.datetime(2020, 8, 15, 3, 42, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 15, 3, 42, 33, tzinfo=datetime.timezone.utc), title='Automatic Storage Structure Selection for hybrid Workload', authors=[arxiv.Result.Author('Hongzhi Wang'), arxiv.Result.Author('Yan Wei'), arxiv.Result.Author('Hao Yan')], summary='In the use of database systems, the design of the storage engine and data\\nmodel directly affects the performance of the database when performing queries.\\nTherefore, the users of the database need to select the storage engine and\\ndesign data model according to the workload encountered. However, in a hybrid\\nworkload, the query set of the database is dynamically changing, and the design\\nof its optimal storage structure is also changing. Motivated by this, we\\npropose an automatic storage structure selection system based on learning cost,\\nwhich is used to dynamically select the optimal storage structure of the\\ndatabase under hybrid workloads. In the system, we introduce a machine learning\\nmethod to build a cost model for the storage engine, and a column-oriented data\\nlayout generation algorithm. Experimental results show that the proposed system\\ncan choose the optimal combination of storage engine and data model according\\nto the current workload, which greatly improves the performance of the default\\nstorage structure. And the system is designed to be compatible with different\\nstorage engines for easy use in practical applications.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2008.06640v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.06640v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.06831v1', updated=datetime.datetime(2020, 8, 16, 3, 23, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 16, 3, 23, 1, tzinfo=datetime.timezone.utc), title='DeepSampling: Selectivity Estimation with Predicted Error and Response Time', authors=[arxiv.Result.Author('Tin Vu'), arxiv.Result.Author('Ahmed Eldawy')], summary='The rapid growth of spatial data urges the research community to find\\nefficient processing techniques for interactive queries on large volumes of\\ndata. Approximate Query Processing (AQP) is the most prominent technique that\\ncan provide real-time answer for ad-hoc queries based on a random sample.\\nUnfortunately, existing AQP methods provide an answer without providing any\\naccuracy metrics due to the complex relationship between the sample size, the\\nquery parameters, the data distribution, and the result accuracy. This paper\\nproposes DeepSampling, a deep-learning-based model that predicts the accuracy\\nof a sample-based AQP algorithm, specially selectivity estimation, given the\\nsample size, the input distribution, and query parameters. The model can also\\nbe reversed to measure the sample size that would produce a desired accuracy.\\nDeepSampling is the first system that provides a reliable tool for existing\\nspatial databases to control the accuracy of AQP.', comment='9 pages, published in DeepSpatial 2020', journal_ref='ACM SIGKDD Workshop on Deep Learning for Spatiotemporal Data,\\n  Applications, and Systems, 2020', doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2008.06831v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.06831v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.06835v1', updated=datetime.datetime(2020, 8, 16, 4, 8, 38, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 16, 4, 8, 38, tzinfo=datetime.timezone.utc), title='Benchmarking database performance for genomic data', authors=[arxiv.Result.Author('Matloob Khushi')], summary='Genomic regions represent features such as gene annotations, transcription\\nfactor binding sites and epigenetic modifications. Performing various genomic\\noperations such as identifying overlapping/non-overlapping regions or nearest\\ngene annotations are common research needs. The data can be saved in a database\\nsystem for easy management, however, there is no comprehensive database\\nbuilt-in algorithm at present to identify overlapping regions. Therefore I have\\ndeveloped a region-mapping (RegMap) SQL-based algorithm to perform genomic\\noperations and have benchmarked the performance of different databases.\\nBenchmarking identified that PostgreSQL extracts overlapping regions much\\nfaster than MySQL. Insertion and data uploads in PostgreSQL were also better,\\nalthough general searching capability of both databases was almost equivalent.\\nIn addition, using the algorithm pair-wise, overlaps of >1000 datasets of\\ntranscription factor binding sites and histone marks, collected from previous\\npublications, were reported and it was found that HNF4G significantly\\nco-locates with cohesin subunit STAG1 (SA1).', comment=None, journal_ref='J Cell Biochem. 2015 Jun;116(6):877-83', doi='10.1002/jcb.25049', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1002/jcb.25049', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2008.06835v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.06835v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.06869v1', updated=datetime.datetime(2020, 8, 16, 10, 3, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 16, 10, 3, 14, tzinfo=datetime.timezone.utc), title='SECODA: Segmentation- and Combination-Based Detection of Anomalies', authors=[arxiv.Result.Author('Ralph Foorthuis')], summary='This study introduces SECODA, a novel general-purpose unsupervised\\nnon-parametric anomaly detection algorithm for datasets containing continuous\\nand categorical attributes. The method is guaranteed to identify cases with\\nunique or sparse combinations of attribute values. Continuous attributes are\\ndiscretized repeatedly in order to correctly determine the frequency of such\\nvalue combinations. The concept of constellations, exponentially increasing\\nweights and discretization cut points, as well as a pruning heuristic are used\\nto detect anomalies with an optimal number of iterations. Moreover, the\\nalgorithm has a low memory imprint and its runtime performance scales linearly\\nwith the size of the dataset. An evaluation with simulated and real-life\\ndatasets shows that this algorithm is able to identify many different types of\\nanomalies, including complex multidimensional instances. An evaluation in terms\\nof a data quality use case with a real dataset demonstrates that SECODA can\\nbring relevant and practical value to real-world settings.', comment='12 pages (including DSAA conference poster), 9 figures, 3 tables.\\n  Presented at DSAA 2017, the IEEE International Conference on Data Science and\\n  Advanced Analytics', journal_ref=None, doi='10.1109/DSAA.2017.35', primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LG', 'stat.OT', '62G07', 'G.3; I.2.6; I.5'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/DSAA.2017.35', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2008.06869v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.06869v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.07176v1', updated=datetime.datetime(2020, 8, 17, 9, 32, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 17, 9, 32, 4, tzinfo=datetime.timezone.utc), title='SDM-RDFizer: An RML Interpreter for the Efficient Creation of RDF Knowledge Graphs', authors=[arxiv.Result.Author('Enrique Iglesias'), arxiv.Result.Author('Samaneh Jozashoori'), arxiv.Result.Author('David Chaves-Fraga'), arxiv.Result.Author('Diego Collarana'), arxiv.Result.Author('Maria-Esther Vidal')], summary='In recent years, the amount of data has increased exponentially, and\\nknowledge graphs have gained attention as data structures to integrate data and\\nknowledge harvested from myriad data sources. However, data complexity issues\\nlike large volume, high-duplicate rate, and heterogeneity usually characterize\\nthese data sources, being required data management tools able to address the\\nimpact negatively of these issues on the knowledge graph creation process. In\\nthis paper, we propose the SDM-RDFizer, an interpreter of the RDF Mapping\\nLanguage (RML), to transform raw data in various formats into an RDF knowledge\\ngraph. SDM-RDFizer implements novel algorithms to execute the logical operators\\nbetween mappings in RML, allowing thus to scale up to complex scenarios where\\ndata is not only broad but has a high-duplication rate. We empirically evaluate\\nthe SDM-RDFizer performance against diverse testbeds with diverse\\nconfigurations of data volume, duplicates, and heterogeneity. The observed\\nresults indicate that SDM-RDFizer is two orders of magnitude faster than state\\nof the art, thus, meaning that SDM-RDFizer an interoperable and scalable\\nsolution for knowledge graph creation. SDM-RDFizer is publicly available as a\\nresource through a Github repository and a DOI.', comment=None, journal_ref=None, doi='10.1145/3340531.3412881', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3340531.3412881', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2008.07176v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.07176v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.08989v1', updated=datetime.datetime(2020, 8, 20, 14, 22, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 20, 14, 22, 20, tzinfo=datetime.timezone.utc), title='Towards Inferring Queries from Simple and Partial Provenance Examples', authors=[arxiv.Result.Author('Amir Gilad'), arxiv.Result.Author('Yuval Moskovitch')], summary='The field of query-by-example aims at inferring queries from output examples\\ngiven by non-expert users, by finding the underlying logic that binds the\\nexamples. However, for a very small set of examples, it is difficult to\\ncorrectly infer such logic. To bridge this gap, previous work suggested\\nattaching explanations to each output example, modeled as provenance, allowing\\nusers to explain the reason behind their choice of example. In this paper, we\\nexplore the problem of inferring queries from a few output examples and\\nintuitive explanations. We propose a two step framework: (1) convert the\\nexplanations into (partial) provenance and (2) infer a query that generates the\\noutput examples using a novel algorithm that employs a graph based approach.\\nThis framework is suitable for non-experts as it does not require the\\nspecification of the provenance in its entirety or an understanding of its\\nstructure. We show promising initial experimental results of our approach.', comment=None, journal_ref=None, doi='10.1145/3340531.3417451', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3340531.3417451', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2008.08989v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.08989v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.09268v1', updated=datetime.datetime(2020, 8, 21, 2, 16, 12, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 21, 2, 16, 12, tzinfo=datetime.timezone.utc), title='Spitz: A Verifiable Database System', authors=[arxiv.Result.Author('Meihui Zhang'), arxiv.Result.Author('Zhongle Xie'), arxiv.Result.Author('Cong Yue'), arxiv.Result.Author('Ziyue Zhong')], summary='Databases in the past have helped businesses maintain and extract insights\\nfrom their data. Today, it is common for a business to involve multiple\\nindependent, distrustful parties. This trend towards decentralization\\nintroduces a new and important requirement to databases: the integrity of the\\ndata, the history, and the execution must be protected. In other words, there\\nis a need for a new class of database systems whose integrity can be verified\\n(or verifiable databases).\\n  In this paper, we identify the requirements and the design challenges of\\nverifiable databases.We observe that the main challenges come from the need to\\nbalance data immutability, tamper evidence, and performance. We first consider\\napproaches that extend existing OLTP and OLAP systems with support for\\nverification. We next examine a clean-slate approach, by describing a new\\nsystem, Spitz, specifically designed for efficiently supporting immutable and\\ntamper-evident transaction management. We conduct a preliminary performance\\nstudy of both approaches against a baseline system, and provide insights on\\ntheir performance.', comment=None, journal_ref=None, doi='10.14778/3415478.3415567', primary_category='cs.DB', categories=['cs.DB', 'cs.CR', 'cs.DC'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3415478.3415567', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2008.09268v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.09268v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.10569v1', updated=datetime.datetime(2020, 8, 24, 17, 15, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 24, 17, 15, 9, tzinfo=datetime.timezone.utc), title='Approximate Partition Selection for Big-Data Workloads using Summary Statistics', authors=[arxiv.Result.Author('Kexin Rong'), arxiv.Result.Author('Yao Lu'), arxiv.Result.Author('Peter Bailis'), arxiv.Result.Author('Srikanth Kandula'), arxiv.Result.Author('Philip Levis')], summary='Many big-data clusters store data in large partitions that support access at\\na coarse, partition-level granularity. As a result, approximate query\\nprocessing via row-level sampling is inefficient, often requiring reads of many\\npartitions. In this work, we seek to answer queries quickly and approximately\\nby reading a subset of the data partitions and combining partial answers in a\\nweighted manner without modifying the data layout. We illustrate how to\\nefficiently perform this query processing using a set of pre-computed summary\\nstatistics, which inform the choice of partitions and weights. We develop novel\\nmeans of using the statistics to assess the similarity and importance of\\npartitions. Our experiments on several datasets and data layouts demonstrate\\nthat to achieve the same relative error compared to uniform partition sampling,\\nour techniques offer from 2.7$\\\\times$ to $70\\\\times$ reduction in the number of\\npartitions read, and the statistics stored per partition require fewer than\\n100KB.', comment=None, journal_ref=None, doi='10.14778/3407790.3407848', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3407790.3407848', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2008.10569v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.10569v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.10925v3', updated=datetime.datetime(2020, 9, 9, 19, 29, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 25, 10, 14, 55, tzinfo=datetime.timezone.utc), title='Replicability and Reproducibility of a Schema Evolution Study in Embedded Databases', authors=[arxiv.Result.Author('Dimitri Braininger'), arxiv.Result.Author('Wolfgang Mauerer'), arxiv.Result.Author('Stefanie Scherzinger')], summary='Ascertaining the feasibility of independent falsification or repetition of\\npublished results is vital to the scientific process, and replication or\\nreproduction experiments are routinely performed in many disciplines.\\nUnfortunately, such studies are only scarcely available in database research,\\nwith few papers dedicated to re-evaluating published results. In this paper, we\\nconduct a case study on replicating and reproducing a study on schema evolution\\nin embedded databases. We obtain exact results for one out of four database\\napplications studied, and come close in two further cases. By reporting\\nresults, efforts, and obstacles encountered, we hope to increase appreciation\\nfor the substantial efforts required to ensure reproducibility. By discussing\\nminutiae details required for reproducible work, we argue that such important,\\nbut often ignored components of scientific work should receive more credit in\\nthe evaluation of future research.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.SE'], links=[arxiv.Result.Link('http://arxiv.org/abs/2008.10925v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.10925v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.10986v3', updated=datetime.datetime(2020, 11, 18, 13, 46, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 25, 13, 26, 4, tzinfo=datetime.timezone.utc), title='On the complexity of query containment and computing certain answers in the presence of ACs', authors=[arxiv.Result.Author('Foto N. Afrati'), arxiv.Result.Author('Matthew Damigos')], summary='We often add arithmetic to extend the expressiveness of query languages and\\nstudy the complexity of problems such as testing query containment and finding\\ncertain answers in the framework of answering queries using views. When adding\\narithmetic comparisons, the complexity of such problems is higher than the\\ncomplexity of their counterparts without them. It has been observed that we can\\nachieve lower complexity if we restrict some of the comparisons in the\\ncontaining query to be closed or open semi-interval comparisons. Here, focusing\\na) on the problem of containment for conjunctive queries with arithmetic\\ncomparisons (CQAC queries, for short), we prove upper bounds on its\\ncomputational complexity and b) on the problem of computing certain answers, we\\nfind large classes of CQAC queries and views where this problem is polynomial.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2008.10986v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.10986v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.11015v4', updated=datetime.datetime(2021, 6, 28, 11, 57, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 24, 15, 6, 26, tzinfo=datetime.timezone.utc), title='Table2Charts: Recommending Charts by Learning Shared Table Representations', authors=[arxiv.Result.Author('Mengyu Zhou'), arxiv.Result.Author('Qingtao Li'), arxiv.Result.Author('Xinyi He'), arxiv.Result.Author('Yuejiang Li'), arxiv.Result.Author('Yibo Liu'), arxiv.Result.Author('Wei Ji'), arxiv.Result.Author('Shi Han'), arxiv.Result.Author('Yining Chen'), arxiv.Result.Author('Daxin Jiang'), arxiv.Result.Author('Dongmei Zhang')], summary='It is common for people to create different types of charts to explore a\\nmulti-dimensional dataset (table). However, to recommend commonly composed\\ncharts in real world, one should take the challenges of efficiency, imbalanced\\ndata and table context into consideration. In this paper, we propose\\nTable2Charts framework which learns common patterns from a large corpus of\\n(table, charts) pairs. Based on deep Q-learning with copying mechanism and\\nheuristic searching, Table2Charts does table-to-sequence generation, where each\\nsequence follows a chart template. On a large spreadsheet corpus with 165k\\ntables and 266k charts, we show that Table2Charts could learn a shared\\nrepresentation of table fields so that recommendation tasks on different chart\\ntypes could mutually enhance each other. Table2Charts outperforms other chart\\nrecommendation systems in both multi-type task (with doubled recall numbers\\nR@3=0.61 and R@1=0.43) and human evaluations.', comment=\"9 + 2(appendix) pages, accepted by KDD'21 conference\", journal_ref=None, doi='10.1145/3447548.3467279', primary_category='cs.DB', categories=['cs.DB', 'cs.CL', 'cs.HC'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3447548.3467279', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2008.11015v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.11015v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.11409v2', updated=datetime.datetime(2020, 9, 1, 14, 6, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 26, 6, 58, 9, tzinfo=datetime.timezone.utc), title='Automatic Integration Issues of Tabular Data for On-Line Analysis Processing', authors=[arxiv.Result.Author('Yuzhao Yang'), arxiv.Result.Author('Jérôme Darmont'), arxiv.Result.Author('Franck Ravat'), arxiv.Result.Author('Olivier Teste')], summary='Companies and individuals produce numerous tabular data. The objective of\\nthis position paper is to draw up the challenges posed by the automatic\\nintegration of data in the form of tables so that they can be cross-analyzed.\\nWe provide a first automatic solution for the integration of such tabular data\\nto allow On-Line Analysis Processing. To fulfil this task, features of tabular\\ndata should be analyzed and the challenge of automatic multidimensional schema\\ngeneration should be addressed. Hence, we propose a typology of tabular data\\nand discuss our idea of an automatic solution.', comment=None, journal_ref=\"16e journ{\\\\'e}es EDA Business Intelligence & Big Data (EDA 2020),\\n  Aug 2020, Lyon, France. pp.5-18\", doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2008.11409v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.11409v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.12330v1', updated=datetime.datetime(2020, 8, 27, 18, 43, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 27, 18, 43, 55, tzinfo=datetime.timezone.utc), title='The Impact of Discretization Method on the Detection of Six Types of Anomalies in Datasets', authors=[arxiv.Result.Author('Ralph Foorthuis')], summary='Anomaly detection is the process of identifying cases, or groups of cases,\\nthat are in some way unusual and do not fit the general patterns present in the\\ndataset. Numerous algorithms use discretization of numerical data in their\\ndetection processes. This study investigates the effect of the discretization\\nmethod on the unsupervised detection of each of the six anomaly types\\nacknowledged in a recent typology of data anomalies. To this end, experiments\\nare conducted with various datasets and SECODA, a general-purpose algorithm for\\nunsupervised non-parametric anomaly detection in datasets with numerical and\\ncategorical attributes. This algorithm employs discretization of continuous\\nattributes, exponentially increasing weights and discretization cut points, and\\na pruning heuristic to detect anomalies with an optimal number of iterations.\\nThe results demonstrate that standard SECODA can detect all six types, but that\\ndifferent discretization methods favor the discovery of certain anomaly types.\\nThe main findings also hold for other detection techniques using\\ndiscretization.', comment='16 pages, 5 figures, 2 tables. Presented at the 30th Benelux\\n  Conference on Artificial Intelligence (BNAIC 2018)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LG', 'stat.ML', '62G07', 'G.3; I.2.6; I.5'], links=[arxiv.Result.Link('http://arxiv.org/abs/2008.12330v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.12330v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.12665v1', updated=datetime.datetime(2020, 8, 28, 14, 6, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 28, 14, 6, 10, tzinfo=datetime.timezone.utc), title='Cache-Efficient Sweeping-Based Interval Joins for Extended Allen Relation Predicates (Extended Version)', authors=[arxiv.Result.Author('Danila Piatov'), arxiv.Result.Author('Sven Helmer'), arxiv.Result.Author('Anton Dignös'), arxiv.Result.Author('Fabio Persia')], summary=\"We develop a family of efficient plane-sweeping interval join algorithms that\\ncan evaluate a wide range of interval predicates such as Allen's relationships\\nand parameterized relationships. Our technique is based on a framework,\\ncomponents of which can be flexibly combined in different manners to support\\nthe required interval relation. In temporal databases, our algorithms can\\nexploit a well-known and flexible access method, the Timeline Index, thus\\nexpanding the set of operations it supports even further. Additionally,\\nemploying a compact data structure, the gapless hash map, we utilize the CPU\\ncache efficiently. In an experimental evaluation, we show that our approach is\\nseveral times faster and scales better than state-of-the-art techniques, while\\nbeing much better suited for real-time event processing.\", comment='24 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2.4'], links=[arxiv.Result.Link('http://arxiv.org/abs/2008.12665v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.12665v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.12763v1', updated=datetime.datetime(2020, 8, 28, 17, 41, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 28, 17, 41, 11, tzinfo=datetime.timezone.utc), title='Relational Data Synthesis using Generative Adversarial Networks: A Design Space Exploration', authors=[arxiv.Result.Author('Ju Fan'), arxiv.Result.Author('Tongyu Liu'), arxiv.Result.Author('Guoliang Li'), arxiv.Result.Author('Junyou Chen'), arxiv.Result.Author('Yuwei Shen'), arxiv.Result.Author('Xiaoyong Du')], summary=\"The proliferation of big data has brought an urgent demand for\\nprivacy-preserving data publishing. Traditional solutions to this demand have\\nlimitations on effectively balancing the tradeoff between privacy and utility\\nof the released data. Thus, the database community and machine learning\\ncommunity have recently studied a new problem of relational data synthesis\\nusing generative adversarial networks (GAN) and proposed various algorithms.\\nHowever, these algorithms are not compared under the same framework and thus it\\nis hard for practitioners to understand GAN's benefits and limitations. To\\nbridge the gaps, we conduct so far the most comprehensive experimental study\\nthat investigates applying GAN to relational data synthesis. We introduce a\\nunified GAN-based framework and define a space of design solutions for each\\ncomponent in the framework, including neural network architectures and training\\nstrategies. We conduct extensive experiments to explore the design space and\\ncompare with traditional data synthesis approaches. Through extensive\\nexperiments, we find that GAN is very promising for relational data synthesis,\\nand provide guidance for selecting appropriate design solutions. We also point\\nout limitations of GAN and identify future research directions.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2008.12763v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.12763v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.12905v1', updated=datetime.datetime(2020, 8, 29, 3, 42, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 29, 3, 42, 3, tzinfo=datetime.timezone.utc), title='Batching and Matching for Food Delivery in Dynamic Road Networks', authors=[arxiv.Result.Author('Manas Joshi'), arxiv.Result.Author('Arshdeep Singh'), arxiv.Result.Author('Sayan Ranu'), arxiv.Result.Author('Amitabha Bagchi'), arxiv.Result.Author('Priyank Karia'), arxiv.Result.Author('Puneet Kala')], summary='Given a stream of food orders and available delivery vehicles, how should\\norders be assigned to vehicles so that the delivery time is minimized? Several\\ndecisions have to be made: (1) assignment of orders to vehicles, (2) grouping\\norders into batches to cope with limited vehicle availability, and (3) adapting\\nto dynamic positions of delivery vehicles. We show that the minimization\\nproblem is not only NP-hard but inapproximable in polynomial time. To mitigate\\nthis computational bottleneck, we develop an algorithm called FoodMatch, which\\nmaps the vehicle assignment problem to that of minimum weight perfect matching\\non a bipartite graph. To further reduce the quadratic construction cost of the\\nbipartite graph, we deploy best-first search to only compute a subgraph that is\\nhighly likely to contain the minimum matching. The solution quality is further\\nenhanced by reducing batching to a graph clustering problem and anticipating\\ndynamic positions of vehicles through angular distance. Extensive experiments\\non food-delivery data from large metropolitan cities establish that FoodMatch\\nis substantially better than baseline strategies on a number of metrics, while\\nbeing efficient enough to handle real-world workloads.', comment='12 pages, 9 figures, Accepted in ICDE 2021 as Short Paper', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://arxiv.org/abs/2008.12905v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.12905v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.13432v1', updated=datetime.datetime(2020, 8, 31, 8, 44, 36, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 31, 8, 44, 36, tzinfo=datetime.timezone.utc), title='VALMOD: A Suite for Easy and Exact Detection of Variable Length Motifs in Data Series', authors=[arxiv.Result.Author('Michele Linardi'), arxiv.Result.Author('Yan Zhu'), arxiv.Result.Author('Themis Palpanas'), arxiv.Result.Author('Eamonn Keogh')], summary='Data series motif discovery represents one of the most useful primitives for\\ndata series mining, with applications to many domains, such as robotics,\\nentomology, seismology, medicine, and climatology, and others. The\\nstate-of-the-art motif discovery tools still require the user to provide the\\nmotif length. Yet, in several cases, the choice of motif length is critical for\\ntheir detection. Unfortunately, the obvious brute-force solution, which tests\\nall lengths within a given range, is computationally untenable, and does not\\nprovide any support for ranking motifs at different resolutions (i.e.,\\nlengths). We demonstrate VALMOD, our scalable motif discovery algorithm that\\nefficiently finds all motifs in a given range of lengths, and outputs a\\nlength-invariant ranking of motifs. Furthermore, we support the analysis\\nprocess by means of a newly proposed meta-data structure that helps the user to\\nselect the most promising pattern length. This demo aims at illustrating in\\ndetail the steps of the proposed approach, showcasing how our algorithm and\\ncorresponding graphical insights enable users to efficiently identify the\\ncorrect motifs. (Paper published in ACM Sigmod Conference 2018.)', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2008.13432v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.13432v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.13447v1', updated=datetime.datetime(2020, 8, 31, 9, 19, 58, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 31, 9, 19, 58, tzinfo=datetime.timezone.utc), title='Matrix Profile Goes MAD: Variable-Length Motif And Discord Discovery in Data Series', authors=[arxiv.Result.Author('Michele Linardi'), arxiv.Result.Author('Yan Zhu'), arxiv.Result.Author('Themis Palpanas'), arxiv.Result.Author('Eamonn Keogh')], summary='In the last fifteen years, data series motif and discord discovery have\\nemerged as two useful and well-used primitives for data series mining, with\\napplications to many domains, including robotics, entomology, seismology,\\nmedicine, and climatology. Nevertheless, the state-of-the-art motif and discord\\ndiscovery tools still require the user to provide the relative length. Yet, in\\nseveral cases, the choice of length is critical and unforgiving. Unfortunately,\\nthe obvious brute-force solution, which tests all lengths within a given range,\\nis computationally untenable. In this work, we introduce a new framework, which\\nprovides an exact and scalable motif and discord discovery algorithm that\\nefficiently finds all motifs and discords in a given range of lengths. We\\nevaluate our approach with five diverse real datasets, and demonstrate that it\\nis up to 20 times faster than the state-of-the-art. Our results also show that\\nremoving the unrealistic assumption that the user knows the correct length, can\\noften produce more intuitive and actionable results, which could have otherwise\\nbeen missed. (Paper published in Data Mining and Knowledge Discovery Journal -\\n2020)', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2008.13447v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.13447v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2008.13482v2', updated=datetime.datetime(2020, 10, 5, 8, 51, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 31, 10, 48, 41, tzinfo=datetime.timezone.utc), title='FunMap: Efficient Execution of Functional Mappings for Knowledge Graph Creation', authors=[arxiv.Result.Author('Samaneh Jozashoori'), arxiv.Result.Author('David Chaves-Fraga'), arxiv.Result.Author('Enrique Iglesias'), arxiv.Result.Author('Maria-Esther Vidal'), arxiv.Result.Author('Oscar Corcho')], summary='Data has exponentially grown in the last years, and knowledge graphs\\nconstitute powerful formalisms to integrate a myriad of existing data sources.\\nTransformation functions -- specified with function-based mapping languages\\nlike FunUL and RML+FnO -- can be applied to overcome interoperability issues\\nacross heterogeneous data sources. However, the absence of engines to\\nefficiently execute these mapping languages hinders their global adoption. We\\npropose FunMap, an interpreter of function-based mapping languages; it relies\\non a set of lossless rewriting rules to push down and materialize the execution\\nof functions in initial steps of knowledge graph creation. Although applicable\\nto any function-based mapping language that supports joins between mapping\\nrules, FunMap feasibility is shown on RML+FnO. FunMap reduces data redundancy,\\ne.g., duplicates and unused attributes, and converts RML+FnO mappings into a\\nset of equivalent rules executable on RML-compliant engines. We evaluate FunMap\\nperformance over real-world testbeds from the biomedical domain. The results\\nindicate that FunMap reduces the execution time of RML-compliant engines by up\\nto a factor of 18, furnishing, thus, a scalable solution for knowledge graph\\ncreation.', comment=None, journal_ref=None, doi='10.1007/978-3-030-62419-4_16', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-030-62419-4_16', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2008.13482v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2008.13482v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.00932v1', updated=datetime.datetime(2021, 6, 2, 4, 42, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 2, 4, 42, 1, tzinfo=datetime.timezone.utc), title='Proposed DBMS for OTT platforms in line with new age requirements', authors=[arxiv.Result.Author('Khushi Shah'), arxiv.Result.Author('Aryan Shah'), arxiv.Result.Author('Charmi Shah'), arxiv.Result.Author('Devansh Shah'), arxiv.Result.Author('Mustafa Africawala'), arxiv.Result.Author('Rushabh Shah'), arxiv.Result.Author('Nishant Doshi')], summary='Database management has become an enormous tool for on-demand content\\ndistribution services, proffering required information and providing custom\\nservices to the user. Also plays a major role for the platforms to manage their\\ndata in such a way that data redundancy is minimized. This paper emphasizes\\nimproving the user experience for the platform by efficiently managing data.\\nKeeping in mind all the new age requirements, especially after COVID-19 the\\nsudden surge in subscription has led the stakeholders to try new things to lead\\nthe OTT market. Collection of shows being the root of the tree here, this paper\\nimprovises the currently existing branches via various tables and suggests some\\nnew features on how the data collected can be utilized for introducing new and\\nmuch-required query results for the consumer.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.00932v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.00932v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.01501v1', updated=datetime.datetime(2021, 6, 2, 23, 2, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 2, 23, 2, 26, tzinfo=datetime.timezone.utc), title='Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins', authors=[arxiv.Result.Author('Sahaana Suri'), arxiv.Result.Author('Ihab F. Ilyas'), arxiv.Result.Author('Christopher Ré'), arxiv.Result.Author('Theodoros Rekatsinas')], summary='Structured data, or data that adheres to a pre-defined schema, can suffer\\nfrom fragmented context: information describing a single entity can be\\nscattered across multiple datasets or tables tailored for specific business\\nneeds, with no explicit linking keys (e.g., primary key-foreign key\\nrelationships or heuristic functions). Context enrichment, or rebuilding\\nfragmented context, using keyless joins is an implicit or explicit step in\\nmachine learning (ML) pipelines over structured data sources. This process is\\ntedious, domain-specific, and lacks support in now-prevalent no-code ML systems\\nthat let users create ML pipelines using just input data and high-level\\nconfiguration files. In response, we propose Ember, a system that abstracts and\\nautomates keyless joins to generalize context enrichment. Our key insight is\\nthat Ember can enable a general keyless join operator by constructing an index\\npopulated with task-specific embeddings. Ember learns these embeddings by\\nleveraging Transformer-based representation learning techniques. We describe\\nour core architectural principles and operators when developing Ember, and\\nempirically demonstrate that Ember allows users to develop no-code pipelines\\nfor five domains, including search, recommendation and question answering, and\\ncan exceed alternatives by up to 39% recall, with as little as a single line\\nconfiguration change.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.01501v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.01501v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.01543v4', updated=datetime.datetime(2022, 10, 4, 20, 59, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 3, 1, 58, 24, tzinfo=datetime.timezone.utc), title='Ver: View Discovery in the Wild', authors=[arxiv.Result.Author('Yue Gong'), arxiv.Result.Author('Zhiru Zhu'), arxiv.Result.Author('Sainyam Galhotra'), arxiv.Result.Author('Raul Castro Fernandez')], summary='We present Ver, a data discovery system that identifies project-join views\\nover large repositories of tables that do not contain join path information,\\nand even when input queries are inaccurate. Ver implements a reference\\narchitecture to solve both the technical (scale and search) and human (semantic\\nambiguity, navigating a large number of results) problems of view discovery. We\\ndemonstrate users find the view they want when using Ver with a user study and\\nwe demonstrate its performance with large-scale end-to-end experiments on\\nreal-world datasets containing tens of millions of join paths.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.01543v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.01543v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.02361v1', updated=datetime.datetime(2021, 6, 4, 9, 19, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 4, 9, 19, 47, tzinfo=datetime.timezone.utc), title='Facade-X: an opinionated approach to SPARQL anything', authors=[arxiv.Result.Author('Enrico Daga'), arxiv.Result.Author('Luigi Asprino'), arxiv.Result.Author('Paul Mulholland'), arxiv.Result.Author('Aldo Gangemi')], summary='The Semantic Web research community understood since its beginning how\\ncrucial it is to equip practitioners with methods to transform non-RDF\\nresources into RDF. Proposals focus on either engineering content\\ntransformations or accessing non-RDF resources with SPARQL. Existing solutions\\nrequire users to learn specific mapping languages (e.g. RML), to know how to\\nquery and manipulate a variety of source formats (e.g. XPATH, JSON-Path), or to\\ncombine multiple languages (e.g. SPARQL Generate). In this paper, we explore an\\nalternative solution and contribute a general-purpose meta-model for converting\\nnon-RDF resources into RDF: Facade-X. Our approach can be implemented by\\noverriding the SERVICE operator and does not require to extend the SPARQL\\nsyntax. We compare our approach with the state of art methods RML and SPARQL\\nGenerate and show how our solution has lower learning demands and cognitive\\ncomplexity, and it is cheaper to implement and maintain, while having\\ncomparable extensibility and efficiency.', comment='Version submitted to the SEMANTICS 2021 EU conference (Accepted May\\n  2021)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.02361v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.02361v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.03085v1', updated=datetime.datetime(2021, 6, 6, 10, 33, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 6, 10, 33, 42, tzinfo=datetime.timezone.utc), title='An Ontology Model for Climatic Data Analysis', authors=[arxiv.Result.Author('Jiantao Wu'), arxiv.Result.Author('Fabrizio Orlandi'), arxiv.Result.Author(\"Declan O'Sullivan\"), arxiv.Result.Author('Soumyabrata Dev')], summary='Recently ontologies have been exploited in a wide range of research areas for\\ndata modeling and data management. They greatly assists in defining the\\nsemantic model of the underlying data combined with domain knowledge. In this\\npaper, we propose the Climate Analysis (CA) Ontology to model climate datasets\\nused by remote sensing analysts. We use the data published by National Oceanic\\nand Atmospheric Administration (NOAA) to further explore how ontology modeling\\ncan be used to facilitate the field of climatic data processing. The idea of\\nthis work is to convert relational climate data to the Resource Description\\nFramework (RDF) data model, so that it can be stored in a graph database and\\neasily accessed through the Web as Linked Data. Typically, this provides\\nclimate researchers, who are interested in datasets such as NOAA, with the\\npotential of enriching and interlinking with other databases. As a result, our\\napproach facilitates data integration and analysis of diverse climatic data\\nsources and allows researchers to interrogate these sources directly on the Web\\nusing the standard SPARQL query language.', comment='Published in IEEE International Geoscience and Remote Sensing\\n  Symposium (IGARSS), 2021', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.03085v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.03085v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.03355v1', updated=datetime.datetime(2021, 6, 7, 6, 8, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 7, 6, 8, 6, tzinfo=datetime.timezone.utc), title='Sub-trajectory Similarity Join with Obfuscation', authors=[arxiv.Result.Author('Yanchuan Chang'), arxiv.Result.Author('Jianzhong Qi'), arxiv.Result.Author('Egemen Tanin'), arxiv.Result.Author('Xingjun Ma'), arxiv.Result.Author('Hanan Samet')], summary=\"User trajectory data is becoming increasingly accessible due to the\\nprevalence of GPS-equipped devices such as smartphones. Many existing studies\\nfocus on querying trajectories that are similar to each other in their\\nentirety. We observe that trajectories partially similar to each other contain\\nuseful information about users' travel patterns which should not be ignored.\\nSuch partially similar trajectories are critical in applications such as\\nepidemic contact tracing. We thus propose to query trajectories that are within\\na given distance range from each other for a given period of time. We formulate\\nthis problem as a sub-trajectory similarity join query named as the STS-Join.\\nWe further propose a distributed index structure and a query algorithm for\\nSTS-Join, where users retain their raw location data and only send obfuscated\\ntrajectories to a server for query processing. This helps preserve user\\nlocation privacy which is vital when dealing with such data. Theoretical\\nanalysis and experiments on real data confirm the effectiveness and the\\nefficiency of our proposed index structure and query algorithm.\", comment=None, journal_ref=None, doi='10.1145/3468791.3468822', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3468791.3468822', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2106.03355v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.03355v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.05620v2', updated=datetime.datetime(2021, 9, 13, 8, 52, 12, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 10, 9, 56, 42, tzinfo=datetime.timezone.utc), title='Efficient Exact k-Flexible Aggregate Nearest Neighbor Search in Road Networks Using the M-tree', authors=[arxiv.Result.Author('Moonyoung Chung'), arxiv.Result.Author('Soon J. Hyun'), arxiv.Result.Author('Woong-Kee Loh')], summary='This study proposes an efficient exact k-flexible aggregate nearest neighbor\\n(k-FANN) search algorithm in road networks using the M-tree. The\\nstate-of-the-art IER-kNN algorithm used the R-tree and pruned off unnecessary\\nnodes based on the Euclidean coordinates of objects in road networks. However,\\nIER-kNN made many unnecessary accesses to index nodes since the Euclidean\\ndistances between objects are significantly different from the actual\\nshortest-path distances between them. In contrast, our algorithm proposed in\\nthis study can greatly reduce unnecessary accesses to index nodes compared with\\nIER-kNN since the M-tree is constructed based on the actual shortest-path\\ndistances between objects. To the best of our knowledge, our algorithm is the\\nfirst exact FANN algorithm that uses the M-tree. We prove that our algorithm\\ndoes not cause any false drop. In conducting a series of experiments using\\nvarious real road network datasets, our algorithm consistently outperformed\\nIER-kNN by up to 6.92 times.', comment='16 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.05620v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.05620v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.08455v1', updated=datetime.datetime(2021, 6, 15, 22, 2, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 15, 22, 2, 59, tzinfo=datetime.timezone.utc), title='Machamp: A Generalized Entity Matching Benchmark', authors=[arxiv.Result.Author('Jin Wang'), arxiv.Result.Author('Yuliang Li'), arxiv.Result.Author('Wataru Hirota')], summary='Entity Matching (EM) refers to the problem of determining whether two\\ndifferent data representations refer to the same real-world entity. It has been\\na long-standing interest of the data management community and many efforts have\\nbeen paid in creating benchmark tasks as well as in developing advanced\\nmatching techniques. However, existing benchmark tasks for EM are limited to\\nthe case where the two data collections of entities are structured tables with\\nthe same schema. Meanwhile, the data collections for matching could be\\nstructured, semi-structured, or unstructured in real-world scenarios of data\\nscience. In this paper, we come up with a new research problem -- Generalized\\nEntity Matching to satisfy this requirement and create a benchmark Machamp for\\nit. Machamp consists of seven tasks having diverse characteristics and thus\\nprovides good coverage of use cases in real applications. We summarize existing\\nEM benchmark tasks for structured tables and conduct a series of processing and\\ncleaning efforts to transform them into matching tasks between tables with\\ndifferent structures. Based on that, we further conduct comprehensive profiling\\nof the proposed benchmark tasks and evaluate popular entity matching approaches\\non them. With the help of Machamp, it is the first time that researchers can\\nevaluate EM techniques between data collections with different structures.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.08455v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.08455v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.09592v2', updated=datetime.datetime(2023, 2, 17, 17, 27, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 17, 15, 18, 23, tzinfo=datetime.timezone.utc), title='Data Lakes: A Survey of Functions and Systems', authors=[arxiv.Result.Author('Rihan Hai'), arxiv.Result.Author('Christos Koutras'), arxiv.Result.Author('Christoph Quix'), arxiv.Result.Author('Matthias Jarke')], summary=\"Data lakes are becoming increasingly prevalent for big data management and\\ndata analytics. In contrast to traditional 'schema-on-write' approaches such as\\ndata warehouses, data lakes are repositories storing raw data in its original\\nformats and providing a common access interface. Despite the strong interest\\nraised from both academia and industry, there is a large body of ambiguity\\nregarding the definition, functions and available technologies for data lakes.\\nA complete, coherent picture of data lake challenges and solutions is still\\nmissing. This survey reviews the development, architectures, and systems of\\ndata lakes. We provide a comprehensive overview of research questions for\\ndesigning and building data lakes. We classify the existing approaches and\\nsystems based on their provided functions for data lakes, which makes this\\nsurvey a useful technical reference for designing, implementing and deploying\\ndata lakes. We hope that the thorough comparison of existing solutions and the\\ndiscussion of open research challenges in this survey will motivate the future\\ndevelopment of data lake research and practice.\", comment='Under review', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.09592v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.09592v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.09764v2', updated=datetime.datetime(2021, 8, 3, 16, 51, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 17, 18, 46, 56, tzinfo=datetime.timezone.utc), title='A probabilistic database approach to autoencoder-based data cleaning', authors=[arxiv.Result.Author('R. R. Mauritz'), arxiv.Result.Author('F. P. J. Nijweide'), arxiv.Result.Author('J. Goseling'), arxiv.Result.Author('M. van Keulen')], summary='Data quality problems are a large threat in data science. In this paper, we\\npropose a data-cleaning autoencoder capable of near-automatic data quality\\nimprovement. It learns the structure and dependencies in the data and uses it\\nas evidence to identify and correct doubtful values. We apply a probabilistic\\ndatabase approach to represent weak and strong evidence for attribute value\\nrepairs. A theoretical framework is provided, and experiments show that it can\\nremove significant amounts of noise (i.e., data quality problems) from\\ncategorical and numeric probabilistic data. Our method does not require clean\\ndata. We do, however, show that manually cleaning a small fraction of the data\\nsignificantly improves performance.', comment='Submitted to ACM Journal of Data and Information Quality, Special\\n  Issue on Deep Learning for Data Quality. Updated with changes made during\\n  peer-review process', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LG', 'H.2.7; I.2.6'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.09764v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.09764v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.09799v1', updated=datetime.datetime(2021, 6, 17, 20, 27, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 17, 20, 27, 3, tzinfo=datetime.timezone.utc), title=\"Introducing PathQuery, Google's Graph Query Language\", authors=[arxiv.Result.Author('Jesse Weaver'), arxiv.Result.Author('Eric Paniagua'), arxiv.Result.Author('Tushar Agarwal'), arxiv.Result.Author('Nicholas Guy'), arxiv.Result.Author('Alexandre Mattos')], summary='We introduce PathQuery, a graph query language developed to scale with\\nGoogle\\'s query and data volumes as well as its internal developer community.\\nPathQuery supports flexible and declarative semantics. We have found that this\\nenables query developers to think in a naturally \"graphy\" design space and to\\navoid the additional cognitive effort of coordinating numerous joins and\\nsubqueries often required to express an equivalent query in a relational space.\\nDespite its traversal-oriented syntactic style, PathQuery has a foundation on a\\ncustom variant of relational algebra -- the exposition of which we presently\\ndefer -- allowing for the application of both common and novel optimizations.\\nWe believe that PathQuery has withstood a \"test of time\" at Google, under both\\nlarge scale and low latency requirements. We thus share herein a language\\ndesign that admits a rigorous declarative semantics, has scaled well in\\npractice, and provides a natural syntax for graph traversals while also\\nadmitting complex graph patterns.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.09799v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.09799v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.10515v1', updated=datetime.datetime(2021, 6, 19, 15, 20, 21, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 19, 15, 20, 21, tzinfo=datetime.timezone.utc), title='A Generic Distributed Clustering Framework for Massive Data', authors=[arxiv.Result.Author('Pingyi Luo'), arxiv.Result.Author('Qiang Huang'), arxiv.Result.Author('Anthony K. H. Tung')], summary='In this paper, we introduce a novel Generic distributEd clustEring frameworK\\n(GEEK) beyond $k$-means clustering to process massive amounts of data. To deal\\nwith different data types, GEEK first converts data in the original feature\\nspace into a unified format of buckets; then, we design a new Seeding method\\nbased on simILar bucKets (SILK) to determine initial seeds. Compared with\\nstate-of-the-art seeding methods such as $k$-means++ and its variants, SILK can\\nautomatically identify the number of initial seeds based on the closeness of\\nshared data objects in similar buckets instead of pre-specifying $k$. Thus, its\\ntime complexity is independent of $k$. With these well-selected initial seeds,\\nGEEK only needs a one-pass data assignment to get the final clusters. We\\nimplement GEEK on a distributed CPU-GPU platform for large-scale clustering. We\\nevaluate the performance of GEEK over five large-scale real-life datasets and\\nshow that GEEK can deal with massive data of different types and is comparable\\nto (or even better than) many state-of-the-art customized GPU-based methods,\\nespecially in large $k$ values.', comment='11 pages, 7 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.10515v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.10515v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.12118v1', updated=datetime.datetime(2021, 6, 23, 1, 19, 18, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 23, 1, 19, 18, tzinfo=datetime.timezone.utc), title='HDMM: Optimizing error of high-dimensional statistical queries under differential privacy', authors=[arxiv.Result.Author('Ryan McKenna'), arxiv.Result.Author('Gerome Miklau'), arxiv.Result.Author('Michael Hay'), arxiv.Result.Author('Ashwin Machanavajjhala')], summary='In this work we describe the High-Dimensional Matrix Mechanism (HDMM), a\\ndifferentially private algorithm for answering a workload of predicate counting\\nqueries. HDMM represents query workloads using a compact implicit matrix\\nrepresentation and exploits this representation to efficiently optimize over (a\\nsubset of) the space of differentially private algorithms for one that is\\nunbiased and answers the input query workload with low expected error. HDMM can\\nbe deployed for both $\\\\epsilon$-differential privacy (with Laplace noise) and\\n$(\\\\epsilon, \\\\delta)$-differential privacy (with Gaussian noise), although the\\ncore techniques are slightly different for each. We demonstrate empirically\\nthat HDMM can efficiently answer queries with lower expected error than\\nstate-of-the-art techniques, and in some cases, it nearly matches existing\\nlower bounds for the particular class of mechanisms we consider.', comment='arXiv admin note: text overlap with arXiv:1808.03537', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.12118v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.12118v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.12505v1', updated=datetime.datetime(2021, 6, 23, 16, 23, 18, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 23, 16, 23, 18, tzinfo=datetime.timezone.utc), title='Mr. Plotter: Unifying Data Reduction Techniques in Storage and Visualization Systems', authors=[arxiv.Result.Author('Sam Kumar'), arxiv.Result.Author('Michael P Andersen'), arxiv.Result.Author('David E. Culler')], summary='As the rate of data collection continues to grow rapidly, developing\\nvisualization tools that scale to immense data sets is a serious and\\never-increasing challenge. Existing approaches generally seek to decouple\\nstorage and visualization systems, performing just-in-time data reduction to\\ntransparently avoid overloading the visualizer. We present a new architecture\\nin which the visualizer and data store are tightly coupled. Unlike systems that\\nread raw data from storage, the performance of our system scales linearly with\\nthe size of the final visualization, essentially independent of the size of the\\ndata. Thus, it scales to massive data sets while supporting interactive\\nperformance (sub-100 ms query latency). This enables a new class of\\nvisualization clients that automatically manage data, quickly and transparently\\nrequesting data from the underlying database without requiring the user to\\nexplicitly initiate queries. It lays a groundwork for supporting truly\\ninteractive exploration of big data and opens new directions for research on\\nscalable information visualization systems.', comment='14 pages; Originally published in May 2018 as a technical report in\\n  the UC Berkeley EECS Technical Report Series (see\\n  https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-85.html)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.12505v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.12505v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.12765v1', updated=datetime.datetime(2021, 6, 24, 4, 25, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 24, 4, 25, 28, tzinfo=datetime.timezone.utc), title='A Novel Approach to Discover Switch Behaviours in Process Mining', authors=[arxiv.Result.Author('Yang Lu'), arxiv.Result.Author('Qifan Chen'), arxiv.Result.Author('Simon Poon')], summary='Process mining is a relatively new subject which builds a bridge between\\nprocess modelling and data mining. An exclusive choice in a process model\\nusually splits the process into different branches. However, in some processes,\\nit is possible to switch from one branch to another. The inductive miner\\nguarantees to return sound process models, but fails to return a precise model\\nwhen there are switch behaviours between different exclusive choice branches\\ndue to the limitation of process trees. In this paper, we present a novel\\nextension to the process tree model to support switch behaviours between\\ndifferent branches of the exclusive choice operator and propose a novel\\nextension to the inductive miner to discover sound process models with switch\\nbehaviours. The proposed discovery technique utilizes the theory of a previous\\nstudy to detect possible switch behaviours. We apply both artificial and\\npublicly-available datasets to evaluate our approach. Our results show that our\\napproach can improve the precision of discovered models by 36% while\\nmaintaining high fitness values compared to the original inductive miner.', comment='ICPM Workshop 2020', journal_ref='Process Mining Workshops. ICPM 2020. Lecture Notes in Business\\n  Information Processing, vol 406. Springer, Cham', doi='10.1007/978-3-030-72693-5_5', primary_category='cs.DB', categories=['cs.DB', 'cs.IR'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-030-72693-5_5', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2106.12765v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.12765v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.13342v2', updated=datetime.datetime(2022, 4, 14, 13, 46, 38, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 24, 22, 44, 30, tzinfo=datetime.timezone.utc), title='The Complexity of Boolean Conjunctive Queries with Intersection Joins', authors=[arxiv.Result.Author('Mahmoud Abo Khamis'), arxiv.Result.Author('George Chichirim'), arxiv.Result.Author('Antonia Kormpa'), arxiv.Result.Author('Dan Olteanu')], summary='Intersection joins over interval data are relevant in spatial and temporal\\ndata settings. A set of intervals join if their intersection is non-empty. In\\ncase of point intervals, the intersection join becomes the standard equality\\njoin.\\n  We establish the complexity of Boolean conjunctive queries with intersection\\njoins by a many-one equivalence to disjunctions of Boolean conjunctive queries\\nwith equality joins. The complexity of any query with intersection joins is\\nthat of the hardest query with equality joins in the disjunction exhibited by\\nour equivalence. This is captured by a new width measure called the IJ-width.\\n  We also introduce a new syntactic notion of acyclicity called iota-acyclicity\\nto characterise the class of Boolean queries with intersection joins that admit\\nlinear time computation modulo a poly-logarithmic factor in the data size.\\nIota-acyclicity is for intersection joins what alpha-acyclicity is for equality\\njoins. It strictly sits between gamma-acyclicity and Berge-acyclicity. The\\nintersection join queries that are not iota-acyclic are at least as hard as the\\nBoolean triangle query with equality joins, which is widely considered not\\ncomputable in linear time.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.13342v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.13342v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.14811v1', updated=datetime.datetime(2021, 6, 28, 15, 34, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 28, 15, 34, 3, tzinfo=datetime.timezone.utc), title='TOPIC: Top-k High-Utility Itemset Discovering', authors=[arxiv.Result.Author('Jiahui Chen'), arxiv.Result.Author('Shicheng Wan'), arxiv.Result.Author('Wensheng Gan'), arxiv.Result.Author('Guoting Chen'), arxiv.Result.Author('Hamido Fujita')], summary='Utility-driven itemset mining is widely applied in many real-world scenarios.\\nHowever, most algorithms do not work for itemsets with negative utilities.\\nSeveral efficient algorithms for high-utility itemset (HUI) mining with\\nnegative utilities have been proposed. These algorithms can find complete HUIs\\nwith or without negative utilities. However, the major problem with these\\nalgorithms is how to select an appropriate minimum utility (minUtil) threshold.\\nTo address this issue, some efficient algorithms for extracting top-k HUIs have\\nbeen proposed, where parameter k is the quantity of HUIs to be discovered.\\nHowever, all of these algorithms can solve only one part of the above problem.\\nIn this paper, we present a method for TOP-k high-utility Itemset disCovering\\n(TOPIC) with positive and negative utility values, which utilizes the\\nadvantages of the above algorithms. TOPIC adopts transaction merging and\\ndatabase projection techniques to reduce the database scanning cost, and\\nutilizes minUtil threshold raising strategies. It also uses an array-based\\nutility technique, which calculates the utility of itemsets and upper bounds in\\nlinear time. We conducted extensive experiments on several real and synthetic\\ndatasets, and the results showed that TOPIC outperforms state-of-the-art\\nalgorithm in terms of runtime, memory costs, and scalability.', comment='Preprint. 5 figures, 11 tables', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.14811v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.14811v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.14830v1', updated=datetime.datetime(2021, 6, 28, 16, 11, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 28, 16, 11, 54, tzinfo=datetime.timezone.utc), title='THUE: Discovering Top-K High Utility Episodes', authors=[arxiv.Result.Author('Shicheng Wan'), arxiv.Result.Author('Jiahui Chen'), arxiv.Result.Author('Wensheng Gan'), arxiv.Result.Author('Guoting Chen'), arxiv.Result.Author('Vikram Goyal')], summary='Episode discovery from an event is a popular framework for data mining tasks\\nand has many real-world applications. An episode is a partially ordered set of\\nobjects (e.g., item, node), and each object is associated with an event type.\\nThis episode can also be considered as a complex event sub-sequence.\\nHigh-utility episode mining is an interesting utility-driven mining task in the\\nreal world. Traditional episode mining algorithms, by setting a threshold,\\nusually return a huge episode that is neither intuitive nor saves time. In\\ngeneral, finding a suitable threshold in a pattern-mining algorithm is a\\ntrivial and time-consuming task. In this paper, we propose a novel algorithm,\\ncalled Top-K High Utility Episode (THUE) mining within the complex event\\nsequence, which redefines the previous mining task by obtaining the K highest\\nepisodes. We introduce several threshold-raising strategies and optimize the\\nepisode-weighted utilization upper bounds to speed up the mining process and\\neffectively reduce the memory cost. Finally, the experimental results on both\\nreal-life and synthetic datasets reveal that the THUE algorithm can offer six\\nto eight orders of magnitude running time performance improvement over the\\nstate-of-the-art algorithm and has low memory consumption.', comment='Preprint. 6 figures, 9 tables', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.14830v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.14830v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.15703v2', updated=datetime.datetime(2021, 11, 17, 16, 14, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 29, 20, 14, 35, tzinfo=datetime.timezone.utc), title='Threshold Queries in Theory and in the Wild', authors=[arxiv.Result.Author('Angela Bonifati'), arxiv.Result.Author('Stefania Dumbrava'), arxiv.Result.Author('George Fletcher'), arxiv.Result.Author('Jan Hidders'), arxiv.Result.Author('Matthias Hofer'), arxiv.Result.Author('Wim Martens'), arxiv.Result.Author('Filip Murlak'), arxiv.Result.Author('Joshua Shinavier'), arxiv.Result.Author('Sławek Staworko'), arxiv.Result.Author('Dominik Tomaszuk')], summary='Threshold queries are an important class of queries that only require\\ncomputing or counting answers up to a specified threshold value. To the best of\\nour knowledge, threshold queries have been largely disregarded in the research\\nliterature, which is surprising considering how common they are in practice. In\\nthis paper, we present a deep theoretical analysis of threshold query\\nevaluation and show that thresholds can be used to significantly improve the\\nasymptotic bounds of state-of-the-art query evaluation algorithms. We also\\nempirically show that threshold queries are significant in practice. In\\nsurprising contrast to conventional wisdom, we found important scenarios in\\nreal-world data sets in which users are interested in computing the results of\\nqueries up to a certain threshold, independent of a ranking function that\\norders the query results by importance.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.15703v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.15703v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2106.16166v2', updated=datetime.datetime(2021, 11, 22, 11, 33, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 30, 16, 0, 47, tzinfo=datetime.timezone.utc), title='A Critical Analysis of Recursive Model Indexes', authors=[arxiv.Result.Author('Marcel Maltry'), arxiv.Result.Author('Jens Dittrich')], summary=\"The recursive model index (RMI) has recently been introduced as a\\nmachine-learned replacement for traditional indexes over sorted data, achieving\\nremarkably fast lookups. Follow-up work focused on explaining RMI's performance\\nand automatically configuring RMIs through enumeration. Unfortunately,\\nconfiguring RMIs involves setting several hyperparameters, the enumeration of\\nwhich is often too time-consuming in practice. Therefore, in this work, we\\nconduct the first inventor-independent broad analysis of RMIs with the goal of\\nunderstanding the impact of each hyperparameter on performance. In particular,\\nwe show that in addition to model types and layer size, error bounds and search\\nalgorithms must be considered to achieve the best possible performance. Based\\non our findings, we develop a simple-to-follow guideline for configuring RMIs.\\nWe evaluate our guideline by comparing the resulting RMIs with a number of\\nstate-of-the-art indexes, both learned and traditional. We show that our simple\\nguideline is sufficient to achieve competitive performance with other learned\\nindexes and RMIs whose configuration was determined using an expensive\\nenumeration procedure. In addition, while carefully reimplementing RMIs, we are\\nable to improve the build time by 2.5x to 6.3x.\", comment='12 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.3.1'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.16166v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.16166v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.00224v1', updated=datetime.datetime(2018, 3, 31, 23, 16, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 3, 31, 23, 16, 1, tzinfo=datetime.timezone.utc), title='A comparative analysis of state-of-the-art SQL-on-Hadoop systems for interactive analytics', authors=[arxiv.Result.Author('Ashish Tapdiya'), arxiv.Result.Author('Daniel Fabbri')], summary='Hadoop is emerging as the primary data hub in enterprises, and SQL represents\\nthe de facto language for data analysis. This combination has led to the\\ndevelopment of a variety of SQL-on-Hadoop systems in use today. While the\\nvarious SQL-on-Hadoop systems target the same class of analytical workloads,\\ntheir different architectures, design decisions and implementations impact\\nquery performance. In this work, we perform a comparative analysis of four\\nstate-of-the-art SQL-on-Hadoop systems (Impala, Drill, Spark SQL and Phoenix)\\nusing the Web Data Analytics micro benchmark and the TPC-H benchmark on the\\nAmazon EC2 cloud platform. The TPC-H experiment results show that, although\\nImpala outperforms other systems (4.41x - 6.65x) in the text format, trade-offs\\nexists in the parquet format, with each system performing best on subsets of\\nqueries. A comprehensive analysis of execution profiles expands upon the\\nperformance results to provide insights into performance variations,\\nperformance bottlenecks and query execution characteristics.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1804.00224v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.00224v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.00370v2', updated=datetime.datetime(2018, 9, 13, 20, 44, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 2, 1, 51, 10, tzinfo=datetime.timezone.utc), title='Differentially Private Hierarchical Count-of-Counts Histograms', authors=[arxiv.Result.Author('Yu-Hsuan Kuo'), arxiv.Result.Author('Cho-Chun Chiu'), arxiv.Result.Author('Daniel Kifer'), arxiv.Result.Author('Michael Hay'), arxiv.Result.Author('Ashwin Machanavajjhala')], summary='We consider the problem of privately releasing a class of queries that we\\ncall hierarchical count-of-counts histograms. Count-of-counts histograms\\npartition the rows of an input table into groups (e.g., group of people in the\\nsame household), and for every integer j report the number of groups of size j.\\nHierarchical count-of-counts queries report count-of-counts histograms at\\ndifferent granularities as per hierarchy defined on an attribute in the input\\ndata (e.g., geographical location of a household at the national, state and\\ncounty levels). In this paper, we introduce this problem, along with\\nappropriate error metrics and propose a differentially private solution that\\ngenerates count-of-counts histograms that are consistent across all levels of\\nthe hierarchy.', comment='13 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1804.00370v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.00370v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.00443v1', updated=datetime.datetime(2018, 4, 2, 10, 22, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 2, 10, 22, 51, tzinfo=datetime.timezone.utc), title='A Note on the Hardness of the Critical Tuple Problem', authors=[arxiv.Result.Author('Egor V. Kostylev'), arxiv.Result.Author('Dan Suciu')], summary='The notion of critical tuple was introduced by Miklau and Suciu (Gerome\\nMiklau and Dan Suciu. A formal analysis of information disclosure in data\\nexchange. J. Comput. Syst. Sci., 73(3):507-534, 2007), who also claimed that\\nthe problem of checking whether a tuple is non-critical is complete for the\\nsecond level of the polynomial hierarchy. Kostylev identified an error in the\\n12-page-long hardness proof. It turns out that the issue is rather fundamental:\\nthe proof can be adapted to show hardness of a relative variant of\\ntuple-non-criticality, but we have neither been able to prove the original\\nclaim nor found an algorithm for it of lower complexity. In this note we state\\nformally the relative variant and present an alternative, simplified proof of\\nits hardness; we also give an NP-hardness proof for the original problem, the\\nbest lower bound we have been able to show. Hence, the precise complexity of\\nthe original critical tuple problem remains open.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1804.00443v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.00443v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.01379v1', updated=datetime.datetime(2018, 3, 19, 4, 37, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 3, 19, 4, 37, 33, tzinfo=datetime.timezone.utc), title='Mining User Behavioral Rules from Smartphone Data through Association Analysis', authors=[arxiv.Result.Author('Iqbal H. Sarker'), arxiv.Result.Author('Flora D. Salim')], summary='The increasing popularity of smart mobile phones and their powerful sensing\\ncapabilities have enabled the collection of rich contextual information and\\nmobile phone usage records through the device logs. This paper formulates the\\nproblem of mining behavioral association rules of individual mobile phone users\\nutilizing their smartphone data. Association rule learning is the most popular\\ntechnique to discover rules utilizing large datasets. However, it is well-known\\nthat a large proportion of association rules generated are redundant. This\\nredundant production makes not only the rule-set unnecessarily large but also\\nmakes the decision making process more complex and ineffective. In this paper,\\nwe propose an approach that effectively identifies the redundancy in\\nassociations and extracts a concise set of behavioral association rules that\\nare non-redundant. The effectiveness of the proposed approach is examined by\\nconsidering the real mobile phone datasets of individual users.', comment='12 pages, Springer, The 22nd Pacific-Asia Conference on Knowledge\\n  Discovery and Data Mining (PAKDD 2018), Melbourne, Australia', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.SE'], links=[arxiv.Result.Link('http://arxiv.org/abs/1804.01379v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.01379v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.02593v1', updated=datetime.datetime(2018, 4, 7, 21, 23, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 7, 21, 23, 16, tzinfo=datetime.timezone.utc), title='IDEBench: A Benchmark for Interactive Data Exploration', authors=[arxiv.Result.Author('Philipp Eichmann'), arxiv.Result.Author('Carsten Binnig'), arxiv.Result.Author('Tim Kraska'), arxiv.Result.Author('Emanuel Zgraggen')], summary='Existing benchmarks for analytical database systems such as TPC-DS and TPC-H\\nare designed for static reporting scenarios. The main metric of these\\nbenchmarks is the performance of running individual SQL queries over a\\nsynthetic database. In this paper, we argue that such benchmarks are not\\nsuitable for evaluating database workloads originating from interactive data\\nexploration (IDE) systems where most queries are ad-hoc, not based on\\npredefined reports, and built incrementally. As a main contribution, we present\\na novel benchmark called IDEBench that can be used to evaluate the performance\\nof database systems for IDE workloads. As opposed to traditional benchmarks for\\nanalytical database systems, our goal is to provide more meaningful workloads\\nand datasets that can be used to benchmark IDE query engines, with a particular\\nfocus on metrics that capture the trade-off between query performance and\\nquality of the result. As a second contribution, this paper evaluates and\\ndiscusses the performance results of selected IDE query engines using our\\nbenchmark. The study includes two commercial systems, as well as two research\\nprototypes (IDEA, approXimateDB/XDB), and one traditional analytical database\\nsystem (MonetDB).', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1804.02593v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.02593v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.02780v2', updated=datetime.datetime(2019, 3, 25, 7, 40, 37, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 9, 0, 51, 11, tzinfo=datetime.timezone.utc), title='Counting Triangles under Updates in Worst-Case Optimal Time', authors=[arxiv.Result.Author('Ahmet Kara'), arxiv.Result.Author('Hung Q. Ngo'), arxiv.Result.Author('Milos Nikolic'), arxiv.Result.Author('Dan Olteanu'), arxiv.Result.Author('Haozhe Zhang')], summary='We consider the problem of incrementally maintaining the triangle count query\\nunder single-tuple updates to the input relations. We introduce an approach\\nthat exhibits a space-time tradeoff such that the space-time product is\\nquadratic in the size of the input database and the update time can be as low\\nas the square root of this size. This lowest update time is worst-case optimal\\nconditioned on the Online Matrix-Vector Multiplication conjecture. The\\nclassical and factorized incremental view maintenance approaches are recovered\\nas special cases of our approach within the space-time tradeoff. In particular,\\nthey require linear-time update maintenance, which is suboptimal. Our approach\\nalso recovers the worst-case optimal time complexity for computing the triangle\\ncount in the non-incremental setting.', comment='simplified notation; incremental maintenance of full triangle query,\\n  4-path count query, count queries with three relations added; improved the\\n  space complexity of the dynamic algorithm maintaining the triangle count\\n  query', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2.4'], links=[arxiv.Result.Link('http://arxiv.org/abs/1804.02780v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.02780v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.04260v1', updated=datetime.datetime(2018, 4, 12, 0, 4, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 12, 0, 4, 5, tzinfo=datetime.timezone.utc), title='Graph Pattern Matching Preserving Label-Repetition Constraints', authors=[arxiv.Result.Author('Houari Mahfoud')], summary='Graph pattern matching is a routine process for a wide variety of\\napplications such as social network analysis. It is typically defined in terms\\nof subgraph isomorphism which is NP-Complete. To lower its complexity, many\\nextensions of graph simulation have been proposed which focus on some\\ntopological constraints of pattern graphs that can be preserved in\\npolynomial-time over data graphs. We discuss in this paper the satisfaction of\\na new topological constraint, called Label-Repetition constraint. To the best\\nof our knowledge, existing polynomial approaches fail to preserve this\\nconstraint, and moreover, one can adopt only subgraph isomorphism for this end\\nwhich is cost-prohibitive. We present first a necessary and sufficient\\ncondition that a data subgraph must satisfy to preserve the Label-Repetition\\nconstraints of the pattern graph. Furthermore, we define matching based on a\\nnotion of triple simulation, an extension of graph simulation by considering\\nthe new topological constraint. We show that with this extension, graph pattern\\nmatching can be performed in polynomial-time, by providing such an algorithm.\\nOur algorithm is sub-quadratic in the size of data graphs only, and quartic in\\ngeneral. We show that our results can be combined with orthogonal approaches\\nfor more expressive graph pattern matching.', comment='19 pages, 4 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://arxiv.org/abs/1804.04260v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.04260v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.05639v1', updated=datetime.datetime(2018, 4, 16, 12, 37, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 16, 12, 37, 35, tzinfo=datetime.timezone.utc), title='NELL2RDF: Reading the Web, and Publishing it as Linked Data', authors=[arxiv.Result.Author('José M. Giménez-García'), arxiv.Result.Author('Maísa Duarte'), arxiv.Result.Author('Antoine Zimmermann'), arxiv.Result.Author('Christophe Gravier'), arxiv.Result.Author('Estevam R. Hruschke Jr.'), arxiv.Result.Author('Pierre Maret')], summary=\"NELL is a system that continuously reads the Web to extract knowledge in form\\nof entities and relations between them. It has been running since January 2010\\nand extracted over 50,000,000 candidate statements. NELL's generated data\\ncomprises all the candidate statements together with detailed information about\\nhow it was generated. This information includes how each component of the\\nsystem contributed to the extraction of the statement, as well as when that\\nhappened and how confident the system is in the veracity of the statement.\\nHowever, the data is only available in an ad hoc CSV format that makes it\\ndifficult to exploit out of the context of NELL. In order to make it more\\nusable for other communities, we adopt Linked Data principles to publish a more\\nstandardized, self-describing dataset with rich provenance metadata.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1804.05639v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.05639v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.06653v1', updated=datetime.datetime(2018, 4, 18, 11, 19, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 18, 11, 19, 1, tzinfo=datetime.timezone.utc), title='Consensus Community Detection in Multilayer Networks using Parameter-free Graph Pruning', authors=[arxiv.Result.Author('Domenico Mandaglio'), arxiv.Result.Author('Alessia Amelio'), arxiv.Result.Author('Andrea Tagarelli')], summary='The clustering ensemble paradigm has emerged as an effective tool for\\ncommunity detection in multilayer networks, which allows for producing\\nconsensus solutions that are designed to be more robust to the algorithmic\\nselection and configuration bias. However, one limitation is related to the\\ndependency on a co-association threshold that controls the degree of consensus\\nin the community structure solution. The goal of this work is to overcome this\\nlimitation with a new framework of ensemble-based multilayer community\\ndetection, which features parameter-free identification of consensus\\ncommunities based on generative models of graph pruning that are able to filter\\nout noisy co-associations. We also present an enhanced version of the\\nmodularity-driven ensemble-based multilayer community detection method, in\\nwhich community memberships of nodes are reconsidered to optimize the\\nmultilayer modularity of the consensus solution. Experimental evidence on\\nreal-world networks confirms the beneficial effect of using model-based\\nfiltering methods and also shows the superiority of the proposed method on\\nstate-of-the-art multilayer community detection.', comment='Accepted as regular paper at The 22nd Pacific-Asia Conference on\\n  Knowledge Discovery and Data Mining (PAKDD 2018)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.SI', 'physics.data-an'], links=[arxiv.Result.Link('http://arxiv.org/abs/1804.06653v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.06653v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.06829v2', updated=datetime.datetime(2018, 4, 23, 13, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 18, 17, 28, 48, tzinfo=datetime.timezone.utc), title='HD-Index: Pushing the Scalability-Accuracy Boundary for Approximate kNN Search in High-Dimensional Spaces', authors=[arxiv.Result.Author('Akhil Arora'), arxiv.Result.Author('Sakshi Sinha'), arxiv.Result.Author('Piyush Kumar'), arxiv.Result.Author('Arnab Bhattacharya')], summary='Nearest neighbor searching of large databases in high-dimensional spaces is\\ninherently difficult due to the curse of dimensionality. A flavor of\\napproximation is, therefore, necessary to practically solve the problem of\\nnearest neighbor search. In this paper, we propose a novel yet simple indexing\\nscheme, HD-Index, to solve the problem of approximate k-nearest neighbor\\nqueries in massive high-dimensional databases. HD-Index consists of a set of\\nnovel hierarchical structures called RDB-trees built on Hilbert keys of\\ndatabase objects. The leaves of the RDB-trees store distances of database\\nobjects to reference objects, thereby allowing efficient pruning using distance\\nfilters. In addition to triangular inequality, we also use Ptolemaic inequality\\nto produce better lower bounds. Experiments on massive (up to billion scale)\\nhigh-dimensional (up to 1000+) datasets show that HD-Index is effective,\\nefficient, and scalable.', comment='PVLDB 11(8):906-919, 2018', journal_ref=None, doi='10.14778/3204028.3204034', primary_category='cs.DB', categories=['cs.DB', 'H.2.4'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3204028.3204034', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1804.06829v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.06829v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.06894v1', updated=datetime.datetime(2018, 4, 18, 19, 49, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 18, 19, 49, 15, tzinfo=datetime.timezone.utc), title='Dichotomies in Ontology-Mediated Querying with the Guarded Fragment', authors=[arxiv.Result.Author('Andre Hernich'), arxiv.Result.Author('Carsten Lutz'), arxiv.Result.Author('Fabio Papacchini'), arxiv.Result.Author('Frank Wolter')], summary=\"We study the complexity of ontology-mediated querying when ontologies are\\nformulated in the guarded fragment of first-order logic (GF). Our general aim\\nis to classify the data complexity on the level of ontologies where query\\nevaluation w.r.t. an ontology O is considered to be in PTime if all (unions of\\nconjunctive) queries can be evaluated in PTime w.r.t. O and coNP-hard if at\\nleast one query is coNP-hard w.r.t. O. We identify several large and relevant\\nfragments of GF that enjoy a dichotomy between PTime and coNP, some of them\\nadditionally admitting a form of counting. In fact, almost all ontologies in\\nthe BioPortal repository fall into these fragments or can easily be rewritten\\nto do so. We then establish a variation of Ladner's Theorem on the existence of\\nNP-intermediate problems and use this result to show that for other fragments,\\nthere is provably no such dichotomy. Again for other fragments (such as full\\nGF), establishing a dichotomy implies the Feder-Vardi conjecture on the\\ncomplexity of constraint satisfaction problems. We also link these results to\\nDatalog-rewritability and study the decidability of whether a given ontology\\nenjoys PTime query evaluation, presenting both positive and negative results.\", comment=None, journal_ref=None, doi='10.1145/3034786.3056108', primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LO'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3034786.3056108', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1804.06894v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.06894v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.07156v1', updated=datetime.datetime(2018, 4, 17, 22, 35, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 17, 22, 35, 47, tzinfo=datetime.timezone.utc), title='Heuristic and Cost-based Optimization for Diverse Provenance Tasks', authors=[arxiv.Result.Author('Xing Niu'), arxiv.Result.Author('Raghav Kapoor'), arxiv.Result.Author('Boris Glavic'), arxiv.Result.Author('Dieter Gawlick'), arxiv.Result.Author('Zhen Hua Liu'), arxiv.Result.Author('Vasudha Krishnaswamy'), arxiv.Result.Author('Venkatesh Radhakrishnan')], summary='A well-established technique for capturing database provenance as annotations\\non data is to instrument queries to propagate such annotations. However, even\\nsophisticated query optimizers often fail to produce efficient execution plans\\nfor instrumented queries. We develop provenance-aware optimization techniques\\nto address this problem. Specifically, we study algebraic equivalences targeted\\nat instrumented queries and alternative ways of instrumenting queries for\\nprovenance capture. Furthermore, we present an extensible heuristic and\\ncost-based optimization framework utilizing these optimizations. Our\\nexperiments confirm that these optimizations are highly effective, improving\\nperformance by several orders of magnitude for diverse provenance tasks.', comment='IEEE Transactions on Knowledge and Data Engineering (TKDE), 2018,\\n  long version, 31 pages. arXiv admin note: substantial text overlap with\\n  arXiv:1701.05513', journal_ref='IEEE Transactions on Knowledge and Data Engineering (TKDE), 2018', doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1804.07156v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.07156v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.07525v1', updated=datetime.datetime(2018, 4, 20, 10, 1, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 20, 10, 1, 43, tzinfo=datetime.timezone.utc), title='Benchmarking Top-K Keyword and Top-K Document Processing with T${}^2$K${}^2$ and T${}^2$K${}^2$D${}^2$', authors=[arxiv.Result.Author('Ciprian-Octavian Truica'), arxiv.Result.Author('Jérôme Darmont'), arxiv.Result.Author('Alexandru Boicea'), arxiv.Result.Author('Florin Radulescu')], summary=\"Top-k keyword and top-k document extraction are very popular text analysis\\ntechniques. Top-k keywords and documents are often computed on-the-fly, but\\nthey exploit weighted vocabularies that are costly to build. To compare\\ncompeting weighting schemes and database implementations, benchmarking is\\ncustomary. To the best of our knowledge, no benchmark currently addresses these\\nproblems. Hence, in this paper, we present T${}^2$K${}^2$, a top-k keywords and\\ndocuments benchmark, and its decision support-oriented evolution\\nT${}^2$K${}^2$D${}^2$. Both benchmarks feature a real tweet dataset and queries\\nwith various complexities and selectivities. They help evaluate weighting\\nschemes and database implementations in terms of computing performance. To\\nillustrate our bench-marks' relevance and genericity, we successfully ran\\nperformance tests on the TF-IDF and Okapi BM25 weighting schemes, on one hand,\\nand on different relational (Oracle, PostgreSQL) and document-oriented\\n(MongoDB) database implementations, on the other hand.\", comment=None, journal_ref='Future Generation Computer Systems, Elsevier, 2018, 85, pp.60-75.\\n  https://www.sciencedirect.com/science/article/pii/S0167739X17323580', doi='10.1016/j.future.2018.02.037', primary_category='cs.DB', categories=['cs.DB', 'cs.IR'], links=[arxiv.Result.Link('http://dx.doi.org/10.1016/j.future.2018.02.037', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1804.07525v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.07525v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.07550v1', updated=datetime.datetime(2018, 4, 20, 11, 11, 36, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 20, 11, 11, 36, tzinfo=datetime.timezone.utc), title='Specialty-Aware Task Assignment in Spatial Crowdsourcing', authors=[arxiv.Result.Author('Tianshu Song'), arxiv.Result.Author('Feng Zhu'), arxiv.Result.Author('Ke Xu')], summary=\"With the rapid development of Mobile Internet, spatial crowdsourcing is\\ngaining more and more attention from both academia and industry.\\n  In spatial crowdsourcing, spatial tasks are sent to workers based on their\\nlocations.\\n  A wide kind of tasks in spatial crowdsourcing are specialty-aware, which are\\ncomplex and need to be completed by workers with different skills\\ncollaboratively.\\n  Existing studies on specialty-aware spatial crowdsourcing assume that each\\nworker has a united charge when performing different tasks, no matter how many\\nskills of her/him are used to complete the task, which is not fair and\\npractical.\\n  In this paper, we study the problem of specialty-aware task assignment in\\nspatial crowdsourcing, where each worker has fine-grained charge for each of\\ntheir skills, and the goal is to maximize the total number of completed tasks\\nbased on tasks' budget and requirements on particular skills.\\n  The problem is proven to be NP-hard. Thus, we propose two efficient\\nheuristics to solve the problem.\\n  Experiments on both synthetic and real datasets demonstrate the effectiveness\\nand efficiency of our solutions.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1804.07550v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.07550v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.08822v1', updated=datetime.datetime(2018, 4, 24, 3, 2, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 24, 3, 2, 54, tzinfo=datetime.timezone.utc), title='In-Browser Split-Execution Support for Interactive Analytics in the Cloud', authors=[arxiv.Result.Author('Kareem El Gebaly'), arxiv.Result.Author('Jimmy Lin')], summary='The canonical analytics architecture today consists of a browser connected to\\na backend in the cloud. In all deployments that we are aware of, the browser is\\nsimply a dumb rendering endpoint. As an alternative, this paper explores\\nsplit-execution architectures that push analytics capabilities into the\\nbrowser. We show that, by taking advantage of typed arrays and asm.js, it is\\npossible to build an analytical RDBMS in JavaScript that runs in a browser,\\nachieving performance rivaling native databases. To support interactive data\\nexploration, our Afterburner prototype automatically generates local\\nmaterialized views from a backend database that are then shipped to the browser\\nto facilitate subsequent interactions seamlessly and efficiently. We compare\\nthis architecture to several alternative deployments, experimentally\\ndemonstrating performance parity, while at the same time providing additional\\nadvantages in terms of administrative and operational simplicity.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1804.08822v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.08822v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.09324v1', updated=datetime.datetime(2018, 4, 25, 2, 30, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 25, 2, 30, 43, tzinfo=datetime.timezone.utc), title='Processing Database Joins over a Shared-Nothing System of Multicore Machines', authors=[arxiv.Result.Author('Abhirup Chakraborty')], summary='To process a large volume of data, modern data management systems use a\\ncollection of machines connected through a network. This paper looks into the\\nfeasibility of scaling up such a shared-nothing system while processing a\\ncompute- and communication-intensive workload---processing distributed joins.\\nBy exploiting multiple processing cores within the individual machines, we\\nimplement a system to process database joins that parallelizes computation\\nwithin each node, pipelines the computation with communication, parallelizes\\nthe communication by allowing multiple simultaneous data transfers\\n(send/receive), and removes synchronization barriers (a scalability bottleneck\\nin a distributed data processing system). Our experimental results show that\\nusing only four threads per node the framework achieves a 3.5x gains in\\nintra-node performance while compared with a single-threaded counterpart.\\nMoreover, with the join processing workload the cluster-wide performance (and\\nspeedup) is observed to be dictated by the intra-node computational loads; this\\nproperty brings a near-linear speedup with increasing nodes in the system, a\\nfeature much desired in modern large-scale data processing system.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1804.09324v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.09324v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.10388v1', updated=datetime.datetime(2018, 4, 27, 8, 33, 12, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 27, 8, 33, 12, tzinfo=datetime.timezone.utc), title='Event Forecasting with Pattern Markov Chains', authors=[arxiv.Result.Author('Elias Alevizos'), arxiv.Result.Author('Alexander Artikis'), arxiv.Result.Author('Georgios Paliouras')], summary='We present a system for online probabilistic event forecasting. We assume\\nthat a user is interested in detecting and forecasting event patterns, given in\\nthe form of regular expressions. Our system can consume streams of events and\\nforecast when the pattern is expected to be fully matched. As more events are\\nconsumed, the system revises its forecasts to reflect possible changes in the\\nstate of the pattern. The framework of Pattern Markov Chains is used in order\\nto learn a probabilistic model for the pattern, with which forecasts with\\nguaranteed precision may be produced, in the form of intervals within which a\\nfull match is expected. Experimental results from real-world datasets are shown\\nand the quality of the produced forecasts is explored, using both precision\\nscores and two other metrics: spread, which refers to the \"focusing resolution\"\\nof a forecast (interval length), and distance, which captures how early a\\nforecast is reported.', comment=None, journal_ref=None, doi='10.1145/3093742.3093920', primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3093742.3093920', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1804.10388v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.10388v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1804.11052v1', updated=datetime.datetime(2018, 4, 30, 5, 45, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 30, 5, 45, 39, tzinfo=datetime.timezone.utc), title='Relational to RDF Data Exchange in Presence of a Shape Expression Schema', authors=[arxiv.Result.Author('Iovka Boneva'), arxiv.Result.Author('Jose Lozano'), arxiv.Result.Author('Sławek Staworko')], summary='We study the relational to RDF data exchange problem, where the tar- get\\nconstraints are specified using Shape Expression schema (ShEx). We investi-\\ngate two fundamental problems: 1) consistency which is checking for a given\\ndata exchange setting whether there always exists a solution for any source\\ninstance, and 2) constructing a universal solution which is a solution that\\nrepresents the space of all solutions. We propose to use typed IRI constructors\\nin source-to- target tuple generating dependencies to create the IRIs of the\\nRDF graph from the values in the relational instance, and we translate ShEx\\ninto a set of target dependencies. We also identify data exchange settings that\\nare key covered, a property that is decidable and guarantees consistency.\\nFurthermore, we show that this property is a sufficient and necessary condition\\nfor the existence of universal solutions for a practical subclass of\\nweakly-recursive ShEx.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1804.11052v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.11052v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.01046v2', updated=datetime.datetime(2019, 12, 9, 8, 32, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 2, 22, 30, 22, tzinfo=datetime.timezone.utc), title='BlazeIt: Optimizing Declarative Aggregation and Limit Queries for Neural Network-Based Video Analytics', authors=[arxiv.Result.Author('Daniel Kang'), arxiv.Result.Author('Peter Bailis'), arxiv.Result.Author('Matei Zaharia')], summary=\"Recent advances in neural networks (NNs) have enabled automatic querying of\\nlarge volumes of video data with high accuracy. While these deep NNs can\\nproduce accurate annotations of an object's position and type in video, they\\nare computationally expensive and require complex, imperative deployment code\\nto answer queries. Prior work uses approximate filtering to reduce the cost of\\nvideo analytics, but does not handle two important classes of queries,\\naggregation and limit queries; moreover, these approaches still require complex\\ncode to deploy. To address the computational and usability challenges of\\nquerying video at scale, we introduce BlazeIt, a system that optimizes queries\\nof spatiotemporal information of objects in video. BlazeIt accepts queries via\\nFrameQL, a declarative extension of SQL for video analytics that enables\\nvideo-specific query optimization. We introduce two new query optimization\\ntechniques in BlazeIt that are not supported by prior work. First, we develop\\nmethods of using NNs as control variates to quickly answer approximate\\naggregation queries with error bounds. Second, we present a novel search\\nalgorithm for cardinality-limited video queries. Through these these\\noptimizations, BlazeIt can deliver up to 83x speedups over the recent\\nliterature on video processing.\", comment=None, journal_ref='PVLDB 2020', doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.01046v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.01046v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.01083v1', updated=datetime.datetime(2018, 5, 3, 1, 57, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 3, 1, 57, 31, tzinfo=datetime.timezone.utc), title='Scalable Semantic Querying of Text', authors=[arxiv.Result.Author('Xiaolan Wang'), arxiv.Result.Author('Aaron Feng'), arxiv.Result.Author('Behzad Golshan'), arxiv.Result.Author('Alon Halevy'), arxiv.Result.Author('George Mihaila'), arxiv.Result.Author('Hidekazu Oiwa'), arxiv.Result.Author('Wang-Chiew Tan')], summary=\"We present the KOKO system that takes declarative information extraction to a\\nnew level by incorporating advances in natural language processing techniques\\nin its extraction language. KOKO is novel in that its extraction language\\nsimultaneously supports conditions on the surface of the text and on the\\nstructure of the dependency parse tree of sentences, thereby allowing for more\\nrefined extractions. KOKO also supports conditions that are forgiving to\\nlinguistic variation of expressing concepts and allows to aggregate evidence\\nfrom the entire document in order to filter extractions.\\n  To scale up, KOKO exploits a multi-indexing scheme and heuristics for\\nefficient extractions. We extensively evaluate KOKO over publicly available\\ntext corpora. We show that KOKO indices take up the smallest amount of space,\\nare notably faster and more effective than a number of prior indexing schemes.\\nFinally, we demonstrate KOKO's scale up on a corpus of 5 million Wikipedia\\narticles.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.01083v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.01083v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.01825v1', updated=datetime.datetime(2018, 5, 3, 9, 57, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 3, 9, 57, 13, tzinfo=datetime.timezone.utc), title='Simplified SPARQL REST API - CRUD on JSON Object Graphs via URI Paths', authors=[arxiv.Result.Author('Markus Schröder'), arxiv.Result.Author('Jörn Hees'), arxiv.Result.Author('Ansgar Bernardi'), arxiv.Result.Author('Daniel Ewert'), arxiv.Result.Author('Peter Klotz'), arxiv.Result.Author('Steffen Stadtmüller')], summary='Within the Semantic Web community, SPARQL is one of the predominant languages\\nto query and update RDF knowledge. However, the complexity of SPARQL, the\\nunderlying graph structure and various encodings are common sources of\\nconfusion for Semantic Web novices.\\n  In this paper we present a general purpose approach to convert any given\\nSPARQL endpoint into a simple to use REST API. To lower the initial hurdle, we\\nrepresent the underlying graph as an interlinked view of nested JSON objects\\nthat can be traversed by the API path.', comment='5 pages, 2 figures, ESWC 2018 demo paper', journal_ref='The Semantic Web: ESWC 2018 Satellite Events - ESWC 2018 Satellite\\n  Events, Heraklion, Crete, Greece, June 3-7, 2018, Revised Selected Papers', doi='10.1007/978-3-319-98192-5_8', primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-319-98192-5_8', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1805.01825v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.01825v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.02009v1', updated=datetime.datetime(2018, 5, 5, 5, 43, 49, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 5, 5, 43, 49, tzinfo=datetime.timezone.utc), title='Efficient Top K Temporal Spatial Keyword Search', authors=[arxiv.Result.Author('Chengyuan Zhang'), arxiv.Result.Author('Lei Zhu'), arxiv.Result.Author('Weiren Yu'), arxiv.Result.Author('Jun Long'), arxiv.Result.Author('Fang Huang'), arxiv.Result.Author('Hongbo Zhao')], summary='Massive amount of data that are geo-tagged and associated with text\\ninformation are being generated at an unprecedented scale in many emerging\\napplications such as location based services and social networks. Due to their\\nimportance, a large body of work has focused on efficiently computing various\\nspatial keyword queries. In this paper,we study the top-$k$ temporal spatial\\nkeyword query which considers three important constraints during the search\\nincluding time, spatial proximity and textual relevance. A novel index\\nstructure, namely SSG-tree, to efficiently insert/delete spatio-temporal web\\nobjects with high rates. Base on SSG-tree an efficient algorithm is developed\\nto support top-k temporal spatial keyword query. We show via extensive\\nexperimentation with real spatial databases that our method has increased\\nperformance over alternate techniques', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.02009v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.02009v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.04156v1', updated=datetime.datetime(2018, 5, 10, 20, 5, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 10, 20, 5, 59, tzinfo=datetime.timezone.utc), title='Computational Social Choice Meets Databases', authors=[arxiv.Result.Author('Benny Kimelfeld'), arxiv.Result.Author('Phokion G. Kolaitis'), arxiv.Result.Author('Julia Stoyanovich')], summary='We develop a novel framework that aims to create bridges between the\\ncomputational social choice and the database management communities. This\\nframework enriches the tasks currently supported in computational social choice\\nwith relational database context, thus making it possible to formulate\\nsophisticated queries about voting rules, candidates, voters, issues, and\\npositions. At the conceptual level, we give rigorous semantics to queries in\\nthis framework by introducing the notions of necessary answers and possible\\nanswers to queries. At the technical level, we embark on an investigation of\\nthe computational complexity of the necessary answers. We establish a number of\\nresults about the complexity of the necessary answers of conjunctive queries\\ninvolving positional scoring rules that contrast sharply with earlier results\\nabout the complexity of the necessary winners.', comment='This is an extended version of \"Computational Social Choice Meets\\n  Databases\" by Kimelfeld, Kolaitis and Stoyanovich, to appear in IJCAI 2018', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.04156v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.04156v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.04642v1', updated=datetime.datetime(2018, 5, 12, 3, 8, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 12, 3, 8, 53, tzinfo=datetime.timezone.utc), title='HOC-Tree: A Novel Index for efficient Spatio-temporal Range Search', authors=[arxiv.Result.Author('Jun Long'), arxiv.Result.Author('Lei Zhu'), arxiv.Result.Author('Chengyuan Zhang'), arxiv.Result.Author('Shuangqiao Lin'), arxiv.Result.Author('Zhan Yang'), arxiv.Result.Author('Xinpan Yuan')], summary='With the rapid development of mobile computing and Web services, a huge\\namount of data with spatial and temporal information have been collected\\neveryday by smart mobile terminals, in which an object is described by its\\nspatial information and temporal information. Motivated by the significance of\\nspatio-temporal range search and the lack of efficient search algorithm, in\\nthis paper, we study the problem of spatio-temporal range search (STRS), a\\nnovel index structure is proposed, called HOC-Tree, which is based on Hilbert\\ncurve and OC-Tree, and takes both spatial and temporal information into\\nconsideration. Based on HOC-Tree, we develop an efficient algorithm to solve\\nthe problem of spatio-temporal range search. Comprehensive experiments on real\\nand synthetic data demonstrate that our method is more efficient than the\\nstate-of-the-art technique.', comment='12 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.04642v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.04642v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.05514v1', updated=datetime.datetime(2018, 5, 15, 1, 18, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 15, 1, 18, 51, tzinfo=datetime.timezone.utc), title='Incremental Database Design using UML-B and Event-B', authors=[arxiv.Result.Author('Ahmed Al-Brashdi'), arxiv.Result.Author('Michael Butler'), arxiv.Result.Author('Abdolbaghi Rezazadeh')], summary='Correct operation of many critical systems is dependent on the data\\nconsistency and integrity properties of underlying databases. Therefore, a\\nverifiable and rigorous database design process is highly desirable. This\\nresearch aims to investigate and deliver a comprehensive and practical approach\\nfor modelling databases in formal methods through layered refinements. The\\nmethodology is being guided by a number of case studies, using abstraction and\\nrefinement in UML-B and verification with the Rodin tool. UML-B is a graphical\\nrepresentation of the Event-B formalism and the Rodin tool supports\\nverification for Event-B and UML-B. Our method guides developers to model\\nrelational databases in UML-B through layered refinement and to specify the\\nnecessary constraints and operations on the database.', comment='In Proceedings IMPEX 2017 and FM&MDD 2017, arXiv:1805.04636', journal_ref='EPTCS 271, 2018, pp. 34-47', doi='10.4204/EPTCS.271.3', primary_category='cs.DB', categories=['cs.DB', 'cs.LO', 'cs.SE', 'D.2 SOFTWARE ENGINEERING;F.3 LOGICS AND MEANINGS OF PROGRAMS'], links=[arxiv.Result.Link('http://dx.doi.org/10.4204/EPTCS.271.3', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1805.05514v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.05514v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.06757v1', updated=datetime.datetime(2018, 5, 17, 13, 34, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 17, 13, 34, 42, tzinfo=datetime.timezone.utc), title='Matching Consecutive Subpatterns Over Streaming Time Series', authors=[arxiv.Result.Author('Rong Kang'), arxiv.Result.Author('Chen Wang'), arxiv.Result.Author('Peng Wang'), arxiv.Result.Author('Yuting Ding'), arxiv.Result.Author('Jianmin Wang')], summary='Pattern matching of streaming time series with lower latency under limited\\ncomputing resource comes to a critical problem, especially as the growth of\\nIndustry 4.0 and Industry Internet of Things. However, against traditional\\nsingle pattern matching model, a pattern may contain multiple subpatterns\\nrepresenting different physical meanings in the real world. Hence, we formulate\\na new problem, called \"consecutive subpatterns matching\", which allows users to\\nspecify a pattern containing several consecutive subpatterns with various\\nspecified thresholds. We propose a novel representation Equal-Length Block\\n(ELB) together with two efficient implementations, which work very well under\\nall Lp-Norms without false dismissals. Extensive experiments are performed on\\nsynthetic and real-world datasets to illustrate that our approach outperforms\\nthe brute-force method and MSM, a multi-step filter mechanism over the\\nmulti-scaled representation by orders of magnitude.', comment='15 pages, 8 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.06757v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.06757v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.07599v1', updated=datetime.datetime(2018, 5, 19, 14, 26, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 19, 14, 26, 14, tzinfo=datetime.timezone.utc), title='A hybrid index model for efficient spatio-temporal search in HBase', authors=[arxiv.Result.Author('Chengyuan Zhangy'), arxiv.Result.Author('Lei Zhuy'), arxiv.Result.Author('Jun Longy'), arxiv.Result.Author('Shuangqiao Liny'), arxiv.Result.Author('Zhan Yangy'), arxiv.Result.Author('Wenti Huang')], summary='With advances in geo-positioning technologies and geo-location services,\\nthere are a rapidly growing massive amount of spatio-temporal data collected in\\nmany applications such as location-aware devices and wireless communication, in\\nwhich an object is described by its spatial location and its timestamp.\\nConsequently, the study of spatio-temporal search which explores both\\ngeo-location information and temporal information of the data has attracted\\nsignificant concern from research organizations and commercial communities.\\nThis work study the problem of spatio-temporal \\\\emph{k}-nearest neighbors\\nsearch (ST$k$NNS), which is fundamental in the spatial temporal queries. Based\\non HBase, a novel index structure is proposed, called \\\\textbf{H}ybrid\\n\\\\textbf{S}patio-\\\\textbf{T}emporal HBase \\\\textbf{I}ndex (\\\\textbf{HSTI} for\\nshort), which is carefully designed and takes both spatial and temporal\\ninformation into consideration to effectively reduce the search space. Based on\\nHSTI, an efficient algorithm is developed to deal with spatio-temporal\\n\\\\emph{k}-nearest neighbors search. Comprehensive experiments on real and\\nsynthetic data clearly show that HSTI is three to five times faster than the\\nstate-of-the-art technique.', comment='11 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.07599v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.07599v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.08169v1', updated=datetime.datetime(2018, 5, 18, 17, 53, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 18, 17, 53, 17, tzinfo=datetime.timezone.utc), title='Cancer Research UK Drug Discovery Process Mining', authors=[arxiv.Result.Author('Haochao Huang')], summary='Background. The Drug Discovery Unit (DDU) of Cancer Research UK (CRUK) is\\nusing the software Dotmatics for storage and analysis of scientific data during\\ndrug discovery process. Whilst the data include event logs, time stamps,\\nactivities, and user information are mostly sitting in the database without\\nfully utilising their potential value. Aims. This dissertation aims at\\nextracting knowledge from event logs data which recorded during drug discovery\\nprocess, to capture the operational business process of the DDU of Cancer\\nResearch UK (CRUK) as it was being executed. It provides the evaluations and\\nmethodologies of drawing the process mining panoramic models for the drug\\ndiscovery process. Thus by enabling the DDU to maximise its efficiency in\\nreviewing its resources and works allocations, patients will benefit from more\\nnew treatments faster. Conclusion. Management of organisations can be benefit\\nfrom the process mining methodologies. Disco is excellent for non-experts on\\nmanagement purposes. ProM is great for expert on research purposes. However,\\nthe process mining is not once and for all but is a regular operation\\nmanagement process. Indeed, event logs needs to be understand more on the\\ntarget organisational behaviours and organisational business process. The\\nresearchers have to be aware that event logs data are the most important and\\npriority elements in process mining.', comment='113 pages, 84 figures/tables', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.08169v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.08169v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.08520v1', updated=datetime.datetime(2018, 5, 22, 11, 50, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 22, 11, 50, 35, tzinfo=datetime.timezone.utc), title='MonetDBLite: An Embedded Analytical Database', authors=[arxiv.Result.Author('Mark Raasveldt'), arxiv.Result.Author('Hannes Mühleisen')], summary='While traditional RDBMSes offer a lot of advantages, they require significant\\neffort to setup and to use. Because of these challenges, many data scientists\\nand analysts have switched to using alternative data management solutions.\\nThese alternatives, however, lack features that are standard for RDBMSes, e.g.\\nout-of-core query execution. In this paper, we introduce the embedded\\nanalytical database MonetDBLite. MonetDBLite is designed to be both highly\\nefficient and easy to use in conjunction with standard analytical tools. It can\\nbe installed using standard package managers, and requires no configuration or\\nserver management. It is designed for OLAP scenarios, and offers\\nnear-instantaneous data transfer between the database and analytical tools, all\\nthe while maintaining the transactional guarantees and ACID properties of a\\nstandard relational system. These properties make MonetDBLite highly suitable\\nas a storage engine for data used in analytics, machine learning and\\nclassification tasks.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.08520v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.08520v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.08650v1', updated=datetime.datetime(2018, 5, 22, 14, 59, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 22, 14, 59, 2, tzinfo=datetime.timezone.utc), title='Cache-based Multi-query Optimization for Data-intensive Scalable Computing Frameworks', authors=[arxiv.Result.Author('Pietro Michiardi'), arxiv.Result.Author('Damiano Carra'), arxiv.Result.Author('Sara Migliorini')], summary='In modern large-scale distributed systems, analytics jobs submitted by\\nvarious users often share similar work, for example scanning and processing the\\nsame subset of data. Instead of optimizing jobs independently, which may result\\nin redundant and wasteful processing, multi-query optimization techniques can\\nbe employed to save a considerable amount of cluster resources. In this work,\\nwe introduce a novel method combining in-memory cache primitives and\\nmulti-query optimization, to improve the efficiency of data-intensive, scalable\\ncomputing frameworks. By careful selection and exploitation of common\\n(sub)expressions, while satisfying memory constraints, our method transforms a\\nbatch of queries into a new, more efficient one which avoids unnecessary\\nrecomputations. To find feasible and efficient execution plans, our method uses\\na cost-based optimization formulation akin to the multiple-choice knapsack\\nproblem. Extensive experiments on a prototype implementation of our system show\\nsignificant benefits of worksharing for both TPC-DS workloads and detailed\\nmicro-benchmarks.', comment='12 pages + references, extended version', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.08650v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.08650v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.11450v1', updated=datetime.datetime(2018, 5, 26, 6, 2, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 26, 6, 2, 40, tzinfo=datetime.timezone.utc), title='Model-based Pricing for Machine Learning in a Data Marketplace', authors=[arxiv.Result.Author('Lingjiao Chen'), arxiv.Result.Author('Paraschos Koutris'), arxiv.Result.Author('Arun Kumar')], summary=\"Data analytics using machine learning (ML) has become ubiquitous in science,\\nbusiness intelligence, journalism and many other domains. While a lot of work\\nfocuses on reducing the training cost, inference runtime and storage cost of ML\\nmodels, little work studies how to reduce the cost of data acquisition, which\\npotentially leads to a loss of sellers' revenue and buyers' affordability and\\nefficiency.\\n  In this paper, we propose a model-based pricing (MBP) framework, which\\ninstead of pricing the data, directly prices ML model instances. We first\\nformally describe the desired properties of the MBP framework, with a focus on\\navoiding arbitrage. Next, we show a concrete realization of the MBP framework\\nvia a noise injection approach, which provably satisfies the desired formal\\nproperties. Based on the proposed framework, we then provide algorithmic\\nsolutions on how the seller can assign prices to models under different market\\nscenarios (such as to maximize revenue). Finally, we conduct extensive\\nexperiments, which validate that the MBP framework can provide high revenue to\\nthe seller, high affordability to the buyer, and also operate on low runtime\\ncost.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.GT', 'cs.LG', 'math.OC', 'stat.ML'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.11450v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.11450v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.11517v3', updated=datetime.datetime(2018, 7, 3, 14, 36, 37, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 29, 14, 50, 24, tzinfo=datetime.timezone.utc), title=\"You Say 'What', I Hear 'Where' and 'Why': (Mis-)Interpreting SQL to Derive Fine-Grained Provenance\", authors=[arxiv.Result.Author('Tobias Müller'), arxiv.Result.Author('Benjamin Dietrich'), arxiv.Result.Author('Torsten Grust')], summary='SQL declaratively specifies what the desired output of a query is. This work\\nshows that a non-standard interpretation of the SQL semantics can, instead,\\ndisclose where a piece of the output originated in the input and why that piece\\nfound its way into the result. We derive such data provenance for very rich SQL\\ndialects (including recursion, windowed aggregates, and user-defined functions)\\nat the fine-grained level of individual table cells. The approach is\\nnon-invasive and implemented as a compositional source-level SQL rewrite: an\\ninput SQL query is transformed into its own interpreter that wields data\\ndependencies instead of regular values. We deliberately design this\\ntransformation to preserve the shape of both data and query, which allows\\nprovenance derivation to scale to complex queries without overwhelming the\\nunderlying database system.', comment='Extended version of an article published in the Proceedings of the\\n  VLDB Endowment (PVLDB, 11(11), August 2018)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2.3; D.2.5'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.11517v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.11517v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.11723v1', updated=datetime.datetime(2018, 5, 29, 21, 46, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 29, 21, 46, 50, tzinfo=datetime.timezone.utc), title='Building your Cross-Platform Application with RHEEM', authors=[arxiv.Result.Author('Sanjay Chawla'), arxiv.Result.Author('Bertty Contreras-Rojas'), arxiv.Result.Author('Zoi Kaoudi'), arxiv.Result.Author('Sebastian Kruse'), arxiv.Result.Author('Jorge-Arnulfo Quiané-Ruiz')], summary='Today, organizations typically perform tedious and costly tasks to juggle\\ntheir code and data across different data processing platforms. Addressing this\\npain and achieving automatic cross-platform data processing is quite\\nchallenging because it requires quite good expertise for all the available data\\nprocessing platforms. In this report, we present Rheem, a general-purpose\\ncross-platform data processing system that alleviates users from the pain of\\nfinding the most efficient data processing platform for a given task. It also\\nsplits a task into subtasks and assigns each subtask to a specific platform to\\nminimize the overall cost (e.g., runtime or monetary cost). To offer\\ncross-platform functionality, it features (i) a robust interface to easily\\ncompose data analytic tasks; (ii) a novel cost-based optimizer able to find the\\nmost efficient platform in almost all cases; and (iii) an executor to\\nefficiently orchestrate tasks over different platforms. As a result, it allows\\nusers to focus on the business logic of their applications rather than on the\\nmechanics of how to compose and execute them. Rheem is released under an open\\nsource license.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.11723v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.11723v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.11728v2', updated=datetime.datetime(2018, 9, 13, 13, 29, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 29, 22, 22, 21, tzinfo=datetime.timezone.utc), title='Sapphire: Querying RDF Data Made Simple', authors=[arxiv.Result.Author('Ahmed El-Roby'), arxiv.Result.Author('Khaled Ammar'), arxiv.Result.Author('Ashraf Aboulnaga'), arxiv.Result.Author('Jimmy Lin')], summary='RDF data in the linked open data (LOD) cloud is very valuable for many\\ndifferent applications. In order to unlock the full value of this data, users\\nshould be able to issue complex queries on the RDF datasets in the LOD cloud.\\nSPARQL can express such complex queries, but constructing SPARQL queries can be\\na challenge to users since it requires knowing the structure and vocabulary of\\nthe datasets being queried. In this paper, we introduce Sapphire, a tool that\\nhelps users write syntactically and semantically correct SPARQL queries without\\nprior knowledge of the queried datasets. Sapphire interactively helps the user\\nwhile typing the query by providing auto-complete suggestions based on the\\nqueried data. After a query is issued, Sapphire provides suggestions on ways to\\nchange the query to better match the needs of the user. We evaluated Sapphire\\nbased on performance experiments and a user study and showed it to be superior\\nto competing approaches.', comment='16 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.11728v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.11728v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.11900v1', updated=datetime.datetime(2018, 5, 30, 11, 14, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 30, 11, 14, 30, tzinfo=datetime.timezone.utc), title='Q-Graph: Preserving Query Locality in Multi-Query Graph Processing', authors=[arxiv.Result.Author('Christian Mayer'), arxiv.Result.Author('Ruben Mayer'), arxiv.Result.Author('Jonas Grunert'), arxiv.Result.Author('Kurt Rothermel'), arxiv.Result.Author('Muhammad Adnan Tariq')], summary='Arising user-centric graph applications such as route planning and\\npersonalized social network analysis have initiated a shift of paradigms in\\nmodern graph processing systems towards multi-query analysis, i.e., processing\\nmultiple graph queries in parallel on a shared graph. These applications\\ngenerate a dynamic number of localized queries around query hotspots such as\\npopular urban areas. However, existing graph processing systems are not yet\\ntailored towards these properties: The employed methods for graph partitioning\\nand synchronization management disregard query locality and dynamism which\\nleads to high query latency. To this end, we propose the system Q-Graph for\\nmulti-query graph analysis that considers query locality on three levels. (i)\\nThe query-aware graph partitioning algorithm Q-cut maximizes query locality to\\nreduce communication overhead. (ii) The method for synchronization management,\\ncalled hybrid barrier synchronization, allows for full exploitation of local\\nqueries spanning only a subset of partitions. (iii) Both methods adapt at\\nruntime to changing query workloads in order to maintain and exploit locality.\\nOur experiments show that Q-cut reduces average query latency by up to 57\\npercent compared to static query-agnostic partitioning algorithms.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.11900v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.11900v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.12033v5', updated=datetime.datetime(2019, 10, 18, 5, 40, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 30, 15, 22, 33, tzinfo=datetime.timezone.utc), title='PIQUE: Progressive Integrated QUery Operator with Pay-As-You-Go Enrichment', authors=[arxiv.Result.Author('Dhrubajyoti Ghosh'), arxiv.Result.Author('Roberto Yus'), arxiv.Result.Author('Yasser Altowim'), arxiv.Result.Author('Sharad Mehrotra')], summary='Big data today in the form of text, images, video, and sensor data needs to\\nbe enriched (i.e., annotated with tags) prior to be effectively queried or\\nanalyzed. Data enrichment (that, depending upon the application could be\\ncompiled code, declarative queries, or expensive machine learning and/or signal\\nprocessing techniques) often cannot be performed in its entirety as a\\npre-processing step at the time of data ingestion. Enriching data as a separate\\noffline step after ingestion makes it unavailable for analysis during the\\nperiod between the ingestion and enrichment. To bridge such a gap, this paper\\nexplores a novel approach that supports progressive data enrichment during\\nquery processing in order to support interactive exploratory analysis. Our\\napproach is based on integrating an operator, entitled PIQUE, to support a\\nprioritized execution of the enrichment functions during query processing.\\nQuery processing with the PIQUE operator significantly outperforms the\\nbaselines in terms of rate at which answer quality improves during query\\nprocessing.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1805.12033v5', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.12033v5', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1805.12502v2', updated=datetime.datetime(2018, 8, 14, 9, 12, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 31, 14, 54, 55, tzinfo=datetime.timezone.utc), title='Improving Machine-based Entity Resolution with Limited Human Effort: A Risk Perspective', authors=[arxiv.Result.Author('Zhaoqiang Chen'), arxiv.Result.Author('Qun Chen'), arxiv.Result.Author('Boyi Hou'), arxiv.Result.Author('Murtadha Ahmed'), arxiv.Result.Author('Zhanhuai Li')], summary='Pure machine-based solutions usually struggle in the challenging\\nclassification tasks such as entity resolution (ER). To alleviate this problem,\\na recent trend is to involve the human in the resolution process, most notably\\nthe crowdsourcing approach. However, it remains very challenging to effectively\\nimprove machine-based entity resolution with limited human effort. In this\\npaper, we investigate the problem of human and machine cooperation for ER from\\na risk perspective. We propose to select the machine-labeled instances at high\\nrisk of being mislabeled for manual verification. For this task, we present a\\nrisk model that takes into consideration the human-labeled instances as well as\\nthe output of machine resolution. Finally, we evaluate the performance of the\\nproposed risk model on real data. Our experiments demonstrate that it can pick\\nup the mislabeled instances with considerably higher accuracy than the existing\\nalternatives. Provided with the same amount of human cost budget, it can also\\nachieve better resolution quality than the state-of-the-art approach based on\\nactive learning.', comment='5 pages, 3 figures', journal_ref=None, doi='10.1145/3242153.3242156', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3242153.3242156', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1805.12502v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.12502v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1809.00089v1', updated=datetime.datetime(2018, 9, 1, 0, 20, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 9, 1, 0, 20, 5, tzinfo=datetime.timezone.utc), title='Eliminating Boundaries in Cloud Storage with Anna', authors=[arxiv.Result.Author('Chenggang Wu'), arxiv.Result.Author('Vikram Sreekanti'), arxiv.Result.Author('Joseph M. Hellerstein')], summary=\"In this paper, we describe how we extended a distributed key-value store\\ncalled Anna into an elastic, multi-tier service for the cloud. In its extended\\nform, Anna is designed to overcome the narrow cost-performance limitations\\ntypical of current cloud storage systems. We describe three key aspects of\\nAnna's new design: multi-master selective replication of hot keys, a vertical\\ntiering of storage layers with different cost-performance tradeoffs, and\\nhorizontal elasticity of each tier to add and remove nodes in response to load\\ndynamics. Anna's policy engine uses these mechanisms to balance service-level\\nobjectives around cost, latency and fault tolerance. Experimental results\\nexplore the behavior of Anna's mechanisms and policy, exhibiting orders of\\nmagnitude efficiency improvements over both commodity cloud KVS services and\\nresearch systems.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1809.00089v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1809.00089v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1809.00405v2', updated=datetime.datetime(2018, 9, 29, 4, 47, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 9, 2, 22, 41, 32, tzinfo=datetime.timezone.utc), title='Query Log Compression for Workload Analytics', authors=[arxiv.Result.Author('Ting Xie'), arxiv.Result.Author('Oliver Kennedy'), arxiv.Result.Author('Varun Chandola')], summary='Analyzing database access logs is a key part of performance tuning, intrusion\\ndetection, benchmark development, and many other database administration tasks.\\nUnfortunately, it is common for production databases to deal with millions or\\neven more queries each day, so these logs must be summarized before they can be\\nused. Designing an appropriate summary encoding requires trading off between\\nconciseness and information content. For example: simple workload sampling may\\nmiss rare, but high impact queries. In this paper, we present LogR, a lossy log\\ncompression scheme suitable use for many automated log analytics tools, as well\\nas for human inspection. We formalize and analyze the space/fidelity trade-off\\nin the context of a broader family of \"pattern\" and \"pattern mixture\" log\\nencodings to which LogR belongs. We show through a series of experiments that\\nLogR compressed encodings can be created efficiently, come with provable\\ninformation-theoretic bounds on their accuracy, and outperform state-of-art log\\nsummarization strategies.', comment='Typos fixed, some irrelevant figures and paragraphs are trimmed', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1809.00405v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1809.00405v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1809.00641v1', updated=datetime.datetime(2018, 9, 3, 16, 19, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 9, 3, 16, 19, 55, tzinfo=datetime.timezone.utc), title='Typed Linear Algebra for Efficient Analytical Querying', authors=[arxiv.Result.Author('João M. Afonso'), arxiv.Result.Author('Gabriel D. Fernandes'), arxiv.Result.Author('João P. Fernandes'), arxiv.Result.Author('Filipe Oliveira'), arxiv.Result.Author('Bruno M. Ribeiro'), arxiv.Result.Author('Rogério Pontes'), arxiv.Result.Author('José N. Oliveira'), arxiv.Result.Author('Alberto J. Proença')], summary='This paper uses typed linear algebra (LA) to represent data and perform\\nanalytical querying in a single, unified framework. The typed approach offers\\nstrong type checking (as in modern programming languages) and a diagrammatic\\nway of expressing queries (paths in LA diagrams). A kernel of LA operators has\\nbeen implemented so that paths extracted from LA diagrams can be executed. The\\napproach is validated and evaluated taking TPC-H benchmark queries as\\nreference. The performance of the LA-based approach is compared with popular\\ndatabase competitors (PostgreSQL and MySQL).', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1809.00641v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1809.00641v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1809.01622v1', updated=datetime.datetime(2018, 9, 5, 17, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 9, 5, 17, 2, tzinfo=datetime.timezone.utc), title='Ranking RDF Instances in Degree-decoupled RDF Graphs', authors=[arxiv.Result.Author('Elisa S. Menendez'), arxiv.Result.Author('Marco A. Casanova'), arxiv.Result.Author('Mohand Boughanem'), arxiv.Result.Author('Luiz André P. Paes Leme')], summary='In the last decade, RDF emerged as a new kind of standardized data model, and\\na sizable body of knowledge from fields such as Information Retrieval was\\nadapted to RDF graphs. One common task in graph databases is to define an\\nimportance score for nodes based on centrality measures, such as PageRank and\\nHITS. The majority of the strategies highly depend on the degree of the node.\\nHowever, in some RDF graphs, called degree-decoupled RDF graphs, the notion of\\nimportance is not directly related to the node degree. Therefore, this work\\nfirst proposes three novel node importance measures, named InfoRank I, II and\\nIII, for degree-decoupled RDF graphs. It then compares the proposed measures\\nwith traditional PageRank and other familiar centrality measures, using with an\\nIMDb dataset.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1809.01622v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1809.01622v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1809.02345v1', updated=datetime.datetime(2018, 9, 7, 8, 21, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 9, 7, 8, 21, 1, tzinfo=datetime.timezone.utc), title='Hierarchical Characteristic Set Merging for Optimizing SPARQL Queries in Heterogeneous RDF', authors=[arxiv.Result.Author('Marios Meimaris'), arxiv.Result.Author('George Papastefanatos')], summary='Characteristic sets (CS) organize RDF triples based on the set of properties\\ncharacterizing their subject nodes. This concept is recently used in indexing\\ntechniques, as it can capture the implicit schema of RDF data. While most\\nCS-based approaches yield significant improvements in space and query\\nperformance, they fail to perform well in the presence of schema heterogeneity,\\ni.e., when the number of CSs becomes very large, resulting in a highly\\npartitioned data organization. In this paper, we address this problem by\\nintroducing a novel technique, for merging CSs based on their hierarchical\\nstructure. Our technique employs a lattice to capture the hierarchical\\nrelationships between CSs, identifies dense CSs and merges dense CSs with their\\nancestors, thus reducing the size of the CSs as well as the links between them.\\nWe implemented our algorithm on top of a relational backbone, where each merged\\nCS is stored in a relational table, and we performed an extensive experimental\\nstudy to evaluate the performance and impact of merging to the storage and\\nquerying of RDF datasets, indicating significant improvements.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1809.02345v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1809.02345v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1809.02631v1', updated=datetime.datetime(2018, 9, 7, 18, 31, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 9, 7, 18, 31, 52, tzinfo=datetime.timezone.utc), title='Pushing the Limits of Encrypted Databases with Secure Hardware', authors=[arxiv.Result.Author('Panagiotis Antonopoulos'), arxiv.Result.Author('Arvind Arasu'), arxiv.Result.Author('Ken Eguro'), arxiv.Result.Author('Joachim Hammer'), arxiv.Result.Author('Raghav Kaushik'), arxiv.Result.Author('Donald Kossmann'), arxiv.Result.Author('Ravi Ramamurthy'), arxiv.Result.Author('Jakub Szymaszek')], summary='Encrypted databases have been studied for more than 10 years and are quickly\\nemerging as a critical technology for the cloud. The current state of the art\\nis to use property-preserving encrypting techniques (e.g., deterministic\\nencryption) to protect the confidentiality of the data and support query\\nprocessing at the same time. Unfortunately, these techniques have many\\nlimitations. Recently, trusted computing platforms (e.g., Intel SGX) have\\nemerged as an alternative to implement encrypted databases. This paper\\ndemonstrates some vulnerabilities and the limitations of this technology, but\\nit also shows how to make best use of it in order to improve on\\nconfidentiality, functionality, and performance.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1809.02631v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1809.02631v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1809.04017v1', updated=datetime.datetime(2018, 9, 11, 16, 42, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 9, 11, 16, 42, 24, tzinfo=datetime.timezone.utc), title='Reducing Uncertainty of Schema Matching via Crowdsourcing with Accuracy Rates', authors=[arxiv.Result.Author('Chen Jason Zhang'), arxiv.Result.Author('Lei Chen'), arxiv.Result.Author('H. V. Jagadish'), arxiv.Result.Author('Mengchen Zhang'), arxiv.Result.Author('Yongxin Tong')], summary=\"Schema matching is a central challenge for data integration systems. Inspired\\nby the popularity and the success of crowdsourcing platforms, we explore the\\nuse of crowdsourcing to reduce the uncertainty of schema matching. Since\\ncrowdsourcing platforms are most effective for simple questions, we assume that\\neach Correspondence Correctness Question (CCQ) asks the crowd to decide whether\\na given correspondence should exist in the correct matching. Furthermore,\\nmembers of a crowd may sometimes return incorrect answers with different\\nprobabilities. Accuracy rates of individual crowd workers are probabilities of\\nreturning correct answers which can be attributes of CCQs as well as\\nevaluations of individual workers. We prove that uncertainty reduction equals\\nto entropy of answers minus entropy of crowds and show how to obtain lower and\\nupper bounds for it. We propose frameworks and efficient algorithms to\\ndynamically manage the CCQs to maximize the uncertainty reduction within a\\nlimited budget of questions. We develop two novel approaches, namely `Single\\nCCQ' and `Multiple CCQ', which adaptively select, publish and manage questions.\\nWe verify the value of our solutions with simulation and real implementation.\", comment='15 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1809.04017v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1809.04017v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1809.04284v1', updated=datetime.datetime(2018, 9, 12, 7, 32, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 9, 12, 7, 32, 29, tzinfo=datetime.timezone.utc), title='An Approach to Handle Big Data Warehouse Evolution', authors=[arxiv.Result.Author('Darja Solodovnikova'), arxiv.Result.Author('Laila Niedrite')], summary='One of the purposes of Big Data systems is to support analysis of data\\ngathered from heterogeneous data sources. Since data warehouses have been used\\nfor several decades to achieve the same goal, they could be leveraged also to\\nprovide analysis of data stored in Big Data systems. The problem of adapting\\ndata warehouse data and schemata to changes in these requirements as well as\\ndata sources has been studied by many researchers worldwide. However,\\ninnovative methods must be developed also to support evolution of data\\nwarehouses that are used to analyze data stored in Big Data systems. In this\\npaper, we propose a data warehouse architecture that allows to perform\\ndifferent kinds of analytical tasks, including OLAP-like analysis, on big data\\nloaded from multiple heterogeneous data sources with different latency and is\\ncapable of processing changes in data sources as well as evolving analysis\\nrequirements. The operation of the architecture is highly based on the metadata\\nthat are outlined in the paper.', comment='In the Pre-proceedings of the Semantics in Big Data Management\\n  workshop, IFIP W.G. 2.6 on Database, Tuesday 18th September 2018 at IFIP\\n  World Computer Congress 2018 - Poznan, Poland', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1809.04284v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1809.04284v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1809.06859v1', updated=datetime.datetime(2018, 9, 18, 15, 10, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 9, 18, 15, 10, 52, tzinfo=datetime.timezone.utc), title=\"HDTCat: let's make HDT scale\", authors=[arxiv.Result.Author('Dennis Diefenbach'), arxiv.Result.Author('Josée M. Giménez-García')], summary='HDT (Header, Dictionary, Triples) is a serialization for RDF. HDT has become\\nvery popular in the last years because it allows to store RDF data with a small\\ndisk footprint, while remaining at the same time queriable. For this reason HDT\\nis often used when scalability becomes an issue. Once RDF data is serialized\\ninto HDT, the disk footprint to store it and the memory footprint to query it\\nare very low. However, generating HDT files from raw text RDF serializations\\n(like N-Triples) is a time-consuming and (especially) memory-consuming task. In\\nthis publication we present HDTCat, an algorithm and command line tool to join\\ntwo HDT files with low memory footprint. HDTCat can be used in a\\ndivide-and-conquer strategy to generate HDT files from huge datasets using a\\nlow-memory footprint.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1809.06859v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1809.06859v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1809.10212v2', updated=datetime.datetime(2018, 12, 18, 0, 0, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 9, 26, 19, 51, 3, tzinfo=datetime.timezone.utc), title='Towards a Hands-Free Query Optimizer through Deep Learning', authors=[arxiv.Result.Author('Ryan Marcus'), arxiv.Result.Author('Olga Papaemmanouil')], summary='Query optimization remains one of the most important and well-studied\\nproblems in database systems. However, traditional query optimizers are complex\\nheuristically-driven systems, requiring large amounts of time to tune for a\\nparticular database and requiring even more time to develop and maintain in the\\nfirst place. In this vision paper, we argue that a new type of query optimizer,\\nbased on deep reinforcement learning, can drastically improve on the\\nstate-of-the-art. We identify potential complications for future research that\\nintegrates deep learning with query optimization, and we describe three novel\\ndeep learning based approaches that can lead the way to end-to-end\\nlearning-based query optimizers.', comment='Published in CIDR19', journal_ref='9th Biennial Conference on Innovative Data Systems Research, 2019', doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1809.10212v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1809.10212v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1809.11084v1', updated=datetime.datetime(2018, 9, 28, 15, 26, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 9, 28, 15, 26, 17, tzinfo=datetime.timezone.utc), title='Reuse and Adaptation for Entity Resolution through Transfer Learning', authors=[arxiv.Result.Author('Saravanan Thirumuruganathan'), arxiv.Result.Author('Shameem A Puthiya Parambath'), arxiv.Result.Author('Mourad Ouzzani'), arxiv.Result.Author('Nan Tang'), arxiv.Result.Author('Shafiq Joty')], summary='Entity resolution (ER) is one of the fundamental problems in data\\nintegration, where machine learning (ML) based classifiers often provide the\\nstate-of-the-art results. Considerable human effort goes into feature\\nengineering and training data creation. In this paper, we investigate a new\\nproblem: Given a dataset D_T for ER with limited or no training data, is it\\npossible to train a good ML classifier on D_T by reusing and adapting the\\ntraining data of dataset D_S from same or related domain? Our major\\ncontributions include (1) a distributed representation based approach to encode\\neach tuple from diverse datasets into a standard feature space; (2)\\nidentification of common scenarios where the reuse of training data can be\\nbeneficial; and (3) five algorithms for handling each of the aforementioned\\nscenarios. We have performed comprehensive experiments on 12 datasets from 5\\ndifferent domains (publications, movies, songs, restaurants, and books). Our\\nexperiments show that our algorithms provide significant benefits such as\\nproviding superior performance for a fixed training data size.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG', 'stat.ML'], links=[arxiv.Result.Link('http://arxiv.org/abs/1809.11084v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1809.11084v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.00234v1', updated=datetime.datetime(2019, 3, 30, 15, 23, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 3, 30, 15, 23, 5, tzinfo=datetime.timezone.utc), title='Uncertainty Annotated Databases - A Lightweight Approach for Approximating Certain Answers (extended version)', authors=[arxiv.Result.Author('Su Feng'), arxiv.Result.Author('Aaron Huber'), arxiv.Result.Author('Boris Glavic'), arxiv.Result.Author('Oliver Kennedy')], summary='Certain answers are a principled method for coping with uncertainty that\\narises in many practical data management tasks. Unfortunately, this method is\\nexpensive and may exclude useful (if uncertain) answers. Thus, users frequently\\nresort to less principled approaches to resolve the uncertainty. In this paper,\\nwe propose Uncertainty Annotated Databases (UA-DBs), which combine an under-\\nand over-approximation of certain answers to achieve the reliability of certain\\nanswers, with the performance of a classical database system. Furthermore, in\\ncontrast to prior work on certain answers, UA-DBs achieve a higher utility by\\nincluding some (explicitly marked) answers that are not certain. UA-DBs are\\nbased on incomplete K-relations, which we introduce to generalize the classical\\nset-based notions of incomplete databases and certain answers to a much larger\\nclass of data models. Using an implementation of our approach, we demonstrate\\nexperimentally that it efficiently produces tight approximations of certain\\nanswers that are of high utility.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.00234v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.00234v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.00850v1', updated=datetime.datetime(2019, 4, 1, 13, 47, 58, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 1, 13, 47, 58, tzinfo=datetime.timezone.utc), title='Boundedness of Conjunctive Regular Path Queries', authors=[arxiv.Result.Author('Pablo Barceló'), arxiv.Result.Author('Diego Figueira'), arxiv.Result.Author('Miguel Romero')], summary='We study the boundedness problem for unions of conjunctive regular path\\nqueries with inverses (UC2RPQs). This is the problem of, given a UC2RPQ,\\nchecking whether it is equivalent to a union of conjunctive queries (UCQ). We\\nshow the problem to be ExpSpace-complete, thus coinciding with the complexity\\nof containment for UC2RPQs. As a corollary, when a UC2RPQ is bounded, it is\\nequivalent to a UCQ of at most triple-exponential size, and in fact we show\\nthat this bound is optimal. We also study better behaved classes of UC2RPQs,\\nnamely acyclic UC2RPQs of bounded thickness, and strongly connected UCRPQs,\\nwhose boundedness problem are, respectively, PSpace-complete and\\n$\\\\Pi^p_2$-complete. Most upper bounds exploit results on limitedness for\\ndistance automata, in particular extending the model with alternation and\\ntwo-wayness, which may be of independent interest.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DM', 'cs.FL', 'cs.LO'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.00850v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.00850v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.01279v1', updated=datetime.datetime(2019, 4, 2, 8, 29, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 2, 8, 29, 55, tzinfo=datetime.timezone.utc), title='Learning a Partitioning Advisor with Deep Reinforcement Learning', authors=[arxiv.Result.Author('Benjamin Hilprecht'), arxiv.Result.Author('Carsten Binnig'), arxiv.Result.Author('Uwe Roehm')], summary='Commercial data analytics products such as Microsoft Azure SQL Data Warehouse\\nor Amazon Redshift provide ready-to-use scale-out database solutions for\\nOLAP-style workloads in the cloud. While the provisioning of a database cluster\\nis usually fully automated by cloud providers, customers typically still have\\nto make important design decisions which were traditionally made by the\\ndatabase administrator such as selecting the partitioning schemes.\\n  In this paper we introduce a learned partitioning advisor for analytical\\nOLAP-style workloads based on Deep Reinforcement Learning (DRL). The main idea\\nis that a DRL agent learns its decisions based on experience by monitoring the\\nrewards for different workloads and partitioning schemes. We evaluate our\\nlearned partitioning advisor in an experimental evaluation with different\\ndatabases schemata and workloads of varying complexity. In the evaluation, we\\nshow that our advisor is not only able to find partitionings that outperform\\nexisting approaches for automated partitioning design but that it also can\\neasily adjust to different deployments. This is especially important in cloud\\nsetups where customers can easily migrate their cluster to a new set of\\n(virtual) machines.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.01279v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.01279v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.01863v1', updated=datetime.datetime(2019, 4, 3, 9, 10, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 3, 9, 10, 29, tzinfo=datetime.timezone.utc), title='Identifying Patient Groups based on Frequent Patterns of Patient Samples', authors=[arxiv.Result.Author('Seyed Amin Tabatabaei'), arxiv.Result.Author('Xixi Lu'), arxiv.Result.Author('Mark Hoogendoorn'), arxiv.Result.Author('Hajo A. Reijers')], summary='Grouping patients meaningfully can give insights about the different types of\\npatients, their needs, and the priorities. Finding groups that are meaningful\\nis however very challenging as background knowledge is often required to\\ndetermine what a useful grouping is. In this paper we propose an approach that\\nis able to find groups of patients based on a small sample of positive examples\\ngiven by a domain expert. Because of that, the approach relies on very limited\\nefforts by the domain experts. The approach groups based on the activities and\\ndiagnostic/billing codes within health pathways of patients. To define such a\\ngrouping based on the sample of patients efficiently, frequent patterns of\\nactivities are discovered and used to measure the similarity between the care\\npathways of other patients to the patients in the sample group. This approach\\nresults in an insightful definition of the group. The proposed approach is\\nevaluated using several datasets obtained from a large university medical\\ncenter. The evaluation shows F1-scores of around 0.7 for grouping kidney injury\\nand around 0.6 for diabetes.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.01863v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.01863v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.02285v1', updated=datetime.datetime(2019, 4, 4, 0, 38, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 4, 0, 38, 59, tzinfo=datetime.timezone.utc), title='HoloDetect: Few-Shot Learning for Error Detection', authors=[arxiv.Result.Author('Alireza Heidari'), arxiv.Result.Author('Joshua McGrath'), arxiv.Result.Author('Ihab F. Ilyas'), arxiv.Result.Author('Theodoros Rekatsinas')], summary='We introduce a few-shot learning framework for error detection. We show that\\ndata augmentation (a form of weak supervision) is key to training high-quality,\\nML-based error detection models that require minimal human involvement. Our\\nframework consists of two parts: (1) an expressive model to learn rich\\nrepresentations that capture the inherent syntactic and semantic heterogeneity\\nof errors; and (2) a data augmentation model that, given a small seed of clean\\nrecords, uses dataset-specific transformations to automatically generate\\nadditional training data. Our key insight is to learn data augmentation\\npolicies from the noisy input dataset in a weakly supervised manner. We show\\nthat our framework detects errors with an average precision of ~94% and an\\naverage recall of ~93% across a diverse array of datasets that exhibit\\ndifferent types and amounts of errors. We compare our approach to a\\ncomprehensive collection of error detection methods, ranging from traditional\\nrule-based methods to ensemble-based and active learning approaches. We show\\nthat data augmentation yields an average improvement of 20 F1 points while it\\nrequires access to 3x fewer labeled examples compared to other ML approaches.', comment='18 pages,', journal_ref='ACM SIGMOD 2019', doi='10.1145/3299869.3319888', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3299869.3319888', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1904.02285v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.02285v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.02344v2', updated=datetime.datetime(2019, 4, 15, 17, 44, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 4, 4, 40, 38, tzinfo=datetime.timezone.utc), title='Mining Precision Interfaces From Query Logs', authors=[arxiv.Result.Author('Qianrui Zhang'), arxiv.Result.Author('Haoci Zhang'), arxiv.Result.Author('Thibault Sellam'), arxiv.Result.Author('Eugene Wu')], summary=\"Interactive tools make data analysis more efficient and more accessible to\\nend-users by hiding the underlying query complexity and exposing interactive\\nwidgets for the parts of the query that matter to the analysis. However,\\ncreating custom tailored (i.e., precise) interfaces is very costly, and\\nautomated approaches are desirable. We propose a syntactic approach that uses\\nqueries from an analysis to generate a tailored interface. We model interface\\nwidgets as functions I(q) -> q' that modify the current analysis query $q$, and\\ninterfaces as the set of queries that its widgets can express. Our system,\\nPrecision Interfaces, analyzes structural changes between input queries from an\\nanalysis, and generates an output interface with widgets to express those\\nchanges. Our experiments on the Sloan Digital Sky Survey query log suggest that\\nPrecision Interfaces can generate useful interfaces for simple unanticipated\\ntasks, and our optimizations can generate interfaces from logs of up to 10,000\\nqueries in <10s.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.02344v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.02344v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.03112v1', updated=datetime.datetime(2019, 4, 5, 15, 11, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 5, 15, 11, 23, tzinfo=datetime.timezone.utc), title='Safe Disassociation of Set-Valued Datasets', authors=[arxiv.Result.Author('Nancy Awad'), arxiv.Result.Author('Bechara Al Bouna'), arxiv.Result.Author('Jean-Francois Couchot'), arxiv.Result.Author('Laurent Philippe')], summary='Disassociation introduced by Terrovitis et al. is a bucketization based\\nanonimyzation technique that divides a set-valued dataset into several clusters\\nto hide the link between individuals and their complete set of items. It\\nincreases the utility of the anonymized dataset, but on the other side, it\\nraises many privacy concerns, one in particular, is when the items are tightly\\ncoupled to form what is called, a cover problem. In this paper, we present safe\\ndisassociation, a technique that relies on partial-suppression, to overcome the\\naforementioned privacy breach encountered when disassociating set-valued\\ndatasets. Safe disassociation allows the $k^m$-anonymity privacy constraint to\\nbe extended to a bucketized dataset and copes with the cover problem. We\\ndescribe our algorithm that achieves the safe disassociation and we provide a\\nset of experiments to demonstrate its efficiency.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.03112v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.03112v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.03711v1', updated=datetime.datetime(2019, 4, 7, 19, 0, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 7, 19, 0, 45, tzinfo=datetime.timezone.utc), title='Neo: A Learned Query Optimizer', authors=[arxiv.Result.Author('Ryan Marcus'), arxiv.Result.Author('Parimarjan Negi'), arxiv.Result.Author('Hongzi Mao'), arxiv.Result.Author('Chi Zhang'), arxiv.Result.Author('Mohammad Alizadeh'), arxiv.Result.Author('Tim Kraska'), arxiv.Result.Author('Olga Papaemmanouil'), arxiv.Result.Author('Nesime Tatbul')], summary='Query optimization is one of the most challenging problems in database\\nsystems. Despite the progress made over the past decades, query optimizers\\nremain extremely complex components that require a great deal of hand-tuning\\nfor specific workloads and datasets. Motivated by this shortcoming and inspired\\nby recent advances in applying machine learning to data management challenges,\\nwe introduce Neo (Neural Optimizer), a novel learning-based query optimizer\\nthat relies on deep neural networks to generate query executions plans. Neo\\nbootstraps its query optimization model from existing optimizers and continues\\nto learn from incoming queries, building upon its successes and learning from\\nits failures. Furthermore, Neo naturally adapts to underlying data patterns and\\nis robust to estimation errors. Experimental results demonstrate that Neo, even\\nwhen bootstrapped from a simple optimizer like PostgreSQL, can learn a model\\nthat offers similar performance to state-of-the-art commercial optimizers, and\\nin some cases even surpass them.', comment=None, journal_ref=None, doi='10.14778/3342263.3342644', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3342263.3342644', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1904.03711v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.03711v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.04702v1', updated=datetime.datetime(2019, 4, 9, 14, 35, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 9, 14, 35, 42, tzinfo=datetime.timezone.utc), title='Modeling Corruption in Eventually-Consistent Graph Databases', authors=[arxiv.Result.Author('Jim Webber'), arxiv.Result.Author('Paul Ezhilchelvan'), arxiv.Result.Author('Isi Mitrani')], summary='We present a model and analysis of an eventually consistent graph database\\nwhere loosely cooperating servers accept concurrent updates to a partitioned,\\ndistributed graph. The model is high-fidelity and preserves design choices from\\ncontemporary graph database management systems. To explore the problem space,\\nwe use two common graph topologies as data models for realistic\\nexperimentation. The analysis reveals, even assuming completely fault-free\\nhardware and bug-free software, that if it is possible for updates to interfere\\nwith one-another, corruption will occur and spread significantly through the\\ngraph within the production database lifetime. Using our model, database\\ndesigners and operators can compute the rate of corruption for their systems\\nand determine whether they are sufficiently dependable for their intended use.', comment='6 pages, 4 Figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC', 'cs.PF'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.04702v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.04702v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.04736v1', updated=datetime.datetime(2019, 4, 9, 15, 36, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 9, 15, 36, 6, tzinfo=datetime.timezone.utc), title='Cold Storage Data Archives: More Than Just a Bunch of Tapes', authors=[arxiv.Result.Author('Bunjamin Memishi'), arxiv.Result.Author('Raja Appuswamy'), arxiv.Result.Author('Marcus Paradies')], summary='The abundance of available sensor and derived data from large scientific\\nexperiments, such as earth observation programs, radio astronomy sky surveys,\\nand high-energy physics already exceeds the storage hardware globally\\nfabricated per year. To that end, cold storage data archives are the---often\\noverlooked---spearheads of modern big data analytics in scientific,\\ndata-intensive application domains. While high-performance data analytics has\\nreceived much attention from the research community, the growing number of\\nproblems in designing and deploying cold storage archives has only received\\nvery little attention.\\n  In this paper, we take the first step towards bridging this gap in knowledge\\nby presenting an analysis of four real-world cold storage archives from three\\ndifferent application domains. In doing so, we highlight (i) workload\\ncharacteristics that differentiate these archives from traditional,\\nperformance-sensitive data analytics, (ii) design trade-offs involved in\\nbuilding cold storage systems for these archives, and (iii) deployment\\ntrade-offs with respect to migration to the public cloud. Based on our\\nanalysis, we discuss several other important research challenges that need to\\nbe addressed by the data management community.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.04736v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.04736v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.05353v3', updated=datetime.datetime(2020, 5, 21, 19, 29, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 10, 13, 27, 53, tzinfo=datetime.timezone.utc), title='Big Data Quality: A systematic literature review and future research directions', authors=[arxiv.Result.Author('Mostafa Mirzaie'), arxiv.Result.Author('Behshid Behkamal'), arxiv.Result.Author('Samad Paydar')], summary='One of the most significant problems of Big Data is to extract knowledge\\nthrough the huge amount of data. The usefulness of the extracted information\\ndepends strongly on data quality. In addition to the importance, data quality\\nhas recently been taken into consideration by the big data community and there\\nis not any comprehensive review conducted in this area. Therefore, the purpose\\nof this study is to review and present the state of the art on the quality of\\nbig data research through a hierarchical framework. The dimensions of the\\nproposed framework cover various aspects in the quality assessment of Big Data\\nincluding 1) the processing types of big data, i.e. stream, batch, and hybrid,\\n2) the main task, and 3) the method used to conduct the task. We compare and\\ncritically review all of the studies reported during the last ten years through\\nour proposed framework to identify which of the available data quality\\nassessment methods have been successfully adopted by the big data community.\\nFinally, we provide a critical discussion on the limitations of existing\\nmethods and offer suggestions on potential valuable research directions that\\ncan be taken in future research in this domain.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.05353v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.05353v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.08679v5', updated=datetime.datetime(2021, 9, 1, 14, 32, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 18, 10, 43, 12, tzinfo=datetime.timezone.utc), title='The Shapley Value of Tuples in Query Answering', authors=[arxiv.Result.Author('Ester Livshits'), arxiv.Result.Author('Leopoldo Bertossi'), arxiv.Result.Author('Benny Kimelfeld'), arxiv.Result.Author('Moshe Sebag')], summary='We investigate the application of the Shapley value to quantifying the\\ncontribution of a tuple to a query answer. The Shapley value is a widely known\\nnumerical measure in cooperative game theory and in many applications of game\\ntheory for assessing the contribution of a player to a coalition game. It has\\nbeen established already in the 1950s, and is theoretically justified by being\\nthe very single wealth-distribution measure that satisfies some natural axioms.\\nWhile this value has been investigated in several areas, it received little\\nattention in data management. We study this measure in the context of\\nconjunctive and aggregate queries by defining corresponding coalition games. We\\nprovide algorithmic and complexity-theoretic results on the computation of\\nShapley-based contributions to query answers; and for the hard cases we present\\napproximation algorithms.', comment=None, journal_ref='Logical Methods in Computer Science, Volume 17, Issue 3 (September\\n  2, 2021) lmcs:8437', doi='10.46298/lmcs-17(3:22)2021', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.46298/lmcs-17(3:22)2021', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1904.08679v5', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.08679v5', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.08741v1', updated=datetime.datetime(2019, 4, 16, 22, 53, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 16, 22, 53, 4, tzinfo=datetime.timezone.utc), title='Mining Closed Episodes with Simultaneous Events', authors=[arxiv.Result.Author('Nikolaj Tatti'), arxiv.Result.Author('Boris Cule')], summary='Sequential pattern discovery is a well-studied field in data mining. Episodes\\nare sequential patterns describing events that often occur in the vicinity of\\neach other. Episodes can impose restrictions to the order of the events, which\\nmakes them a versatile technique for describing complex patterns in the\\nsequence. Most of the research on episodes deals with special cases such as\\nserial, parallel, and injective episodes, while discovering general episodes is\\nunderstudied.\\n  In this paper we extend the definition of an episode in order to be able to\\nrepresent cases where events often occur simultaneously. We present an\\nefficient and novel miner for discovering frequent and closed general episodes.\\nSuch a task presents unique challenges. Firstly, we cannot define closure based\\non frequency. We solve this by computing a more conservative closure that we\\nuse to reduce the search space and discover the closed episodes as a\\npostprocessing step. Secondly, episodes are traditionally presented as directed\\nacyclic graphs. We argue that this representation has drawbacks leading to\\nredundancy in the output. We solve these drawbacks by defining a subset\\nrelationship in such a way that allows us to remove the redundant episodes. We\\ndemonstrate the efficiency of our algorithm and the need for using closed\\nepisodes empirically on synthetic and real-world datasets.', comment=None, journal_ref=None, doi='10.1145/2020408.2020589', primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/2020408.2020589', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1904.08741v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.08741v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.09262v1', updated=datetime.datetime(2019, 4, 19, 16, 45, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 19, 16, 45, 20, tzinfo=datetime.timezone.utc), title='Approximate Queries and Representations for Large Data Sequences', authors=[arxiv.Result.Author('Hagit Shatkay'), arxiv.Result.Author('Stanley B. Zdonik')], summary='Many new database application domains such as experimental sciences and\\nmedicine are characterized by large sequences as their main form of data. Using\\napproximate representation can significantly reduce the required storage and\\nsearch space. A good choice of representation, can support a broad new class of\\napproximate queries, needed in these domains. These queries are concerned with\\napplication dependent features of the data as opposed to the actual sampled\\npoints. We introduce a new notion of generalized approximate queries and a\\ngeneral divide and conquer approach that supports them. This approach uses\\nfamilies of real-valued functions as an approximate representation. We present\\nan algorithm for realizing our technique, and the results of applying it to\\nmedical cardiology data.\\n  (Extended version is available in Tech Report CS-95-03, Dept of Computer\\nScience, Brown University.\\nhttp://cs.brown.edu/research/pubs/techreports/reports/CS-95-03.html)', comment='One of the earliest papers on similarity queries over time series\\n  data, and of symbolic representation of signals (cardio-signals and ECG in\\n  particular). Presents the idea of feature conservation under transformation,\\n  and uses piecewise linear or simple-polynomial approximation', journal_ref='Proceedings of the International Conference on Data Engineering\\n  (ICDE), 1996, pp. 536-545', doi='10.1109/ICDE.1996.492204', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/ICDE.1996.492204', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1904.09262v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.09262v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.09399v1', updated=datetime.datetime(2019, 4, 20, 4, 9, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 20, 4, 9, 1, tzinfo=datetime.timezone.utc), title='Mining Rules Incrementally over Large Knowledge Bases', authors=[arxiv.Result.Author('Xiaofeng Zhou'), arxiv.Result.Author('Ali Sadeghian'), arxiv.Result.Author('Daisy Zhe Wang')], summary='Multiple web-scale Knowledge Bases, e.g., Freebase, YAGO, NELL, have been\\nconstructed using semi-supervised or unsupervised information extraction\\ntechniques and many of them, despite their large sizes, are continuously\\ngrowing. Much research effort has been put into mining inference rules from\\nknowledge bases. To address the task of rule mining over evolving web-scale\\nknowledge bases, we propose a parallel incremental rule mining framework. Our\\napproach is able to efficiently mine rules based on the relational model and\\napply updates to large knowledge bases; we propose an alternative metric that\\nreduces computation complexity without compromising quality; we apply multiple\\noptimization techniques that reduce runtime by more than 2 orders of magnitude.\\nExperiments show that our approach efficiently scales to web-scale knowledge\\nbases and saves over 90% time compared to the state-of-the-art batch rule\\nmining system. We also apply our optimization techniques to the batch rule\\nmining algorithm, reducing runtime by more than half compared to the\\nstate-of-the-art. To the best of our knowledge, our incremental rule mining\\nsystem is the first that handles updates to web-scale knowledge bases.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.09399v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.09399v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.10217v1', updated=datetime.datetime(2019, 4, 23, 9, 23, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 23, 9, 23, 47, tzinfo=datetime.timezone.utc), title='Crowdsourced Truth Discovery in the Presence of Hierarchies for Knowledge Fusion', authors=[arxiv.Result.Author('Woohwan Jung'), arxiv.Result.Author('Younghoon Kim'), arxiv.Result.Author('Kyuseok Shim')], summary='Existing works for truth discovery in categorical data usually assume that\\nclaimed values are mutually exclusive and only one among them is correct.\\nHowever, many claimed values are not mutually exclusive even for functional\\npredicates due to their hierarchical structures. Thus, we need to consider the\\nhierarchical structure to effectively estimate the trustworthiness of the\\nsources and infer the truths. We propose a probabilistic model to utilize the\\nhierarchical structures and an inference algorithm to find the truths. In\\naddition, in the knowledge fusion, the step of automatically extracting\\ninformation from unstructured data (e.g., text) generates a lot of false\\nclaims. To take advantages of the human cognitive abilities in understanding\\nunstructured data, we utilize crowdsourcing to refine the result of the truth\\ndiscovery. We propose a task assignment algorithm to maximize the accuracy of\\nthe inferred truths. The performance study with real-life datasets confirms the\\neffectiveness of our truth inference and task assignment algorithms.', comment=None, journal_ref='Proceedings of the 22nd International Conference on Extending\\n  Database Technology, 2019. pp. 205-216', doi='10.5441/002/edbt.2019.19', primary_category='cs.DB', categories=['cs.DB', 'I.2.6'], links=[arxiv.Result.Link('http://dx.doi.org/10.5441/002/edbt.2019.19', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1904.10217v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.10217v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.11121v1', updated=datetime.datetime(2019, 4, 25, 1, 50, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 25, 1, 50, 52, tzinfo=datetime.timezone.utc), title='Declarative Recursive Computation on an RDBMS, or, Why You Should Use a Database For Distributed Machine Learning', authors=[arxiv.Result.Author('Dimitrije Jankov'), arxiv.Result.Author('Shangyu Luo'), arxiv.Result.Author('Binhang Yuan'), arxiv.Result.Author('Zhuhua Cai'), arxiv.Result.Author('Jia Zou'), arxiv.Result.Author('Chris Jermaine'), arxiv.Result.Author('Zekai J. Gao')], summary=\"A number of popular systems, most notably Google's TensorFlow, have been\\nimplemented from the ground up to support machine learning tasks. We consider\\nhow to make a very small set of changes to a modern relational database\\nmanagement system (RDBMS) to make it suitable for distributed learning\\ncomputations. Changes include adding better support for recursion, and\\noptimization and execution of very large compute plans. We also show that there\\nare key advantages to using an RDBMS as a machine learning platform. In\\nparticular, learning based on a database management system allows for trivial\\nscaling to large data sets and especially large models, where different\\ncomputational units operate on different parts of a model that may be too large\\nto fit into RAM.\", comment=None, journal_ref=None, doi='10.14778/3317315.3317323', primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3317315.3317323', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1904.11121v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.11121v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.11201v1', updated=datetime.datetime(2019, 4, 25, 8, 20, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 25, 8, 20, 5, tzinfo=datetime.timezone.utc), title='GPU-based Efficient Join Algorithms on Hadoop', authors=[arxiv.Result.Author('Hongzhi Wang'), arxiv.Result.Author('Ning Li'), arxiv.Result.Author('Zheng Wang'), arxiv.Result.Author('Jianing Li')], summary='The growing data has brought tremendous pressure for query processing and\\nstorage, so there are many studies that focus on using GPU to accelerate join\\noperation, which is one of the most important operations in modern database\\nsystems. However, existing GPU acceleration join operation researches are not\\nvery suitable for the join operation on big data. Based on this, this paper\\nspeeds up nested loop join, hash join and theta join, combining Hadoop with\\nGPU, which is also the first to use GPU to accelerate theta join. At the same\\ntime, after the data pre-filtering and pre-processing, using Map-Reduce and\\nHDFS in Hadoop proposed in this paper, the larger data table can be handled,\\ncompared to existing GPU acceleration methods. Also with Map-Reduce in Hadoop,\\nthe algorithm proposed in this paper can estimate the number of results more\\naccurately and allocate the appropriate storage space without unnecessary\\ncosts, making it more efficient. The rigorous experiments show that the\\nproposed method can obtain 1.5 to 2 times the speedup, compared to the\\ntraditional GPU acceleration equi join algorithm. And in the synthetic data\\nset, the GPU version of the proposed method can get 1.3 to 2 times the speedup,\\ncompared to CPU version.', comment='39 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.11201v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.11201v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.11653v1', updated=datetime.datetime(2019, 4, 26, 2, 31, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 26, 2, 31, 53, tzinfo=datetime.timezone.utc), title='Regular Expression Matching on billion-nodes Graphs', authors=[arxiv.Result.Author('Hongzhi Wang'), arxiv.Result.Author('Jiabao Han'), arxiv.Result.Author('Bin Shao'), arxiv.Result.Author('Jianzhong Li')], summary='In many applications, it is necessary to retrieve pairs of vertices with the\\npath between them satisfying certain constraints, since regular expression is a\\npowerful tool to describe patterns of a sequence. To meet such requirements, in\\nthis paper, we define regular expression (RE) query on graphs to use regular\\nexpression to represent the constraints between vertices. To process RE queries\\non large graphs such as social networks, we propose the RE query processing\\nmethod with the index size sublinear to the graph size. Considering that large\\ngraphs may be randomly distributed in multiple machines, the parallel RE\\nprocessing algorithms are presented without the assumption of graph\\ndistribution. To achieve high efficiency for complex RE query processing, we\\ndevelop cost-based query optimization strategies with only a small size\\nstatistical information which is suitable for querying large graphs.\\nComprehensive experimental results show that this approach works scale well for\\nlarge graphs.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.11653v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.11653v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.11827v2', updated=datetime.datetime(2019, 5, 7, 13, 32, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 26, 13, 7, 53, tzinfo=datetime.timezone.utc), title='AlphaClean: Automatic Generation of Data Cleaning Pipelines', authors=[arxiv.Result.Author('Sanjay Krishnan'), arxiv.Result.Author('Eugene Wu')], summary='The analyst effort in data cleaning is gradually shifting away from the\\ndesign of hand-written scripts to building and tuning complex pipelines of\\nautomated data cleaning libraries. Hyper-parameter tuning for data cleaning is\\nvery different than hyper-parameter tuning for machine learning since the\\npipeline components and objective functions have structure that tuning\\nalgorithms can exploit. This paper proposes a framework, called AlphaClean,\\nthat rethinks parameter tuning for data cleaning pipelines. AlphaClean provides\\nusers with a rich library to define data quality measures with weighted sums of\\nSQL aggregate queries. AlphaClean applies generate-then-search framework where\\neach pipelined cleaning operator contributes candidate transformations to a\\nshared pool. Asynchronously, in separate threads, a search algorithm sequences\\nthem into cleaning pipelines that maximize the user-defined quality measures.\\nThis architecture allows AlphaClean to apply a number of optimizations\\nincluding incremental evaluation of the quality measures and learning dynamic\\npruning rules to reduce the search space. Our experiments on real and synthetic\\nbenchmarks suggest that AlphaClean finds solutions of up-to 9x higher quality\\nthan naively applying state-of-the-art parameter tuning methods, is\\nsignificantly more robust to straggling data cleaning methods and redundancy in\\nthe data cleaning library, and can incorporate state-of-the-art cleaning\\nsystems such as HoloClean as cleaning operators.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.11827v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.11827v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.12344v1', updated=datetime.datetime(2019, 4, 28, 16, 49, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 28, 16, 49, 59, tzinfo=datetime.timezone.utc), title='Towards a New Extracting and Querying Approach of Fuzzy Summaries', authors=[arxiv.Result.Author('Ines Benali-Sougui'), arxiv.Result.Author('Minyar Sassi Hidri'), arxiv.Result.Author('Amel Grissa-Touzi')], summary='Diversification of DB applications highlighted the limitations of relational\\ndatabase management system (RDBMS) particularly on the modeling plan. In fact,\\nin the real world, we are increasingly faced with the situation where\\napplications need to handle imprecise data and to offer a flexible querying to\\ntheir users. Several theoretical solutions have been proposed. However, the\\nimpact of this work in practice remained negligible with the exception of a few\\nresearch prototypes based on the formal model GEFRED. In this chapter, the\\nauthors propose a new approach for exploitation of fuzzy relational databases\\n(FRDB) described by the model GEFRED. This approach consists of 1) a new\\ntechnique for extracting summary fuzzy data, Fuzzy SAINTETIQ, based on the\\nclassification of fuzzy data and formal concepts analysis; 2) an approach of\\nassessing flexible queries in the context of FDB based on the set of fuzzy\\nsummaries generated by our fuzzy SAINTETIQ system; 3) an approach of repairing\\nand substituting unanswered query.', comment='22 pages, 6 figures, 8 tables. Multidisciplinary Approaches to\\n  Service-Oriented Engineering, 2018. arXiv admin note: text overlap with\\n  arXiv:1401.0494', journal_ref=None, doi='10.4018/978-1-5225-5951-1.ch015', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.4018/978-1-5225-5951-1.ch015', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1904.12344v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.12344v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1904.13251v1', updated=datetime.datetime(2019, 4, 29, 14, 37, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 4, 29, 14, 37, 28, tzinfo=datetime.timezone.utc), title='From Digitalization to Data-Driven Decision Making in Container Terminals', authors=[arxiv.Result.Author('Leonard Heilig'), arxiv.Result.Author('Robert Stahlbock'), arxiv.Result.Author('Stefan Voß')], summary='With the new opportunities emerging from the current wave of digitalization,\\nterminal planning and management need to be revisited by taking a data-driven\\nperspective. Business analytics, as a practice of extracting insights from\\noperational data, assists in reducing uncertainties using predictions and helps\\nto identify and understand causes of inefficiencies, disruptions, and anomalies\\nin intra- and inter-organizational terminal operations. Despite the growing\\ncomplexity of data within and around container terminals, a lack of data-driven\\napproaches in the context of container terminals can be identified. In this\\nchapter, the concept of business analytics for supporting terminal planning and\\nmanagement is introduced. The chapter specifically focuses on data mining\\napproaches and provides a comprehensive overview on applications in container\\nterminals and related research. As such, we aim to establish a data-driven\\nperspective on terminal planning and management, complementing the traditional\\noptimization perspective.', comment='20 pages, 5 figures, book chapter', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1904.13251v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.13251v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.00687v1', updated=datetime.datetime(2019, 5, 2, 12, 3, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 2, 12, 3, 52, tzinfo=datetime.timezone.utc), title='Towards a Novel Cooperative Logistics Information System Framework', authors=[arxiv.Result.Author('Fares Zaidi'), arxiv.Result.Author('Laurent Amanton'), arxiv.Result.Author('Eric Sanlaville')], summary='Supply Chains and Logistics have a growing importance in global economy.\\nSupply Chain Information Systems over the world are heterogeneous and each one\\ncan both produce and receive massive amounts of structured and unstructured\\ndata in real-time, which are usually generated by information systems,\\nconnected objects or manually by humans. This heterogeneity is due to Logistics\\nInformation Systems components and processes that are developed by different\\nmodelling methods and running on many platforms; hence, decision making process\\nis difficult in such multi-actor environment. In this paper we identify some\\ncurrent challenges and integration issues between separately designed Logistics\\nInformation Systems (LIS), and we propose a Distributed Cooperative Logistics\\nPlatform (DCLP) framework based on NoSQL, which facilitates real-time\\ncooperation between stakeholders and improves decision making process in a\\nmulti-actor environment. We included also a case study of Hospital Supply Chain\\n(HSC), and a brief discussion on perspectives and future scope of work.', comment=None, journal_ref='7th International Conference on Information Systems, Logistics and\\n  Supply Chain (ILS2018), Jul 2018, Lyon, France', doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.00687v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.00687v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.00774v1', updated=datetime.datetime(2019, 5, 2, 14, 35, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 2, 14, 35, 47, tzinfo=datetime.timezone.utc), title='Can the Optimizer Cost be Used to Predict Query Execution Times?', authors=[arxiv.Result.Author('Anthony Kleerekoper'), arxiv.Result.Author('Javier Navaridas'), arxiv.Result.Author('Mikel Lujan')], summary='Predicting the execution time of queries is an important problem with\\napplications in scheduling, service level agreements and error detection.\\nDuring query planning, a cost is associated with the chosen execution plan and\\nused to rank competing plans. It would be convenient to use that cost to\\npredict execution time, but it has been claimed in the literature that this is\\nnot possible. In this paper, we thoroughly investigate this claim considering\\nboth linear and non-linear models. We find that the accuracy using more complex\\nmodels with only the optimizer cost is comparable to the reported accuracy in\\nthe literature. The most accurate method in the literature is nearest-neighbour\\nregression which does not produce a model. The published results used a large\\nfeature set to identify nearest neighbours. We show that it is possible to\\nachieve the same level of accuracy using only the cost to identify nearest\\nneighbours. Using a smaller feature set brings the advantages of reduced\\noverhead in terms of both storage space for the training data and the time to\\nproduce a prediction.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.00774v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.00774v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.00983v1', updated=datetime.datetime(2019, 5, 2, 22, 4, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 2, 22, 4, 10, tzinfo=datetime.timezone.utc), title='SUMMARIZED: Efficient Framework for Analyzing Multidimensional Process Traces under Edit-distance Constraint', authors=[arxiv.Result.Author('Phuong Nguyen'), arxiv.Result.Author('Vatche Ishakian'), arxiv.Result.Author('Vinod Muthusamy'), arxiv.Result.Author('Aleksander Slominski')], summary='Domains such as scientific workflows and business processes exhibit data\\nmodels with complex relationships between objects. This relationship is\\ntypically represented as sequences, where each data item is annotated with\\nmulti-dimensional attributes. There is a need to analyze this data for\\noperational insights. For example, in business processes, users are interested\\nin clustering process traces into smaller subsets to discover less complex\\nprocess models. This requires expensive computation of similarity metrics\\nbetween sequence-based data. Related work on dimension reduction and embedding\\nmethods do not take into account the multi-dimensional attributes of data, and\\ndo not address the interpretability of data in the embedding space (i.e., by\\nfavoring vector-based representation). In this work, we introduce Summarized, a\\nframework for efficient analysis on sequence-based multi-dimensional data using\\nintuitive and user-controlled summarizations. We introduce summarization\\nschemes that provide tunable trade-offs between the quality and efficiency of\\nanalysis tasks and derive an error model for summary-based similarity under an\\nedit-distance constraint. Evaluations using real-world datasets show the\\neffectives of our framework.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.00983v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.00983v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.01351v1', updated=datetime.datetime(2019, 5, 3, 19, 53, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 3, 19, 53, 11, tzinfo=datetime.timezone.utc), title='In Defense of Synthetic Data', authors=[arxiv.Result.Author('Luke Rodriguez'), arxiv.Result.Author('Bill Howe')], summary='Synthetic datasets have long been thought of as second-rate, to be used only\\nwhen \"real\" data collected directly from the real world is unavailable. But\\nthis perspective assumes that raw data is clean, unbiased, and trustworthy,\\nwhich it rarely is. Moreover, the benefits of synthetic data for privacy and\\nfor bias correction are becoming increasingly important in any domain that\\nworks with people. Curated synthetic datasets - synthetic data derived from\\nminimal perturbations of real data - enable early stage product development and\\ncollaboration, protect privacy, afford reproducibility, increase dataset\\ndiversity in research, and protect disadvantaged groups from problematic\\ninferences on the original data that reflects systematic discrimination. Rather\\nthan representing a departure from the true state of the world, in this paper\\nwe argue that properly generated synthetic data is a step towards responsible\\nand equitable research and development of machine learning systems.', comment='Discussion paper at FATES on the Web 2019', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.01351v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.01351v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.01425v1', updated=datetime.datetime(2019, 5, 4, 3, 59, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 4, 3, 59, 5, tzinfo=datetime.timezone.utc), title='Learning Functional Dependencies with Sparse Regression', authors=[arxiv.Result.Author('Zhihan Guo'), arxiv.Result.Author('Theodoros Rekatsinas')], summary='We study the problem of discovering functional dependencies (FD) from a noisy\\ndataset. We focus on FDs that correspond to statistical dependencies in a\\ndataset and draw connections between FD discovery and structure learning in\\nprobabilistic graphical models. We show that discovering FDs from a noisy\\ndataset is equivalent to learning the structure of a graphical model over\\nbinary random variables, where each random variable corresponds to a functional\\nof the dataset attributes. We build upon this observation to introduce AutoFD a\\nconceptually simple framework in which learning functional dependencies\\ncorresponds to solving a sparse regression problem. We show that our methods\\ncan recover true functional dependencies across a diverse array of real-world\\nand synthetic datasets, even in the presence of noisy or missing data. We find\\nthat AutoFD scales to large data instances with millions of tuples and hundreds\\nof attributes while it yields an average F1 improvement of 2 times against\\nstate-of-the-art FD discovery methods.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.01425v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.01425v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.02010v1', updated=datetime.datetime(2019, 5, 6, 13, 2, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 6, 13, 2, 54, tzinfo=datetime.timezone.utc), title='Errata Note: Discovering Order Dependencies through Order Compatibility', authors=[arxiv.Result.Author('Parke Godfrey'), arxiv.Result.Author('Lukasz Golab'), arxiv.Result.Author('Mehdi Kargar'), arxiv.Result.Author('Divesh Srivastava'), arxiv.Result.Author('Jaroslaw Szlichta')], summary='A number of extensions to the classical notion of functional dependencies\\nhave been proposed to express and enforce application semantics. One of these\\nextensions is that of order dependencies (ODs), which express rules involving\\norder. The article entitled \"Discovering Order Dependencies through Order\\nCompatibility\" by Consonni et al., published in the EDBT conference proceedings\\nin March 2019, investigates the OD discovery problem. They claim to prove that\\ntheir OD discovery algorithm, OCDDISCOVER, is complete, as well as being\\nsignificantly more efficient in practice than the state-of-the-art. They\\nfurther claim that the implementation of the existing FASTOD algorithm\\n(ours)-we shared our code base with the authors-which they benchmark against is\\nflawed, as OCDDISCOVER and FASTOD report different sets of ODs over the same\\ndata sets.\\n  In this rebuttal, we show that their claim of completeness is, in fact, not\\ntrue. Built upon their incorrect claim, OCDDISCOVER\\'s pruning rules are overly\\naggressive, and prune parts of the search space that contain legitimate ODs.\\nThis is the reason their approach appears to be \"faster\" in practice. Finally,\\nwe show that Consonni et al. misinterpret our set-based canonical form for ODs,\\nleading to an incorrect claim that our FASTOD implementation has an error.', comment='5', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.02010v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.02010v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.02069v1', updated=datetime.datetime(2019, 5, 6, 14, 47, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 6, 14, 47, 33, tzinfo=datetime.timezone.utc), title='Mixing set and bag semantics', authors=[arxiv.Result.Author('Wilmer Ricciotti'), arxiv.Result.Author('James Cheney')], summary=\"The conservativity theorem for nested relational calculus implies that query\\nexpressions can freely use nesting and unnesting, yet as long as the query\\nresult type is a flat relation, these capabilities do not lead to an increase\\nin expressiveness over flat relational queries. Moreover, Wong showed how such\\nqueries can be translated to SQL via a constructive rewriting algorithm. While\\nthis result holds for queries over either set or multiset semantics, to the\\nbest of our knowledge, the questions of conservativity and normalization have\\nnot been studied for queries that mix set and bag collections, or provide\\nduplicate-elimination operations such as SQL's\\n$\\\\mathtt{SELECT}~\\\\mathtt{DISTINCT}$. In this paper we formalize the problem,\\nand present partial progress: specifically, we introduce a calculus with both\\nset and multiset collection types, along with natural mappings from sets to\\nbags and vice versa, present a set of valid rewrite rules for normalizing such\\nqueries, and give an inductive characterization of a set of queries whose\\nnormal forms can be translated to SQL. We also consider examples that do not\\nappear straightforward to translate to SQL, illustrating that the relative\\nexpressiveness of flat and nested queries with mixed set and multiset semantics\\nremains an open question.\", comment='DBPL 2019 -- short paper', journal_ref=None, doi='10.1145/3315507.3330202', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3315507.3330202', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1905.02069v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.02069v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.02157v1', updated=datetime.datetime(2019, 5, 6, 17, 17, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 6, 17, 17, 44, tzinfo=datetime.timezone.utc), title='BlockLite: A Lightweight Emulator for Public Blockchains', authors=[arxiv.Result.Author('Xinying Wang'), arxiv.Result.Author('Abdullah Al-Mamun'), arxiv.Result.Author('Feng Yan'), arxiv.Result.Author('Mohammad Sadoghi'), arxiv.Result.Author('Dongfang Zhao')], summary='Blockchain is an enabler of many emerging decentralized applications in areas\\nof cryptocurrency, Internet of Things, smart healthcare, among many others.\\nAlthough various open-source blockchain frameworks are available, the\\ninfrastructure is complex enough and difficult for many users to modify or test\\nout new research ideas. To make it worse, many advantages of blockchain systems\\ncan be demonstrated only at large scales, e.g., thousands of nodes, which are\\nnot always available to researchers. This demo paper presents a lightweight\\nsingle-node emulator of blockchain systems, namely \\\\mbox{BlockLite}, designed\\nto be executing real proof-of-work workload along with peer-to-peer network\\ncommunications and hash-based immutability. BlockLite employs a preprocessing\\napproach to avoid the per-node computation overhead at runtime and thus scales\\nto thousands of nodes. Moreover, BlockLite offers an easy-to-use programming\\ninterface allowing for a Lego-like customization to the system, e.g. new ad-hoc\\nconsensus protocols.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.02157v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.02157v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.03061v1', updated=datetime.datetime(2019, 5, 4, 3, 1, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 4, 3, 1, 20, tzinfo=datetime.timezone.utc), title='Generalized formal model of big data', authors=[arxiv.Result.Author('Shakhovska Nataliya'), arxiv.Result.Author('Veres Oleh'), arxiv.Result.Author('Hirnyak Mariia')], summary='This article dwells on the basic characteristic features of the Big Data\\ntechnologies. It is analyzed the existing definition of the \"big data\" term.\\nThe article proposes and describes the elements of the generalized formal model\\nof big data. It is analyzed the peculiarities of the application of the\\nproposed model components. It described the fundamental differences between Big\\nData technology and business analytics. Big Data is supported by the\\ndistributed file system Google File System technology, Cassandra, HBase, Lustre\\nand ZFS, by the MapReduce and Hadoop programming constructs and many other\\nsolutions. According to the experts, such as McKinsey Institute, the\\nmanufacturing, healthcare, trade, administration and control of individual\\nmovements undergo the transformations under the influence of the Big Data.', comment=None, journal_ref='ECONTECHMOD. AN INTERNATIONAL QUARTERLY JOURNAL - 2016, Vol. 05,\\n  No. 2, 33-38', doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.03061v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.03061v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.04037v1', updated=datetime.datetime(2019, 5, 10, 9, 46, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 10, 9, 46, 1, tzinfo=datetime.timezone.utc), title='Metadata Management for Textual Documents in Data Lakes', authors=[arxiv.Result.Author('Pegdwendé Sawadogo'), arxiv.Result.Author('Tokio Kibata'), arxiv.Result.Author('Jérôme Darmont')], summary='Data lakes have emerged as an alternative to data warehouses for the storage,\\nexploration and analysis of big data. In a data lake, data are stored in a raw\\nstate and bear no explicit schema. Thence, an efficient metadata system is\\nessential to avoid the data lake turning to a so-called data swamp. Existing\\nworks about managing data lake metadata mostly focus on structured and\\nsemi-structured data, with little research on unstructured data. Thus, we\\npropose in this paper a methodological approach to build and manage a metadata\\nsystem that is specific to textual documents in data lakes. First, we make an\\ninventory of usual and meaningful metadata to extract. Then, we apply some\\nspecific techniques from the text mining and information retrieval domains to\\nextract, store and reuse these metadata within the COREL research project, in\\norder to validate our proposals.', comment=None, journal_ref='21st International Conference on Enterprise Information Systems\\n  (ICEIS 2019), May 2019, Heraklion, Greece. pp.72-83', doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.04037v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.04037v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.04795v1', updated=datetime.datetime(2019, 5, 12, 21, 39, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 12, 21, 39, 3, tzinfo=datetime.timezone.utc), title='NFTracer: A Non-Fungible Token Tracking Proof-of-Concept Using Hyperledger Fabric', authors=[arxiv.Result.Author('Mustafa Bal'), arxiv.Result.Author('Caitlin Ner')], summary='Various start-up developers and academic researchers have investigated the\\nusage of blockchain as a data storage medium due to the advantages offered by\\nits tamper-proof and decentralized nature. However, there have not been many\\nattempts to provide a standard platform for virtually storing the states of\\nunique tangible entities and their subsequent modifications. In this paper, we\\npropose NFTracer, a non-fungible token tracking proof-of-concept based on\\nHyperledger Composer and Hyperledger Fabric Blockchain. To achieve the\\ncapabilities of our platform, we use NFTracer to build an artwork auction and a\\nreal estate auction, which vary in technical complexity and demonstrate the\\nadvantages of being able to track entities and their resulting modifications in\\na decentralized manner. We also present its accompanying modular architecture\\nand system components, and discuss possible future works on NFTracer.', comment='9 pages, 3 figures, 5 algorithms, preprint', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.04795v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.04795v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.06167v4', updated=datetime.datetime(2020, 8, 21, 15, 40, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 15, 13, 28, 56, tzinfo=datetime.timezone.utc), title='A Survey of Blocking and Filtering Techniques for Entity Resolution', authors=[arxiv.Result.Author('George Papadakis'), arxiv.Result.Author('Dimitrios Skoutas'), arxiv.Result.Author('Emmanouil Thanos'), arxiv.Result.Author('Themis Palpanas')], summary='Efficiency techniques are an integral part of Entity Resolution, since its\\ninfancy. In this survey, we organized the bulk of works in the field into\\nBlocking, Filtering and hybrid techniques, facilitating their understanding and\\nuse. We also provided an in-dept coverage of each category, further classifying\\nthe corresponding works into novel sub-categories. Lately, the efficiency\\ntechniques have received more attention, due to the rise of Big Data. This\\nincludes large volumes of semi-structured data, which pose challenges not only\\nto the scalability of efficiency techniques, but also to their core\\nassumptions: the requirement of Blocking for schema knowledge and of Filtering\\nfor high similarity thresholds. The former led to the introduction of\\nschema-agnostic Blocking in conjunction with Block Processing techniques, while\\nthe latter led to more relaxed criteria of similarity. Our survey covers these\\nnew fields in detail, putting in context all relevant works.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.06167v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.06167v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.06170v1', updated=datetime.datetime(2019, 5, 15, 13, 31, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 15, 13, 31, 47, tzinfo=datetime.timezone.utc), title='MinoanER: Schema-Agnostic, Non-Iterative, Massively Parallel Resolution of Web Entities', authors=[arxiv.Result.Author('Vasilis Efthymiou'), arxiv.Result.Author('George Papadakis'), arxiv.Result.Author('Kostas Stefanidis'), arxiv.Result.Author('Vassilis Christophides')], summary='Entity Resolution (ER) aims to identify different descriptions in various\\nKnowledge Bases (KBs) that refer to the same entity. ER is challenged by the\\nVariety, Volume and Veracity of entity descriptions published in the Web of\\nData. To address them, we propose the MinoanER framework that simultaneously\\nfulfills full automation, support of highly heterogeneous entities, and massive\\nparallelization of the ER process. MinoanER leverages a token-based similarity\\nof entities to define a new metric that derives the similarity of neighboring\\nentities from the most important relations, as they are indicated only by\\nstatistics. A composite blocking method is employed to capture different\\nsources of matching evidence from the content, neighbors, or names of entities.\\nThe search space of candidate pairs for comparison is compactly abstracted by a\\nnovel disjunctive blocking graph and processed by a non-iterative, massively\\nparallel matching algorithm that consists of four generic, schema-agnostic\\nmatching rules that are quite robust with respect to their internal\\nconfiguration. We demonstrate that the effectiveness of MinoanER is comparable\\nto existing ER tools over real KBs exhibiting low Variety, but it outperforms\\nthem significantly when matching KBs with high Variety.', comment='Presented at EDBT 20019', journal_ref=None, doi='10.5441/002/edbt.2019.33', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.5441/002/edbt.2019.33', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1905.06170v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.06170v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.06256v1', updated=datetime.datetime(2019, 5, 8, 8, 14, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 8, 8, 14, 19, tzinfo=datetime.timezone.utc), title='A Scalable Learned Index Scheme in Storage Systems', authors=[arxiv.Result.Author('Pengfei Li'), arxiv.Result.Author('Yu Hua'), arxiv.Result.Author('Pengfei Zuo'), arxiv.Result.Author('Jingnan Jia')], summary='Index structures are important for efficient data access, which have been\\nwidely used to improve the performance in many in-memory systems. Due to high\\nin-memory overheads, traditional index structures become difficult to process\\nthe explosive growth of data, let alone providing low latency and high\\nthroughput performance with limited system resources. The promising learned\\nindexes leverage deep-learning models to complement existing index structures\\nand obtain significant memory savings. However, the learned indexes fail to\\nbecome scalable due to the heavy inter-model dependency and expensive\\nretraining. To address these problems, we propose a scalable learned index\\nscheme to construct different linear regression models according to the data\\ndistribution. Moreover, the used models are independent so as to reduce the\\ncomplexity of retraining and become easy to partition and store the data into\\ndifferent pages, blocks or distributed systems. Our experimental results show\\nthat compared with state-of-the-art schemes, AIDEL improves the insertion\\nperformance by about 2$\\\\times$ and provides comparable lookup performance,\\nwhile efficiently supporting scalability.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG', 'stat.ML'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.06256v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.06256v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.06397v3', updated=datetime.datetime(2020, 8, 19, 16, 38, 49, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 15, 19, 15, 11, tzinfo=datetime.timezone.utc), title='End-to-End Entity Resolution for Big Data: A Survey', authors=[arxiv.Result.Author('Vassilis Christophides'), arxiv.Result.Author('Vasilis Efthymiou'), arxiv.Result.Author('Themis Palpanas'), arxiv.Result.Author('George Papadakis'), arxiv.Result.Author('Kostas Stefanidis')], summary='One of the most important tasks for improving data quality and the\\nreliability of data analytics results is Entity Resolution (ER). ER aims to\\nidentify different descriptions that refer to the same real-world entity, and\\nremains a challenging problem. While previous works have studied specific\\naspects of ER (and mostly in traditional settings), in this survey, we provide\\nfor the first time an end-to-end view of modern ER workflows, and of the novel\\naspects of entity indexing and matching methods in order to cope with more than\\none of the Big Data characteristics simultaneously. We present the basic\\nconcepts, processing steps and execution strategies that have been proposed by\\ndifferent communities, i.e., database, semantic Web and machine learning, in\\norder to cope with the loose structuredness, extreme diversity, high speed and\\nlarge scale of entity descriptions used by real-world applications. Finally, we\\nprovide a synthetic discussion of the existing approaches, and conclude with a\\ndetailed presentation of open research directions.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.06397v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.06397v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.06425v2', updated=datetime.datetime(2019, 9, 12, 0, 0, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 15, 20, 30, 44, tzinfo=datetime.timezone.utc), title='An Empirical Analysis of Deep Learning for Cardinality Estimation', authors=[arxiv.Result.Author('Jennifer Ortiz'), arxiv.Result.Author('Magdalena Balazinska'), arxiv.Result.Author('Johannes Gehrke'), arxiv.Result.Author('S. Sathiya Keerthi')], summary='We implement and evaluate deep learning for cardinality estimation by\\nstudying the accuracy, space and time trade-offs across several architectures.\\nWe find that simple deep learning models can learn cardinality estimations\\nacross a variety of datasets (reducing the error by 72% - 98% on average\\ncompared to PostgreSQL). In addition, we empirically evaluate the impact of\\ninjecting cardinality estimates produced by deep learning models into the\\nPostgreSQL optimizer. In many cases, the estimates from these models lead to\\nbetter query plans across all datasets, reducing the runtimes by up to 49% on\\nselect-project-join workloads. As promising as these models are, we also\\ndiscuss and address some of the challenges of using them in practice.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.06425v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.06425v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.06480v1', updated=datetime.datetime(2019, 5, 16, 0, 19, 49, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 16, 0, 19, 49, tzinfo=datetime.timezone.utc), title='The CEDAR Workbench: An Ontology-Assisted Environment for Authoring Metadata that Describe Scientific Experiments', authors=[arxiv.Result.Author('Rafael S. Gonçalves'), arxiv.Result.Author(\"Martin J. O'Connor\"), arxiv.Result.Author('Marcos Martínez-Romero'), arxiv.Result.Author('Attila L. Egyedi'), arxiv.Result.Author('Debra Willrett'), arxiv.Result.Author('John Graybeal'), arxiv.Result.Author('Mark A. Musen')], summary='The Center for Expanded Data Annotation and Retrieval (CEDAR) aims to\\nrevolutionize the way that metadata describing scientific experiments are\\nauthored. The software we have developed--the CEDAR Workbench--is a suite of\\nWeb-based tools and REST APIs that allows users to construct metadata\\ntemplates, to fill in templates to generate high-quality metadata, and to share\\nand manage these resources. The CEDAR Workbench provides a versatile,\\nREST-based environment for authoring metadata that are enriched with terms from\\nontologies. The metadata are available as JSON, JSON-LD, or RDF for easy\\nintegration in scientific applications and reusability on the Web. Users can\\nleverage our APIs for validating and submitting metadata to external\\nrepositories. The CEDAR Workbench is freely available and open-source.', comment=None, journal_ref=None, doi='10.1007/978-3-319-68204-4_10', primary_category='cs.DB', categories=['cs.DB', 'cs.DL'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-319-68204-4_10', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1905.06480v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.06480v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.06760v1', updated=datetime.datetime(2019, 5, 16, 14, 2, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 16, 14, 2, 29, tzinfo=datetime.timezone.utc), title='Persistent Buffer Management with Optimistic Consistency', authors=[arxiv.Result.Author('Lucas Lersch'), arxiv.Result.Author('Wolfgang Lehner'), arxiv.Result.Author('Ismail Oukid')], summary='Finding the best way to leverage non-volatile memory (NVM) on modern database\\nsystems is still an open problem. The answer is far from trivial since the\\nclear boundary between memory and storage present in most systems seems to be\\nincompatible with the intrinsic memory-storage duality of NVM. Rather than\\ntreating NVM either solely as memory or solely as storage, in this work we\\npropose how NVM can be simultaneously used as both in the context of modern\\ndatabase systems. We design a persistent buffer pool on NVM, enabling pages to\\nbe directly read/written by the CPU (like memory) while recovering corrupted\\npages after a failure (like storage). The main benefits of our approach are an\\neasy integration in the existing database architectures, reduced costs (by\\nreplacing DRAM with NVM), and faster peak-performance recovery.', comment=None, journal_ref='Proceedings of the 15th International Workshop on Data Management\\n  on New Hardware, Article No. 14 (2019) 1-3', doi='10.1145/3329785.3329931', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3329785.3329931', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1905.06760v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.06760v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.07113v1', updated=datetime.datetime(2019, 5, 17, 4, 50, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 17, 4, 50, 53, tzinfo=datetime.timezone.utc), title='High Throughput Push Based Storage Manager', authors=[arxiv.Result.Author('Ye Zhu')], summary='The storage manager, as a key component of the database system, is\\nresponsible for organizing, reading, and delivering data to the execution\\nengine for processing. According to the data serving mechanism, existing\\nstorage managers are either pull-based, incurring high latency, or push-based,\\nleading to a high number of I/O requests when the CPU is busy. To improve these\\nshortcomings, this thesis proposes a push-based prefetching strategy in a\\ncolumn-wise storage manager. The proposed strategy implements an efficient\\ncache layer to store shared data among queries to reduce the number of I/O\\nrequests. The capacity of the cache is maintained by a time access-aware\\neviction mechanism. Our strategy enables the storage manager to coordinate\\nmultiple queries by merging their requests and dynamically generate an optimal\\nread order that maximizes the overall I/O throughput. We evaluated our storage\\nmanager both over a disk-based redundant array of independent disks (RAID) and\\nan NVM Express (NVMe) solid-state drive (SSD). With the high read performance\\nof the SSD, we successfully minimized the total read time and number of I/O\\naccesses.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.07113v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.07113v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.07663v1', updated=datetime.datetime(2019, 5, 19, 0, 1, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 19, 0, 1, 5, tzinfo=datetime.timezone.utc), title='Regions In a Linked Dataset For Change Detection', authors=[arxiv.Result.Author('Anuj Singh')], summary='Linked Datasets (LDs) are constantly evolving and the applications using a\\nLinked Dataset (LD) may face several issues such as outdated data or broken\\ninterlinks due to evolution of the dataset. To overcome these issues, the\\ndetection of changes in LDs during their evolution has proven crucial. As LDs\\nevolve frequently, the change detection during the evolution should also be\\ndone at frequent intervals. However, due to limitation of available\\ncomputational resources such as capacity to fetch data from LD and time to\\ndetect changes, the frequent change detection may not be possible with existing\\nchange detection techniques. This research proposes to explore the notion of\\nprioritization of regions (subsets) in LDs for change detection with the aim of\\nachieving optimal accuracy and efficient use of available computational\\nresources. This will facilitate the detection of changes in an evolving LD at\\nfrequent intervals and will allow the applications to update their data closest\\nto real-time data.', comment='It is a doctoral consortium paper, which was accepted in ISWC 2018\\n  but was not published there as the author was not able to attend the\\n  conference', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.07663v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.07663v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.08337v1', updated=datetime.datetime(2019, 5, 20, 20, 29, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 20, 20, 29, 44, tzinfo=datetime.timezone.utc), title='Ingesting High-Velocity Streaming Graphs from Social Media Sources', authors=[arxiv.Result.Author('Subhasis Dasgupta'), arxiv.Result.Author('Aditya Bagchi'), arxiv.Result.Author('Amarnath Gupta')], summary='Many data science applications like social network analysis use graphs as\\ntheir primary form of data. However, acquiring graph-structured data from\\nsocial media presents some interesting challenges. The first challenge is the\\nhigh data velocity and bursty nature of the social media data. The second\\nchallenge is that the complex nature of the data makes the ingestion process\\nexpensive. If we want to store the streaming graph data in a graph database, we\\nface a third challenge -- the database is very often unable to sustain the\\ningestion of high-velocity, high-burst data. We have developed an adaptive\\nbuffering mechanism and a graph compression technique that effectively\\nmitigates the problem. A novel aspect of our method is that the adaptive\\nbuffering algorithm uses the data rate, the data content as well as the CPU\\nresources of the database machine to determine an optimal data ingestion\\nmechanism. We further show that an ingestion-time graph-compression strategy\\nimproves the efficiency of the data ingestion into the database. We have\\nverified the efficacy of our ingestion optimization strategy through extensive\\nexperiments.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG', 'cs.SI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.08337v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.08337v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.09251v2', updated=datetime.datetime(2020, 6, 9, 19, 21, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 22, 17, 14, 20, tzinfo=datetime.timezone.utc), title='IPAW 2020 Preprint: Efficient Computation of Provenance for Query Result Exploration', authors=[arxiv.Result.Author('Murali Mani'), arxiv.Result.Author('Naveenkumar Singaraj'), arxiv.Result.Author('Zhenyan Liu')], summary='Users typically interact with a database by asking queries and examining the\\nresults. We refer to the user examining the query results and asking follow-up\\nquestions as query result exploration. Our work builds on two decades of\\nprovenance research useful for query result exploration. Three approaches for\\ncomputing provenance have been described in the literature: lazy, eager, and\\nhybrid. We investigate lazy and eager approaches that utilize constraints that\\nwe have identified in the context of query result exploration, as well as novel\\nhybrid approaches. For the TPC-H benchmark, these constraints are applicable to\\n19 out of the 22 queries, and result in a better performance for all queries\\nthat have a join. Furthermore, the performance benefits from our approaches are\\nsignificant, sometimes several orders of magnitude.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.09251v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.09251v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.09359v1', updated=datetime.datetime(2019, 5, 22, 20, 44, 36, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 22, 20, 44, 36, tzinfo=datetime.timezone.utc), title='Towards Global Asset Management in Blockchain Systems', authors=[arxiv.Result.Author('Victor Zakhary'), arxiv.Result.Author('Mohammad Javad Amiri'), arxiv.Result.Author('Sujaya Maiyya'), arxiv.Result.Author('Divyakant Agrawal'), arxiv.Result.Author('Amr El Abbadi')], summary=\"Permissionless blockchains (e.g., Bitcoin, Ethereum, etc) have shown a wide\\nsuccess in implementing global scale peer-to-peer cryptocurrency systems. In\\nsuch blockchains, new currency units are generated through the mining process\\nand are used in addition to transaction fees to incentivize miners to maintain\\nthe blockchain. Although it is clear how currency units are generated and\\ntransacted on, it is unclear how to use the infrastructure of permissionless\\nblockchains to manage other assets than the blockchain's currency units (e.g.,\\ncars, houses, etc). In this paper, we propose a global asset management system\\nby unifying permissioned and permissionless blockchains. A governmental\\npermissioned blockchain authenticates the registration of end-user assets\\nthrough smart contract deployments on a permissionless blockchain. Afterwards,\\nend-users can transact on their assets through smart contract function calls\\n(e.g., sell a car, rent a room in a house, etc). In return, end-users get paid\\nin currency units of the same blockchain or other blockchains through atomic\\ncross-chain transactions and governmental offices receive taxes on these\\ntransactions in cryptocurrency units.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CR', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.09359v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.09359v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.09624v2', updated=datetime.datetime(2019, 7, 26, 13, 0, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 23, 12, 49, 58, tzinfo=datetime.timezone.utc), title='COBS: a Compact Bit-Sliced Signature Index', authors=[arxiv.Result.Author('Timo Bingmann'), arxiv.Result.Author('Phelim Bradley'), arxiv.Result.Author('Florian Gauger'), arxiv.Result.Author('Zamin Iqbal')], summary=\"We present COBS, a COmpact Bit-sliced Signature index, which is a cross-over\\nbetween an inverted index and Bloom filters. Our target application is to index\\n$k$-mers of DNA samples or $q$-grams from text documents and process\\napproximate pattern matching queries on the corpus with a user-chosen coverage\\nthreshold. Query results may contain a number of false positives which\\ndecreases exponentially with the query length. We compare COBS to seven other\\nindex software packages on 100000 microbial DNA samples. COBS' compact but\\nsimple data structure outperforms the other indexes in construction time and\\nquery performance with Mantis by Pandey et al. in second place. However, unlike\\nMantis and other previous work, COBS does not need the complete index in RAM\\nand is thus designed to scale to larger document sets.\", comment=\"To appear in 26th International Symposium on String Processing and\\n  Information Retrieval (SPIRE'19)\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS', 'cs.IR', 'H.3.3; H.3.1'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.09624v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.09624v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.10482v1', updated=datetime.datetime(2019, 5, 24, 23, 34, 48, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 24, 23, 34, 48, tzinfo=datetime.timezone.utc), title='Multi-Model Investigative Exploration of Social Media Data with boutique: A Case Study in Public Health', authors=[arxiv.Result.Author('Junan Guo'), arxiv.Result.Author('Subhasis Dasgupta'), arxiv.Result.Author('Amarnath Gupta')], summary='We present our experience with a data science problem in Public Health, where\\nresearchers use social media (Twitter) to determine whether the public shows\\nawareness of HIV prevention measures offered by Public Health campaigns. To\\nhelp the researcher, we develop an investigative exploration system called\\nboutique that allows a user to perform a multi-step visualization and\\nexploration of data through a dashboard interface. Unique features of boutique\\nincludes its ability to handle heterogeneous types of data provided by a\\npolystore, and its ability to use computation as part of the investigative\\nexploration process. In this paper, we present the design of the boutique\\nmiddleware and walk through an investigation process for a real-life problem.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.HC', 'cs.IR', 'cs.SI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.10482v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.10482v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.12133v1', updated=datetime.datetime(2019, 5, 28, 23, 26, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 28, 23, 26, 8, tzinfo=datetime.timezone.utc), title='One SQL to Rule Them All', authors=[arxiv.Result.Author('Edmon Begoli'), arxiv.Result.Author('Tyler Akidau'), arxiv.Result.Author('Fabian Hueske'), arxiv.Result.Author('Julian Hyde'), arxiv.Result.Author('Kathryn Knight'), arxiv.Result.Author('Kenneth Knowles')], summary='Real-time data analysis and management are increasingly critical for today`s\\nbusinesses. SQL is the de facto lingua franca for these endeavors, yet support\\nfor robust streaming analysis and management with SQL remains limited. Many\\napproaches restrict semantics to a reduced subset of features and/or require a\\nsuite of non-standard constructs. Additionally, use of event timestamps to\\nprovide native support for analyzing events according to when they actually\\noccurred is not pervasive, and often comes with important limitations. We\\npresent a three-part proposal for integrating robust streaming into the SQL\\nstandard, namely: (1) time-varying relations as a foundation for classical\\ntables as well as streaming data, (2) event time semantics, (3) a limited set\\nof optional keyword extensions to control the materialization of time-varying\\nquery results. Motivated and illustrated using examples and lessons learned\\nfrom implementations in Apache Calcite, Apache Flink, and Apache Beam, we show\\nhow with these minimal additions it is possible to utilize the complete suite\\nof standard SQL semantics to perform robust stream processing.', comment=None, journal_ref=None, doi='10.1145/3299869.3314040', primary_category='cs.DB', categories=['cs.DB', 'H.2.3'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3299869.3314040', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1905.12133v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.12133v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.12411v1', updated=datetime.datetime(2019, 5, 29, 13, 18, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 29, 13, 18, 3, tzinfo=datetime.timezone.utc), title='Designing and Implementing Data Warehouse for Agricultural Big Data', authors=[arxiv.Result.Author('Vuong M. Ngo'), arxiv.Result.Author('Nhien-An Le-Khac'), arxiv.Result.Author('M-Tahar Kechadi')], summary='In recent years, precision agriculture that uses modern information and\\ncommunication technologies is becoming very popular. Raw and semi-processed\\nagricultural data are usually collected through various sources, such as:\\nInternet of Thing (IoT), sensors, satellites, weather stations, robots, farm\\nequipment, farmers and agribusinesses, etc. Besides, agricultural datasets are\\nvery large, complex, unstructured, heterogeneous, non-standardized, and\\ninconsistent. Hence, the agricultural data mining is considered as Big Data\\napplication in terms of volume, variety, velocity and veracity. It is a key\\nfoundation to establishing a crop intelligence platform, which will enable\\nresource efficient agronomy decision making and recommendations. In this paper,\\nwe designed and implemented a continental level agricultural data warehouse by\\ncombining Hive, MongoDB and Cassandra. Our data warehouse capabilities: (1)\\nflexible schema; (2) data integration from real agricultural multi datasets;\\n(3) data science and business intelligent support; (4) high performance; (5)\\nhigh storage; (6) security; (7) governance and monitoring; (8) replication and\\nrecovery; (9) consistency, availability and partition tolerant; (10)\\ndistributed and cloud deployment. We also evaluate the performance of our data\\nwarehouse.', comment='Business intelligent, data warehouse, constellation schema, Big Data,\\n  precision agriculture', journal_ref='BigData 2019', doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LG', 'cs.PF'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.12411v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.12411v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.12744v2', updated=datetime.datetime(2020, 1, 24, 21, 41, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 29, 21, 32, 23, tzinfo=datetime.timezone.utc), title='Fair Decision Making using Privacy-Protected Data', authors=[arxiv.Result.Author('Satya Kuppam'), arxiv.Result.Author('Ryan Mckenna'), arxiv.Result.Author('David Pujol'), arxiv.Result.Author('Michael Hay'), arxiv.Result.Author('Ashwin Machanavajjhala'), arxiv.Result.Author('Gerome Miklau')], summary='Data collected about individuals is regularly used to make decisions that\\nimpact those same individuals. We consider settings where sensitive personal\\ndata is used to decide who will receive resources or benefits. While it is well\\nknown that there is a tradeoff between protecting privacy and the accuracy of\\ndecisions, we initiate a first-of-its-kind study into the impact of formally\\nprivate mechanisms (based on differential privacy) on fair and equitable\\ndecision-making. We empirically investigate novel tradeoffs on two real-world\\ndecisions made using U.S. Census data (allocation of federal funds and\\nassignment of voting rights benefits) as well as a classic apportionment\\nproblem. Our results show that if decisions are made using an\\n$\\\\epsilon$-differentially private version of the data, under strict privacy\\nconstraints (smaller $\\\\epsilon$), the noise added to achieve privacy may\\ndisproportionately impact some groups over others. We propose novel measures of\\nfairness in the context of randomized differentially private algorithms and\\nidentify a range of causes of outcome disparities.', comment='12 pages, 4 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.12744v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.12744v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.13011v1', updated=datetime.datetime(2019, 5, 29, 17, 56, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 29, 17, 56, 55, tzinfo=datetime.timezone.utc), title=\"Don't Persist All : Efficient Persistent Data Structures\", authors=[arxiv.Result.Author('Pratyush Mahapatra'), arxiv.Result.Author('Mark D. Hill'), arxiv.Result.Author('Michael M. Swift')], summary='Data structures used in software development have inbuilt redundancy to\\nimprove software reliability and to speed up performance. Examples include a\\nDoubly Linked List which allows a faster deletion due to the presence of the\\nprevious pointer. With the introduction of Persistent Memory, storing the\\nredundant data fields into persistent memory adds a significant write overhead,\\nand reduces performance. In this work, we focus on three data structures -\\nDoubly Linked List, B+Tree and Hashmap, and showcase alternate partly\\npersistent implementations where we only store a limited set of data fields to\\npersistent memory. After a crash/restart, we use the persistent data fields to\\nrecreate the data structures along with the redundant data fields. We compare\\nour implementation with the base implementation and show that we achieve\\nspeedups around 5-20% for some data structures, and up to 165% for a\\nflush-dominated data structure.', comment='10 pages, 12 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.13011v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.13011v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1905.13376v1', updated=datetime.datetime(2019, 5, 31, 1, 48, 58, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 5, 31, 1, 48, 58, tzinfo=datetime.timezone.utc), title='Efficient Multiway Hash Join on Reconfigurable Hardware', authors=[arxiv.Result.Author('Kunle Olukotun'), arxiv.Result.Author('Raghu Prabhakar'), arxiv.Result.Author('Rekha Singhal'), arxiv.Result.Author('Jeffrey D. Ullman'), arxiv.Result.Author('Yaqi Zhang')], summary=\"We propose the algorithms for performing multiway joins using a new type of\\ncoarse grain reconfigurable hardware accelerator~-- ``Plasticine''~-- that,\\ncompared with other accelerators, emphasizes high compute capability and high\\non-chip communication bandwidth. Joining three or more relations in a single\\nstep, i.e. multiway join, is efficient when the join of any two relations\\nyields too large an intermediate relation. We show at least 200X speedup for a\\nsequence of binary hash joins execution on Plasticine over CPU. We further show\\nthat in some realistic cases, a Plasticine-like accelerator can make 3-way\\njoins more efficient than a cascade of binary hash joins on the same hardware,\\nby a factor of up to 45X.\", comment='20 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1905.13376v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1905.13376v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.01627v1', updated=datetime.datetime(2019, 7, 2, 20, 50, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 2, 20, 50, 1, tzinfo=datetime.timezone.utc), title='Rule Applicability on RDF Triplestore Schemas', authors=[arxiv.Result.Author('Paolo Pareti'), arxiv.Result.Author('George Konstantinidis'), arxiv.Result.Author('Timothy J. Norman'), arxiv.Result.Author('Murat Şensoy')], summary='Rule-based systems play a critical role in health and safety, where policies\\ncreated by experts are usually formalised as rules. When dealing with\\nincreasingly large and dynamic sources of data, as in the case of Internet of\\nThings (IoT) applications, it becomes important not only to efficiently apply\\nrules, but also to reason about their applicability on datasets confined by a\\ncertain schema. In this paper we define the notion of a triplestore schema\\nwhich models a set of RDF graphs. Given a set of rules and such a schema as\\ninput we propose a method to determine rule applicability and produce output\\nschemas. Output schemas model the graphs that would be obtained by running the\\nrules on the graph models of the input schema. We present two approaches: one\\nbased on computing a canonical (critical) instance of the schema, and a novel\\napproach based on query rewriting. We provide theoretical, complexity and\\nevaluation results that show the superior efficiency of our rewriting approach.', comment='AI for Internet of Things Workshop, co-located with the 28th\\n  International Joint Conference on Artificial Intelligence (IJCAI-19)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1907.01627v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.01627v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.01831v3', updated=datetime.datetime(2019, 10, 19, 22, 56, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 3, 10, 15, 7, tzinfo=datetime.timezone.utc), title='GeoPrune: Efficiently Finding Shareable Vehicles Based on Geometric Properties', authors=[arxiv.Result.Author('Yixin Xu'), arxiv.Result.Author('Jianzhong Qi'), arxiv.Result.Author('Renata Borovica-Gajic'), arxiv.Result.Author('Lars Kulik')], summary='On-demand ride-sharing is rapidly growing.Matching trip requests to vehicles\\nefficiently is critical for the service quality of ride-sharing. To match trip\\nrequests with vehicles, a prune-and-select scheme is commonly used. The pruning\\nstage identifies feasible vehicles that can satisfy the trip constraints (e.g.,\\ntrip time). The selection stage selects the optimal one(s) from the feasible\\nvehicles. The pruning stage is crucial to reduce the complexity of the\\nselection stage and to achieve efficient matching. We propose an effective and\\nefficient pruning algorithm called GeoPrune. GeoPrune represents the time\\nconstraints of trip requests using circles and ellipses, which can be computed\\nand updated efficiently. Experiments on real-world datasets show that GeoPrune\\nreduces the number of vehicle candidates in nearly all cases by an order of\\nmagnitude and the update cost by two to three orders of magnitude compared to\\nthe state-of-the-art.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'eess.SP'], links=[arxiv.Result.Link('http://arxiv.org/abs/1907.01831v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.01831v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.01885v1', updated=datetime.datetime(2019, 7, 3, 12, 32, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 3, 12, 32, 50, tzinfo=datetime.timezone.utc), title='A Software Framework and Datasets for the Analysis of Graph Measures on RDF Graphs', authors=[arxiv.Result.Author('Matthäus Zloch'), arxiv.Result.Author('Maribel Acosta'), arxiv.Result.Author('Daniel Hienert'), arxiv.Result.Author('Stefan Dietze'), arxiv.Result.Author('Stefan Conrad')], summary='As the availability and the inter-connectivity of RDF datasets grow, so does\\nthe necessity to understand the structure of the data. Understanding the\\ntopology of RDF graphs can guide and inform the development of, e.g. synthetic\\ndataset generators, sampling methods, index structures, or query optimizers. In\\nthis work, we propose two resources: (i) a software framework able to acquire,\\nprepare, and perform a graph-based analysis on the topology of large RDF\\ngraphs, and (ii) results on a graph-based analysis of 280 datasets from the LOD\\nCloud with values for 28 graph measures computed with the framework. We present\\na preliminary analysis based on the proposed resources and point out\\nimplications for synthetic dataset generators. Finally, we identify a set of\\nmeasures, that can be used to characterize graphs in the Semantic Web.', comment='Submitted at ESWC 2019, Resources Track. 15 pages, 5 figures, 2\\n  tables', journal_ref=None, doi='10.5281/zenodo.2109469, 10.5281/zenodo.1214433', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.5281/zenodo.2109469,', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://dx.doi.org/10.5281/zenodo.1214433', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1907.01885v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.01885v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.02790v1', updated=datetime.datetime(2019, 7, 5, 12, 16, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 5, 12, 16, 30, tzinfo=datetime.timezone.utc), title='Interlinking Heterogeneous Data for Smart Energy Systems', authors=[arxiv.Result.Author('Fabrizio Orlandi'), arxiv.Result.Author('Alan Meehan'), arxiv.Result.Author('Murhaf Hossari'), arxiv.Result.Author('Soumyabrata Dev'), arxiv.Result.Author(\"Declan O'Sullivan\"), arxiv.Result.Author('Tarek AlSkaif')], summary='Smart energy systems in general, and solar energy analysis in particular,\\nhave recently gained increasing interest. This is mainly due to stronger focus\\non smart energy saving solutions and recent developments in photovoltaic (PV)\\ncells. Various data-driven and machine-learning frameworks are being proposed\\nby the research community. However, these frameworks perform their analysis -\\nand are designed on - specific, heterogeneous and isolated datasets,\\ndistributed across different sites and sources, making it hard to compare\\nresults and reproduce the analysis on similar data. We propose an approach\\nbased on Web (W3C) standards and Linked Data technologies for representing and\\nconverting PV and weather records into an Resource Description Framework (RDF)\\ngraph-based data format. This format, and the presented approach, is ideal in a\\ndata integration scenario where data needs to be converted into homogeneous\\nform and different datasets could be interlinked for distributed analysis.', comment='Published in Proc. International Conference on Smart Energy Systems\\n  and Technologies (SEST), 2019', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1907.02790v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.02790v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.03936v1', updated=datetime.datetime(2019, 7, 9, 1, 43, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 9, 1, 43, 54, tzinfo=datetime.timezone.utc), title='Property Graph Exchange Format', authors=[arxiv.Result.Author('Hirokazu Chiba'), arxiv.Result.Author('Ryota Yamanaka'), arxiv.Result.Author('Shota Matsumoto')], summary='Recently, a variety of database implementations adopting the property graph\\nmodel have emerged. However, interoperable management of graph data on these\\nimplementations is challenging due to the differences in data models and\\nformats. Here, we redefine the property graph model incorporating the\\ndifferences in the existing models and propose interoperable serialization\\nformats for property graphs. The model is independent of specific\\nimplementations and provides a basis of interoperable management of property\\ngraph data. The proposed serialization is not only general but also intuitive,\\nthus it is useful for creating and maintaining graph data. To demonstrate the\\npractical use of our model and serialization, we implemented converters from\\nour serialization into existing formats, which can then be loaded into various\\ngraph databases. This work provides a basis of an interoperable platform for\\ncreating, exchanging, and utilizing property graph data.', comment='4 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1907.03936v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.03936v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.04643v1', updated=datetime.datetime(2019, 6, 28, 12, 26, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 6, 28, 12, 26, 9, tzinfo=datetime.timezone.utc), title='Multi-source Relations for Contextual Data Mining in Learning Analytics', authors=[arxiv.Result.Author('Julie Bu Daher'), arxiv.Result.Author('Armelle Brun'), arxiv.Result.Author('Anne Boyer')], summary=\"The goals of Learning Analytics (LA) are manifold, among which helping\\nstudents to understand their academic progress and improving their learning\\nprocess, which are at the core of our work. To reach this goal, LA relies on\\neducational data: students' traces of activities on VLE, or academic,\\nsocio-demographic information, information about teachers, pedagogical\\nresources, curricula, etc. The data sources that contain such information are\\nmultiple and diverse. Data mining, specifically pattern mining, aims at\\nextracting valuable and understandable information from large datasets. In our\\nwork, we assume that multiple educational data sources form a rich dataset that\\ncan result in valuable patterns. Mining such data is thus a promising way to\\nreach the goal of helping students. However, heterogeneity and interdependency\\nwithin data lead to high computational complexity. We thus aim at designing low\\ncomplex pattern mining algorithms that mine multi-source data, taking into\\nconsideration the dependency and heterogeneity among sources. The patterns\\nformed are meaningful and interpretable, they can thus be directly used for\\nstudents.\", comment='Learning and Student Analytics Conference 2018', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1907.04643v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.04643v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.06250v1', updated=datetime.datetime(2019, 7, 14, 17, 24, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 14, 17, 24, 39, tzinfo=datetime.timezone.utc), title='Delivery, consistency, and determinism: rethinking guarantees in distributed stream processing', authors=[arxiv.Result.Author('Artem Trofimov'), arxiv.Result.Author('Igor E. Kuralenok'), arxiv.Result.Author('Nikita Marshalkin'), arxiv.Result.Author('Boris Novikov')], summary='Consistency requirements for state-of-the-art stream processing systems are\\ndefined in terms of delivery guarantees. Exactly-once is the strongest one and\\nthe most desirable for end-user. However, there are several issues regarding\\nthis concept. Commonly used techniques that enforce exactly-once produce\\nsignificant performance overhead. Besides, the notion of exactly-once is not\\nformally defined and does not capture all properties that provide stream\\nprocessing systems supporting this guarantee. In this paper, we introduce a\\nformal framework that allows us to define streaming guarantees more regularly.\\nWe demonstrate that the properties of delivery, consistency, and determinism\\nare tightly connected within distributed stream processing. We also show that\\nhaving lightweight determinism, it is possible to provide exactly-once with\\nalmost no performance overhead. Experiments show that the proposed approach can\\nsignificantly outperform alternative industrial solutions.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1907.06250v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.06250v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.06295v1', updated=datetime.datetime(2019, 7, 14, 23, 13, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 14, 23, 13, 19, tzinfo=datetime.timezone.utc), title='An Approach Based on Bayesian Networks for Query Selectivity Estimation', authors=[arxiv.Result.Author('Max Halford'), arxiv.Result.Author('Philippe Saint-Pierre'), arxiv.Result.Author('Frank Morvan')], summary='The efficiency of a query execution plan depends on the accuracy of the\\nselectivity estimates given to the query optimiser by the cost model. The cost\\nmodel makes simplifying assumptions in order to produce said estimates in a\\ntimely manner. These assumptions lead to selectivity estimation errors that\\nhave dramatic effects on the quality of the resulting query execution plans. A\\nconvenient assumption that is ubiquitous among current cost models is to assume\\nthat attributes are independent with each other. However, it ignores potential\\ncorrelations which can have a huge negative impact on the accuracy of the cost\\nmodel. In this paper we attempt to relax the attribute value independence\\nassumption without unreasonably deteriorating the accuracy of the cost model.\\nWe propose a novel approach based on a particular type of Bayesian networks\\ncalled Chow-Liu trees to approximate the distribution of attribute values\\ninside each relation of a database. Our results on the TPC-DS benchmark show\\nthat our method is an order of magnitude more precise than other approaches\\nwhilst remaining reasonably efficient in terms of time and space.', comment=None, journal_ref='Lecture Notes in Computer Science, volume 11447, 2019, pages 3-19', doi='10.1007/978-3-030-18579-4_1', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-030-18579-4_1', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1907.06295v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.06295v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.06946v1', updated=datetime.datetime(2019, 7, 16, 11, 43, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 16, 11, 43, 59, tzinfo=datetime.timezone.utc), title='A Subjective Interestingness measure for Business Intelligence explorations', authors=[arxiv.Result.Author('Alexandre Chanson'), arxiv.Result.Author('Ben Crulis'), arxiv.Result.Author('Nicolas Labroche'), arxiv.Result.Author('Patrick Marcel')], summary=\"This paper addresses the problem of defining a subjective interestingness\\nmeasure for BI exploration. Such a measure involves prior modeling of the\\nbelief of the user. The complexity of this problem lies in the impossibility to\\nask the user about the degree of belief in each element composing their\\nknowledge prior to the writing of a query. To this aim, we propose to\\nautomatically infer this user belief based on the user's past interactions over\\na data cube, the cube schema and other users past activities. We express the\\nbelief under the form of a probability distribution over all the query parts\\npotentially accessible to the user, and use a random walk to learn this\\ndistribution. This belief is then used to define a first Subjective\\nInterestingness measure over multidimensional queries. Experiments conducted on\\nsimulated and real explorations show how this new subjective interestingness\\nmeasure relates to prototypical and real user behaviors, and that query parts\\noffer a reasonable proxy to infer user belief.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1907.06946v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.06946v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.07405v1', updated=datetime.datetime(2019, 7, 17, 9, 17, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 17, 9, 17, 27, tzinfo=datetime.timezone.utc), title='In-Depth Benchmarking of Graph Database Systems with the Linked Data Benchmark Council (LDBC) Social Network Benchmark (SNB)', authors=[arxiv.Result.Author('Florin Rusu'), arxiv.Result.Author('Zhiyi Huang')], summary='In this study, we present the first results of a complete implementation of\\nthe LDBC SNB benchmark -- interactive short, interactive complex, and business\\nintelligence -- in two native graph database systems---Neo4j and TigerGraph. In\\naddition to thoroughly evaluating the performance of all of the 46 queries in\\nthe benchmark on four scale factors -- SF-1, SF-10, SF-100, and SF-1000 -- and\\nthree computing architectures -- on premise and in the cloud -- we also measure\\nthe bulk loading time and storage size. Our results show that TigerGraph is\\nconsistently outperforming Neo4j on the majority of the queries---by two or\\nmore orders of magnitude (100X factor) on certain interactive complex and\\nbusiness intelligence queries. The gap increases with the size of the data\\nsince only TigerGraph is able to scale to SF-1000---Neo4j finishes only 12 of\\nthe 25 business intelligence queries in reasonable time. Nonetheless, Neo4j is\\ngenerally faster at bulk loading graph data up to SF-100. A key to our study is\\nthe active involvement of the vendors in the tuning of their platforms. In\\norder to encourage reproducibility, we make all the code, scripts, and\\nconfiguration parameters publicly available online.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1907.07405v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.07405v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.08671v1', updated=datetime.datetime(2019, 7, 19, 20, 8, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 19, 20, 8, 47, tzinfo=datetime.timezone.utc), title='Linked Crunchbase: A Linked Data API and RDF Data Set About Innovative Companies', authors=[arxiv.Result.Author('Michael Färber')], summary='Crunchbase is an online platform collecting information about startups and\\ntechnology companies, including attributes and relations of companies, people,\\nand investments. Data contained in Crunchbase is, to a large extent, not\\navailable elsewhere, making Crunchbase to a unique data source. In this paper,\\nwe present how to bring Crunchbase to the Web of Data so that its data can be\\nused in the machine-readable RDF format by anyone on the Web. First, we give\\ninsights into how we developed and hosted a Linked Data API for Crunchbase and\\nhow sameAs links to other data sources are integrated. Then, we present our\\nmethod for crawling RDF data based on this API to build a custom Crunchbase RDF\\nknowledge graph. We created an RDF data set with over 347 million triples,\\nincluding 781k people, 659k organizations, and 343k investments. Our Crunchbase\\nLinked Data API is available online at http://linked-crunchbase.org.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1907.08671v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.08671v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.09387v1', updated=datetime.datetime(2019, 7, 22, 16, 1, 21, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 22, 16, 1, 21, tzinfo=datetime.timezone.utc), title='Application of Data Stream Processing Technologies in Industry 4.0 -- What is Missing?', authors=[arxiv.Result.Author('Guenter Hesse'), arxiv.Result.Author('Werner Sinzig'), arxiv.Result.Author('Christoph Matthies'), arxiv.Result.Author('Matthias Uflacker')], summary='Industry 4.0 is becoming more and more important for manufacturers as the\\ndevelopments in the area of Internet of Things advance. Another technology\\ngaining more attention is data stream processing systems. Although such\\nstreaming frameworks seem to be a natural fit for Industry 4.0 scenarios, their\\napplication in this context is still low. The contributions in this paper are\\nthreefold. Firstly, we present industry findings that we derived from site\\ninspections with a focus on Industry 4.0. Moreover, our view on Industry 4.0\\nand important related aspects is elaborated. As a third contribution, we\\nillustrate our opinion on why data stream processing technologies could act as\\nan enabler for Industry 4.0 and point out possible obstacles on this way.', comment='Accepted at 2019 International Conference on Data Science, Technology\\n  and Applications (DATA). The final authenticated version will be available\\n  online in the conference proceedings - ISBN: 978-989-758-377-3, ISSN:\\n  2184-285X', journal_ref='Proceedings of the 8th International Conference on Data Science,\\n  Technology and Applications (DATA), 2019, pages 304-310', doi='10.5220/0007950203040310', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.5220/0007950203040310', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1907.09387v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.09387v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.10125v1', updated=datetime.datetime(2019, 7, 23, 20, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 23, 20, 56, tzinfo=datetime.timezone.utc), title='Generalized Deletion Propagation on Counting Conjunctive Query Answers', authors=[arxiv.Result.Author('Debmalya Panigrahi'), arxiv.Result.Author('Shweta Patwa'), arxiv.Result.Author('Sudeepa Roy')], summary='We investigate the computational complexity of minimizing the source\\nside-effect in order to remove a given number of tuples from the output of a\\nconjunctive query. In particular, given a multi-relational database $D$, a\\nconjunctive query $Q$, and a positive integer $k$ as input, the goal is to find\\na minimum subset of input tuples to remove from D that would eliminate at least\\n$k$ output tuples from $Q(D)$. This problem generalizes the well-studied\\ndeletion propagation problem in databases. In addition, it encapsulates the\\nnotion of intervention for aggregate queries used in data analysis with\\napplications to explaining interesting observations on the output. We show a\\ndichotomy in the complexity of this problem for the class of full conjunctive\\nqueries without self-joins by giving a characterization on the structure of $Q$\\nthat makes the problem either polynomial-time solvable or NP-hard. Our proof of\\nthis dichotomy result already gives an exact algorithm in the easy cases; we\\ncomplement this by giving an approximation algorithm for the hard cases of the\\nproblem.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1907.10125v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.10125v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.10528v1', updated=datetime.datetime(2019, 7, 24, 15, 42, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 24, 15, 42, 22, tzinfo=datetime.timezone.utc), title='The sameAs Problem: A Survey on Identity Management in the Web of Data', authors=[arxiv.Result.Author('Joe Raad'), arxiv.Result.Author('Nathalie Pernelle'), arxiv.Result.Author('Fatiha Saïs'), arxiv.Result.Author('Wouter Beek'), arxiv.Result.Author('Frank van Harmelen')], summary='In a decentralised knowledge representation system such as the Web of Data,\\nit is common and indeed desirable for different knowledge graphs to overlap.\\nWhenever multiple names are used to denote the same thing, owl:sameAs\\nstatements are needed in order to link the data and foster reuse. Whilst the\\ndeductive value of such identity statements can be extremely useful in\\nenhancing various knowledge-based systems, incorrect use of identity can have\\nwide-ranging effects in a global knowledge space like the Web of Data. With\\nseveral works already proven that identity in the Web is broken, this survey\\ninvestigates the current state of this \"sameAs problem\". An open discussion\\nhighlights the main weaknesses suffered by solutions in the literature, and\\ndraws open challenges to be faced in the future.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1907.10528v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.10528v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.10603v1', updated=datetime.datetime(2019, 7, 24, 10, 47, 21, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 24, 10, 47, 21, tzinfo=datetime.timezone.utc), title='Semi Automatic Construction of ShEx and SHACL Schemas', authors=[arxiv.Result.Author('Iovka Boneva'), arxiv.Result.Author('Jérémie Dusart'), arxiv.Result.Author('Daniel Fernández Álvarez'), arxiv.Result.Author('Jose Emilio Labra Gayo')], summary='We present a method for the construction of SHACL or ShEx constraints for an\\nexisting RDF dataset. It has two components that are used conjointly: an\\nalgorithm for automatic schema construction, and an interactive workflow for\\nediting the schema. The schema construction algorithm takes as input sets of\\nsample nodes and constructs a shape constraint for every sample set. It can be\\nparametrized by a schema pattern that defines structural requirements for the\\nschema to be constructed. Schema patterns are also used to feed the algorithm\\nwith relevant information about the dataset coming from a domain expert or from\\nsome ontology. The interactive workflow provides useful information about the\\ndataset, shows validation results w.r.t. the schema under construction, and\\noffers schema editing operations that combined with the schema construction\\nalgorithm allow to build a complex ShEx or SHACL schema.', comment='Work in progress', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1907.10603v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.10603v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.10914v1', updated=datetime.datetime(2019, 7, 25, 9, 19, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 25, 9, 19, 30, tzinfo=datetime.timezone.utc), title='Applying Constraint Logic Programming to SQL Semantic Analysis', authors=[arxiv.Result.Author('Fernando Sáenz-Pérez')], summary='This paper proposes the use of Constraint Logic Programming (CLP) to model\\nSQL queries in a data-independent abstract layer by focusing on some semantic\\nproperties for signalling possible errors in such queries. First, we define a\\ntranslation from SQL to Datalog, and from Datalog to CLP, so that solving this\\nCLP program will give information about inconsistency, tautology, and possible\\nsimplifications. We use different constraint domains which are mapped to SQL\\ntypes, and propose them to cooperate for improving accuracy. Our approach\\nleverages a deductive system that includes SQL and Datalog, and we present an\\nimplementation in this system which is currently being tested in classroom,\\nshowing its advantages and differences with respect to other approaches, as\\nwell as some performance data. This paper is under consideration for acceptance\\nin TPLP.', comment='Paper presented at the 35th International Conference on Logic\\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\\n  16 pages', journal_ref='Theory and Practice of Logic Programming 19 (2019) 808-825', doi='10.1017/S1471068419000206', primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LO', 'cs.PL', '68T35 (Primary) 68N17, 68P15 (Secondary)'], links=[arxiv.Result.Link('http://dx.doi.org/10.1017/S1471068419000206', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1907.10914v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.10914v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.11803v1', updated=datetime.datetime(2019, 7, 26, 22, 17, 7, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 26, 22, 17, 7, tzinfo=datetime.timezone.utc), title='qwLSH: Cache-conscious Indexing for Processing Similarity Search Query Workloads in High-Dimensional Spaces', authors=[arxiv.Result.Author('Omid Jafari'), arxiv.Result.Author('John Ossorgin'), arxiv.Result.Author('Parth Nagarkar')], summary='Similarity search queries in high-dimensional spaces are an important type of\\nqueries in many domains such as image processing, machine learning, etc. Since\\nexact similarity search indexing techniques suffer from the well-known curse of\\ndimensionality in high-dimensional spaces, approximate search techniques are\\noften utilized instead. Locality Sensitive Hashing (LSH) has been shown to be\\nan effective approximate search method for solving similarity search queries in\\nhigh-dimensional spaces. Often times, queries in real-world settings arrive as\\npart of a query workload. LSH and its variants are particularly designed to\\nsolve single queries effectively. They suffer from one major drawback while\\nexecuting query workloads: they do not take into consideration important data\\ncharacteristics for effective cache utilization while designing the index\\nstructures. In this paper, we present qwLSH, an index structure for efficiently\\nprocessing similarity search query workloads in high-dimensional spaces. We\\nintelligently divide a given cache during processing of a query workload by\\nusing novel cost models. Experimental results show that, given a query\\nworkload, qwLSH is able to perform faster than existing techniques due to its\\nunique cost models and strategies.', comment='Extended version of the published work', journal_ref=\"In Proceedings of ICMR '19, pp. 329-333. ACM, 2019\", doi='10.1145/3323873.3325048', primary_category='cs.DB', categories=['cs.DB', 'H.2.4'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3323873.3325048', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1907.11803v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.11803v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.12415v2', updated=datetime.datetime(2019, 8, 2, 16, 18, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 29, 13, 28, 59, tzinfo=datetime.timezone.utc), title='sql4ml A declarative end-to-end workflow for machine learning', authors=[arxiv.Result.Author('Nantia Makrynioti'), arxiv.Result.Author('Ruy Ley-Wild'), arxiv.Result.Author('Vasilis Vassalos')], summary='We present sql4ml, a system for expressing supervised machine learning (ML)\\nmodels in SQL and automatically training them in TensorFlow. The primary\\nmotivation for this work stems from the observation that in many data science\\ntasks there is a back-and-forth between a relational database that stores the\\ndata and a machine learning framework. Data preprocessing and feature\\nengineering typically happen in a database, whereas learning is usually\\nexecuted in separate ML libraries. This fragmented workflow requires from the\\nusers to juggle between different programming paradigms and software systems.\\nWith sql4ml the user can express both feature engineering and ML algorithms in\\nSQL, while the system translates this code to an appropriate representation for\\ntraining inside a machine learning framework. We describe our translation\\nmethod, present experimental results from applying it on three well-known ML\\nalgorithms and discuss the usability benefits from concentrating the entire\\nworkflow on the database side.', comment='14 pages, 9 figures, replaced code repository link', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG', 'cs.SE'], links=[arxiv.Result.Link('http://arxiv.org/abs/1907.12415v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.12415v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1907.12817v2', updated=datetime.datetime(2019, 7, 31, 7, 14, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 30, 10, 0, 52, tzinfo=datetime.timezone.utc), title='Increasing Scalability of Process Mining using Event Dataframes: How Data Structure Matters', authors=[arxiv.Result.Author('Alessandro Berti')], summary='Process Mining is a branch of Data Science that aims to extract\\nprocess-related information from event data contained in information systems,\\nthat is steadily increasing in amount. Many algorithms, and a general-purpose\\nopen source framework (ProM 6), have been developed in the last years for\\nprocess discovery, conformance checking, machine learning on event data.\\nHowever, in very few cases scalability has been a target, prioritizing the\\nquality of the output over the execution speed and the optimization of\\nresources. This is making progressively more difficult to apply process mining\\nwith mainstream workstations on real-life event data with any open source\\nprocess mining framework. Hence, exploring more scalable storage techniques,\\nin-memory data structures, more performant algorithms is a strictly incumbent\\nneed. In this paper, we propose the usage of mainstream columnar storages and\\ndataframes to increase the scalability of process mining. These can replace the\\nclassic event log structures in most tasks, but require completely different\\nimplementations with regards to mainstream process mining algorithms.\\nDataframes will be defined, some algorithms on such structures will be\\npresented and their complexity will be calculated.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1907.12817v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1907.12817v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.00928v1', updated=datetime.datetime(2019, 7, 12, 13, 54, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 7, 12, 13, 54, 15, tzinfo=datetime.timezone.utc), title='A Multi-Media Exchange Format for Time-Series Dataset Curation', authors=[arxiv.Result.Author('Philipp M. Scholl'), arxiv.Result.Author('Benjamin Völker'), arxiv.Result.Author('Bernd Becker'), arxiv.Result.Author('Kristof Van Laerhoven')], summary='Exchanging data as character-separated values (CSV) is slow, cumbersome and\\nerror-prone. Especially for time-series data, which is common in Activity\\nRecognition, synchronizing several independently recorded sensors is\\nchallenging. Adding second level evidence, like video recordings from multiple\\nangles and time-coded annotations, further complicates the matter of curating\\nsuch data. A possible alternative is to make use of standardized multi-media\\nformats. Sensor data can be encoded in audio format, and time-coded\\ninformation, like annotations, as subtitles. Video data can be added easily.\\nAll this media can be merged into a single container file, which makes the\\nissue of synchronization explicit. The incurred performance overhead by this\\nencoding is shown to be negligible and compression can be applied to optimize\\nstorage and transmission overhead.', comment='HASCA Book Contribution, Keywords: Data Curation; Activity\\n  Recognition; Multi-Media Format; Data Storage; Comma-Separated-Values', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1908.00928v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.00928v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.03206v1', updated=datetime.datetime(2019, 8, 8, 21, 27, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 8, 21, 27, 35, tzinfo=datetime.timezone.utc), title='Managing the Complexity of Processing Financial Data at Scale -- an Experience Report', authors=[arxiv.Result.Author('Sebastian Frischbier'), arxiv.Result.Author('Mario Paic'), arxiv.Result.Author('Alexander Echler'), arxiv.Result.Author('Christian Roth')], summary=\"Financial markets are extremely data-driven and regulated. Participants rely\\non notifications about significant events and background information that meet\\ntheir requirements regarding timeliness, accuracy, and completeness. As one of\\nEurope's leading providers of financial data and regulatory solutions vwd\\nprocesses a daily average of 18 billion notifications from 500+ data sources\\nfor 30 million symbols. Our large-scale geo-distributed systems handle daily\\npeak rates of 1+ million notifications/sec. In this paper we give practical\\ninsights about the different types of complexity we face regarding the data we\\nprocess, the systems we operate, and the regulatory constraints we must comply\\nwith. We describe the volume, variety, velocity, and veracity of the data we\\nprocess, the infrastructure we operate, and the architecture we apply. We\\nillustrate the load patterns created by trading and how the markets' attention\\nto the Brexit vote and similar events stressed our systems.\", comment=\"12 pages, 2 figures, to be published in the proceedings of the 10th\\n  Complex Systems Design & Management conference (CSD&M'19) by Springer\", journal_ref=None, doi='10.1007/978-3-030-34843-4_2', primary_category='cs.DB', categories=['cs.DB', 'cs.DC', 'q-fin.GN'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-030-34843-4_2', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1908.03206v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.03206v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.04083v1', updated=datetime.datetime(2019, 8, 12, 10, 57, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 12, 10, 57, 1, tzinfo=datetime.timezone.utc), title='An Efficient Skyline Computation Framework', authors=[arxiv.Result.Author('Rui Liu'), arxiv.Result.Author('Dominique Li')], summary='Skyline computation aims at looking for the set of tuples that are not worse\\nthan any other tuples in all dimensions from a multidimensional database. In\\nthis paper, we present SDI (Skyline on Dimension Index), a dimension indexing\\nconducted general framework to skyline computation. We prove that to determine\\nwhether a tuple belongs to the skyline, it is enough to compare this tuple with\\na bounded subset of skyline tuples in an arbitrary dimensional index, but not\\nwith all existing skyline tuples. Base on SDI, we also show that any skyline\\ntuple can be used to stop the whole skyline computation process with outputting\\nthe complete set of all skyline tuples. We develop an efficient algorithm\\nSDI-RS that significantly reduces the skyline computation time, of which the\\nspace and time complexity can be guaranteed. Our experimental evaluation shows\\nthat SDI-RS outperforms the baseline algorithms in general and is especially\\nvery efficient on high-dimensional data.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://arxiv.org/abs/1908.04083v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.04083v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.04464v2', updated=datetime.datetime(2019, 11, 25, 5, 20, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 13, 2, 28, 37, tzinfo=datetime.timezone.utc), title='Linking Graph Entities with Multiplicity and Provenance', authors=[arxiv.Result.Author('Jixue Liu'), arxiv.Result.Author('Selasi Kwashie'), arxiv.Result.Author('Jiuyong Li'), arxiv.Result.Author('Lin Liu'), arxiv.Result.Author('Michael Bewong')], summary='Entity linking and resolution is a fundamental database problem with\\napplications in data integration, data cleansing, information retrieval,\\nknowledge fusion, and knowledge-base population. It is the task of accurately\\nidentifying multiple, differing, and possibly contradicting representations of\\nthe same real-world entity in data. In this work, we propose an entity linking\\nand resolution system capable of linking entities across different databases\\nand mentioned-entities extracted from text data. Our entity linking/resolution\\nsolution, called Certus, uses a graph model to represent the profiles of\\nentities. The graph model is versatile, thus, it is capable of handling\\nmultiple values for an attribute or a relationship, as well as the provenance\\ndescriptions of the values. Provenance descriptions of a value provide the\\nsettings of the value, such as validity periods, sources, security\\nrequirements, etc. This paper presents the architecture for the entity linking\\nsystem, the logical, physical, and indexing models used in the system, and the\\ngeneral linking process. Furthermore, we demonstrate the performance of update\\noperations of the physical storage models when the system is implemented in two\\nstate-of-the-art database management systems, HBase and Postgres.', comment='7 pages, 5 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1908.04464v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.04464v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.04509v1', updated=datetime.datetime(2019, 8, 13, 6, 19, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 13, 6, 19, 26, tzinfo=datetime.timezone.utc), title='On the Complexity of Checking Transactional Consistency', authors=[arxiv.Result.Author('Ranadeep Biswas'), arxiv.Result.Author('Constantin Enea')], summary='Transactions simplify concurrent programming by enabling computations on\\nshared data that are isolated from other concurrent computations and are\\nresilient to failures. Modern databases provide different consistency models\\nfor transactions corresponding to different tradeoffs between consistency and\\navailability. In this work, we investigate the problem of checking whether a\\ngiven execution of a transactional database adheres to some consistency model.\\nWe show that consistency models like read committed, read atomic, and causal\\nconsistency are polynomial time checkable while prefix consistency and snapshot\\nisolation are NP-complete in general. These results complement a previous\\nNP-completeness result concerning serializability. Moreover, in the context of\\nNP-complete consistency models, we devise algorithms that are polynomial time\\nassuming that certain parameters in the input executions, e.g., the number of\\nsessions, are fixed. We evaluate the scalability of these algorithms in the\\ncontext of several production databases.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.PL'], links=[arxiv.Result.Link('http://arxiv.org/abs/1908.04509v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.04509v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.04772v2', updated=datetime.datetime(2020, 3, 14, 12, 1, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 13, 17, 32, 39, tzinfo=datetime.timezone.utc), title='Adaptive Learning of Aggregate Analytics under Dynamic Workloads', authors=[arxiv.Result.Author('Fotis Savva'), arxiv.Result.Author('Christos Anagnostopoulos'), arxiv.Result.Author('Peter Triantafillou')], summary=\"Large organizations have seamlessly incorporated data-driven decision making\\nin their operations. However, as data volumes increase, expensive big data\\ninfrastructures are called to rescue. In this setting, analytics tasks become\\nvery costly in terms of query response time, resource consumption, and money in\\ncloud deployments, especially when base data are stored across geographically\\ndistributed data centers. Therefore, we introduce an adaptive Machine Learning\\nmechanism which is light-weight, stored client-side, can estimate the answers\\nof a variety of aggregate queries and can avoid the big data backend. The\\nestimations are performed in milliseconds are inexpensive and accurate as the\\nmechanism learns from past analytical-query patterns. However, as analytic\\nqueries are ad-hoc and analysts' interests change over time we develop\\nsolutions that can swiftly and accurately detect such changes and adapt to new\\nquery patterns. The capabilities of our approach are demonstrated using\\nextensive evaluation with real and synthetic datasets.\", comment='12 pages, 9 figures', journal_ref=None, doi='10.1109/BigData47090.2019.9006267', primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/BigData47090.2019.9006267', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1908.04772v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.04772v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.06265v2', updated=datetime.datetime(2019, 9, 7, 8, 18, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 17, 9, 4, 2, tzinfo=datetime.timezone.utc), title='Towards an Integrated Graph Algebra for Graph Pattern Matching with Gremlin (Extended Version)', authors=[arxiv.Result.Author('Harsh Thakkar'), arxiv.Result.Author('Dharmen Punjani'), arxiv.Result.Author('Soeren Auer'), arxiv.Result.Author('Maria-Esther Vidal')], summary='Graph data management (also called NoSQL) has revealed beneficial\\ncharacteristics in terms of flexibility and scalability by differently\\nbalancing between query expressivity and schema flexibility. This peculiar\\nadvantage has resulted into an unforeseen race of developing new task-specific\\ngraph systems, query languages and data models, such as property graphs,\\nkey-value, wide column, resource description framework (RDF), etc. Present-day\\ngraph query languages are focused towards flexible graph pattern matching (aka\\nsub-graph matching), whereas graph computing frameworks aim towards providing\\nfast parallel (distributed) execution of instructions. The consequence of this\\nrapid growth in the variety of graph-based data management systems has resulted\\nin a lack of standardization. Gremlin, a graph traversal language, and machine\\nprovides a common platform for supporting any graph computing system (such as\\nan OLTP graph database or OLAP graph processors). We present a formalization of\\ngraph pattern matching for Gremlin queries. We also study, discuss and\\nconsolidate various existing graph algebra operators into an integrated graph\\nalgebra.', comment='This is an extended version of an article formally published at DEXA\\n  2017', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1908.06265v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.06265v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.06553v1', updated=datetime.datetime(2019, 8, 19, 1, 36, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 19, 1, 36, 52, tzinfo=datetime.timezone.utc), title='LabelECG: A Web-based Tool for Distributed Electrocardiogram Annotation', authors=[arxiv.Result.Author('Zijian Ding'), arxiv.Result.Author('Shan Qiu'), arxiv.Result.Author('Yutong Guo'), arxiv.Result.Author('Jianping Lin'), arxiv.Result.Author('Li Sun'), arxiv.Result.Author('Dapeng Fu'), arxiv.Result.Author('Zhen Yang'), arxiv.Result.Author('Chengquan Li'), arxiv.Result.Author('Yang Yu'), arxiv.Result.Author('Long Meng'), arxiv.Result.Author('Tingting Lv'), arxiv.Result.Author('Dan Li'), arxiv.Result.Author('Ping Zhang')], summary='Electrocardiography plays an essential role in diagnosing and screening\\ncardiovascular diseases in daily healthcare. Deep neural networks have shown\\nthe potentials to improve the accuracies of arrhythmia detection based on\\nelectrocardiograms (ECGs). However, more ECG records with ground truth are\\nneeded to promote the development and progression of deep learning techniques\\nin automatic ECG analysis. Here we propose a web-based tool for ECG viewing and\\nannotating, LabelECG. With the facilitation of unified data management,\\nLabelECG is able to distribute large cohorts of ECGs to dozens of technicians\\nand physicians, who can simultaneously make annotations through web-browsers on\\nPCs, tablets and cell phones. Along with the doctors from four hospitals in\\nChina, we applied LabelECG to support the annotations of about 15,000 12-lead\\nresting ECG records in three months. These annotated ECGs have successfully\\nsupported the First China ECG intelligent Competition. La-belECG will be freely\\naccessible on the Internet to support similar researches, and will also be\\nupgraded through future works.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'eess.SP'], links=[arxiv.Result.Link('http://arxiv.org/abs/1908.06553v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.06553v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.06719v1', updated=datetime.datetime(2019, 8, 19, 12, 0, 57, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 19, 12, 0, 57, tzinfo=datetime.timezone.utc), title='AFrame: Extending DataFrames for Large-Scale Modern Data Analysis (Extended Version)', authors=[arxiv.Result.Author('Phanwadee Sinthong'), arxiv.Result.Author('Michael J. Carey')], summary='Analyzing the increasingly large volumes of data that are available today,\\npossibly including the application of custom machine learning models, requires\\nthe utilization of distributed frameworks. This can result in serious\\nproductivity issues for \"normal\" data scientists. This paper introduces AFrame,\\na new scalable data analysis package powered by a Big Data management system\\nthat extends the data scientists\\' familiar DataFrame operations to efficiently\\noperate on managed data at scale. AFrame is implemented as a layer on top of\\nApache AsterixDB, transparently scaling out the execution of DataFrame\\noperations and machine learning model invocation through a parallel,\\nshared-nothing big data management system. AFrame incrementally constructs\\nSQL++ queries and leverages AsterixDB\\'s semistructured data management\\nfacilities, user-defined function support, and live data ingestion support. In\\norder to evaluate the proposed approach, this paper also introduces an\\nextensible micro-benchmark for use in evaluating DataFrame performance in both\\nsingle-node and distributed settings via a collection of representative\\nanalytic operations. This paper presents the architecture of AFrame, describes\\nthe underlying capabilities of AsterixDB that efficiently support modern data\\nanalytic operations, and utilizes the proposed benchmark to evaluate and\\ncompare the performance and support for large-scale data analyses provided by\\nalternative DataFrame libraries.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1908.06719v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.06719v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.06801v1', updated=datetime.datetime(2019, 8, 15, 13, 28, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 15, 13, 28, 50, tzinfo=datetime.timezone.utc), title='Towards Efficient Discriminative Pattern Mining in Hybrid Domains', authors=[arxiv.Result.Author('Yoshitaka Kameya')], summary='Discriminative pattern mining is a data mining task in which we find patterns\\nthat distinguish transactions in the class of interest from those in other\\nclasses, and is also called emerging pattern mining or subgroup discovery. One\\npractical problem in discriminative pattern mining is how to handle numeric\\nvalues in the input dataset. In this paper, we propose an algorithm for\\ndiscriminative pattern mining that can deal with a transactional dataset in a\\nhybrid domain, i.e. the one that includes both symbolic and numeric values. We\\nalso show the execution results of a prototype implementation of the proposed\\nalgorithm for two standard benchmark datasets.', comment='This paper is an English version of the paper originally presented in\\n  the 17th Forum on Information Technology (FIT 2018), a Japanese domestic\\n  conference held during September 19-21, 2018', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1908.06801v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.06801v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.07431v1', updated=datetime.datetime(2019, 8, 20, 15, 28, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 20, 15, 28, 26, tzinfo=datetime.timezone.utc), title='On the Diversity of Memory and Storage Technologies', authors=[arxiv.Result.Author('Ismail Oukid'), arxiv.Result.Author('Lucas Lersch')], summary='The last decade has seen tremendous developments in memory and storage\\ntechnologies, starting with Flash Memory and continuing with the upcoming\\nStorage-Class Memories. Combined with an explosion of data processing, data\\nanalytics, and machine learning, this led to a segmentation of the memory and\\nstorage market. Consequently, the traditional storage hierarchy, as we know it\\ntoday, might be replaced by a multitude of storage hierarchies, with\\npotentially different depths, each tailored for specific workloads. In this\\ncontext, we explore in this \"Kurz Erkl\\\\\"art\" the state of memory technologies\\nand reflect on their future use with a focus on data management systems.', comment='This is a post-peer-review, pre-copyedit version of an article\\n  published in Datenbank-Spektrum. The final authenticated version is available\\n  online at: http://dx.doi.org/10.1007/s13222-018-0287-8', journal_ref='Datenbank-Spektrum, Volume 18, Issue 2, 2018, Pages 121-127', doi='10.1007/s13222-018-0287-8', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/s13222-018-0287-8', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1908.07431v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.07431v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.07723v1', updated=datetime.datetime(2019, 8, 21, 7, 7, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 21, 7, 7, 27, tzinfo=datetime.timezone.utc), title='Improved Cardinality Estimation by Learning Queries Containment Rates', authors=[arxiv.Result.Author('Rojeh Hayek'), arxiv.Result.Author('Oded Shmueli')], summary=\"The containment rate of query Q1 in query Q2 over database D is the\\npercentage of Q1's result tuples over D that are also in Q2's result over D. We\\ndirectly estimate containment rates between pairs of queries over a specific\\ndatabase. For this, we use a specialized deep learning scheme, CRN, which is\\ntailored to representing pairs of SQL queries. Result-cardinality estimation is\\na core component of query optimization. We describe a novel approach for\\nestimating queries result-cardinalities using estimated containment rates among\\nqueries. This containment rate estimation may rely on CRN or embed, unchanged,\\nknown cardinality estimation methods. Experimentally, our novel approach for\\nestimating cardinalities, using containment rates between queries, on a\\nchallenging real-world database, realizes significant improvements to state of\\nthe art cardinality estimation methods.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1908.07723v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.07723v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.08654v1', updated=datetime.datetime(2019, 8, 23, 3, 39, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 23, 3, 39, 14, tzinfo=datetime.timezone.utc), title='Efficient Join Processing Over Incomplete Data Streams (Technical Report)', authors=[arxiv.Result.Author('Weilong Ren'), arxiv.Result.Author('Xiang Lian'), arxiv.Result.Author('Kambiz Ghazinour')], summary='For decades, the join operator over fast data streams has always drawn much\\nattention from the database community, due to its wide spectrum of real-world\\napplications, such as online clustering, intrusion detection, sensor data\\nmonitoring, and so on. Existing works usually assume that the underlying\\nstreams to be joined are complete (without any missing values). However, this\\nassumption may not always hold, since objects from streams may contain some\\nmissing attributes, due to various reasons such as packet losses, network\\ncongestion/failure, and so on. In this paper, we formalize an important\\nproblem, namely join over incomplete data streams (Join-iDS), which retrieves\\njoining object pairs from incomplete data streams with high confidences. We\\ntackle the Join-iDS problem in the style of \"data imputation and query\\nprocessing at the same time\". To enable this style, we design an effective and\\nefficient cost-model-based imputation method via deferential dependency (DD),\\ndevise effective pruning strategies to reduce the Join-iDS search space, and\\npropose efficient algorithms via our proposed cost-model-based data\\nsynopsis/indexes. Extensive experiments have been conducted to verify the\\nefficiency and effectiveness of our proposed Join-iDS approach on both real and\\nsynthetic data sets.', comment='11 pages, 11 figures, accepted conference paper for CIKM19', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1908.08654v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.08654v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.08656v2', updated=datetime.datetime(2020, 9, 12, 8, 54, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 23, 4, 5, 25, tzinfo=datetime.timezone.utc), title='Revisiting Wedge Sampling for Budgeted Maximum Inner Product Search', authors=[arxiv.Result.Author('Stephan S. Lorenzen'), arxiv.Result.Author('Ninh Pham')], summary='Top-k maximum inner product search (MIPS) is a central task in many machine\\nlearning applications. This paper extends top-k MIPS with a budgeted setting,\\nthat asks for the best approximate top-k MIPS given a limit of B computational\\noperations. We investigate recent advanced sampling algorithms, including wedge\\nand diamond sampling to solve it. Though the design of these sampling schemes\\nnaturally supports budgeted top-k MIPS, they suffer from the linear cost from\\nscanning all data points to retrieve top-k results and the performance\\ndegradation for handling negative inputs.\\n  This paper makes two main contributions. First, we show that diamond sampling\\nis essentially a combination between wedge sampling and basic sampling for\\ntop-k MIPS. Our theoretical analysis and empirical evaluation show that wedge\\nis competitive (often superior) to diamond on approximating top-k MIPS\\nregarding both efficiency and accuracy. Second, we propose a series of\\nalgorithmic engineering techniques to deploy wedge sampling on budgeted top-k\\nMIPS. Our novel deterministic wedge-based algorithm runs significantly faster\\nthan the state-of-the-art methods for budgeted and exact top-k MIPS while\\nmaintaining the top-5 precision at least 80% on standard recommender system\\ndata sets.', comment='ECML-PKDD 2020', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1908.08656v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.08656v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.10177v2', updated=datetime.datetime(2019, 8, 29, 12, 2, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 27, 13, 12, 21, tzinfo=datetime.timezone.utc), title='Datalog Reasoning over Compressed RDF Knowledge Bases', authors=[arxiv.Result.Author('Pan Hu'), arxiv.Result.Author('Jacopo Urbani'), arxiv.Result.Author('Boris Motik'), arxiv.Result.Author('Ian Horrocks')], summary='Materialisation is often used in RDF systems as a preprocessing step to\\nderive all facts implied by given RDF triples and rules. Although widely used,\\nmaterialisation considers all possible rule applications and can use a lot of\\nmemory for storing the derived facts, which can hinder performance. We present\\na novel materialisation technique that compresses the RDF triples so that the\\nrules can sometimes be applied to multiple facts at once, and the derived facts\\ncan be represented using structure sharing. Our technique can thus require less\\nspace, as well as skip certain rule applications. Our experiments show that our\\ntechnique can be very effective: when the rules are relatively simple, our\\nsystem is both faster and requires less memory than prominent state-of-the-art\\nRDF systems.', comment='CIKM 2019', journal_ref=None, doi='10.1145/3357384.3358147', primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3357384.3358147', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1908.10177v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.10177v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.10693v1', updated=datetime.datetime(2019, 8, 28, 12, 48, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 28, 12, 48, 44, tzinfo=datetime.timezone.utc), title='DDSketch: A fast and fully-mergeable quantile sketch with relative-error guarantees', authors=[arxiv.Result.Author('Charles Masson'), arxiv.Result.Author('Jee E. Rim'), arxiv.Result.Author('Homin K. Lee')], summary='Summary statistics such as the mean and variance are easily maintained for\\nlarge, distributed data streams, but order statistics (i.e., sample quantiles)\\ncan only be approximately summarized. There is extensive literature on\\nmaintaining quantile sketches where the emphasis has been on bounding the rank\\nerror of the sketch while using little memory. Unfortunately, rank error\\nguarantees do not preclude arbitrarily large relative errors, and this often\\noccurs in practice when the data is heavily skewed. Given the distributed\\nnature of contemporary large-scale systems, another crucial property for\\nquantile sketches is mergeablility, i.e., several combined sketches must be as\\naccurate as a single sketch of the same data. We present the first\\nfully-mergeable, relative-error quantile sketching algorithm with formal\\nguarantees. The sketch is extremely fast and accurate, and is currently being\\nused by Datadog at a wide-scale.', comment='11 pages, 11 figures, VLDB', journal_ref='PVLDB, 12(12): 2195-2205, 2019', doi='10.14778/3352063.3352135', primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3352063.3352135', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1908.10693v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.10693v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.11612v2', updated=datetime.datetime(2019, 10, 28, 3, 33, 38, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 30, 9, 36, 28, tzinfo=datetime.timezone.utc), title='Parameter-free Structural Diversity Search', authors=[arxiv.Result.Author('Jinbin Huang'), arxiv.Result.Author('Xin Huang'), arxiv.Result.Author('Yuanyuan Zhu'), arxiv.Result.Author('Jianliang Xu')], summary='The problem of structural diversity search is to find the top-k vertices with\\nthe largest structural diversity in a graph. However, when identifying distinct\\nsocial contexts, existing structural diversity models (e.g., t-sized component,\\nt-core, and t-brace) are sensitive to an input parameter of t. To address this\\ndrawback, we propose a parameter-free structural diversity model. Specifically,\\nwe propose a novel notation of discriminative core, which automatically models\\nvarious kinds of social contexts without parameter t. Leveraging on\\ndiscriminative cores and h-index, the structural diversity score for a vertex\\nis calculated. We study the problem of parameter-free structural diversity\\nsearch in this paper. An efficient top-k search algorithm with a well-designed\\nupper bound for pruning is proposed. Extensive experiment results demonstrate\\nthe parameter sensitivity of existing t-core based model and verify the\\nsuperiority of our methods.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1908.11612v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.11612v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1908.11740v2', updated=datetime.datetime(2019, 10, 9, 8, 54, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 8, 30, 13, 48, 31, tzinfo=datetime.timezone.utc), title='Parallel In-Memory Evaluation of Spatial Joins', authors=[arxiv.Result.Author('Dimitrios Tsitsigkos'), arxiv.Result.Author('Panagiotis Bouros'), arxiv.Result.Author('Nikos Mamoulis'), arxiv.Result.Author('Manolis Terrovitis')], summary='The spatial join is a popular operation in spatial database systems and its\\nevaluation is a well-studied problem. As main memories become bigger and faster\\nand commodity hardware supports parallel processing, there is a need to revamp\\nclassic join algorithms which have been designed for I/O-bound processing. In\\nview of this, we study the in-memory and parallel evaluation of spatial joins,\\nby re-designing a classic partitioning-based algorithm to consider alternative\\napproaches for space partitioning. Our study shows that, compared to a\\nstraightforward implementation of the algorithm, our tuning can improve\\nperformance significantly. We also show how to select appropriate partitioning\\nparameters based on data statistics, in order to tune the algorithm for the\\ngiven join inputs. Our parallel implementation scales gracefully with the\\nnumber of threads reducing the cost of the join to at most one second even for\\njoin inputs with tens of millions of rectangles.', comment=\"Extended version of the SIGSPATIAL'19 paper under the same title\", journal_ref='27th ACM SIGSPATIAL International Conference on Advances in\\n  Geographic Information Systems (ACM SIGSPATIAL GIS 2019), Chicago, Illinois,\\n  USA, November 5-8, 2019', doi='10.1145/3347146.3359343', primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3347146.3359343', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1908.11740v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1908.11740v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.00837v2', updated=datetime.datetime(2020, 11, 2, 20, 44, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 3, 6, 3, 16, tzinfo=datetime.timezone.utc), title='RCC: Resilient Concurrent Consensus for High-Throughput Secure Transaction Processing', authors=[arxiv.Result.Author('Suyash Gupta'), arxiv.Result.Author('Jelle Hellings'), arxiv.Result.Author('Mohammad Sadoghi')], summary='Recently, we saw the emergence of consensus-based database systems that\\npromise resilience against failures, strong data provenance, and federated data\\nmanagement. Typically, these fully-replicated systems are operated on top of a\\nprimary-backup consensus protocol, which limits the throughput of these systems\\nto the capabilities of a single replica (the primary). To push throughput\\nbeyond this single-replica limit, we propose concurrent consensus. In\\nconcurrent consensus, replicas independently propose transactions, thereby\\nreducing the influence of any single replica on performance. To put this idea\\nin practice, we propose our RCC paradigm that can turn any primary-backup\\nconsensus protocol into a concurrent consensus protocol by running many\\nconsensus instances concurrently. RCC is designed with performance in mind and\\nrequires minimal coordination between instances. Furthermore, RCC also promises\\nincreased resilience against failures. We put the design of RCC to the test by\\nimplementing it in ResilientDB, our high-performance resilient blockchain\\nfabric, and comparing it with state-of-the-art primary-backup consensus\\nprotocols. Our experiments show that RCC achieves up to 2.75x higher throughput\\nthan other consensus protocols and can be scaled to 91 replicas.', comment='To appear in the proceedings of 37th IEEE International Conference on\\n  Data Engineering (ICDE) 2021', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.00837v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.00837v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.02105v1', updated=datetime.datetime(2019, 11, 5, 22, 0, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 5, 22, 0, 15, tzinfo=datetime.timezone.utc), title='Spatial-Temporal Cluster Relations -- A Foundation for Trajectory Cluster Lifetime Analysis', authors=[arxiv.Result.Author('Ivens Portugal'), arxiv.Result.Author('Paulo Alencar'), arxiv.Result.Author('Donald Cowan')], summary='Spatial-temporal data, that is information about objects that exist at a\\nparticular location and time period, are rich in value and, as a consequence,\\nthe target of so many initiative efforts. Clustering approaches aim at grouping\\ndatapoints based on similar properties for classification tasks. These\\napproaches have been widely used in domains such as human mobility, ecology,\\nhealth and astronomy. However, clustering approaches typically address only the\\nstatic nature of a cluster, and do not take into consideration its dynamic\\naspects. A desirable approach needs to investigate relations between dynamic\\nclusters and their elements that can be used to derive new insights about what\\nhappened to the clusters during their lifetimes. A fundamental step towards\\nthis goal is to provide a formal definition of spatial-temporal cluster\\nrelations. This report introduces, describes, and formalizes 14 novel\\nspatial-temporal cluster relations that may occur during the existence of a\\ncluster and involve both trajectory-cluster membership conditions and\\ncluster-cluster comparisons. We evaluate the proposed relations with a\\ndiscussion on how they are able to interpret complex cases that are difficult\\nto be distinguished without a formal relation specification. We conclude the\\nreport by summarizing our results and describing avenues for further research.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.02105v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.02105v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.02282v4', updated=datetime.datetime(2021, 1, 21, 13, 39, 38, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 6, 9, 59, 56, tzinfo=datetime.timezone.utc), title='A Hybrid Approach To Hierarchical Density-based Cluster Selection', authors=[arxiv.Result.Author('Claudia Malzer'), arxiv.Result.Author('Marcus Baum')], summary=\"HDBSCAN is a density-based clustering algorithm that constructs a cluster\\nhierarchy tree and then uses a specific stability measure to extract flat\\nclusters from the tree. We show how the application of an additional threshold\\nvalue can result in a combination of DBSCAN* and HDBSCAN clusters, and\\ndemonstrate potential benefits of this hybrid approach when clustering data of\\nvariable densities. In particular, our approach is useful in scenarios where we\\nrequire a low minimum cluster size but want to avoid an abundance of\\nmicro-clusters in high-density regions. The method can directly be applied to\\nHDBSCAN's tree of cluster candidates and does not require any modifications to\\nthe hierarchy itself. It can easily be integrated as an addition to existing\\nHDBSCAN implementations.\", comment='6 pages. Conference: 2020 IEEE International Conference on\\n  Multisensor Fusion and Integration for Intelligent Systems (MFI)', journal_ref='2020 IEEE International Conference on Multisensor Fusion and\\n  Integration for Intelligent Systems (MFI), Karlsruhe, Germany, 2020, pp.\\n  223-228', doi='10.1109/MFI49285.2020.9235263', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/MFI49285.2020.9235263', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1911.02282v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.02282v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.02646v1', updated=datetime.datetime(2019, 10, 15, 11, 37, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 15, 11, 37, 56, tzinfo=datetime.timezone.utc), title='Optimizing Semi-Stream CACHEJOIN for Near-Real-Time Data Warehousing', authors=[arxiv.Result.Author('M. Asif Naeem'), arxiv.Result.Author('Erum Mehmood'), arxiv.Result.Author('M G Abbas'), arxiv.Result.Author('Noreen Jamil')], summary='Streaming data join is a critical process in the field of near-real-time data\\nwarehousing. For this purpose, an adaptive semi-stream join algorithm called\\nCACHEJOIN (Cache Join) focusing non-uniform stream data is provided in the\\nliterature. However, this algorithm cannot exploit the memory and CPU resources\\noptimally and consequently it leaves its service rate suboptimal due to\\nsequential execution of both of its phases, called stream-probing (SP) phase\\nand disk-probing (DP) phase. By integrating the advantages of CACHEJOIN, in\\nthis paper we present two modifications in it. First is called P-CACHEJOIN\\n(Parallel Cache Join) that enables the parallel processing of two phases in\\nCACHEJOIN. This increases number of joined stream records and therefore\\nimproves throughput considerably. Second is called OP-CACHEJOIN (Optimized\\nParallel Cache Join) that implements a parallel loading of stored data into\\nmemory while the DP phase is executing. We present the performance analysis of\\nboth of our approaches with existing CACHEJOIN empirically using synthetic\\nskewed dataset.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.02646v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.02646v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.03291v1', updated=datetime.datetime(2019, 11, 8, 14, 43, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 8, 14, 43, 26, tzinfo=datetime.timezone.utc), title='PnyxDB: a Lightweight Leaderless Democratic Byzantine Fault Tolerant Replicated Datastore', authors=[arxiv.Result.Author('Loïck Bonniot'), arxiv.Result.Author('Christoph Neumann'), arxiv.Result.Author('François Taïani')], summary='Byzantine-Fault-Tolerant (BFT) systems are rapidly emerging as a viable\\ntechnology for production-grade systems, notably in closed consortia\\ndeployments for nancial and supply-chain applications. Unfortunately, most\\nalgorithms proposed so far to coordinate these systems suffer from substantial\\nscalability issues, and lack important features to implement Internet-scale\\ngovernance mechanisms. In this paper, we observe that many application\\nworkloads offer little concurrency, and propose PnyxDB, an\\neventually-consistent Byzantine Fault Tolerant replicated datastore that\\nexhibits both high scalability and low latency. Our approach is based on\\nconditional endorsements, that allow nodes to specify the set of transactions\\nthat must not be committed for the endorsement to be valid. In addition to its\\nhigh scalability, PnyxDB supports application-level voting, i.e. individual\\nnodes are able to endorse or reject a transaction according to\\napplication-defined policies without compromising consistency. We provide a\\ncomparison against BFTSMaRt and Tendermint, two competitors with different\\ndesign aims, and show that our implementation speeds up commit latencies by a\\nfactor of 11, remaining below 5 seconds in a worldwide geodistributed\\ndeployment of 180 nodes.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.03291v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.03291v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.04948v1', updated=datetime.datetime(2019, 11, 9, 23, 15, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 9, 23, 15, 13, tzinfo=datetime.timezone.utc), title='EntropyDB: A Probabilistic Approach to Approximate Query Processing', authors=[arxiv.Result.Author('Laurel Orr'), arxiv.Result.Author('Magdalena Balazinska'), arxiv.Result.Author('Dan Suciu')], summary='We present EntropyDB, an interactive data exploration system that uses a\\nprobabilistic approach to generate a small, query-able summary of a dataset.\\nDeparting from traditional summarization techniques, we use the Principle of\\nMaximum Entropy to generate a probabilistic representation of the data that can\\nbe used to give approximate query answers. We develop the theoretical framework\\nand formulation of our probabilistic representation and show how to use it to\\nanswer queries. We then present solving techniques, give two critical\\noptimizations to improve preprocessing time and query execution time, and\\nexplore methods to reduce query error. Lastly, we experimentally evaluate our\\nwork using a 5 GB dataset of flights within the United States and a 210 GB\\ndataset from an astronomy particle simulation. While our current work only\\nsupports linear queries, we show that our technique can successfully answer\\nqueries faster than sampling while introducing, on average, no more error than\\nsampling and can better distinguish between rare and nonexistent values. We\\nalso discuss extensions that can allow for data updates and linear queries over\\njoins.', comment='arXiv admin note: text overlap with arXiv:1703.03856', journal_ref='VLDB Journal 2019', doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.04948v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.04948v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.05900v1', updated=datetime.datetime(2019, 11, 14, 2, 32, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 14, 2, 32, 16, tzinfo=datetime.timezone.utc), title='Proceedings of the Third Workshop on Software Foundations for Data Interoperability (SFDI2019+), October 28, 2019, Fukuoka, Japan', authors=[arxiv.Result.Author('Soichiro Hidaka'), arxiv.Result.Author('Yasunori Ishihara'), arxiv.Result.Author('Zachary G. Ives')], summary='This volume contains the papers presented at the Third Workshop on Software\\nFoundations for Data Interoperability (SFDI2019+) held on October 28, 2019, in\\nFukuoka, co-located with the 11th Asia-Pasific Symposium on Internetware\\n(Internetware 2019). One regular paper and six short papers have been accepted\\nfor presentation, each of which has its unique ideas and/or interesting results\\non interoperability of autonomic distributed data. Moreover, SFDI2019+ featured\\ntwo keynote talks by Hiroyuki Seki (Nagoya University) and Zinovy Diskin\\n(McMaster University), which introduce concepts and directions novel to the\\nseries of SFDI workshops so far.', comment='Proceedings of the Third Workshop on Software Foundations for Data\\n  Interoperability (SFDI2019+), October 28, 2019, Fukuoka, Japan', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.PL'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.05900v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.05900v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.05921v3', updated=datetime.datetime(2020, 8, 31, 16, 7, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 14, 3, 40, 32, tzinfo=datetime.timezone.utc), title='Programmable View Update Strategies on Relations', authors=[arxiv.Result.Author('Van-Dang Tran'), arxiv.Result.Author('Hiroyuki Kato'), arxiv.Result.Author('Zhenjiang Hu')], summary='View update is an important mechanism that allows updates on a view by\\ntranslating them into the corresponding updates on the base relations. The\\nexisting literature has shown the ambiguity of translating view updates. To\\naddress this ambiguity, we propose a robust language-based approach for making\\nview update strategies programmable and validatable. Specifically, we introduce\\na novel approach to use Datalog to describe these update strategies. We propose\\na validation algorithm to check the well-behavedness of the written Datalog\\nprograms. We present a fragment of the Datalog language for which our\\nvalidation is both sound and complete. This fragment not only has good\\nproperties in theory but is also useful for solving practical view updates.\\nFurthermore, we develop an algorithm for optimizing user-written programs to\\nefficiently implement updatable views in relational database management\\nsystems. We have implemented our proposed approach. The experimental results\\nshow that our framework is feasible and efficient in practice.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.05921v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.05921v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.06311v3', updated=datetime.datetime(2020, 6, 3, 4, 54, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 14, 18, 51, 59, tzinfo=datetime.timezone.utc), title='Sato: Contextual Semantic Type Detection in Tables', authors=[arxiv.Result.Author('Dan Zhang'), arxiv.Result.Author('Yoshihiko Suhara'), arxiv.Result.Author('Jinfeng Li'), arxiv.Result.Author('Madelon Hulsebos'), arxiv.Result.Author('Çağatay Demiralp'), arxiv.Result.Author('Wang-Chiew Tan')], summary='Detecting the semantic types of data columns in relational tables is\\nimportant for various data preparation and information retrieval tasks such as\\ndata cleaning, schema matching, data discovery, and semantic search. However,\\nexisting detection approaches either perform poorly with dirty data, support\\nonly a limited number of semantic types, fail to incorporate the table context\\nof columns or rely on large sample sizes for training data. We introduce Sato,\\na hybrid machine learning model to automatically detect the semantic types of\\ncolumns in tables, exploiting the signals from the context as well as the\\ncolumn values. Sato combines a deep learning model trained on a large-scale\\ntable corpus with topic modeling and structured prediction to achieve\\nsupport-weighted and macro average F1 scores of 0.925 and 0.735, respectively,\\nexceeding the state-of-the-art performance by a significant margin. We\\nextensively analyze the overall and per-type performance of Sato, discussing\\nhow individual modeling components, as well as feature categories, contribute\\nto its performance.', comment=\"VLDB'20\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CL', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.06311v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.06311v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.06577v1', updated=datetime.datetime(2019, 11, 15, 11, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 15, 11, 50, tzinfo=datetime.timezone.utc), title='Learning Models over Relational Data: A Brief Tutorial', authors=[arxiv.Result.Author('Maximilian Schleich'), arxiv.Result.Author('Dan Olteanu'), arxiv.Result.Author('Mahmoud Abo-Khamis'), arxiv.Result.Author('Hung Q. Ngo'), arxiv.Result.Author('XuanLong Nguyen')], summary='This tutorial overviews the state of the art in learning models over\\nrelational databases and makes the case for a first-principles approach that\\nexploits recent developments in database research.\\n  The input to learning classification and regression models is a training\\ndataset defined by feature extraction queries over relational databases. The\\nmainstream approach to learning over relational data is to materialize the\\ntraining dataset, export it out of the database, and then learn over it using a\\nstatistical package. This approach can be expensive as it requires the\\nmaterialization of the training dataset. An alternative approach is to cast the\\nmachine learning problem as a database problem by transforming the\\ndata-intensive component of the learning task into a batch of aggregates over\\nthe feature extraction query and by computing this batch directly over the\\ninput database.\\n  The tutorial highlights a variety of techniques developed by the database\\ntheory and systems communities to improve the performance of the learning task.\\nThey rely on structural properties of the relational data and of the feature\\nextraction query, including algebraic (semi-ring), combinatorial (hypertree\\nwidth), statistical (sampling), or geometric (distance) structure. They also\\nrely on factorized computation, code specialization, query compilation, and\\nparallelization.', comment='10 pages, 1 figure', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2.4; I.2.6'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.06577v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.06577v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.07225v1', updated=datetime.datetime(2019, 11, 17, 12, 38, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 17, 12, 38, 47, tzinfo=datetime.timezone.utc), title='Concept-oriented model: Modeling and processing data using functions', authors=[arxiv.Result.Author('Alexandr Savinov')], summary='We describe a new logical data model, called the concept-oriented model\\n(COM). It uses mathematical functions as first-class constructs for data\\nrepresentation and data processing as opposed to using exclusively sets in\\nconventional set-oriented models. Functions and function composition are used\\nas primary semantic units for describing data connectivity instead of relations\\nand relation composition (join), respectively. Grouping and aggregation are\\nalso performed by using (accumulate) functions providing an alternative to\\ngroup-by and reduce operations. This model was implemented in an open source\\ndata processing toolkit examples of which are used to illustrate the model and\\nits operations. The main benefit of this model is that typical data processing\\ntasks become simpler and more natural when using functions in comparison to\\nadopting sets and set operations.', comment='14 pages, 1 figure', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.07225v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.07225v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.07673v1', updated=datetime.datetime(2019, 11, 18, 14, 50, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 18, 14, 50, 31, tzinfo=datetime.timezone.utc), title='Using Mapping Languages for Building Legal Knowledge Graphs from XML Files', authors=[arxiv.Result.Author('Ademar Crotti Junior'), arxiv.Result.Author('Fabrizio Orlandi'), arxiv.Result.Author(\"Declan O'Sullivan\"), arxiv.Result.Author('Christian Dirschl'), arxiv.Result.Author('Quentin Reul')], summary='This paper presents our experience on building RDF knowledge graphs for an\\nindustrial use case in the legal domain. The information contained in legal\\ninformation systems are often accessed through simple keyword interfaces and\\npresented as a simple list of hits. In order to improve search accuracy one may\\navail of knowledge graphs, where the semantics of the data can be made\\nexplicit. Significant research effort has been invested in the area of building\\nknowledge graphs from semi-structured text documents, such as XML, with the\\nprevailing approach being the use of mapping languages. In this paper, we\\npresent a semantic model for representing legal documents together with an\\nindustrial use case. We also present a set of use case requirements based on\\nthe proposed semantic model, which are used to compare and discuss the use of\\nstate-of-the-art mapping languages for building knowledge graphs for legal\\ndata.', comment=\"Presented at the 2nd International Contextualized Knowledge Graphs\\n  Workshop (CKG'19) at the 18th International Semantic Web Conference (ISWC)\\n  2019\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.07673v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.07673v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.08614v1', updated=datetime.datetime(2019, 11, 19, 22, 25, 25, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 19, 22, 25, 25, tzinfo=datetime.timezone.utc), title='PDBMine: A Reformulation of the Protein Data Bank to Facilitate Structural Data Mining', authors=[arxiv.Result.Author('Casey A Cole'), arxiv.Result.Author('Christopher Ott'), arxiv.Result.Author('Diego Valdes'), arxiv.Result.Author('Homayoun Valafar')], summary='Large scale initiatives such as the Human Genome Project, Structural\\nGenomics, and individual research teams have provided large deposits of genomic\\nand proteomic data. The transfer of data to knowledge has become one of the\\nexisting challenges, which is a consequence of capturing data in databases that\\nare optimally designed for archiving and not mining. In this research, we have\\ntargeted the Protein Databank (PDB) and demonstrated a transformation of its\\ncontent, named PDBMine, that reduces storage space by an order of magnitude,\\nand allows for powerful mining in relation to the topic of protein structure\\ndetermination. We have demonstrated the utility of PDBMine in exploring the\\nprevalence of dimeric and trimeric amino acid sequences and provided a\\nmechanism of predicting protein structure.', comment='6 pages, 8 figures, IEEE Annual Conf. on Computational Science &\\n  Computational Intelligence (CSCI), December 2019', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'q-bio.BM'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.08614v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.08614v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.09356v1', updated=datetime.datetime(2019, 11, 21, 9, 20, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 21, 9, 20, 24, tzinfo=datetime.timezone.utc), title='Schemaless Queries over Document Tables with Dependencies', authors=[arxiv.Result.Author('Mustafa Canim'), arxiv.Result.Author('Cristina Cornelio'), arxiv.Result.Author('Arun Iyengar'), arxiv.Result.Author('Ryan Musa'), arxiv.Result.Author('Mariano Rodrigez Muro')], summary='Unstructured enterprise data such as reports, manuals and guidelines often\\ncontain tables. The traditional way of integrating data from these tables is\\nthrough a two-step process of table detection/extraction and mapping the table\\nlayouts to an appropriate schema. This can be an expensive process. In this\\npaper we show that by using semantic technologies (RDF/SPARQL and database\\ndependencies) paired with a simple but powerful way to transform tables with\\nnon-relational layouts, it is possible to offer query answering services over\\nthese tables with minimal manual work or domain-specific mappings. Our method\\nenables users to exploit data in tables embedded in documents with little\\neffort, not only for simple retrieval queries, but also for structured queries\\nthat require joining multiple interrelated tables.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.09356v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.09356v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.11184v1', updated=datetime.datetime(2019, 11, 25, 19, 33, 25, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 25, 19, 33, 25, tzinfo=datetime.timezone.utc), title='Managing Variability in Relational Databases by VDBMS', authors=[arxiv.Result.Author('Parisa Ataei'), arxiv.Result.Author('Qiaoran Li'), arxiv.Result.Author('Eric Walkingshaw'), arxiv.Result.Author('Arash Termehchy')], summary='Variability inherently exists in databases in various contexts which creates\\ndatabase variants. For example, variants of a database could have different\\nschemas/content (database evolution problem), variants of a database could root\\nfrom different sources (data integration problem), variants of a database could\\nbe deployed differently for specific application domain (deploying a database\\nfor different configurations of a software system), etc. Unfortunately, while\\nthere are specific solutions to each of the problems arising in these contexts,\\nthere is no general solution that accounts for variability in databases and\\naddresses managing variability within a database. In this paper, we formally\\ndefine variational databases (VDBs) and statically-typed variational relational\\nalgebra (VRA) to query VDBs---both database and queries explicitly account for\\nvariation. We also design and implement variational database management system\\n(VDBMS) to run variational queries over a VDB effectively and efficiently. To\\nassess this, we generate two VDBs from real-world databases in the context of\\nsoftware development and database evolution with a set of experimental queries\\nfor each.', comment='15 pages, 11 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.PL'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.11184v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.11184v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.11387v1', updated=datetime.datetime(2019, 11, 26, 8, 0, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 26, 8, 0, 29, tzinfo=datetime.timezone.utc), title='Cracking In-Memory Database Index A Case Study for Adaptive Radix Tree Index', authors=[arxiv.Result.Author('Gang Wu'), arxiv.Result.Author('Yidong Song'), arxiv.Result.Author('Guodong Zhao'), arxiv.Result.Author('Wei Sun'), arxiv.Result.Author('Donghong Han'), arxiv.Result.Author('Baiyou Qiao'), arxiv.Result.Author('Guoren Wang'), arxiv.Result.Author('Ye Yuan')], summary='Indexes provide a method to access data in databases quickly. It can improve\\nthe response speed of subsequent queries by building a complete index in\\nadvance. However, it also leads to a huge overhead of the continuous updating\\nduring creating the index. An in-memory database usually has a higher query\\nprocessing performance than disk databases and is more suitable for real-time\\nquery processing. Therefore, there is an urgent need to reduce the index\\ncreation and update cost for in-memory databases. Database cracking technology\\nis currently recognized as an effective method to reduce the index\\ninitialization time. However, conventional cracking algorithms are focused on\\nsimple column data structure rather than those complex index structure for\\nin-memory databases. In order to show the feasibility of in-memory database\\nindex cracking and promote to future more extensive research, this paper\\nconducted a case study on the Adaptive Radix Tree (ART), a popular tree index\\nstructure of in-memory databases. On the basis of carefully examining the ART\\nindex construction overhead, an algorithm using auxiliary data structures to\\ncrack the ART index is proposed.', comment='12 pages,15 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.11387v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.11387v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.11689v1', updated=datetime.datetime(2019, 11, 26, 16, 48, 25, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 26, 16, 48, 25, tzinfo=datetime.timezone.utc), title='Join Query Optimization with Deep Reinforcement Learning Algorithms', authors=[arxiv.Result.Author('Jonas Heitz'), arxiv.Result.Author('Kurt Stockinger')], summary='Join query optimization is a complex task and is central to the performance\\nof query processing. In fact it belongs to the class of NP-hard problems.\\nTraditional query optimizers use dynamic programming (DP) methods combined with\\na set of rules and restrictions to avoid exhaustive enumeration of all possible\\njoin orders. However, DP methods are very resource intensive. Moreover, given\\nsimplifying assumptions of attribute independence, traditional query optimizers\\nrely on erroneous cost estimations, which can lead to suboptimal query plans.\\nRecent success of deep reinforcement learning (DRL) creates new opportunities\\nfor the field of query optimization to tackle the above-mentioned problems. In\\nthis paper, we present our DRL-based Fully Observed Optimizer (FOOP) which is a\\ngeneric query optimization framework that enables plugging in different machine\\nlearning algorithms. The main idea of FOOP is to use a data-adaptive learning\\nquery optimizer that avoids exhaustive enumerations of join orders and is thus\\nsignificantly faster than traditional approaches based on dynamic programming.\\nIn particular, we evaluate various DRL-algorithms and show that Proximal Policy\\nOptimization significantly outperforms Q-learning based algorithms. Finally we\\ndemonstrate how ensemble learning techniques combined with DRL can further\\nimprove the query optimizer.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.11689v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.11689v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.11727v1', updated=datetime.datetime(2019, 11, 26, 18, 3, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 26, 18, 3, 16, tzinfo=datetime.timezone.utc), title='Starling: A Scalable Query Engine on Cloud Function Services', authors=[arxiv.Result.Author('Matthew Perron'), arxiv.Result.Author('Raul Castro Fernandez'), arxiv.Result.Author('David DeWitt'), arxiv.Result.Author('Samuel Madden')], summary='Much like on-premises systems, the natural choice for running database\\nanalytics workloads in the cloud is to provision a cluster of nodes to run a\\ndatabase instance. However, analytics workloads are often bursty or low volume,\\nleaving clusters idle much of the time, meaning customers pay for compute\\nresources even when unused. The ability of cloud function services, such as AWS\\nLambda or Azure Functions, to run small, fine granularity tasks make them\\nappear to be a natural choice for query processing in such settings. But\\nimplementing an analytics system on cloud functions comes with its own set of\\nchallenges. These include managing hundreds of tiny stateless\\nresource-constrained workers, handling stragglers, and shuffling data through\\nopaque cloud services. In this paper we present Starling, a query execution\\nengine built on cloud function services that employs number of techniques to\\nmitigate these challenges, providing interactive query latency at a lower total\\ncost than provisioned systems with low-to-moderate utilization. In particular,\\non a 1TB TPC-H dataset in cloud storage, Starling is less expensive than the\\nbest provisioned systems for workloads when queries arrive 1 minute apart or\\nmore. Starling also has lower latency than competing systems reading from cloud\\nobject stores and can scale to larger datasets.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.11727v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.11727v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.12933v1', updated=datetime.datetime(2019, 11, 29, 3, 9, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 29, 3, 9, 41, tzinfo=datetime.timezone.utc), title='Mining Approximate Acyclic Schemes from Relations', authors=[arxiv.Result.Author('Batya Kenig'), arxiv.Result.Author('Pranay Mundra'), arxiv.Result.Author('Guna Prasad'), arxiv.Result.Author('Babak Salimi'), arxiv.Result.Author('Dan Suciu')], summary='Acyclic schemes have numerous applications in databases and in machine\\nlearning, such as improved design, more efficient storage, and increased\\nperformance for queries and machine learning algorithms. Multivalued\\ndependencies (MVDs) are the building blocks of acyclic schemes. The discovery\\nfrom data of both MVDs and acyclic schemes is more challenging than other forms\\nof data dependencies, such as Functional Dependencies, because these\\ndependencies do not hold on subsets of data, and because they are very\\nsensitive to noise in the data; for example a single wrong or missing tuple may\\ninvalidate the schema. In this paper we present Maimon, a system for\\ndiscovering approximate acyclic schemes and MVDs from data. We give a\\nprincipled definition of approximation, by using notions from information\\ntheory, then describe the two components of Maimon: mining for approximate\\nMVDs, then reconstructing acyclic schemes from approximate MVDs. We conduct an\\nexperimental evaluation of Maimon on 20 real-world datasets, and show that it\\ncan scale up to 1M rows, and up to 30 columns.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.12933v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.12933v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1911.13014v1', updated=datetime.datetime(2019, 11, 29, 9, 35, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 29, 9, 35, 4, tzinfo=datetime.timezone.utc), title='SOSD: A Benchmark for Learned Indexes', authors=[arxiv.Result.Author('Andreas Kipf'), arxiv.Result.Author('Ryan Marcus'), arxiv.Result.Author('Alexander van Renen'), arxiv.Result.Author('Mihail Stoian'), arxiv.Result.Author('Alfons Kemper'), arxiv.Result.Author('Tim Kraska'), arxiv.Result.Author('Thomas Neumann')], summary='A groundswell of recent work has focused on improving data management systems\\nwith learned components. Specifically, work on learned index structures has\\nproposed replacing traditional index structures, such as B-trees, with learned\\nmodels. Given the decades of research committed to improving index structures,\\nthere is significant skepticism about whether learned indexes actually\\noutperform state-of-the-art implementations of traditional structures on\\nreal-world data. To answer this question, we propose a new benchmarking\\nframework that comes with a variety of real-world datasets and baseline\\nimplementations to compare against. We also show preliminary results for\\nselected index structures, and find that learned models indeed often outperform\\nstate-of-the-art implementations, and are therefore a promising direction for\\nfuture research.', comment='NeurIPS 2019 Workshop on Machine Learning for Systems', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1911.13014v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1911.13014v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.00323v1', updated=datetime.datetime(2019, 12, 1, 5, 42, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 1, 5, 42, 35, tzinfo=datetime.timezone.utc), title='HCA-DBSCAN: HyperCube Accelerated Density Based Spatial Clustering for Applications with Noise', authors=[arxiv.Result.Author('Vinayak Mathur'), arxiv.Result.Author('Jinesh Mehta'), arxiv.Result.Author('Sanjay Singh')], summary='Density-based clustering has found numerous applications across various\\ndomains. The Density-Based Spatial Clustering of Applications with Noise\\n(DBSCAN) algorithm is capable of finding clusters of varied shapes that are not\\nlinearly separable, at the same time it is not sensitive to outliers in the\\ndata. Combined with the fact that the number of clusters in the data are not\\nrequired apriori makes DBSCAN really powerfully. Slower performance (O(n2))\\nlimits its applications. In this work, we present a new clustering algorithm,\\nthe HyperCube Accelerated DBSCAN(HCA-DBSCAN) which uses a combination of\\ndistance-based aggregation by overlaying the data with customized grids. We use\\nrepresentative points to reduce the number of comparisons that need to be\\ncomputed. Experimental results show that the proposed algorithm achieves a\\nsignificant run time speedup of up to 58.27% when compared to other\\nimprovements that try to reduce the time complexity of theDBSCAN algorithm', comment='9 pages, Sets and Partitions workshop at NeurIPS 2019', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1912.00323v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.00323v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.00426v2', updated=datetime.datetime(2020, 3, 28, 17, 17, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 1, 15, 16, 46, tzinfo=datetime.timezone.utc), title='Area Queries Based on Voronoi Diagrams', authors=[arxiv.Result.Author('Yang Li')], summary='The area query, to find all elements contained in a specified area from a\\ncertain set of spatial objects, is a very important spatial query widely\\nrequired in various fields. A number of approaches have been proposed to\\nimplement this query, the best known of which is to obtain a rough candidate\\nset through spatial indexes and then refine the candidates through geometric\\nvalidations to get the final result. When the shape of the query area is a\\nrectangle, this method has very high efficiency. However, when the query area\\nis irregular, the candidate set is usually much larger than the final result\\nset, which means a lot of redundant detection needs to be done, thus the\\nefficiency is greatly limited. In view of this issue, we propose a method of\\niteratively generating candidates based on Voronoi diagrams and apply it to\\narea queries. The experimental results indicate that with our approach, the\\nnumber of candidates in the process of area query is greatly reduced and the\\nefficiency of the query is significantly improved.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1912.00426v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.00426v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.00736v1', updated=datetime.datetime(2019, 11, 29, 14, 29, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 11, 29, 14, 29, 4, tzinfo=datetime.timezone.utc), title='Prototype Selection Based on Clustering and Conformance Metrics for Model Discovery', authors=[arxiv.Result.Author('Mohammadreza Fani Sani'), arxiv.Result.Author('Mathilde Boltenhagen'), arxiv.Result.Author('Wil van der Aalst')], summary='Process discovery aims at automatically creating process models on the basis\\nof event data captured during the execution of business processes. Process\\ndiscovery algorithms tend to use all of the event data to discover a process\\nmodel. This attitude sometimes leads to discover imprecise and/or complex\\nprocess models that may conceal important information of processes. To address\\nthis problem, several techniques, from data filtering to model repair, have\\nbeen elaborated in the literature. In this paper, we introduce a new\\nincremental prototype selection algorithm based on clustering of process\\ninstances. The method aims to iteratively compute a unique process model with a\\ndifferent set of selected prototypes, i.e., representative of whole event data\\nand stops when conformance metrics decrease. The proposed method has been\\nimplemented in both the ProM and the RapidProM platforms. We applied the\\nproposed method on several real event data with state-of-the-art, process\\ndiscovery algorithms. Results show that using the proposed method leads to\\nimprove the general quality of discovered process models.', comment='15 pages, 7 Figures, 2 tables', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1912.00736v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.00736v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.00937v1', updated=datetime.datetime(2019, 12, 2, 17, 7, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 2, 17, 7, 43, tzinfo=datetime.timezone.utc), title='Lambada: Interactive Data Analytics on Cold Data using Serverless Cloud Infrastructure', authors=[arxiv.Result.Author('Ingo Müller'), arxiv.Result.Author('Renato Marroquín'), arxiv.Result.Author('Gustavo Alonso')], summary='The promise of ultimate elasticity and operational simplicity of serverless\\ncomputing has recently lead to an explosion of research in this area. In the\\ncontext of data analytics, the concept sounds appealing, but due to the\\nlimitations of current offerings, there is no consensus yet on whether or not\\nthis approach is technically and economically viable. In this paper, we\\nidentify interactive data analytics on cold data as a use case where serverless\\ncomputing excels. We design and implement Lambada, a system following a purely\\nserverless architecture, in order to illustrate when and how serverless\\ncomputing should be employed for data analytics. We propose several system\\ncomponents that overcome the previously known limitations inherent in the\\nserverless paradigm as well as additional ones we identify in this work. We can\\nshow that, thanks to careful design, a serverless query processing system can\\nbe at the same time one order of magnitude faster and two orders of magnitude\\ncheaper compared to commercial Query-as-a-Service systems, the only alternative\\nwith similar operational simplicity.', comment=None, journal_ref=None, doi='10.1145/3318464.3389758', primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3318464.3389758', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1912.00937v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.00937v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.01519v2', updated=datetime.datetime(2020, 2, 5, 14, 8, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 3, 16, 48, 5, tzinfo=datetime.timezone.utc), title='Ontologies for the Virtual Materials Marketplace', authors=[arxiv.Result.Author('Martin Thomas Horsch'), arxiv.Result.Author('Silvia Chiacchiera'), arxiv.Result.Author('Michael A. Seaton'), arxiv.Result.Author('Ilian T. Todorov'), arxiv.Result.Author('Karel Šindelka'), arxiv.Result.Author('Martin Lísal'), arxiv.Result.Author('Barbara Andreon'), arxiv.Result.Author('Esteban Bayro Kaiser'), arxiv.Result.Author('Gabriele Mogni'), arxiv.Result.Author('Gerhard Goldbeck'), arxiv.Result.Author('Ralf Kunze'), arxiv.Result.Author('Georg Summer'), arxiv.Result.Author('Andreas Fiseni'), arxiv.Result.Author('Hauke Brüning'), arxiv.Result.Author('Peter Schiffels'), arxiv.Result.Author('Welchy Leite Cavalcanti')], summary='The Virtual Materials Marketplace (VIMMP) project, which develops an open\\nplatform for providing and accessing services related to materials modelling,\\nis presented with a focus on its ontology development and data technology\\naspects. Within VIMMP, a system of marketplace-level ontologies is developed to\\ncharacterize services, models, and interactions between users; the European\\nMaterials and Modelling Ontology (EMMO) is employed as a top-level ontology.\\nThe ontologies are used to annotate data that are stored in the ZONTAL Space\\ncomponent of VIMMP and to support the ingest and retrieval of data and metadata\\nat the VIMMP marketplace frontend.', comment=\"The Virtual Materials Marketplace (VIMMP) project is funded from the\\n  European Union's Horizon 2020 research and innovation programme under grant\\n  agreement no. 760907\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/1912.01519v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.01519v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.01668v1', updated=datetime.datetime(2019, 12, 3, 20, 10, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 3, 20, 10, 31, tzinfo=datetime.timezone.utc), title='Learning Multi-dimensional Indexes', authors=[arxiv.Result.Author('Vikram Nathan'), arxiv.Result.Author('Jialin Ding'), arxiv.Result.Author('Mohammad Alizadeh'), arxiv.Result.Author('Tim Kraska')], summary='Scanning and filtering over multi-dimensional tables are key operations in\\nmodern analytical database engines. To optimize the performance of these\\noperations, databases often create clustered indexes over a single dimension or\\nmulti-dimensional indexes such as R-trees, or use complex sort orders (e.g.,\\nZ-ordering). However, these schemes are often hard to tune and their\\nperformance is inconsistent across different datasets and queries. In this\\npaper, we introduce Flood, a multi-dimensional in-memory index that\\nautomatically adapts itself to a particular dataset and workload by jointly\\noptimizing the index structure and data storage. Flood achieves up to three\\norders of magnitude faster performance for range scans with predicates than\\nstate-of-the-art multi-dimensional indexes or sort orders on real-world\\ndatasets and workloads. Our work serves as a building block towards an\\nend-to-end learned database system.', comment=None, journal_ref=None, doi='10.1145/3318464.3380579', primary_category='cs.DB', categories=['cs.DB', 'cs.DS', 'cs.LG'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3318464.3380579', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1912.01668v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.01668v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.02127v2', updated=datetime.datetime(2020, 6, 3, 18, 43, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 4, 17, 20, 33, tzinfo=datetime.timezone.utc), title='Directly Mapping RDF Databases to Property Graph Databases', authors=[arxiv.Result.Author('Renzo Angles'), arxiv.Result.Author('Harsh Thakkar'), arxiv.Result.Author('Dominik Tomaszuk')], summary='RDF triplestores and property graph databases are two approaches for data\\nmanagement which are based on modeling, storing, and querying graph-like data.\\nIn spite of such common principles, they present special features that\\ncomplicate the task of database interoperability. While there exist some\\nmethods to transform RDF graphs into property graphs, and vice versa, they lack\\ncompatibility and a solid formal foundation. This paper presents three direct\\nmappings (schema-dependent and schema-independent) for transforming an RDF\\ndatabase into a property graph database, including data and schema. We show\\nthat two of the proposed mappings satisfy the properties of semantics\\npreservation and information preservation. The existence of both mappings\\nallows us to conclude that the property graph data model subsumes the\\ninformation capacity of the RDF data model.', comment='This work has been accepted and published at the IEEE Access Journal\\n  DOI: 10.1109/ACCESS.2020.2993117', journal_ref='IEEE Access Volume 8, 2020', doi='10.1109/ACCESS.2020.2993117', primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/ACCESS.2020.2993117', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1912.02127v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.02127v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.02947v1', updated=datetime.datetime(2019, 12, 6, 1, 59, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 6, 1, 59, 43, tzinfo=datetime.timezone.utc), title='Towards Interpretable and Learnable Risk Analysis for Entity Resolution', authors=[arxiv.Result.Author('Zhaoqiang Chen'), arxiv.Result.Author('Qun Chen'), arxiv.Result.Author('Boyi Hou'), arxiv.Result.Author('Tianyi Duan'), arxiv.Result.Author('Zhanhuai Li'), arxiv.Result.Author('Guoliang Li')], summary='Machine-learning-based entity resolution has been widely studied. However,\\nsome entity pairs may be mislabeled by machine learning models and existing\\nstudies do not study the risk analysis problem -- predicting and interpreting\\nwhich entity pairs are mislabeled. In this paper, we propose an interpretable\\nand learnable framework for risk analysis, which aims to rank the labeled pairs\\nbased on their risks of being mislabeled. We first describe how to\\nautomatically generate interpretable risk features, and then present a\\nlearnable risk model and its training technique. Finally, we empirically\\nevaluate the performance of the proposed approach on real data. Our extensive\\nexperiments have shown that the learning risk model can identify the mislabeled\\npairs with considerably higher accuracy than the existing alternatives.', comment='14 pages, Accepted to SIGMOD 2020', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1912.02947v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.02947v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.03417v1', updated=datetime.datetime(2019, 12, 7, 2, 42, 48, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 7, 2, 42, 48, tzinfo=datetime.timezone.utc), title='AutoBlock: A Hands-off Blocking Framework for Entity Matching', authors=[arxiv.Result.Author('Wei Zhang'), arxiv.Result.Author('Hao Wei'), arxiv.Result.Author('Bunyamin Sisman'), arxiv.Result.Author('Xin Luna Dong'), arxiv.Result.Author('Christos Faloutsos'), arxiv.Result.Author('David Page')], summary='Entity matching seeks to identify data records over one or multiple data\\nsources that refer to the same real-world entity. Virtually every entity\\nmatching task on large datasets requires blocking, a step that reduces the\\nnumber of record pairs to be matched. However, most of the traditional blocking\\nmethods are learning-free and key-based, and their successes are largely built\\non laborious human effort in cleaning data and designing blocking keys.\\n  In this paper, we propose AutoBlock, a novel hands-off blocking framework for\\nentity matching, based on similarity-preserving representation learning and\\nnearest neighbor search. Our contributions include: (a) Automation: AutoBlock\\nfrees users from laborious data cleaning and blocking key tuning. (b)\\nScalability: AutoBlock has a sub-quadratic total time complexity and can be\\neasily deployed for millions of records. (c) Effectiveness: AutoBlock\\noutperforms a wide range of competitive baselines on multiple large-scale,\\nreal-world datasets, especially when datasets are dirty and/or unstructured.', comment=\"In The Thirteenth ACM International Conference on Web Search and Data\\n  Mining (WSDM '20), February 3-7, 2020, Houston, TX, USA. ACM, Anchorage,\\n  Alaska, USA , 9 pages\", journal_ref=None, doi='10.1145/3336191.3371813', primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3336191.3371813', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1912.03417v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.03417v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.06229v2', updated=datetime.datetime(2020, 6, 1, 7, 37, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 12, 21, 35, 55, tzinfo=datetime.timezone.utc), title='Optimal Two-Sided Market Mechanism Design for Large-Scale Data Sharing and Trading in Massive IoT Networks', authors=[arxiv.Result.Author('Tao Zhang'), arxiv.Result.Author('Quanyan Zhu')], summary='The development of the Internet of Things (IoT) generates a significant\\namount of data that contains valuable knowledge for system operations and\\nbusiness opportunities. Since the data is the property of the IoT data owners,\\nthe access to the data requires permission from the data owners, which gives\\nrise to a potential market opportunity for the IoT data sharing and trading to\\ncreate economic values and market opportunities for both data owners and\\nbuyers. In this work, we leverage optimal mechanism design theory to develop a\\nmonopolist matching platform for data trading over massive IoT networks. The\\nproposed mechanism is composed of a pair of matching and payment rules for each\\nside of the market. We analyze the incentive compatibility of the market and\\ncharacterize the optimal mechanism with a class of cut-off matching rules for\\nboth welfare-maximization and revenue-maximization mechanisms and study three\\nmatching behaviors including complete-matched, bottom-eliminated, and\\ntop-reserved.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.SI', 'cs.SY', 'eess.SY'], links=[arxiv.Result.Link('http://arxiv.org/abs/1912.06229v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.06229v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.07091v1', updated=datetime.datetime(2019, 12, 15, 19, 4, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 15, 19, 4, 19, tzinfo=datetime.timezone.utc), title='Drawbacks and Proposed Solutions for Real-time Processing on Existing State-of-the-art Locality Sensitive Hashing Techniques', authors=[arxiv.Result.Author('Omid Jafari'), arxiv.Result.Author('Khandker Mushfiqul Islam'), arxiv.Result.Author('Parth Nagarkar')], summary='Nearest-neighbor query processing is a fundamental operation for many image\\nretrieval applications. Often, images are stored and represented by\\nhigh-dimensional vectors that are generated by feature-extraction algorithms.\\nSince tree-based index structures are shown to be ineffective for high\\ndimensional processing due to the well-known \"Curse of Dimensionality\",\\napproximate nearest neighbor techniques are used for faster query processing.\\nLocality Sensitive Hashing (LSH) is a very popular and efficient approximate\\nnearest neighbor technique that is known for its sublinear query processing\\ncomplexity and theoretical guarantees. Nowadays, with the emergence of\\ntechnology, several diverse application domains require real-time\\nhigh-dimensional data storing and processing capacity. Existing LSH techniques\\nare not suitable to handle real-time data and queries. In this paper, we\\ndiscuss the challenges and drawbacks of existing LSH techniques for processing\\nreal-time high-dimensional image data. Additionally, through experimental\\nanalysis, we propose improvements for existing state-of-the-art LSH techniques\\nfor efficient processing of high-dimensional image data.', comment='Accepted and Presented at the 5th International Conference on Signal\\n  and Image Processing (SIGI-2019), Dubai, UAE', journal_ref=None, doi='10.5121/csit.2019.91410', primary_category='cs.DB', categories=['cs.DB', 'H.2.4'], links=[arxiv.Result.Link('http://dx.doi.org/10.5121/csit.2019.91410', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1912.07091v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.07091v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.07172v1', updated=datetime.datetime(2019, 12, 16, 3, 13, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 16, 3, 13, 17, tzinfo=datetime.timezone.utc), title='Lauca: Generating Application-Oriented Synthetic Workloads', authors=[arxiv.Result.Author('Yuming Li'), arxiv.Result.Author('Rong Zhang'), arxiv.Result.Author('Yuchen Li'), arxiv.Result.Author('Ke Shu'), arxiv.Result.Author('Shuyan Zhang'), arxiv.Result.Author('Aoying Zhou')], summary='The synthetic workload is essential and critical to the performance\\nevaluation of database systems. When evaluating the database performance for a\\nspecific application, the similarity between synthetic workload and real\\napplication workload determines the credibility of evaluation results. However,\\nthe workload currently used for performance evaluation is difficult to have the\\nsame workload characteristics as the target application, which leads to\\ninaccurate evaluation results. To address this problem, we propose a workload\\nduplicator (Lauca) that can generate synthetic workloads with highly similar\\nperformance metrics for specific applications. To the best of our knowledge,\\nLauca is the first application-oriented transactional workload generator. By\\ncarefully studying the application-oriented synthetic workload generation\\nproblem, we present the key workload characteristics (transaction logic and\\ndata access distribution) of online transaction processing (OLTP) applications,\\nand propose novel workload characterization and generation algorithms, which\\nguarantee the high fidelity of synthetic workloads. We conduct extensive\\nexperiments using workloads from TPC-C, SmallBank and micro benchmarks on both\\nMySQL and PostgreSQL databases, and experimental results show that Lauca\\nconsistently generates high-quality synthetic workloads.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1912.07172v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.07172v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.07777v3', updated=datetime.datetime(2020, 1, 10, 20, 48, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 17, 1, 34, 5, tzinfo=datetime.timezone.utc), title='Mosaic: A Sample-Based Database System for Open World Query Processing', authors=[arxiv.Result.Author('Laurel Orr'), arxiv.Result.Author('Samuel Ainsworth'), arxiv.Result.Author('Walter Cai'), arxiv.Result.Author('Kevin Jamieson'), arxiv.Result.Author('Magda Balazinska'), arxiv.Result.Author('Dan Suciu')], summary='Data scientists have relied on samples to analyze populations of interest for\\ndecades. Recently, with the increase in the number of public data repositories,\\nsample data has become easier to access. It has not, however, become easier to\\nanalyze. This sample data is arbitrarily biased with an unknown sampling\\nprobability, meaning data scientists must manually debias the sample with\\ncustom techniques to avoid inaccurate results. In this vision paper, we propose\\nMosaic, a database system that treats samples as first-class citizens and\\nallows users to ask questions over populations represented by these samples.\\nAnswering queries over biased samples is non-trivial as there is no existing,\\nstandard technique to answer population queries when the sampling probability\\nis unknown. In this paper, we show how our envisioned system solves this\\nproblem by having a unique sample-based data model with extensions to the SQL\\nlanguage. We propose how to perform population query answering using biased\\nsamples and give preliminary results for one of our novel query answering\\ntechniques.', comment='CIDR 2020', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/1912.07777v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.07777v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.08010v1', updated=datetime.datetime(2019, 12, 17, 13, 45, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 17, 13, 45, 10, tzinfo=datetime.timezone.utc), title='Querying Linked Data: An Experimental Evaluation of State-of-the-Art Interfaces', authors=[arxiv.Result.Author('Gabriela Montoya'), arxiv.Result.Author('Ilkcan Keles'), arxiv.Result.Author('Katja Hose')], summary='The adoption of Semantic Web technologies, and in particular the Open Data\\ninitiative, has contributed to the steady growth of the number of datasets and\\ntriples accessible on the Web. Most commonly, queries over RDF data are\\nevaluated over SPARQL endpoints. Recently, however, alternatives such as TPF\\nhave been proposed with the goal of shifting query processing load from the\\nserver running the SPARQL endpoint towards the client that issued the query.\\nAlthough these interfaces have been evaluated against standard benchmarks and\\ntestbeds that showed their benefits over previous work in general, a\\nfine-granular evaluation of what types of queries exploit the strengths of the\\ndifferent available interfaces has never been done. In this paper, we present\\nthe results of our in-depth evaluation of existing RDF interfaces. In addition,\\nwe also examine the influence of the backend on the performance of these\\ninterfaces. Using representative and diverse query loads based on the query log\\nof a public SPARQL endpoint, we stress test the different interfaces and\\nbackends and identify their strengths and weaknesses.', comment='18 pages, 14 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1912.08010v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.08010v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.08026v2', updated=datetime.datetime(2020, 10, 29, 17, 18, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 17, 14, 8, 1, tzinfo=datetime.timezone.utc), title='ORCA: a Benchmark for Data Web Crawlers', authors=[arxiv.Result.Author('Michael Röder'), arxiv.Result.Author('Geraldo de Souza'), arxiv.Result.Author('Denis Kuchelev'), arxiv.Result.Author('Abdelmoneim Amer Desouki'), arxiv.Result.Author('Axel-Cyrille Ngonga Ngomo')], summary='The number of RDF knowledge graphs available on the Web grows constantly.\\nGathering these graphs at large scale for downstream applications hence\\nrequires the use of crawlers. Although Data Web crawlers exist, and general Web\\ncrawlers could be adapted to focus on the Data Web, there is currently no\\nbenchmark to fairly evaluate their performance. Our work closes this gap by\\npresenting the Orca benchmark. Orca generates a synthetic Data Web, which is\\ndecoupled from the original Web and enables a fair and repeatable comparison of\\nData Web crawlers. Our evaluations show that Orca can be used to reveal the\\ndifferent advantages and disadvantages of existing crawlers. The benchmark is\\nopen-source and available at https://github.com/dice-group/orca.', comment='8 pages, submitted to a conference', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.PF'], links=[arxiv.Result.Link('http://arxiv.org/abs/1912.08026v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.08026v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.09018v2', updated=datetime.datetime(2020, 7, 2, 13, 39, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 19, 5, 5, 51, tzinfo=datetime.timezone.utc), title='Detecting Incorrect Behavior of Cloud Databases as an Outsider', authors=[arxiv.Result.Author('Cheng Tan'), arxiv.Result.Author('Changgeng Zhao'), arxiv.Result.Author('Shuai Mu'), arxiv.Result.Author('Michael Walfish')], summary='Cloud DBs offer strong properties, including serializability, sometimes\\ncalled the gold standard database correctness property. But cloud DBs are\\ncomplicated black boxes, running in a different administrative domain from\\ntheir clients; thus, clients might like to know whether the DBs are meeting\\ntheir contract. A core difficulty is that the underlying problem here, namely\\nverifying serializability, is NP-complete. Nevertheless, we hypothesize that on\\nreal-world workloads, verifying serializability is tractable, and we treat the\\nquestion as a systems problem, for the first time. We build Cobra, which tames\\nthe underlying search problem by blending a new encoding of the problem,\\nhardware acceleration, and a careful choice of a suitable SMT solver. cobra\\nalso introduces a technique to address the challenge of garbage collection in\\nthis context. cobra improves over natural baselines by at least 10x in the\\nproblem size it can handle, while imposing modest overhead on clients.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1912.09018v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.09018v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.09127v2', updated=datetime.datetime(2019, 12, 26, 8, 1, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 19, 11, 8, 18, tzinfo=datetime.timezone.utc), title='Fast Mining of Spatial Frequent Wordset from Social Database', authors=[arxiv.Result.Author('Yongmi Lee'), arxiv.Result.Author('Kwang Woo Nam'), arxiv.Result.Author('Keun Ho Ryu')], summary='In this paper, we propose an algorithm that extracts spatial frequent\\npatterns to explain the relative characteristics of a specific location from\\nthe available social data. This paper proposes a spatial social data model\\nwhich includes spatial social data, spatial support, spatial frequent patterns,\\nspatial partition, and spatial clustering; these concepts are used for\\ndescribing the exploration algorithm of spatial frequent patterns. With these\\ndefined concepts as the foundation, an SFP-tree structure that maintains not\\nonly the frequent words but also the frequent cells was proposed, and an\\nSFP-growth algorithm that explores the frequent patterns on the basis of this\\nSFP-tree was proposed.', comment='published in Spatial Information Research, vol.25, pp. 271-280', journal_ref='Spatial Information Research, 25(2), 271-280 (2017)', doi='10.1007/s41324-017-0094-6', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/s41324-017-0094-6', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1912.09127v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.09127v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.11098v1', updated=datetime.datetime(2019, 12, 23, 20, 37, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 23, 20, 37, 40, tzinfo=datetime.timezone.utc), title='Towards Deterministic Decomposable Circuits for Safe Queries', authors=[arxiv.Result.Author('Mikaël Monet'), arxiv.Result.Author('Dan Olteanu')], summary='There exist two approaches for exact probabilistic inference of UCQs on\\ntuple-independent databases. In the extensional approach, query evaluation is\\nperformed within a DBMS by exploiting the structure of the query. In the\\nintensional approach, one first builds a representation of the lineage of the\\nquery on the database, then computes the probability of the lineage. In this\\npaper we propose a new technique to construct lineage representations as\\ndeterministic decomposable circuits in PTIME. The technique can apply to a\\nclass of UCQs that has been conjectured to separate the complexity of the two\\napproaches. We test our technique experimentally, and show that it succeeds on\\nall the queries of this class up to a certain size parameter, i.e., over $20$\\nmillion queries.', comment=\"10 pages. Appeared in the workshop AMW'18\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1912.11098v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.11098v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.11977v2', updated=datetime.datetime(2020, 4, 8, 6, 41, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 27, 4, 2, 6, tzinfo=datetime.timezone.utc), title='Real Time Pattern Matching with Dynamic Normalization', authors=[arxiv.Result.Author('Renzhi Wu'), arxiv.Result.Author('Sergey Sukhanov'), arxiv.Result.Author('Christian Debes')], summary='Pattern matching in time series data streams is considered to be an essential\\ndata mining problem that still stays challenging for many practical scenarios.\\nDifferent factors such as noise, varying amplitude scale or shift, signal\\nstretches or shrinks in time are all leading to performance degradation of many\\nexisting pattern matching algorithms. In this paper, we introduce a dynamic\\nz-normalization mechanism allowing for proper signal scaling even under\\nsignificant time and amplitude distortions. Based on that, we further propose a\\nDynamic Time Warping-based real-time pattern matching method to recover hidden\\npatterns that can be distorted in both time and amplitude. We evaluate our\\nproposed method on synthetic and real-world scenarios under realistic\\nconditions demonstrating its high operational characteristics comparing to\\nother state-of-the-art pattern matching methods.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.MM'], links=[arxiv.Result.Link('http://arxiv.org/abs/1912.11977v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.11977v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1912.12610v1', updated=datetime.datetime(2019, 12, 29, 8, 55, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 12, 29, 8, 55, 27, tzinfo=datetime.timezone.utc), title='The Impact of Negation on the Complexity of the Shapley Value in Conjunctive Queries', authors=[arxiv.Result.Author('Alon Reshef'), arxiv.Result.Author('Benny Kimelfeld'), arxiv.Result.Author('Ester Livshits')], summary='The Shapley value is a conventional and well-studied function for determining\\nthe contribution of a player to the coalition in a cooperative game. Among its\\napplications in a plethora of domains, it has recently been proposed to use the\\nShapley value for quantifying the contribution of a tuple to the result of a\\ndatabase query. In particular, we have a thorough understanding of the\\ntractability frontier for the class of Conjunctive Queries (CQs) and aggregate\\nfunctions over CQs. It has also been established that a tractable (randomized)\\nmultiplicative approximation exists for every union of CQs. Nevertheless, all\\nof these results are based on the monotonicity of CQs. In this work, we\\ninvestigate the implication of negation on the complexity of Shapley\\ncomputation, in both the exact and approximate senses. We generalize a known\\ndichotomy to account for negated atoms. We also show that negation\\nfundamentally changes the complexity of approximation. We do so by drawing a\\nconnection to the problem of deciding whether a tuple is \"relevant\" to a query,\\nand by analyzing its complexity.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/1912.12610v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1912.12610v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.00160v2', updated=datetime.datetime(2020, 3, 18, 20, 27, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 1, 7, 20, 23, tzinfo=datetime.timezone.utc), title='ResilientDB: Global Scale Resilient Blockchain Fabric', authors=[arxiv.Result.Author('Suyash Gupta'), arxiv.Result.Author('Sajjad Rahnama'), arxiv.Result.Author('Jelle Hellings'), arxiv.Result.Author('Mohammad Sadoghi')], summary='Recent developments in blockchain technology have inspired innovative new\\ndesigns in resilient distributed and database systems. At their core, these\\nblockchain applications typically use Byzantine fault-tolerant consensus\\nprotocols to maintain a common state across all replicas, even if some replicas\\nare faulty or malicious. Unfortunately, existing consensus protocols are not\\ndesigned to deal with geo-scale deployments in which many replicas spread\\nacross a geographically large area participate in consensus. To address this,\\nwe present the Geo-Scale Byzantine FaultTolerant consensus protocol (GeoBFT).\\nGeoBFT is designed for excellent scalability by using a topological-aware\\ngrouping of replicas in local clusters, by introducing parallelization of\\nconsensus at the local level, and by minimizing communication between clusters.\\nTo validate our vision of high-performance geo-scale resilient distributed\\nsystems, we implement GeoBFT in our efficient ResilientDB permissioned\\nblockchain fabric. We show that GeoBFT is not only sound and provides great\\nscalability, but also outperforms state-of-the-art consensus protocols by a\\nfactor of six in geo-scale deployments.', comment=None, journal_ref='PVLDB 13 (2020) 868-883', doi='10.14778/3380750.3380757', primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3380750.3380757', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2002.00160v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.00160v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.01531v1', updated=datetime.datetime(2020, 2, 4, 20, 52, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 4, 20, 52, 55, tzinfo=datetime.timezone.utc), title='Providing Insights for Queries affected by Failures and Stragglers', authors=[arxiv.Result.Author('Bruhathi Sundarmurthy'), arxiv.Result.Author('Harshad Deshmukh'), arxiv.Result.Author('Paris Koutris'), arxiv.Result.Author('Jeffrey Naughton')], summary='Interactive time responses are a crucial requirement for users analyzing\\nlarge amounts of data. Such analytical queries are typically run in a\\ndistributed setting, with data being sharded across thousands of nodes for high\\nthroughput. However, providing real-time analytics is still a very big\\nchallenge; with data distributed across thousands of nodes, the probability\\nthat some of the required nodes are unavailable or very slow during query\\nexecution is very high and unavailability may result in slow execution or even\\nfailures. The sheer magnitude of data and users increase resource contention\\nand this exacerbates the phenomenon of stragglers and node failures during\\nexecution. In this paper, we propose a novel solution to alleviate the\\nstraggler/failure problem that exploits existing efficient partitioning\\nproperties of the data, particularly, co-hash partitioned data, and provides\\napproximate answers along with confidence bounds to queries affected by\\nfailed/straggler nodes. We consider aggregate queries that involve joins, group\\nbys, having clauses and a subclass of nested subqueries. Finally, we validate\\nour approach through extensive experiments on the TPC-H dataset.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.01531v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.01531v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.01582v2', updated=datetime.datetime(2020, 5, 18, 16, 32, 57, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 5, 0, 10, 54, tzinfo=datetime.timezone.utc), title='A workload-adaptive mechanism for linear queries under local differential privacy', authors=[arxiv.Result.Author('Ryan McKenna'), arxiv.Result.Author('Raj Kumar Maity'), arxiv.Result.Author('Arya Mazumdar'), arxiv.Result.Author('Gerome Miklau')], summary='We propose a new mechanism to accurately answer a user-provided set of linear\\ncounting queries under local differential privacy (LDP). Given a set of linear\\ncounting queries (the workload) our mechanism automatically adapts to provide\\naccuracy on the workload queries. We define a parametric class of mechanisms\\nthat produce unbiased estimates of the workload, and formulate a constrained\\noptimization problem to select a mechanism from this class that minimizes\\nexpected total squared error. We solve this optimization problem numerically\\nusing projected gradient descent and provide an efficient implementation that\\nscales to large workloads. We demonstrate the effectiveness of our\\noptimization-based approach in a wide variety of settings, showing that it\\noutperforms many competitors, even outperforming existing mechanisms on the\\nworkloads for which they were intended.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.01582v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.01582v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.02017v1', updated=datetime.datetime(2020, 2, 5, 22, 11, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 5, 22, 11, 45, tzinfo=datetime.timezone.utc), title='Observations on Porting In-memory KV stores to Persistent Memory', authors=[arxiv.Result.Author('Brian Choi'), arxiv.Result.Author('Parv Saxena'), arxiv.Result.Author('Ryan Huang'), arxiv.Result.Author('Randal Burns')], summary='Systems that require high-throughput and fault tolerance, such as key-value\\nstores and databases, are looking to persistent memory to combine the\\nperformance of in-memory systems with the data-consistent fault-tolerance of\\nnonvolatile stores. Persistent memory devices provide fast bytea-ddressable\\naccess to non-volatile memory. We analyze the design space when integrating\\npersistent memory into in-memory key value stores and quantify performance\\ntradeoffs between throughput, latency, and and recovery time. Previous works\\nhave explored many design choices, but did not quantify the tradeoffs. We\\nimplement persistent memory support in Redis and Memcached, adapting the data\\nstructures of each to work in two modes: (1) with all data in persistent memory\\nand (2) a hybrid mode that uses persistent memory for key/value data and\\nnon-volatile memory for indexing and metadata. Our experience reveals three\\nactionable design principles that hold in Redis and Memcached, despite their\\nvery different implementations. We conclude that the hybrid design increases\\nthroughput and decreases latency at a minor cost in recovery time and code\\ncomplexity', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.02017v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.02017v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.03063v1', updated=datetime.datetime(2020, 2, 8, 1, 35, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 8, 1, 35, 50, tzinfo=datetime.timezone.utc), title='Storyboard: Optimizing Precomputed Summaries for Aggregation', authors=[arxiv.Result.Author('Edward Gan'), arxiv.Result.Author('Peter Bailis'), arxiv.Result.Author('Moses Charikar')], summary='An emerging class of data systems partition their data and precompute\\napproximate summaries (i.e., sketches and samples) for each segment to reduce\\nquery costs. They can then aggregate and combine the segment summaries to\\nestimate results without scanning the raw data. However, given limited storage\\nspace each summary introduces approximation errors that affect query accuracy.\\nFor instance, systems that use existing mergeable summaries cannot reduce query\\nerror below the error of an individual precomputed summary. We introduce\\nStoryboard, a query system that optimizes item frequency and quantile summaries\\nfor accuracy when aggregating over multiple segments. Compared to conventional\\nmergeable summaries, Storyboard leverages additional memory available for\\nsummary construction and aggregation to derive a more precise combined result.\\nThis reduces error by up to 25x over interval aggregations and 4.4x over data\\ncube aggregations on industrial datasets compared to standard summarization\\nmethods, with provable worst-case error guarantees.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.03063v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.03063v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.03686v1', updated=datetime.datetime(2020, 2, 10, 12, 37, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 10, 12, 37, 3, tzinfo=datetime.timezone.utc), title='Optimization of Retrieval Algorithms on Large Scale Knowledge Graphs', authors=[arxiv.Result.Author('Jens Dörpinghaus'), arxiv.Result.Author('Andreas Stefan')], summary='Knowledge graphs have been shown to play an important role in recent\\nknowledge mining and discovery, for example in the field of life sciences or\\nbioinformatics. Although a lot of research has been done on the field of query\\noptimization, query transformation and of course in storing and retrieving\\nlarge scale knowledge graphs the field of algorithmic optimization is still a\\nmajor challenge and a vital factor in using graph databases. Few researchers\\nhave addressed the problem of optimizing algorithms on large scale labeled\\nproperty graphs. Here, we present two optimization approaches and compare them\\nwith a naive approach of directly querying the graph database. The aim of our\\nwork is to determine limiting factors of graph databases like Neo4j and we\\ndescribe a novel solution to tackle these challenges. For this, we suggest a\\nclassification schema to differ between the complexity of a problem on a graph\\ndatabase. We evaluate our optimization approaches on a test system containing a\\nknowledge graph derived biomedical publication data enriched with text mining\\ndata. This dense graph has more than 71M nodes and 850M relationships. The\\nresults are very encouraging and - depending on the problem - we were able to\\nshow a speedup of a factor between 44 and 3839.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.03686v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.03686v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.05837v1', updated=datetime.datetime(2020, 2, 14, 1, 23, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 14, 1, 23, 54, tzinfo=datetime.timezone.utc), title='PushdownDB: Accelerating a DBMS using S3 Computation', authors=[arxiv.Result.Author('Xiangyao Yu'), arxiv.Result.Author('Matt Youill'), arxiv.Result.Author('Matthew Woicik'), arxiv.Result.Author('Abdurrahman Ghanem'), arxiv.Result.Author('Marco Serafini'), arxiv.Result.Author('Ashraf Aboulnaga'), arxiv.Result.Author('Michael Stonebraker')], summary='This paper studies the effectiveness of pushing parts of DBMS analytics\\nqueries into the Simple Storage Service (S3) engine of Amazon Web Services\\n(AWS), using a recently released capability called S3 Select. We show that some\\nDBMS primitives (filter, projection, aggregation) can always be\\ncost-effectively moved into S3. Other more complex operations (join, top-K,\\ngroup-by) require reimplementation to take advantage of S3 Select and are often\\ncandidates for pushdown. We demonstrate these capabilities through\\nexperimentation using a new DBMS that we developed, PushdownDB. Experimentation\\nwith a collection of queries including TPC-H queries shows that PushdownDB is\\non average 30% cheaper and 6.7X faster than a baseline that does not use S3\\nSelect.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.05837v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.05837v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.05945v3', updated=datetime.datetime(2020, 7, 15, 16, 27, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 14, 10, 10, 43, tzinfo=datetime.timezone.utc), title='Online Process Monitoring Using Incremental State-Space Expansion: An Exact Algorithm', authors=[arxiv.Result.Author('Daniel Schuster'), arxiv.Result.Author('Sebastiaan J. van Zelst')], summary=\"The execution of (business) processes generates valuable traces of event data\\nin the information systems employed within companies. Recently, approaches for\\nmonitoring the correctness of the execution of running processes have been\\ndeveloped in the area of process mining, i.e., online conformance checking. The\\nadvantages of monitoring a process' conformity during its execution are clear,\\ni.e., deviations are detected as soon as they occur and countermeasures can\\nimmediately be initiated to reduce the possible negative effects caused by\\nprocess deviations. Existing work in online conformance checking only allows\\nfor obtaining approximations of non-conformity, e.g., overestimating the actual\\nseverity of the deviation. In this paper, we present an exact, parameter-free,\\nonline conformance checking algorithm that computes conformance checking\\nresults on the fly. Our algorithm exploits the fact that the conformance\\nchecking problem can be reduced to a shortest path problem, by incrementally\\nexpanding the search space and reusing previously computed intermediate\\nresults. Our experiments show that our algorithm outperforms comparable\\nstate-of-the-art approximation algorithms.\", comment=None, journal_ref=None, doi='10.1007/978-3-030-58666-9_9', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-030-58666-9_9', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2002.05945v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.05945v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.06163v6', updated=datetime.datetime(2020, 4, 15, 22, 47, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 14, 18, 27, 37, tzinfo=datetime.timezone.utc), title='Cleaning Denial Constraint Violations through Relaxation', authors=[arxiv.Result.Author('Stella Giannakopoulou'), arxiv.Result.Author('Manos Karpathiotakis'), arxiv.Result.Author('Anastasia Ailamaki')], summary='Data cleaning is a time-consuming process that depends on the data analysis\\nthat users perform. Existing solutions treat data cleaning as a separate\\noffline process that takes place before analysis begins. Applying data cleaning\\nbefore analysis assumes a priori knowledge of the inconsistencies and the query\\nworkload, thereby requiring effort on understanding and cleaning the data that\\nis unnecessary for the analysis. We propose an approach that performs\\nprobabilistic repair of denial constraint violations on-demand, driven by the\\nexploratory analysis that users perform. We introduce Daisy, a system that\\nseamlessly integrates data cleaning into the analysis by relaxing query\\nresults. Daisy executes analytical query-workloads over dirty data by weaving\\ncleaning operators into the query plan. Our evaluation shows that Daisy adapts\\nto the workload and outperforms traditional offline cleaning on both synthetic\\nand real-world workloads.', comment='To appear in SIGMOD 2020 proceedings', journal_ref=None, doi='10.1145/3318464.3389775', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3318464.3389775', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2002.06163v6', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.06163v6', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.08828v4', updated=datetime.datetime(2022, 10, 4, 11, 18, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 20, 16, 7, 18, tzinfo=datetime.timezone.utc), title='The Complexity of Aggregates over Extractions by Regular Expressions', authors=[arxiv.Result.Author('Johannes Doleschal'), arxiv.Result.Author('Benny Kimelfeld'), arxiv.Result.Author('Wim Martens')], summary='Regular expressions with capture variables, also known as regex-formulas,\\nextract relations of spans (intervals identified by their start and end\\nindices) from text. In turn, the class of regular document spanners is the\\nclosure of the regex formulas under the Relational Algebra. We investigate the\\ncomputational complexity of querying text by aggregate functions, such as sum,\\naverage, and quantile, on top of regular document spanners. To this end, we\\nformally define aggregate functions over regular document spanners and analyze\\nthe computational complexity of exact and approximate computation. More\\nprecisely, we show that in a restricted case, all studied aggregate functions\\ncan be computed in polynomial time. In general, however, even though exact\\ncomputation is intractable, some aggregates can still be approximated with\\nfully polynomial-time randomized approximation schemes (FPRAS).', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.FL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.08828v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.08828v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.09091v1', updated=datetime.datetime(2020, 2, 21, 2, 10, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 21, 2, 10, 42, tzinfo=datetime.timezone.utc), title='Facilitating SQL Query Composition and Analysis', authors=[arxiv.Result.Author('Zainab Zolaktaf'), arxiv.Result.Author('Mostafa Milani'), arxiv.Result.Author('Rachel Pottinger')], summary='Formulating efficient SQL queries requires several cycles of tuning and\\nexecution, particularly for inexperienced users. We examine methods that can\\naccelerate and improve this interaction by providing insights about SQL queries\\nprior to execution. We achieve this by predicting properties such as the query\\nanswer size, its run-time, and error class. Unlike existing approaches, our\\napproach does not rely on any statistics from the database instance or query\\nexecution plans. This is particularly important in settings with limited access\\nto the database instance. Our approach is based on using data-driven machine\\nlearning techniques that rely on large query workloads to model SQL queries and\\ntheir properties. We evaluate the utility of neural network models and\\ntraditional machine learning models. We use two real-world query workloads: the\\nSloan Digital Sky Survey (SDSS) and the SQLShare query workload. Empirical\\nresults show that the neural network models are more accurate in predicting the\\nquery error class, achieving a higher F-measure on classes with fewer samples\\nas well as performing better on other problems such as run-time and answer size\\nprediction. These results are encouraging and confirm that SQL query workloads\\nand data-driven machine learning methods can be leveraged to facilitate query\\ncomposition and analysis.', comment='Full version of the ACM SIGMOD 2020 paper', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.09091v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.09091v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.09172v2', updated=datetime.datetime(2021, 11, 9, 12, 19, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 21, 7, 59, 22, tzinfo=datetime.timezone.utc), title='Star Pattern Fragments: Accessing Knowledge Graphs through Star Patterns', authors=[arxiv.Result.Author('Christian Aebeloe'), arxiv.Result.Author('Ilkcan Keles'), arxiv.Result.Author('Gabriela Montoya'), arxiv.Result.Author('Katja Hose')], summary='The Semantic Web offers access to a vast Web of interlinked information\\naccessible via SPARQL endpoints. Such endpoints offer a well-defined interface\\nto retrieve results for complex SPARQL queries. The computational load for\\nprocessing such SPARQL endpoints offer access to a vast amount of interlinked\\ninformation. While they offer a well-defined interface for efficiently\\nretrieving results for complex SPARQL queries, complex query loads can easily\\noverload or crash endpoints as all the computational load of answering the\\nqueries resides entirely with the server hosting the endpoint. Recently\\nproposed interfaces, such as Triple Pattern Fragments, have therefore shifted\\nsome of the query processing load from the server to the client at the expense\\nof increased network traffic in the case of non-selective triple patterns. This\\npaper therefore proposes Star Pattern Fragments (SPF), an RDF interface\\nenabling a better load balancing between server and client by decomposing\\nSPARQL queries into star-shaped subqueries, evaluating them on the server side.\\nExperiments using synthetic data (WatDiv), as well as real data (DBpedia), show\\nthat SPF does not only significantly reduce network traffic, it is also up to\\ntwo orders of magnitude faster than the state-of-the-art interfaces under high\\nquery load.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.09172v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.09172v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.09361v1', updated=datetime.datetime(2020, 2, 21, 15, 33, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 21, 15, 33, 53, tzinfo=datetime.timezone.utc), title='Crowdsourced Collective Entity Resolution with Relational Match Propagation', authors=[arxiv.Result.Author('Jiacheng Huang'), arxiv.Result.Author('Wei Hu'), arxiv.Result.Author('Zhifeng Bao'), arxiv.Result.Author('Yuzhong Qu')], summary='Knowledge bases (KBs) store rich yet heterogeneous entities and facts. Entity\\nresolution (ER) aims to identify entities in KBs which refer to the same\\nreal-world object. Recent studies have shown significant benefits of involving\\nhumans in the loop of ER. They often resolve entities with pairwise similarity\\nmeasures over attribute values and resort to the crowds to label uncertain\\nones. However, existing methods still suffer from high labor costs and\\ninsufficient labeling to some extent. In this paper, we propose a novel\\napproach called crowdsourced collective ER, which leverages the relationships\\nbetween entities to infer matches jointly rather than independently.\\nSpecifically, it iteratively asks human workers to label picked entity pairs\\nand propagates the labeling information to their neighbors in distance. During\\nthis process, we address the problems of candidate entity pruning,\\nprobabilistic propagation, optimal question selection and error-tolerant truth\\ninference. Our experiments on real-world datasets demonstrate that, compared\\nwith state-of-the-art methods, our approach achieves superior accuracy with\\nmuch less labeling.', comment='Accepted by the 36th IEEE International Conference on Data\\n  Engineering (ICDE 2020)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.09361v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.09361v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.09440v3', updated=datetime.datetime(2021, 9, 28, 3, 44, 7, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 21, 17, 40, 20, tzinfo=datetime.timezone.utc), title='A Toolkit for Generating Code Knowledge Graphs', authors=[arxiv.Result.Author('Ibrahim Abdelaziz'), arxiv.Result.Author('Julian Dolby'), arxiv.Result.Author('Jamie McCusker'), arxiv.Result.Author('Kavitha Srinivas')], summary='Knowledge graphs have been proven extremely useful in powering diverse\\napplications in semantic search and natural language understanding. In this\\npaper, we present GraphGen4Code, a toolkit to build code knowledge graphs that\\ncan similarly power various applications such as program search, code\\nunderstanding, bug detection, and code automation. GraphGen4Code uses generic\\ntechniques to capture code semantics with the key nodes in the graph\\nrepresenting classes, functions, and methods. Edges indicate function usage\\n(e.g., how data flows through function calls, as derived from program analysis\\nof real code), and documentation about functions (e.g., code documentation,\\nusage documentation, or forum discussions such as StackOverflow). Our toolkit\\nuses named graphs in RDF to model graphs per program, or can output graphs as\\nJSON. We show the scalability of the toolkit by applying it to 1.3 million\\nPython files drawn from GitHub, 2,300 Python modules, and 47 million forum\\nposts. This results in an integrated code graph with over 2 billion triples. We\\nmake the toolkit to build such graphs as well as the sample extraction of the 2\\nbillion triples graph publicly available to the community for use.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.09440v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.09440v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.09799v2', updated=datetime.datetime(2020, 2, 29, 18, 38, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 23, 0, 53, 4, tzinfo=datetime.timezone.utc), title='Sample Debiasing in the Themis Open World Database System (Extended Version)', authors=[arxiv.Result.Author('Laurel Orr'), arxiv.Result.Author('Magda Balazinska'), arxiv.Result.Author('Dan Suciu')], summary='Open world database management systems assume tuples not in the database\\nstill exist and are becoming an increasingly important area of research. We\\npresent Themis, the first open world database that automatically rebalances\\narbitrarily biased samples to approximately answer queries as if they were\\nissued over the entire population. We leverage apriori population aggregate\\ninformation to develop and combine two different approaches for automatic\\ndebiasing: sample reweighting and Bayesian network probabilistic modeling. We\\nbuild a prototype of Themis and demonstrate that Themis achieves higher query\\naccuracy than the default AQP approach, an alternative sample reweighting\\ntechnique, and a variety of Bayesian network models while maintaining\\ninteractive query response times. We also show that \\\\name is robust to\\ndifferences in the support between the sample and population, a key use case\\nwhen using social media samples.', comment='SIGMOD 2020', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.09799v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.09799v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.10688v1', updated=datetime.datetime(2020, 2, 25, 6, 2, 7, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 25, 6, 2, 7, tzinfo=datetime.timezone.utc), title='A metric Suite for Systematic Quality Assessment of Linked Open Data', authors=[arxiv.Result.Author('Behshid Behkamal'), arxiv.Result.Author('Moshen Kahani'), arxiv.Result.Author('Ebrahim Bagheri'), arxiv.Result.Author('Majid Sazvar')], summary='Abstract- The vision of the Linked Open Data (LOD) initiative is to provide a\\ndistributed model for publishing and meaningfully interlinking open data. The\\nrealization of this goal depends strongly on the quality of the data that is\\npublished as a part of the LOD. This paper focuses on the systematic quality\\nassessment of datasets prior to publication on the LOD cloud. To this end, we\\nidentify important quality deficiencies that need to be avoided and/or resolved\\nprior to the publication of a dataset. We then propose a set of metrics to\\nmeasure these quality deficiencies in a dataset. This way, we enable the\\nassessment and identification of undesirable quality characteristics of a\\ndataset through our proposed metrics. This will help publishers to filter out\\nlow-quality data based on the quality assessment results, which in turn enables\\ndata consumers to make better and more informed decisions when using the open\\ndatasets.', comment='20 pages, 12 tables', journal_ref='IJICT Vol. 8, No. 3 (2016) 27-45', doi=None, primary_category='cs.DB', categories=['cs.DB', '97R50', 'E.0; E.1'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.10688v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.10688v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.11622v1', updated=datetime.datetime(2020, 2, 26, 17, 3, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 26, 17, 3, 28, tzinfo=datetime.timezone.utc), title='Revisiting compact RDF stores based on k2-trees', authors=[arxiv.Result.Author('Nieves R. Brisaboa'), arxiv.Result.Author('Ana Cerdeira-Pena'), arxiv.Result.Author('Guillermo de Bernardo'), arxiv.Result.Author('Antonio Fariña')], summary='We present a new compact representation to efficiently store and query large\\nRDF datasets in main memory. Our proposal, called BMatrix, is based on the\\nk2-tree, a data structure devised to represent binary matrices in a compressed\\nway, and aims at improving the results of previous state-of-the-art\\nalternatives, especially in datasets with a relatively large number of\\npredicates. We introduce our technique, together with some improvements on the\\nbasic k2-tree that can be applied to our solution in order to boost\\ncompression. Experimental results in the flagship RDF dataset DBPedia show that\\nour proposal achieves better compression than existing alternatives, while\\nyielding competitive query times, particularly in the most frequent triple\\npatterns and in queries with unbound predicate, in which we outperform existing\\nsolutions.', comment=\"This research has received funding from the European Union's Horizon\\n  2020 research and innovation programme under the Marie Sklodowska-Curie\\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.11622v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.11622v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2002.12459v1', updated=datetime.datetime(2020, 2, 27, 21, 50, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 2, 27, 21, 50, 40, tzinfo=datetime.timezone.utc), title='Fast Join Project Query Evaluation using Matrix Multiplication', authors=[arxiv.Result.Author('Shaleen Deep'), arxiv.Result.Author('Xiao Hu'), arxiv.Result.Author('Paraschos Koutris')], summary='In the last few years, much effort has been devoted to developing join\\nalgorithms in order to achieve worst-case optimality for join queries over\\nrelational databases. Towards this end, the database community has had\\nconsiderable success in developing succinct algorithms that achieve worst-case\\noptimal runtime for full join queries, i.e the join is over all variables\\npresent in the input database. However, not much is known about join evaluation\\nwith {\\\\em projections} beyond some simple techniques of pushing down the\\nprojection operator in the query execution plan. Such queries have a large\\nnumber of applications in entity matching, graph analytics and searching over\\ncompressed graphs. In this paper, we study how a class of join queries with\\nprojections can be evaluated faster using worst-case optimal algorithms\\ntogether with matrix multiplication. Crucially, our algorithms are\\nparameterized by the output size of the final result, allowing for choice of\\nthe best execution strategy. We implement our algorithms as a subroutine and\\ncompare the performance with state-of-the-art techniques to show they can be\\nimproved upon by as much as 50x. More importantly, our experiments indicate\\nthat matrix multiplication is a useful operation that can help speed up join\\nprocessing owing to highly optimized open source libraries that are also highly\\nparallelizable.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2002.12459v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.12459v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.00773v3', updated=datetime.datetime(2021, 3, 29, 1, 7, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 2, 11, 30, 35, tzinfo=datetime.timezone.utc), title='Top-K Deep Video Analytics: A Probabilistic Approach', authors=[arxiv.Result.Author('Ziliang Lai'), arxiv.Result.Author('Chenxia Han'), arxiv.Result.Author('Chris Liu'), arxiv.Result.Author('Pengfei Zhang'), arxiv.Result.Author('Eric Lo'), arxiv.Result.Author('Ben Kao')], summary='The impressive accuracy of deep neural networks (DNNs) has created great\\ndemands on practical analytics over video data. Although efficient and\\naccurate, the latest video analytic systems have not supported analytics beyond\\nselection and aggregation queries. In data analytics, Top-K is a very important\\nanalytical operation that enables analysts to focus on the most important\\nentities. In this paper, we present Everest, the first system that supports\\nefficient and accurate Top-K video analytics. Everest ranks and identifies the\\nmost interesting frames/moments from videos with probabilistic guarantees.\\nEverest is a system built with a careful synthesis of deep computer vision\\nmodels, uncertain data management, and Top-K query processing. Evaluations on\\nreal-world videos and the latest Visual Road benchmark show that Everest\\nachieves between 14.3x to 20.6x higher efficiency than baseline approaches with\\nhigh result accuracy', comment='14 pages, 9 figures, 8 tables', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.00773v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.00773v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.01064v1', updated=datetime.datetime(2020, 3, 2, 17, 50, 7, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 2, 17, 50, 7, tzinfo=datetime.timezone.utc), title='Bridging the Gap Between Theory and Practice on Insertion-Intensive Database', authors=[arxiv.Result.Author('Sepanta Zeighami'), arxiv.Result.Author('Raymond Chi-Wing Wong')], summary='With the prevalence of online platforms, today, data is being generated and\\naccessed by users at a very high rate. Besides, applications such as stock\\ntrading or high frequency trading require guaranteed low delays for performing\\nan operation on a database. It is consequential to design databases that\\nguarantee data insertion and query at a consistently high rate without\\nintroducing any long delay during insertion. In this paper, we propose Nested\\nB-trees (NB-trees), an index that can achieve a consistently high insertion\\nrate on large volumes of data, while providing asymptotically optimal query\\nperformance that is very efficient in practice. Nested B-trees support\\ninsertions at rates higher than LSM-trees, the state-of-the-art index for\\ninsertion-intensive workloads, while avoiding their long insertion delays and\\nimproving on their query performance. They approach the query performance of\\nB-trees when complemented with Bloom filters. In our experiments, NB-trees had\\nworst-case delays up to 1000 smaller than LevelDB, RocksDB and bLSM, commonly\\nused LSM-tree data-stores, could perform queries more than 4 times faster than\\nLevelDB and 1.5 times faster than bLSM and RocksDB, while also outperforming\\nthem in terms of average insertion rate.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.01064v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.01064v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.01075v1', updated=datetime.datetime(2020, 3, 2, 18, 9, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 2, 18, 9, 43, tzinfo=datetime.timezone.utc), title='Constant delay enumeration with FPT-preprocessing for conjunctive queries of bounded submodular width', authors=[arxiv.Result.Author('Christoph Berkholz'), arxiv.Result.Author('Nicole Schweikardt')], summary=\"Marx (STOC~2010, J.~ACM 2013) introduced the notion of submodular width of a\\nconjunctive query (CQ) and showed that for any class $\\\\Phi$ of Boolean CQs of\\nbounded submodular width, the model-checking problem for $\\\\Phi$ on the class of\\nall finite structures is fixed-parameter tractable (FPT). Note that for\\nnon-Boolean queries, the size of the query result may be far too large to be\\ncomputed entirely within FPT time. We investigate the free-connex variant of\\nsubmodular width and generalise Marx's result to non-Boolean queries as\\nfollows: For every class $\\\\Phi$ of CQs of bounded free-connex submodular width,\\nwithin FPT-preprocessing time we can build a data structure that allows to\\nenumerate, without repetition and with constant delay, all tuples of the query\\nresult. Our proof builds upon Marx's splitting routine to decompose the query\\nresult into a union of results; but we have to tackle the additional technical\\ndifficulty to ensure that these can be enumerated efficiently.\", comment='This is the full version of the conference contribution with the same\\n  title that appeared at MFCS 2019', journal_ref='Proceedings of the 44th International Symposium on Mathematical\\n  Foundations of Computer Science, pp. 58:1-58:15, 2019', doi='10.4230/LIPIcs.MFCS.2019.58', primary_category='cs.DB', categories=['cs.DB', 'cs.LO'], links=[arxiv.Result.Link('http://dx.doi.org/10.4230/LIPIcs.MFCS.2019.58', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2003.01075v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.01075v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.01974v1', updated=datetime.datetime(2020, 3, 4, 9, 52, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 4, 9, 52, 9, tzinfo=datetime.timezone.utc), title='Flow Computation in Temporal Interaction Networks', authors=[arxiv.Result.Author('Chrysanthi Kosyfaki'), arxiv.Result.Author('Nikos Mamoulis'), arxiv.Result.Author('Evaggelia Pitoura'), arxiv.Result.Author('Panayiotis Tsaparas')], summary='Temporal interaction networks capture the history of activities between\\nentities along a timeline. At each interaction, some quantity of data (money,\\ninformation, kbytes, etc.) flows from one vertex of the network to another.\\nFlow-based analysis can reveal important information. For instance, financial\\nintelligent units (FIUs) are interested in finding subgraphs in transactions\\nnetworks with significant flow of money transfers. In this paper, we introduce\\nthe flow computation problem in an interaction network or a subgraph thereof.\\nWe propose and study two models of flow computation, one based on a greedy flow\\ntransfer assumption and one that finds the maximum possible flow. We show that\\nthe greedy flow computation problem can be easily solved by a single scan of\\nthe interactions in time order. For the harder maximum flow problem, we propose\\ngraph precomputation and simplification approaches that can greatly reduce its\\ncomplexity in practice. As an application of flow computation, we formulate and\\nsolve the problem of flow pattern search, where, given a graph pattern, the\\nobjective is to find its instances and their flows in a large interaction\\nnetwork. We evaluate our algorithms using real datasets. The results show that\\nthe techniques proposed in this paper can greatly reduce the cost of flow\\ncomputation and pattern enumeration.', comment='13 pages, 27 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.01974v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.01974v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.02391v1', updated=datetime.datetime(2020, 3, 5, 1, 2, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 5, 1, 2, 47, tzinfo=datetime.timezone.utc), title='Order-Preserving Key Compression for In-Memory Search Trees', authors=[arxiv.Result.Author('Huanchen Zhang'), arxiv.Result.Author('Xiaoxuan Liu'), arxiv.Result.Author('David G. Andersen'), arxiv.Result.Author('Michael Kaminsky'), arxiv.Result.Author('Kimberly Keeton'), arxiv.Result.Author('Andrew Pavlo')], summary=\"We present the High-speed Order-Preserving Encoder (HOPE) for in-memory\\nsearch trees. HOPE is a fast dictionary-based compressor that encodes arbitrary\\nkeys while preserving their order. HOPE's approach is to identify common key\\npatterns at a fine granularity and exploit the entropy to achieve high\\ncompression rates with a small dictionary. We first develop a theoretical model\\nto reason about order-preserving dictionary designs. We then select six\\nrepresentative compression schemes using this model and implement them in HOPE.\\nThese schemes make different trade-offs between compression rate and encoding\\nspeed. We evaluate HOPE on five data structures used in databases: SuRF, ART,\\nHOT, B+tree, and Prefix B+tree. Our experiments show that using HOPE allows the\\nsearch trees to achieve lower query latency (up to 40\\\\% lower) and better\\nmemory efficiency (up to 30\\\\% smaller) simultaneously for most string key\\nworkloads.\", comment=\"SIGMOD'20 version + Appendix\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.02391v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.02391v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.02542v2', updated=datetime.datetime(2020, 7, 14, 7, 34, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 5, 11, 38, 21, tzinfo=datetime.timezone.utc), title='Efficient and Effective Similar Subtrajectory Search with Deep Reinforcement Learning', authors=[arxiv.Result.Author('Zheng Wang'), arxiv.Result.Author('Cheng Long'), arxiv.Result.Author('Gao Cong'), arxiv.Result.Author('Yiding Liu')], summary='Similar trajectory search is a fundamental problem and has been well studied\\nover the past two decades. However, the similar subtrajectory search (SimSub)\\nproblem, aiming to return a portion of a trajectory (i.e., a subtrajectory)\\nwhich is the most similar to a query trajectory, has been mostly disregarded\\ndespite that it could capture trajectory similarity in a finer-grained way and\\nmany applications take subtrajectories as basic units for analysis. In this\\npaper, we study the SimSub problem and develop a suite of algorithms including\\nboth exact and approximate ones. Among those approximate algorithms, two that\\nare based on deep reinforcement learning stand out and outperform those\\nnon-learning based algorithms in terms of effectiveness and efficiency. We\\nconduct experiments on real-world trajectory datasets, which verify the\\neffectiveness and efficiency of the proposed algorithms.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.02542v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.02542v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.04074v2', updated=datetime.datetime(2020, 3, 16, 12, 20, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 9, 12, 35, 46, tzinfo=datetime.timezone.utc), title='NoSQL Databases: Yearning for Disambiguation', authors=[arxiv.Result.Author('Chaimae Asaad'), arxiv.Result.Author('Karim Baïna'), arxiv.Result.Author('Mounir Ghogho')], summary='The demanding requirements of the new Big Data intensive era raised the need\\nfor flexible storage systems capable of handling huge volumes of unstructured\\ndata and of tackling the challenges that traditional databases were facing.\\nNoSQL Databases, in their heterogeneity, are a powerful and diverse set of\\ndatabases tailored to specific industrial and business needs. However, the lack\\nof theoretical background creates a lack of consensus even among experts about\\nmany NoSQL concepts, leading to ambiguity and confusion. In this paper, we\\npresent a survey of NoSQL databases and their classification by data model\\ntype. We also conduct a benchmark in order to compare different NoSQL databases\\nand distinguish their characteristics. Additionally, we present the major areas\\nof ambiguity and confusion around NoSQL databases and their related concepts,\\nand attempt to disambiguate them.', comment='18 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2.1; H.2.2'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.04074v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.04074v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.04410v1', updated=datetime.datetime(2020, 3, 9, 21, 3, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 9, 21, 3, 52, tzinfo=datetime.timezone.utc), title='A Note On Operator-Level Query Execution Cost Modeling', authors=[arxiv.Result.Author('Wentao Wu')], summary='External query execution cost modeling using query execution feedback has\\nfound its way in various database applications such as admission control and\\nquery scheduling. Existing techniques in general fall into two categories,\\nplan-level cost modeling and operator-level cost modeling. It has been shown in\\nthe literature that operator-level cost modeling can often significantly\\noutperform plan-level cost modeling. In this paper, we study operator-level\\ncost modeling from a robustness perspective. We address two main challenges in\\npractice regarding limited execution feedback (for certain operators) and mixed\\ncost estimates due to the use of multiple cost modeling techniques. We propose\\na framework that deals with these issues and present a comprehensive analysis\\nof this framework. We further provide a case study to demonstrate the efficacy\\nof our framework in the context of index tuning, which is itself a new\\napplication of external cost modeling techniques.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.04410v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.04410v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.04915v1', updated=datetime.datetime(2020, 3, 10, 18, 10, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 10, 18, 10, 16, tzinfo=datetime.timezone.utc), title='Managing Data Lineage of O&G Machine Learning Models: The Sweet Spot for Shale Use Case', authors=[arxiv.Result.Author('Raphael Thiago'), arxiv.Result.Author('Renan Souza'), arxiv.Result.Author('L. Azevedo'), arxiv.Result.Author('E. Soares'), arxiv.Result.Author('Rodrigo Santos'), arxiv.Result.Author('Wallas Santos'), arxiv.Result.Author('Max De Bayser'), arxiv.Result.Author('M. Cardoso'), arxiv.Result.Author('M. Moreno'), arxiv.Result.Author('Renato Cerqueira')], summary='Machine Learning (ML) has increased its role, becoming essential in several\\nindustries. However, questions around training data lineage, such as \"where has\\nthe dataset used to train this model come from?\"; the introduction of several\\nnew data protection legislation; and, the need for data governance\\nrequirements, have hindered the adoption of ML models in the real world. In\\nthis paper, we discuss how data lineage can be leveraged to benefit the ML\\nlifecycle to build ML models to discover sweet-spots for shale oil and gas\\nproduction, a major application in the Oil and Gas O&G Industry.', comment='Author preprint of paper accepted at the 2020 European Association of\\n  Geoscientists and Engineers (EAGE) Digitalization Conference and Exhibition', journal_ref='2020 European Association of Geoscientists and Engineers (EAGE)\\n  Digitalization Conference and Exhibition', doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CY', 'cs.DC', 'cs.LG', '65Y05, 68P15', 'I.2; H.2; C.4; J.2'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.04915v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.04915v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.05043v1', updated=datetime.datetime(2020, 3, 11, 0, 13, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 11, 0, 13, 17, tzinfo=datetime.timezone.utc), title='Crop Knowledge Discovery Based on Agricultural Big Data Integration', authors=[arxiv.Result.Author('Vuong M. Ngo'), arxiv.Result.Author('M-Tahar Kechadi')], summary='Nowadays, the agricultural data can be generated through various sources,\\nsuch as: Internet of Thing (IoT), sensors, satellites, weather stations,\\nrobots, farm equipment, agricultural laboratories, farmers, government agencies\\nand agribusinesses. The analysis of this big data enables farmers, companies\\nand agronomists to extract high business and scientific knowledge, improving\\ntheir operational processes and product quality. However, before analysing this\\ndata, different data sources need to be normalised, homogenised and integrated\\ninto a unified data representation. In this paper, we propose an agricultural\\ndata integration method using a constellation schema which is designed to be\\nflexible enough to incorporate other datasets and big data models. We also\\napply some methods to extract knowledge with the view to improve crop yield;\\nthese include finding suitable quantities of soil properties, herbicides and\\ninsecticides for both increasing crop yield and protecting the environment.', comment='5 pages', journal_ref='ICMLSC-2020', doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CY', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.05043v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.05043v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.06415v3', updated=datetime.datetime(2020, 6, 22, 17, 38, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 13, 17, 57, 8, tzinfo=datetime.timezone.utc), title='mmLSH: A Practical and Efficient Technique for Processing Approximate Nearest Neighbor Queries on Multimedia Data', authors=[arxiv.Result.Author('Omid Jafari'), arxiv.Result.Author('Parth Nagarkar'), arxiv.Result.Author('Jonathan Montaño')], summary='Many large multimedia applications require efficient processing of nearest\\nneighbor queries. Often, multimedia data are represented as a collection of\\nimportant high-dimensional feature vectors. Existing Locality Sensitive Hashing\\n(LSH) techniques require users to find top-k similar feature vectors for each\\nof the feature vectors that represent the query object. This leads to wasted\\nand redundant work due to two main reasons: 1) not all feature vectors may\\ncontribute equally in finding the top-k similar multimedia objects, and 2)\\nfeature vectors are treated independently during query processing.\\nAdditionally, there is no theoretical guarantee on the returned multimedia\\nresults. In this work, we propose a practical and efficient indexing approach\\nfor finding top-k approximate nearest neighbors for multimedia data using LSH\\ncalled mmLSH, which can provide theoretical guarantees on the returned\\nmultimedia results. Additionally, we present a buffer-conscious strategy to\\nspeed up the query processing. Experimental evaluation shows significant gains\\nin performance time and accuracy for different real multimedia datasets when\\ncompared against state-of-the-art LSH techniques.', comment='Submitted to SISAP 2020', journal_ref='SISAP 2020. Lecture Notes in Computer Science, vol 12440.\\n  Springer, Cham', doi='10.1007/978-3-030-60936-8_4', primary_category='cs.DB', categories=['cs.DB', 'H.2.4'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-030-60936-8_4', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2003.06415v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.06415v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.06613v1', updated=datetime.datetime(2020, 3, 14, 12, 8, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 14, 12, 8, 6, tzinfo=datetime.timezone.utc), title='ML-AQP: Query-Driven Approximate Query Processing based on Machine Learning', authors=[arxiv.Result.Author('Fotis Savva'), arxiv.Result.Author('Christos Anagnostopoulos'), arxiv.Result.Author('Peter Triantafillou')], summary=\"As more and more organizations rely on data-driven decision making,\\nlarge-scale analytics become increasingly important. However, an analyst is\\noften stuck waiting for an exact result. As such, organizations turn to Cloud\\nproviders that have infrastructure for efficiently analyzing large quantities\\nof data. But, with increasing costs, organizations have to optimize their\\nusage. Having a cheap alternative that provides speed and efficiency will go a\\nlong way. Concretely, we offer a solution that can provide approximate answers\\nto aggregate queries, relying on Machine Learning (ML), which is able to work\\nalongside Cloud systems. Our developed lightweight ML-led system can be stored\\non an analyst's local machine or deployed as a service to instantly answer\\nanalytic queries, having low response times and monetary/computational costs\\nand energy footprint. To accomplish this we leverage the knowledge obtained by\\npreviously answered queries and build ML models that can estimate the result of\\nnew queries in an efficient and inexpensive manner. The capabilities of our\\nsystem are demonstrated using extensive evaluation with both real and synthetic\\ndatasets/workloads and well known benchmarks.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.06613v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.06613v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.06708v1', updated=datetime.datetime(2020, 3, 14, 21, 28, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 14, 21, 28, 43, tzinfo=datetime.timezone.utc), title='Scrutinizer: A Mixed-Initiative Approach to Large-Scale, Data-Driven Claim Verification', authors=[arxiv.Result.Author('Georgios Karagiannis'), arxiv.Result.Author('Mohammed Saeed'), arxiv.Result.Author('Paolo Papotti'), arxiv.Result.Author('Immanuel Trummer')], summary='Organizations such as the International Energy Agency (IEA) spend significant\\namounts of time and money to manually fact check text documents summarizing\\ndata. The goal of the Scrutinizer system is to reduce verification overheads by\\nsupporting human fact checkers in translating text claims into SQL queries on\\nan associated database.\\n  Scrutinizer coordinates teams of human fact checkers. It reduces verification\\ntime by proposing queries or query fragments to the users. Those proposals are\\nbased on claim text classifiers, that gradually improve during the verification\\nof a large document. In addition, Scrutinizer uses tentative execution of query\\ncandidates to narrow down the set of alternatives. The verification process is\\ncontrolled by a cost-based optimizer. It optimizes the interaction with users\\nand prioritizes claim verifications. For the latter, it considers expected\\nverification overheads as well as the expected claim utility as training\\nsamples for the classifiers. We evaluate the Scrutinizer system using\\nsimulations and a user study, based on actual claims and data and using\\nprofessional fact checkers employed by IEA. Our experiments consistently\\ndemonstrate significant savings in verification time, without reducing result\\naccuracy.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.06708v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.06708v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.06875v1', updated=datetime.datetime(2020, 3, 15, 17, 36, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 15, 17, 36, 55, tzinfo=datetime.timezone.utc), title='Recommending Deployment Strategies for Collaborative Tasks', authors=[arxiv.Result.Author('Dong Wei'), arxiv.Result.Author('Senjuti Basu Roy'), arxiv.Result.Author('Sihem Amer-Yahia')], summary='Our work contributes to aiding requesters in deploying collaborative tasks in\\ncrowdsourcing. We initiate the study of recommending deployment strategies for\\ncollaborative tasks to requesters that are consistent with deployment\\nparameters they desire: a lower-bound on the quality of the crowd contribution,\\nan upper-bound on the latency of task completion, and an upper-bound on the\\ncost incurred by paying workers. A deployment strategy is a choice of value for\\nthree dimensions: Structure (whether to solicit the workforce sequentially or\\nsimultaneously), Organization (to organize it collaboratively or\\nindependently), and Style (to rely solely on the crowd or to combine it with\\nmachine algorithms). We propose StratRec, an optimization-driven middle layer\\nthat recommends deployment strategies and alternative deployment parameters to\\nrequesters by accounting for worker availability. Our solutions are grounded in\\ndiscrete optimization and computational geometry techniques that produce\\nresults with theoretical guarantees. We present extensive experiments on Amazon\\nMechanical Turk and conduct synthetic experiments to validate the qualitative\\nand scalability aspects of StratRec.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.06875v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.06875v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.06880v6', updated=datetime.datetime(2023, 1, 24, 15, 32, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 15, 17, 50, 18, tzinfo=datetime.timezone.utc), title='Grammars for Document Spanners', authors=[arxiv.Result.Author('Liat Peterfreund')], summary='We propose a new grammar-based language for defining information-extractors\\nfrom documents (text) that is built upon the well-studied framework of document\\nspanners for extracting structured data from text. While previously studied\\nformalisms for document spanners are mainly based on regular expressions, we\\nuse an extension of context-free grammars, called {extraction grammars}, to\\ndefine the new class of context-free spanners. Extraction grammars are simply\\ncontext-free grammars extended with variables that capture interval positions\\nof the document, namely spans. While regular expressions are efficient for\\ntokenizing and tagging, context-free grammars are also efficient for capturing\\nstructural properties. Indeed, we show that context-free spanners are strictly\\nmore expressive than their regular counterparts. We reason about the expressive\\npower of our new class and present a pushdown-automata model that captures it.\\nWe show that extraction grammars can be evaluated with polynomial data\\ncomplexity. Nevertheless, as the degree of the polynomial depends on the query,\\nwe present an enumeration algorithm for unambiguous extraction grammars that,\\nafter quintic preprocessing, outputs the results sequentially, without\\nrepetitions, with a constant delay between every two consecutive ones.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.06880v6', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.06880v6', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.07302v2', updated=datetime.datetime(2020, 4, 9, 8, 3, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 16, 16, 15, 46, tzinfo=datetime.timezone.utc), title='Dash: Scalable Hashing on Persistent Memory', authors=[arxiv.Result.Author('Baotong Lu'), arxiv.Result.Author('Xiangpeng Hao'), arxiv.Result.Author('Tianzheng Wang'), arxiv.Result.Author('Eric Lo')], summary='Byte-addressable persistent memory (PM) brings hash tables the potential of\\nlow latency, cheap persistence and instant recovery. The recent advent of Intel\\nOptane DC Persistent Memory Modules (DCPMM) further accelerates this trend.\\nMany new hash table designs have been proposed, but most of them were based on\\nemulation and perform sub-optimally on real PM. They were also piece-wise and\\npartial solutions that side-step many important properties, in particular good\\nscalability, high load factor and instant recovery. We present Dash, a holistic\\napproach to building dynamic and scalable hash tables on real PM hardware with\\nall the aforementioned properties. Based on Dash, we adapted two popular\\ndynamic hashing schemes (extendible hashing and linear hashing). On a 24-core\\nmachine with Intel Optane DCPMM, we show that compared to state-of-the-art,\\nDash-enabled hash tables can achieve up to ~3.9X higher performance with up to\\nover 90% load factor and an instant recovery time of 57ms regardless of data\\nsize.', comment='To appear at VLDB 2020 (PVLDB Vol. 13 Issue 8)', journal_ref='PVLDB, 13(8): 1147-1161, 2020', doi='10.14778/3389133.3389134', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3389133.3389134', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2003.07302v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.07302v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.07316v2', updated=datetime.datetime(2020, 3, 19, 11, 30, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 16, 16, 38, 56, tzinfo=datetime.timezone.utc), title='Equivalent Rewritings on Path Views with Binding Patterns', authors=[arxiv.Result.Author('Julien Romero'), arxiv.Result.Author('Nicoleta Preda'), arxiv.Result.Author('Antoine Amarilli'), arxiv.Result.Author('Fabian Suchanek')], summary='A view with a binding pattern is a parameterized query on a database. Such\\nviews are used, e.g., to model Web services. To answer a query on such views,\\nthe views have to be orchestrated together in execution plans. We show how\\nqueries can be rewritten into equivalent execution plans, which are guaranteed\\nto deliver the same results as the query on all databases. We provide a correct\\nand complete algorithm to find these plans for path views and atomic queries.\\nFinally, we show that our method can be used to answer queries on real-world\\nWeb services.', comment=\"33 pages including 16 pages of main text. This is the full version of\\n  the ESWC'2020 article, which integrates all reviewer feedback, with the same\\n  text as the publisher version except minor changes. Several corrections\\n  relative to the first version\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.07316v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.07316v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.07432v2', updated=datetime.datetime(2020, 3, 22, 11, 6, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 16, 20, 32, 20, tzinfo=datetime.timezone.utc), title='Hihooi: A Database Replication Middleware for Scaling Transactional Databases Consistently', authors=[arxiv.Result.Author('Michael A. Georgiou'), arxiv.Result.Author('Aristodemos Paphitis'), arxiv.Result.Author('Michael Sirivianos'), arxiv.Result.Author('Herodotos Herodotou')], summary='With the advent of the Internet and Internet-connected devices, modern\\nbusiness applications can experience rapid increases as well as variability in\\ntransactional workloads. Database replication has been employed to scale\\nperformance and improve availability of relational databases but past\\napproaches have suffered from various issues including limited scalability,\\nperformance versus consistency tradeoffs, and requirements for database or\\napplication modifications. This paper presents Hihooi, a replication-based\\nmiddleware system that is able to achieve workload scalability, strong\\nconsistency guarantees, and elasticity for existing transactional databases at\\na low cost. A novel replication algorithm enables Hihooi to propagate database\\nmodifications asynchronously to all replicas at high speeds, while ensuring\\nthat all replicas are consistent. At the same time, a fine-grained routing\\nalgorithm is used to load balance incoming transactions to available replicas\\nin a consistent way. Our thorough experimental evaluation with several\\nwell-established benchmarks shows how Hihooi is able to achieve almost linear\\nworkload scalability for transactional databases.', comment='16 pages, 11 figures, 7 tables', journal_ref=None, doi='10.1109/TKDE.2020.2987560', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/TKDE.2020.2987560', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2003.07432v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.07432v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.08031v7', updated=datetime.datetime(2021, 2, 10, 6, 49, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 18, 3, 54, 51, tzinfo=datetime.timezone.utc), title='PolyFit: Polynomial-based Indexing Approach for Fast Approximate Range Aggregate Queries', authors=[arxiv.Result.Author('Zhe Li'), arxiv.Result.Author('Tsz Nam Chan'), arxiv.Result.Author('Man Lung Yiu'), arxiv.Result.Author('Christian S. Jensen')], summary='Range aggregate queries find frequent application in data analytics. In some\\nuse cases, approximate results are preferred over accurate results if they can\\nbe computed rapidly and satisfy approximation guarantees. Inspired by a recent\\nindexing approach, we provide means of representing a discrete point data set\\nby continuous functions that can then serve as compact index structures. More\\nspecifically, we develop a polynomial-based indexing approach, called PolyFit,\\nfor processing approximate range aggregate queries. PolyFit is capable of\\nsupporting multiple types of range aggregate queries, including COUNT, SUM, MIN\\nand MAX aggregates, with guaranteed absolute and relative error bounds.\\nExperiment results show that PolyFit is faster and more accurate and compact\\nthan existing learned index structures.', comment='13 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.08031v7', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.08031v7', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.08170v1', updated=datetime.datetime(2020, 3, 18, 11, 58, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 18, 11, 58, 1, tzinfo=datetime.timezone.utc), title='Discovering Business Area Effects to Process Mining Analysis Using Clustering and Influence Analysis', authors=[arxiv.Result.Author('Teemu Lehto'), arxiv.Result.Author('Markku Hinkka')], summary='A common challenge for improving business processes in large organizations is\\nthat business people in charge of the operations are lacking a fact-based\\nunderstanding of the execution details, process variants, and exceptions taking\\nplace in business operations. While existing process mining methodologies can\\ndiscover these details based on event logs, it is challenging to communicate\\nthe process mining findings to business people. In this paper, we present a\\nnovel methodology for discovering business areas that have a significant effect\\non the process execution details. Our method uses clustering to group similar\\ncases based on process flow characteristics and then influence analysis for\\ndetecting those business areas that correlate most with the discovered\\nclusters. Our analysis serves as a bridge between BPM people and business,\\npeople facilitating the knowledge sharing between these groups. We also present\\nan example analysis based on publicly available real-life purchase order\\nprocess data.', comment='12 pages. Paper accepted in 23rd International Conference on Business\\n  Information Systems (BIS 2020) to be published in a proceedings edition of\\n  the Lecture Notes in Business Information Processing', journal_ref='Abramowicz W., Klein G. (eds) Business Information Systems. BIS\\n  2020. Lecture Notes in Business Information Processing, vol 389. Springer,\\n  Cham', doi='10.1007/978-3-030-53337-3_18', primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-030-53337-3_18', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2003.08170v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.08170v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.09074v1', updated=datetime.datetime(2020, 3, 20, 2, 37, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 20, 2, 37, 40, tzinfo=datetime.timezone.utc), title='A Migratory Near Memory Processing Architecture Applied to Big Data Problems', authors=[arxiv.Result.Author('Ed T. Upchurch')], summary='Servers produced by mainstream vendors are inefficient in processing Big Data\\nqueries due to bottlenecks inherent in the fundamental architecture of these\\nsystems. Current server blades contain multicore processors connected to DRAM\\nmemory and disks by an interconnection chipset. The multicore processor chips\\nperform all the computations while the DRAM and disks store the data but have\\nno processing capability. To perform a database query, data must be moved back\\nand forth between DRAM and a small cache as well as between DRAM and disks. For\\nBig Data applications this data movement in onerous. Migratory Near Memory\\nServers address this bottleneck by placing large numbers of lightweight\\nprocessors directly into the memory system. These processors operate directly\\non the relations, vertices and edges of Big Data applications in place without\\nhaving to shuttle large quantities of data back and forth between DRAM, cache\\nand heavyweight multicore processors. This paper addresses the application of\\nsuch an architecture to relational database SELECT and JOIN queries.\\nPreliminary results indicate end-to-end orders of magnitude speedup.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.09074v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.09074v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.09541v2', updated=datetime.datetime(2020, 5, 13, 11, 4, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 21, 0, 55, 26, tzinfo=datetime.timezone.utc), title='A Synopses Data Engine for Interactive Extreme-Scale Analytics', authors=[arxiv.Result.Author('Antonis Kontaxakis'), arxiv.Result.Author('Nikos Giatrakos'), arxiv.Result.Author('Antonios Deligiannakis')], summary='In this work, we detail the design and structure of a Synopses Data Engine\\n(SDE) which combines the virtues of parallel processing and stream\\nsummarization towards delivering interactive analytics at extreme scale. Our\\nSDE is built on top of Apache Flink and implements a synopsis-as-a-service\\nparadigm. In that it achieves (a) concurrently maintaining thousands of\\nsynopses of various types for thousands of streams on demand, (b) reusing\\nmaintained synopses among various concurrent workflows, (c) providing data\\nsummarization facilities even for cross-(Big Data) platform workflows, (d)\\npluggability of new synopses on-the-fly, (e) increased potential for workflow\\nexecution optimization. The proposed SDE is useful for interactive analytics at\\nextreme scales because it enables (i) enhanced horizontal scalability, i.e.,\\nnot only scaling out the computation to a number of processing units available\\nin a computer cluster, but also harnessing the processing load assigned to each\\nby operating on carefully-crafted data summaries, (ii) vertical scalability,\\ni.e., scaling the computation to very high numbers of processed streams and\\n(iii) federated scalability i.e., scaling the computation beyond single\\nclusters and clouds by controlling the communication required to answer global\\nqueries posed over a number of potentially geo-dispersed clusters.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.09541v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.09541v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.09769v1', updated=datetime.datetime(2020, 3, 21, 23, 40, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 21, 23, 40, 44, tzinfo=datetime.timezone.utc), title='Translation of Array-Based Loops to Distributed Data-Parallel Programs', authors=[arxiv.Result.Author('Leonidas Fegaras'), arxiv.Result.Author('Md Hasanuzzaman Noor')], summary='Large volumes of data generated by scientific experiments and simulations\\ncome in the form of arrays, while programs that analyze these data are\\nfrequently expressed in terms of array operations in an imperative, loop-based\\nlanguage. But, as datasets grow larger, new frameworks in distributed Big Data\\nanalytics have become essential tools to large-scale scientific computing.\\nScientists, who are typically comfortable with numerical analysis tools but are\\nnot familiar with the intricacies of Big Data analytics, must now learn to\\nconvert their loop-based programs to distributed data-parallel programs. We\\npresent a novel framework for translating programs expressed as array-based\\nloops to distributed data parallel programs that is more general and efficient\\nthan related work. Although our translations are over sparse arrays, we extend\\nour framework to handle packed arrays, such as tiled matrices, without\\nsacrificing performance. We report on a prototype implementation on top of\\nSpark and evaluate the performance of our system relative to hand-written\\nprograms.', comment='This is the extended version of a paper that will appear at VLDB 2020\\n  (PVLDB Vol. 13)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.PL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.09769v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.09769v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.11105v1', updated=datetime.datetime(2020, 3, 19, 3, 32, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 19, 3, 32, 4, tzinfo=datetime.timezone.utc), title='EQL -- an extremely easy to learn knowledge graph query language, achieving highspeed and precise search', authors=[arxiv.Result.Author('Han Liu'), arxiv.Result.Author('Shantao Liu')], summary=\"EQL, also named as Extremely Simple Query Language, can be widely used in the\\nfield of knowledge graph, precise search, strong artificial intelligence,\\ndatabase, smart speaker ,patent search and other fields. EQL adopt the\\nprinciple of minimalism in design and pursues simplicity and easy to learn so\\nthat everyone can master it quickly. EQL language and lambda calculus are\\ninterconvertible, that reveals the mathematical nature of EQL language, and\\nlays a solid foundation for rigor and logical integrity of EQL language. The\\nEQL language and a comprehensive knowledge graph system with the world's\\ncommonsense can together form the foundation of strong AI in the future, and\\nmake up for the current lack of understanding of world's commonsense by current\\nAI system. EQL language can be used not only by humans, but also as a basic\\nlanguage for data query and data exchange between robots.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.11105v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.11105v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.11546v1', updated=datetime.datetime(2020, 3, 25, 15, 4, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 25, 15, 4, 5, tzinfo=datetime.timezone.utc), title='MultiRI: Fast Subgraph Matching in Labeled Multigraphs', authors=[arxiv.Result.Author('Giovanni Micale'), arxiv.Result.Author('Vincenzo Bonnici'), arxiv.Result.Author('Alfredo Ferro'), arxiv.Result.Author('Dennis Shasha'), arxiv.Result.Author('Rosalba Giugno'), arxiv.Result.Author('Alfredo Pulvirenti')], summary='The Subgraph Matching (SM) problem consists of finding all the embeddings of\\na given small graph, called the query, into a large graph, called the target.\\nThe SM problem has been widely studied for simple graphs, i.e. graphs where\\nthere is exactly one edge between two nodes and nodes have single labels, but\\nfew approaches have been devised for labeled multigraphs, i.e. graphs having\\npossibly multiple labels on nodes in which pair of nodes may have multiple\\nlabeled edges between them. Here we present MultiRI, a novel algorithm for the\\nSub-Multigraph Matching (SMM) problem, i.e. subgraph matching in labeled\\nmultigraphs. MultiRI improves on the state-of-the-art by computing\\ncompatibility domains and symmetry breaking conditions on query nodes to filter\\nthe search space of possible solutions. Empirically, we show that MultiRI\\noutperforms the state-of-the-art method for the SMM problem in both synthetic\\nand real graphs, with a multiplicative speedup between five and ten for large\\ngraphs, by using a limited amount of memory.', comment='Submitted for pubblication on January 2019', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'E.1; F.2.0'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.11546v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.11546v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.11547v2', updated=datetime.datetime(2020, 12, 14, 16, 37, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 25, 16, 19, 41, tzinfo=datetime.timezone.utc), title='A Survey on Trajectory Data Management, Analytics, and Learning', authors=[arxiv.Result.Author('Sheng Wang'), arxiv.Result.Author('Zhifeng Bao'), arxiv.Result.Author('J. Shane Culpepper'), arxiv.Result.Author('Gao Cong')], summary='Recent advances in sensor and mobile devices have enabled an unprecedented\\nincrease in the availability and collection of urban trajectory data, thus\\nincreasing the demand for more efficient ways to manage and analyze the data\\nbeing produced. In this survey, we comprehensively review recent research\\ntrends in trajectory data management, ranging from trajectory pre-processing,\\nstorage, common trajectory analytic tools, such as querying spatial-only and\\nspatial-textual trajectory data, and trajectory clustering. We also explore\\nfour closely related analytical tasks commonly used with trajectory data in\\ninteractive or real-time processing. Deep trajectory learning is also reviewed\\nfor the first time. Finally, we outline the essential qualities that a\\ntrajectory data management system should possess in order to maximize\\nflexibility.', comment='Accepted to the ACM Computing Surveys', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.11547v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.11547v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.12396v1', updated=datetime.datetime(2020, 3, 27, 13, 5, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 27, 13, 5, 11, tzinfo=datetime.timezone.utc), title='Time Series Data Cleaning: From Anomaly Detection to Anomaly Repairing (Technical Report)', authors=[arxiv.Result.Author('Aoqian Zhang'), arxiv.Result.Author('Shaoxu Song'), arxiv.Result.Author('Jianmin Wang'), arxiv.Result.Author('Philip S. Yu')], summary='Errors are prevalent in time series data, such as GPS trajectories or sensor\\nreadings. Existing methods focus more on anomaly detection but not on repairing\\nthe detected anomalies. By simply filtering out the dirty data via anomaly\\ndetection, applications could still be unreliable over the incomplete time\\nseries. Instead of simply discarding anomalies, we propose to (iteratively)\\nrepair them in time series data, by creatively bonding the beauty of temporal\\nnature in anomaly detection with the widely considered minimum change principle\\nin data repairing. Our major contributions include: (1) a novel framework of\\niterative minimum repairing (IMR) over time series data, (2) explicit analysis\\non convergence of the proposed iterative minimum repairing, and (3) efficient\\nestimation of parameters in each iteration. Remarkably, with incremental\\ncomputation, we reduce the complexity of parameter estimation from O(n) to\\nO(1). Experiments on real datasets demonstrate the superiority of our proposal\\ncompared to the state-of-the-art approaches. In particular, we show that (the\\nproposed) repairing indeed improves the time series classification application.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.12396v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.12396v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2003.13831v1', updated=datetime.datetime(2020, 3, 30, 21, 36, 49, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 30, 21, 36, 49, tzinfo=datetime.timezone.utc), title='Consistency and Certain Answers in Relational to RDF Data Exchange with Shape Constraints', authors=[arxiv.Result.Author('Iovka Boneva'), arxiv.Result.Author('Jose Lozano'), arxiv.Result.Author('Sławek Staworko')], summary='We investigate the data exchange from relational databases to RDF graphs\\ninspired by R2RML with the addition of target shape schemas. We study the\\nproblems of consistency i.e., checking that every source instance admits a\\nsolution, and certain query answering i.e., finding answers present in every\\nsolution. We identify the class of constructive relational to RDF data exchange\\nthat uses IRI constructors and full tgds (with no existential variables) in its\\nsource to target dependencies. We show that the consistency problem is\\ncoNP-complete. We introduce the notion of universal simulation solution that\\nallows to compute certain query answers to any class of queries that is robust\\nunder simulation. One such class are nested regular expressions (NREs) that are\\nforward i.e., do not use the inverse operation. Using universal simulation\\nsolution renders tractable the computation of certain answers to forward NREs\\n(data-complexity). Finally, we present a number of results that show that\\nrelaxing the restrictions of the proposed framework leads to an increase in\\ncomplexity.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.13831v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.13831v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.00190v2', updated=datetime.datetime(2020, 4, 17, 16, 52, 48, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 1, 1, 52, 11, tzinfo=datetime.timezone.utc), title='Technical Report: Developing a Working Data Hub', authors=[arxiv.Result.Author('Vijay Gadepally'), arxiv.Result.Author('Jeremy Kepner')], summary='Data forms a key component of any enterprise. The need for high quality and\\neasy access to data is further amplified by organizations wishing to leverage\\nmachine learning or artificial intelligence for their operations. To this end,\\nmany organizations are building resources for managing heterogenous data,\\nproviding end-users with an organization wide view of available data, and\\nacting as a centralized repository for data owned/collected by an organization.\\nVery broadly, we refer to these class of techniques as a \"data hub.\" While\\nthere is no clear definition of what constitutes a data hub, some of the key\\ncharacteristics include: data catalog; links to data sets or owners of data\\nsets or centralized data repository; basic ability to serve / visualize data\\nsets; access control policies that ensure secure data access and respects\\npolicies of data owners; and computing capabilities tied with data hub\\ninfrastructure. Of course, developing such a data hub entails numerous\\nchallenges. This document provides background in databases, data management and\\noutlines best practices and recommendations for developing and deploying a\\nworking data hub.', comment='Fixes typographical errors; references updated; minor content updates\\n  in Section 4. arXiv admin note: substantial text overlap with\\n  arXiv:1905.03592', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.00190v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.00190v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.00253v1', updated=datetime.datetime(2020, 4, 1, 6, 45, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 1, 6, 45, 50, tzinfo=datetime.timezone.utc), title='Leveraging Data Preparation, HBase NoSQL Storage, and HiveQL Querying for COVID-19 Big Data Analytics Projects', authors=[arxiv.Result.Author('Karim Baïna')], summary='Epidemiologist, Scientists, Statisticians, Historians, Data engineers and\\nData scientists are working on finding descriptive models and theories to\\nexplain COVID-19 expansion phenomena or on building analytics predictive models\\nfor learning the apex of COVID-19 confimed cases, recovered cases, and deaths\\nevolution curves. In CRISP-DM life cycle, 75% of time is consumed only by data\\npreparation phase causing lot of pressions and stress on scientists and data\\nscientists building machine learning models. This paper aims to help reducing\\ndata preparation efforts by presenting detailed schemas design and data\\npreparation technical scripts for formatting and storing Johns Hopkins\\nUniversity COVID-19 daily data in HBase NoSQL data store, and enabling HiveQL\\nCOVID-19 data querying in a relational Hive SQL-like style.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2.8'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.00253v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.00253v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.01124v1', updated=datetime.datetime(2020, 4, 2, 16, 53, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 2, 16, 53, 4, tzinfo=datetime.timezone.utc), title='Nass: A New Approach to Graph Similarity Search', authors=[arxiv.Result.Author('Jongik Kim')], summary='In this paper, we study the problem of graph similarity search with graph\\nedit distance (GED) constraints. Due to the NP-hardness of GED computation,\\nexisting solutions to this problem adopt the filtering-and-verification\\nframework with a main focus on the filtering phase to generate a small number\\nof candidate graphs. However, they have a limitation that the number of\\ncandidates grows extremely rapidly as a GED threshold increases. To address the\\nlimitation, we propose a new approach that utilizes GED computation results in\\ngenerating candidate graphs. The main idea is that whenever we identify a\\nresult graph of the query, we immediately regenerate candidate graphs using a\\nsubset of pre-computed graphs similar to the identified result graph. To speed\\nup GED computation, we also develop a novel GED computation algorithm. The\\nproposed algorithm reduces the search space for GED computation by utilizing a\\nseries of filtering techniques, which have been used to generate candidates in\\nexisting solutions. Experimental results on real datasets demonstrate the\\nproposed approach significantly outperforms the state-of-the art techniques.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.01124v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.01124v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.02012v1', updated=datetime.datetime(2020, 4, 4, 20, 35, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 4, 20, 35, 30, tzinfo=datetime.timezone.utc), title='Regular Path Query Evaluation on Streaming Graphs', authors=[arxiv.Result.Author('Anil Pacaci'), arxiv.Result.Author('Angela Bonifati'), arxiv.Result.Author('M. Tamer Özsu')], summary='We study persistent query evaluation over streaming graphs, which is becoming\\nincreasingly important. We focus on navigational queries that determine if\\nthere exists a path between two entities that satisfies a user-specified\\nconstraint. We adopt the Regular Path Query (RPQ) model that specifies\\nnavigational patterns with labeled constraints. We propose deterministic\\nalgorithms to efficiently evaluate persistent RPQs under both arbitrary and\\nsimple path semantics in a uniform manner. Experimental analysis on real and\\nsynthetic streaming graphs shows that the proposed algorithms can process up to\\ntens of thousands of edges per second and efficiently answer RPQs that are\\ncommonly used in real-world workloads.', comment='A shorter version of this paper has been accepted for publication in\\n  2020 International Conference on Management of Data (SIGMOD 2020)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.02012v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.02012v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.02308v1', updated=datetime.datetime(2020, 4, 5, 20, 21, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 5, 20, 21, 13, tzinfo=datetime.timezone.utc), title='Learning Over Dirty Data Without Cleaning', authors=[arxiv.Result.Author('Jose Picado'), arxiv.Result.Author('John Davis'), arxiv.Result.Author('Arash Termehchy'), arxiv.Result.Author('Ga Young Lee')], summary='Real-world datasets are dirty and contain many errors. Examples of these\\nissues are violations of integrity constraints, duplicates, and inconsistencies\\nin representing data values and entities. Learning over dirty databases may\\nresult in inaccurate models. Users have to spend a great deal of time and\\neffort to repair data errors and create a clean database for learning.\\nMoreover, as the information required to repair these errors is not often\\navailable, there may be numerous possible clean versions for a dirty database.\\nWe propose DLearn, a novel relational learning system that learns directly over\\ndirty databases effectively and efficiently without any preprocessing. DLearn\\nleverages database constraints to learn accurate relational models over\\ninconsistent and heterogeneous data. Its learned models represent patterns over\\nall possible clean instances of the data in a usable form. Our empirical study\\nindicates that DLearn learns accurate models over large real-world databases\\nefficiently.', comment=\"To be published in Proceedings of the 2020 ACM SIGMOD International\\n  Conference on Management of Data (SIGMOD'20)\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.02308v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.02308v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.02564v1', updated=datetime.datetime(2020, 4, 6, 11, 22, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 6, 11, 22, 43, tzinfo=datetime.timezone.utc), title='DySky: Dynamic Skyline Queries on Uncertain Graphs', authors=[arxiv.Result.Author('Suman Banerjee'), arxiv.Result.Author('Bithika Pal')], summary='Given a graph, and a set of query vertices (subset of the vertices), the\\ndynamic skyline query problem returns a subset of data vertices (other than\\nquery vertices) which are not dominated by other data vertices based on certain\\ndistance measure. In this paper, we study the dynamic skyline query problem on\\nuncertain graphs (DySky). The input to this problem is an uncertain graph, a\\nsubset of its nodes as query vertices, and the goal here is to return all the\\ndata vertices which are not dominated by others. We employ two distance\\nmeasures in uncertain graphs, namely, \\\\emph{Majority Distance}, and\\n\\\\emph{Expected Distance}. Our approach is broadly divided into three steps:\\n\\\\emph{Pruning}, \\\\emph{Distance Computation}, and \\\\emph{Skyline Vertex Set\\nGeneration}. We implement the proposed methodology with three publicly\\navailable datasets and observe that it can find out skyline vertex set without\\ntaking much time even for million sized graphs if expected distance is\\nconcerned. Particularly, the pruning strategy reduces the computational time\\nsignificantly.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.02564v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.02564v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.02570v1', updated=datetime.datetime(2020, 4, 6, 11, 34, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 6, 11, 34, 26, tzinfo=datetime.timezone.utc), title='Dynamic Ridesharing in Peak Travel Periods', authors=[arxiv.Result.Author('Hui Luo'), arxiv.Result.Author('Zhifeng Bao'), arxiv.Result.Author('Farhana M. Choudhury'), arxiv.Result.Author('J. Shane Culpepper')], summary=\"In this paper, we study a variant of the dynamic ridesharing problem with a\\nspecific focus on peak hours: Given a set of drivers and rider requests, we aim\\nto match drivers to each rider request by achieving two objectives: maximizing\\nthe served rate and minimizing the total additional distance, subject to a\\nseries of spatio-temporal constraints. Our problem can be distinguished from\\nexisting work in three aspects: (1) Previous work did not fully explore the\\nimpact of peak travel periods where the number of rider requests is much\\ngreater than the number of available drivers. (2) Existing solutions usually\\nrely on single objective optimization techniques, such as minimizing the total\\ntravel cost. (3) When evaluating the overall system performance, the runtime\\nspent on updating drivers' trip schedules as per incoming rider requests should\\nbe incorporated, while it is excluded by most existing solutions. We propose an\\nindex structure together with a set of pruning rules and an efficient algorithm\\nto include new riders into drivers' existing trip schedule. To answer new rider\\nrequests effectively, we propose two algorithms that match drivers with rider\\nrequests. Finally, we perform extensive experiments on a large-scale test\\ncollection to validate the proposed methods.\", comment=None, journal_ref=None, doi='10.1109/TKDE.2019.2961341', primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/TKDE.2019.2961341', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2004.02570v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.02570v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.03352v3', updated=datetime.datetime(2020, 8, 3, 1, 25, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 7, 13, 27, 2, tzinfo=datetime.timezone.utc), title='GeoFlink: A Distributed and Scalable Framework for the Real-time Processing of Spatial Streams', authors=[arxiv.Result.Author('Salman Ahmed Shaikh'), arxiv.Result.Author('Komal Mariam'), arxiv.Result.Author('Hiroyuki Kitagawa'), arxiv.Result.Author('Kyoung-Sook Kim')], summary='Apache Flink is an open-source system for scalable processing of batch and\\nstreaming data. Flink does not natively support efficient processing of spatial\\ndata streams, which is a requirement of many applications dealing with spatial\\ndata. Besides Flink, other scalable spatial data processing platforms including\\nGeoSpark, Spatial Hadoop, etc. do not support streaming workloads and can only\\nhandle static/batch workloads. To fill this gap, we present GeoFlink, which\\nextends Apache Flink to support spatial data types, indexes and continuous\\nqueries over spatial data streams. To enable the efficient processing of\\nspatial continuous queries and for the effective data distribution across Flink\\ncluster nodes, a gird-based index is introduced. GeoFlink currently supports\\nspatial range, spatial $k$NN and spatial join queries on point data type. An\\nextensive experimental study on real spatial data streams shows that GeoFlink\\nachieves significantly higher query throughput than ordinary Flink processing.', comment='CIKM 2020 Preprint', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.03352v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.03352v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.03477v1', updated=datetime.datetime(2020, 4, 7, 15, 26, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 7, 15, 26, 50, tzinfo=datetime.timezone.utc), title='An Algorithm for Context-Free Path Queries over Graph Databases', authors=[arxiv.Result.Author('Ciro M. Medeiros'), arxiv.Result.Author('Martin A. Musicante'), arxiv.Result.Author('Umberto S. Costa')], summary='RDF (Resource Description Framework) is a standard language to represent\\ngraph databases. Query languages for RDF databases usually include primitives\\nto support path queries, linking pairs of vertices of the graph that are\\nconnected by a path of labels belonging to a given language. Languages such as\\nSPARQL include support for paths defined by regular languages (by means of\\nRegular Expressions). A context-free path query is a path query whose language\\ncan be defined by a context-free grammar. Context-free path queries can be used\\nto implement queries such as the \"same generation queries\", that are not\\nexpressible by Regular Expressions. In this paper, we present a novel algorithm\\nfor context-free path query processing. We prove the correctness of our\\napproach and show its run-time and memory complexity. We show the viability of\\nour approach by means of a prototype implemented in Go. We run our prototype\\nusing the same cases of study as proposed in recent works, comparing our\\nresults with another, recently published algorithm. The experiments include\\nboth synthetic and real RDF databases. Our algorithm can be seen as a step\\nforward, towards the implementation of more expressive query languages.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS', 'cs.FL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.03477v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.03477v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.03710v1', updated=datetime.datetime(2020, 4, 7, 21, 5, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 7, 21, 5, 22, tzinfo=datetime.timezone.utc), title='DataFed: Towards Reproducible Research via Federated Data Management', authors=[arxiv.Result.Author('Dale Stansberry'), arxiv.Result.Author('Suhas Somnath'), arxiv.Result.Author('Jessica Breet'), arxiv.Result.Author('Gregory Shutt'), arxiv.Result.Author('Mallikarjun Shankar')], summary='The increasingly collaborative, globalized nature of scientific research\\ncombined with the need to share data and the explosion in data volumes present\\nan urgent need for a scientific data management system (SDMS). An SDMS presents\\na logical and holistic view of data that greatly simplifies and empowers data\\norganization, curation, searching, sharing, dissemination, etc. We present\\nDataFed -- a lightweight, distributed SDMS that spans a federation of storage\\nsystems within a loosely-coupled network of scientific facilities. Unlike\\nexisting SDMS offerings, DataFed uses high-performance and scalable user\\nmanagement and data transfer technologies that simplify deployment,\\nmaintenance, and expansion of DataFed. DataFed provides web-based and\\ncommand-line interfaces to manage data and integrate with complex scientific\\nworkflows. DataFed represents a step towards reproducible scientific research\\nby enabling reliable staging of the correct data at the desired environment.', comment='Part of conference proceedings at the 6th Annual Conference on\\n  Computational Science & Computational Intelligence held at Las Vegas, NV, USA\\n  on Dec 05-07 2019', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.03710v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.03710v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.03716v1', updated=datetime.datetime(2020, 4, 7, 21, 9, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 7, 21, 9, 59, tzinfo=datetime.timezone.utc), title='Maintaining Triangle Queries under Updates', authors=[arxiv.Result.Author('Ahmet Kara'), arxiv.Result.Author('Milos Nikolic'), arxiv.Result.Author('Hung Q. Ngo'), arxiv.Result.Author('Dan Olteanu'), arxiv.Result.Author('Haozhe Zhang')], summary='We consider the problem of incrementally maintaining the triangle queries\\nwith arbitrary free variables under single-tuple updates to the input\\nrelations. We introduce an approach called IVM$^\\\\epsilon$ that exhibits a\\ntrade-off between the update time, the space, and the delay for the enumeration\\nof the query result, such that the update time ranges from the square root to\\nlinear in the database size while the delay ranges from constant to linear\\ntime. IVM$^\\\\epsilon$ achieves Pareto worst-case optimality in the update-delay\\nspace conditioned on the Online Matrix-Vector Multiplication conjecture. It is\\nstrongly Pareto optimal for the triangle queries with zero or three free\\nvariables and weakly Pareto optimal for the triangle queries with one or two\\nfree variables.', comment='47 pages, 18 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2.4'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.03716v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.03716v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.03814v1', updated=datetime.datetime(2020, 4, 8, 5, 15, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 8, 5, 15, 47, tzinfo=datetime.timezone.utc), title='Bao: Learning to Steer Query Optimizers', authors=[arxiv.Result.Author('Ryan Marcus'), arxiv.Result.Author('Parimarjan Negi'), arxiv.Result.Author('Hongzi Mao'), arxiv.Result.Author('Nesime Tatbul'), arxiv.Result.Author('Mohammad Alizadeh'), arxiv.Result.Author('Tim Kraska')], summary='Query optimization remains one of the most challenging problems in data\\nmanagement systems. Recent efforts to apply machine learning techniques to\\nquery optimization challenges have been promising, but have shown few practical\\ngains due to substantive training overhead, inability to adapt to changes, and\\npoor tail performance. Motivated by these difficulties and drawing upon a long\\nhistory of research in multi-armed bandits, we introduce Bao (the BAndit\\nOptimizer). Bao takes advantage of the wisdom built into existing query\\noptimizers by providing per-query optimization hints. Bao combines modern tree\\nconvolutional neural networks with Thompson sampling, a decades-old and\\nwell-studied reinforcement learning algorithm. As a result, Bao automatically\\nlearns from its mistakes and adapts to changes in query workloads, data, and\\nschema. Experimentally, we demonstrate that Bao can quickly (an order of\\nmagnitude faster than previous approaches) learn strategies that improve\\nend-to-end query execution performance, including tail latency. In cloud\\nenvironments, we show that Bao can offer both reduced costs and better\\nperformance compared with a sophisticated commercial system.', comment=None, journal_ref=None, doi='10.1145/3448016.3452838', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3448016.3452838', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2004.03814v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.03814v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.04139v1', updated=datetime.datetime(2020, 4, 8, 17, 50, 18, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 8, 17, 50, 18, tzinfo=datetime.timezone.utc), title='Fast and Reliable Missing Data Contingency Analysis with Predicate-Constraints', authors=[arxiv.Result.Author('Xi Liang'), arxiv.Result.Author('Zechao Shang'), arxiv.Result.Author('Aaron J. Elmore'), arxiv.Result.Author('Sanjay Krishnan'), arxiv.Result.Author('Michael J. Franklin')], summary='Today, data analysts largely rely on intuition to determine whether missing\\nor withheld rows of a dataset significantly affect their analyses. We propose a\\nframework that can produce automatic contingency analysis, i.e., the range of\\nvalues an aggregate SQL query could take, under formal constraints describing\\nthe variation and frequency of missing data tuples. We describe how to process\\nSUM, COUNT, AVG, MIN, and MAX queries in these conditions resulting in hard\\nerror bounds with testable constraints. We propose an optimization algorithm\\nbased on an integer program that reconciles a set of such constraints, even if\\nthey are overlapping, conflicting, or unsatisfiable, into such bounds. Our\\nexperiments on real-world datasets against several statistical imputation and\\ninference baselines show that statistical techniques can have a deceptively\\nhigh error rate that is often unpredictable. In contrast, our framework offers\\nhard bounds that are guaranteed to hold if the constraints are not violated. In\\nspite of these hard bounds, we show competitive accuracy to statistical\\nbaselines.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.04139v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.04139v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.04286v1', updated=datetime.datetime(2020, 4, 8, 22, 37, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 8, 22, 37, 39, tzinfo=datetime.timezone.utc), title='The Effects of Different JSON Representations on Querying Knowledge Graphs', authors=[arxiv.Result.Author('Masoud Salehpour'), arxiv.Result.Author('Joseph G. Davis')], summary='Knowledge Graphs (KGs) have emerged as the de-facto standard for modeling and\\nquerying datasets with a graph-like structure in the Semantic Web domain. Our\\nfocus is on the performance challenges associated with querying KGs. We\\ndeveloped three informationally equivalent JSON-based representations for KGs,\\nnamely, Subject-based Name/Value (JSON-SNV), Documents of Triples (JSON-DT),\\nand Chain-based Name/Value (JSON-CNV). We analyzed the effects of these\\nrepresentations on query performance by storing them on two prominent\\ndocument-based Data Management Systems (DMSs), namely, MongoDB and Couchbase\\nand executing a set of benchmark queries over them. We also compared the\\nexecution times with row-store Virtuoso, column-store Virtuoso, and\\n\\\\mbox{Blazegraph} as three major DMSs with different architectures (aka,\\nRDF-stores). Our results indicate that the representation type has a\\nsignificant performance impact on query execution. For instance, the JSON-SNV\\noutperforms others by nearly one order of magnitude to execute subject-subject\\njoin queries. This and the other results presented in this paper can assist in\\nmore accurate benchmarking of the emerging DMSs.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.04286v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.04286v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.05065v2', updated=datetime.datetime(2020, 4, 13, 1, 15, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 10, 15, 0, 29, tzinfo=datetime.timezone.utc), title='On Multiple Semantics for Declarative Database Repairs', authors=[arxiv.Result.Author('Amir Gilad'), arxiv.Result.Author('Daniel Deutch'), arxiv.Result.Author('Sudeepa Roy')], summary='We study the problem of database repairs through a rule-based framework that\\nwe refer to as Delta Rules. Delta Rules are highly expressive and allow\\nspecifying complex, cross-relations repair logic associated with Denial\\nConstraints, Causal Rules, and allowing to capture Database Triggers of\\ninterest. We show that there are no one-size-fits-all semantics for repairs in\\nthis inclusive setting, and we consequently introduce multiple alternative\\nsemantics, presenting the case for using each of them. We then study the\\nrelationships between the semantics in terms of their output and the complexity\\nof computation. Our results formally establish the tradeoff between the\\npermissiveness of the semantics and its computational complexity. We\\ndemonstrate the usefulness of the framework in capturing multiple data repair\\nscenarios for an Academic Search database and the TPC-H databases, showing how\\nusing different semantics affects the repair in terms of size and runtime, and\\nexamining the relationships between the repairs. We also compare our approach\\nwith SQL triggers and a state-of-the-art data repair system.', comment=None, journal_ref='SIGMOD 2020', doi='10.1145/3318464.3389721', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3318464.3389721', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2004.05065v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.05065v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.05076v2', updated=datetime.datetime(2020, 4, 26, 17, 46, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 10, 15, 34, 15, tzinfo=datetime.timezone.utc), title='Cheetah: Accelerating Database Queries with Switch Pruning', authors=[arxiv.Result.Author('Muhammad Tirmazi'), arxiv.Result.Author('Ran Ben Basat'), arxiv.Result.Author('Jiaqi Gao'), arxiv.Result.Author('Minlan Yu')], summary='Modern database systems are growing increasingly distributed and struggle to\\nreduce query completion time with a large volume of data. In this paper, we\\nleverage programmable switches in the network to partially offload query\\ncomputation to the switch. While switches provide high performance, they have\\nresource and programming constraints that make implementing diverse queries\\ndifficult. To fit in these constraints, we introduce the concept of data\\n\\\\emph{pruning} -- filtering out entries that are guaranteed not to affect\\noutput. The database system then runs the same query but on the pruned data,\\nwhich significantly reduces processing time. We propose pruning algorithms for\\na variety of queries. We implement our system, Cheetah, on a Barefoot Tofino\\nswitch and Spark. Our evaluation on multiple workloads shows $40 - 200\\\\%$\\nimprovement in the query completion time compared to Spark.', comment='To appear in ACM SIGMOD 2020', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.NI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.05076v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.05076v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.05297v2', updated=datetime.datetime(2021, 3, 4, 16, 55, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 11, 3, 47, 27, tzinfo=datetime.timezone.utc), title='Graphsurge: Graph Analytics on View Collections Using Differential Computation', authors=[arxiv.Result.Author('Siddhartha Sahu'), arxiv.Result.Author('Semih Salihoglu')], summary='This paper presents the design and implementation of a new open-source\\nview-based graph analytics system called Graphsurge. Graphsurge is designed to\\nsupport applications that analyze multiple snapshots or views of a large-scale\\ngraph. Users program Graphsurge through a declarative graph view definition\\nlanguage (GVDL) to create views over input graphs and a Differential\\nDataflow-based programming API to write analytics computations. A key feature\\nof GVDL is the ability to organize views into view collections, which allows\\nGraphsurge to automatically share computation across views, without users\\nwriting any incrementalization code, by performing computations differentially.\\nWe then introduce two optimization problems that naturally arise in our\\nsetting. First is the collection ordering problem to determine the order of\\nviews that leads to minimum differences across consecutive views. We prove this\\nproblem is NP-hard and show a constant-factor approximation algorithm drawn\\nfrom literature. Second is the collection splitting problem to decide on which\\nviews to run computations differentially vs from scratch, for which we present\\nan adaptive solution that makes decisions at runtime. We present extensive\\nexperiments to demonstrate the benefits of running computations differentially\\nfor view collections and our collection ordering and splitting optimizations.', comment=None, journal_ref=None, doi='10.1145/3448016.3452837', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3448016.3452837', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2004.05297v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.05297v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.05345v1', updated=datetime.datetime(2020, 4, 11, 9, 24, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 11, 9, 24, 51, tzinfo=datetime.timezone.utc), title='Locality-Sensitive Hashing Scheme based on Longest Circular Co-Substring', authors=[arxiv.Result.Author('Yifan Lei'), arxiv.Result.Author('Qiang Huang'), arxiv.Result.Author('Mohan Kankanhalli'), arxiv.Result.Author('Anthony K. H. Tung')], summary='Locality-Sensitive Hashing (LSH) is one of the most popular methods for\\n$c$-Approximate Nearest Neighbor Search ($c$-ANNS) in high-dimensional spaces.\\nIn this paper, we propose a novel LSH scheme based on the Longest Circular\\nCo-Substring (LCCS) search framework (LCCS-LSH) with a theoretical guarantee.\\nWe introduce a novel concept of LCCS and a new data structure named Circular\\nShift Array (CSA) for $k$-LCCS search. The insight of LCCS search framework is\\nthat close data objects will have a longer LCCS than the far-apart ones with\\nhigh probability. LCCS-LSH is \\\\emph{LSH-family-independent}, and it supports\\n$c$-ANNS with different kinds of distance metrics. We also introduce a\\nmulti-probe version of LCCS-LSH and conduct extensive experiments over five\\nreal-life datasets. The experimental results demonstrate that LCCS-LSH\\noutperforms state-of-the-art LSH schemes.', comment='16 pages, 10 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.05345v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.05345v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.05517v1', updated=datetime.datetime(2020, 4, 12, 0, 59, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 12, 0, 59, 9, tzinfo=datetime.timezone.utc), title='A Relational Matrix Algebra and its Implementation in a Column Store', authors=[arxiv.Result.Author('Oksana Dolmatova'), arxiv.Result.Author('Nikolaus Augsten'), arxiv.Result.Author('Michael H. Boehlen')], summary='Analytical queries often require a mixture of relational and linear algebra\\noperations applied to the same data. This poses a challenge to analytic systems\\nthat must bridge the gap between relations and matrices. Previous work has\\nmainly strived to fix the problem at the implementation level. This paper\\nproposes a principled solution at the logical level. We introduce the\\nrelational matrix algebra (RMA), which seamlessly integrates linear algebra\\noperations into the relational model and eliminates the dichotomy between\\nmatrices and relations. RMA is closed: All our relational matrix operations are\\nperformed on relations and result in relations; no additional data structure is\\nrequired. Our implementation in MonetDB shows the feasibility of our approach,\\nand empirical evaluations suggest that in-database analytics performs well for\\nmixed workloads.', comment='16 pages, 18 figures', journal_ref=None, doi='10.1145/3318464.3389747', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3318464.3389747', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2004.05517v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.05517v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.05648v1', updated=datetime.datetime(2020, 4, 8, 23, 4, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 8, 23, 4, 16, tzinfo=datetime.timezone.utc), title='A Comparative Analysis of Knowledge Graph Query Performance', authors=[arxiv.Result.Author('Masoud Salehpour'), arxiv.Result.Author('Joseph G. Davis')], summary='As Knowledge Graphs (KGs) continue to gain widespread momentum for use in\\ndifferent domains, storing the relevant KG content and efficiently executing\\nqueries over them are becoming increasingly important. A range of Data\\nManagement Systems (DMSs) have been employed to process KGs. This paper aims to\\nprovide an in-depth analysis of query performance across diverse DMSs and KG\\nquery types. Our aim is to provide a fine-grained, comparative analysis of four\\nmajor DMS types, namely, row-, column-, graph-, and document-stores, against\\nmajor query types, namely, subject-subject, subject-object, tree-like, and\\noptional joins. In particular, we analyzed the performance of row-store\\nVirtuoso, column-store Virtuoso, Blazegraph (i.e., graph-store), and MongoDB\\n(i.e., document-store) using five well-known benchmarks, namely, BSBM, WatDiv,\\nFishMark, BowlognaBench, and BioBench-Allie. Our results show that no single\\nDMS displays superior query performance across the four query types. In\\nparticular, row- and column-store Virtuoso are a factor of 3-8 faster for\\ntree-like joins, Blazegraph performs around one order of magnitude faster for\\nsubject-object joins, and MongoDB performs over one order of magnitude faster\\nfor high-selective queries.', comment='arXiv admin note: substantial text overlap with arXiv:2004.04286', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.05648v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.05648v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.05712v1', updated=datetime.datetime(2020, 4, 12, 22, 58, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 12, 22, 58, 46, tzinfo=datetime.timezone.utc), title='A1: A Distributed In-Memory Graph Database', authors=[arxiv.Result.Author('Chiranjeeb Buragohain'), arxiv.Result.Author('Knut Magne Risvik'), arxiv.Result.Author('Paul Brett'), arxiv.Result.Author('Miguel Castro'), arxiv.Result.Author('Wonhee Cho'), arxiv.Result.Author('Joshua Cowhig'), arxiv.Result.Author('Nikolas Gloy'), arxiv.Result.Author('Karthik Kalyanaraman'), arxiv.Result.Author('Richendra Khanna'), arxiv.Result.Author('John Pao'), arxiv.Result.Author('Matthew Renzelmann'), arxiv.Result.Author('Alex Shamis'), arxiv.Result.Author('Timothy Tan'), arxiv.Result.Author('Shuheng Zheng')], summary='A1 is an in-memory distributed database used by the Bing search engine to\\nsupport complex queries over structured data. The key enablers for A1 are\\navailability of cheap DRAM and high speed RDMA (Remote Direct Memory Access)\\nnetworking in commodity hardware. A1 uses FaRM as its underlying storage layer\\nand builds the graph abstraction and query engine on top. The combination of\\nin-memory storage and RDMA access requires rethinking how data is allocated,\\norganized and queried in a large distributed system. A single A1 cluster can\\nstore tens of billions of vertices and edges and support a throughput of 350+\\nmillion of vertex reads per second with end to end query latency in single\\ndigit milliseconds. In this paper we describe the A1 data model, RDMA optimized\\ndata structures and query execution.', comment=None, journal_ref=None, doi='10.1145/3318464.3386135', primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3318464.3386135', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2004.05712v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.05712v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.05722v1', updated=datetime.datetime(2020, 4, 12, 23, 56, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 12, 23, 56, 6, tzinfo=datetime.timezone.utc), title='Complaint-driven Training Data Debugging for Query 2.0', authors=[arxiv.Result.Author('Weiyuan Wu'), arxiv.Result.Author('Lampros Flokas'), arxiv.Result.Author('Eugene Wu'), arxiv.Result.Author('Jiannan Wang')], summary='As the need for machine learning (ML) increases rapidly across all industry\\nsectors, there is a significant interest among commercial database providers to\\nsupport \"Query 2.0\", which integrates model inference into SQL queries.\\nDebugging Query 2.0 is very challenging since an unexpected query result may be\\ncaused by the bugs in training data (e.g., wrong labels, corrupted features).\\nIn response, we propose Rain, a complaint-driven training data debugging\\nsystem. Rain allows users to specify complaints over the query\\'s intermediate\\nor final output, and aims to return a minimum set of training examples so that\\nif they were removed, the complaints would be resolved. To the best of our\\nknowledge, we are the first to study this problem. A naive solution requires\\nretraining an exponential number of ML models. We propose two novel heuristic\\napproaches based on influence functions which both require linear retraining\\nsteps. We provide an in-depth analytical and empirical analysis of the two\\napproaches and conduct extensive experiments to evaluate their effectiveness\\nusing four real-world datasets. Results show that Rain achieves the highest\\nrecall@k among all the baselines while still returns results interactively.', comment='Proceedings of the 2020 ACM SIGMOD International Conference on\\n  Management of Data', journal_ref=None, doi='10.1145/3318464.3389696', primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3318464.3389696', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2004.05722v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.05722v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.05951v1', updated=datetime.datetime(2020, 4, 13, 14, 7, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 13, 14, 7, 28, tzinfo=datetime.timezone.utc), title='SLIM: Scalable Linkage of Mobility Data', authors=[arxiv.Result.Author('Fuat Basık'), arxiv.Result.Author('Hakan Ferhatosmanoğlu'), arxiv.Result.Author('Buğra Gedik')], summary='We present a scalable solution to link entities across mobility datasets\\nusing their spatio-temporal information. This is a fundamental problem in many\\napplications such as linking user identities for security, understanding\\nprivacy limitations of location based services, or producing a unified dataset\\nfrom multiple sources for urban planning. Such integrated datasets are also\\nessential for service providers to optimise their services and improve business\\nintelligence. In this paper, we first propose a mobility based representation\\nand similarity computation for entities. An efficient matching process is then\\ndeveloped to identify the final linked pairs, with an automated mechanism to\\ndecide when to stop the linkage. We scale the process with a locality-sensitive\\nhashing (LSH) based approach that significantly reduces candidate pairs for\\nmatching. To realize the effectiveness and efficiency of our techniques in\\npractice, we introduce an algorithm called SLIM. In the experimental\\nevaluation, SLIM outperforms the two existing state-of-the-art approaches in\\nterms of precision and recall. Moreover, the LSH-based approach brings two to\\nfour orders of magnitude speedup.', comment='To Appear in Sigmod 2020', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.05951v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.05951v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.06101v1', updated=datetime.datetime(2020, 4, 13, 17, 59, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 13, 17, 59, 27, tzinfo=datetime.timezone.utc), title='Near-Optimal Distributed Band-Joins through Recursive Partitioning', authors=[arxiv.Result.Author('Rundong Li'), arxiv.Result.Author('Wolfgang Gatterbauer'), arxiv.Result.Author('Mirek Riedewald')], summary='We consider running-time optimization for band-joins in a distributed system,\\ne.g., the cloud. To balance load across worker machines, input has to be\\npartitioned, which causes duplication. We explore how to resolve this tension\\nbetween maximum load per worker and input duplication for band-joins between\\ntwo relations. Previous work suffered from high optimization cost or considered\\npartitionings that were too restricted (resulting in suboptimal join\\nperformance). Our main insight is that recursive partitioning of the\\njoin-attribute space with the appropriate split scoring measure can achieve\\nboth low optimization cost and low join cost. It is the first approach that is\\nnot only effective for one-dimensional band-joins but also for joins on\\nmultiple attributes. Experiments indicate that our method is able to find\\npartitionings that are within 10% of the lower bound for both maximum load per\\nworker and input duplication for a broad range of settings, significantly\\nimproving over previous work.', comment=None, journal_ref=None, doi='10.1145/3318464.3389750', primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3318464.3389750', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2004.06101v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.06101v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.06203v1', updated=datetime.datetime(2020, 4, 8, 23, 12, 12, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 8, 23, 12, 12, tzinfo=datetime.timezone.utc), title='Knowledge Graphs for Processing Scientific Data: Challenges and Prospects', authors=[arxiv.Result.Author('Masoud Salehpour'), arxiv.Result.Author('Joseph G. Davis')], summary='There is growing interest in the use of Knowledge Graphs (KGs) for the\\nrepresentation, exchange, and reuse of scientific data. While KGs offer the\\nprospect of improving the infrastructure for working with scalable and reusable\\nscholarly data consistent with the FAIR (Findability, Accessibility,\\nInteroperability, and Reusability) principles, the state-of-the-art Data\\nManagement Systems (DMSs) for processing large KGs leave somewhat to be\\ndesired. In this paper, we studied the performance of some of the major DMSs in\\nthe context of querying KGs with the goal of providing a finely-grained,\\ncomparative analysis of DMSs representing each of the four major DMS types. We\\nexperimented with four well-known scientific KGs, namely, Allie, Cellcycle,\\nDrugBank, and LinkedSPL against Virtuoso, Blazegraph, RDF-3X, and MongoDB as\\nthe representative DMSs. Our results suggest that the DMSs display limitations\\nin processing complex queries on the KG datasets. Depending on the query type,\\nthe performance differentials can be several orders of magnitude. Also, no\\nsingle DMS appears to offer consistently superior performance. We present an\\nanalysis of the underlying issues and outline two integrated approaches and\\nproposals for resolving the problem.', comment='arXiv admin note: text overlap with arXiv:2004.04286,\\n  arXiv:2004.05648', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.06203v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.06203v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.06530v1', updated=datetime.datetime(2020, 4, 12, 20, 13, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 12, 20, 13, 23, tzinfo=datetime.timezone.utc), title='BugDoc: Algorithms to Debug Computational Processes', authors=[arxiv.Result.Author('Raoni Lourenço'), arxiv.Result.Author('Juliana Freire'), arxiv.Result.Author('Dennis Shasha')], summary='Data analysis for scientific experiments and enterprises, large-scale\\nsimulations, and machine learning tasks all entail the use of complex\\ncomputational pipelines to reach quantitative and qualitative conclusions. If\\nsome of the activities in a pipeline produce erroneous outputs, the pipeline\\nmay fail to execute or produce incorrect results. Inferring the root cause(s)\\nof such failures is challenging, usually requiring time and much human thought,\\nwhile still being error-prone. We propose a new approach that makes use of\\niteration and provenance to automatically infer the root causes and derive\\nsuccinct explanations of failures. Through a detailed experimental evaluation,\\nwe assess the cost, precision, and recall of our approach compared to the state\\nof the art. Our experimental data and processing software is available for use,\\nreproducibility, and enhancement.', comment='To appear in SIGMOD 2020. arXiv admin note: text overlap with\\n  arXiv:2002.04640', journal_ref=None, doi='10.1145/3318464.3389763', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3318464.3389763', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2004.06530v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.06530v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.06653v1', updated=datetime.datetime(2020, 4, 11, 8, 35, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 11, 8, 35, 54, tzinfo=datetime.timezone.utc), title='Efficient Suspected Infected Crowds Detection Based on Spatio-Temporal Trajectories', authors=[arxiv.Result.Author('Huajun He'), arxiv.Result.Author('Ruiyuan Li'), arxiv.Result.Author('Rubin Wang'), arxiv.Result.Author('Jie Bao'), arxiv.Result.Author('Yu Zheng'), arxiv.Result.Author('Tianrui Li')], summary='Virus transmission from person to person is an emergency event facing the\\nglobal public. Early detection and isolation of potentially susceptible crowds\\ncan effectively control the epidemic of its disease. Existing metrics can not\\ncorrectly address the infected rate on trajectories. To solve this problem, we\\npropose a novel spatio-temporal infected rate (IR) measure based on human\\nmoving trajectories that can adequately describe the risk of being infected by\\na given query trajectory of a patient. Then, we manage source data through an\\nefficient spatio-temporal index to make our system more scalable, and can\\nquickly query susceptible crowds from massive trajectories. Besides, we design\\nseveral pruning strategies that can effectively reduce calculations. Further,\\nwe design a spatial first time (SFT) index, which enables us to quickly query\\nmultiple trajectories without much I/O consumption and data redundancy. The\\nperformance of the solutions is demonstrated in experiments based on real and\\nsynthetic trajectory datasets that have shown the effectiveness and efficiency\\nof our solutions.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.06653v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.06653v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.07498v2', updated=datetime.datetime(2020, 6, 17, 20, 33, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 16, 7, 37, 24, tzinfo=datetime.timezone.utc), title='Sieve: A Middleware Approach to Scalable Access Control for Database Management Systems', authors=[arxiv.Result.Author('Primal Pappachan'), arxiv.Result.Author('Roberto Yus'), arxiv.Result.Author('Sharad Mehrotra'), arxiv.Result.Author('Johann-Christoph Freytag')], summary=\"Current approaches of enforcing FGAC in Database Management Systems (DBMS) do\\nnot scale in scenarios when the number of policies are in the order of\\nthousands. This paper identifies such a use case in the context of emerging\\nsmart spaces wherein systems may be required by legislation, such as Europe's\\nGDPR and California's CCPA, to empower users to specify who may have access to\\ntheir data and for what purposes. We present Sieve, a layered approach of\\nimplementing FGAC in existing database systems, that exploits a variety of it's\\nfeatures such as UDFs, index usage hints, query explain; to scale to large\\nnumber of policies. Given a query, Sieve exploits it's context to filter the\\npolicies that need to be checked. Sieve also generates guarded expressions that\\nsaves on evaluation cost by grouping the policies and cuts the read cost by\\nexploiting database indices. Our experimental results, on two DBMS and two\\ndifferent datasets, show that Sieve scales to large data sets and to large\\npolicy corpus thus supporting real-time access in applications including\\nemerging smart environments.\", comment='Extended version of the paper submitted to Very Large Data Bases\\n  (VLDB) and is now under review', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.07498v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.07498v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.07585v1', updated=datetime.datetime(2020, 4, 16, 10, 52, 25, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 16, 10, 52, 25, tzinfo=datetime.timezone.utc), title='ForkBase: Immutable, Tamper-evident Storage Substrate for Branchable Applications', authors=[arxiv.Result.Author('Qian Lin'), arxiv.Result.Author('Kaiyuan Yang'), arxiv.Result.Author('Tien Tuan Anh Dinh'), arxiv.Result.Author('Qingchao Cai'), arxiv.Result.Author('Gang Chen'), arxiv.Result.Author('Beng Chin Ooi'), arxiv.Result.Author('Pingcheng Ruan'), arxiv.Result.Author('Sheng Wang'), arxiv.Result.Author('Zhongle Xie'), arxiv.Result.Author('Meihui Zhang'), arxiv.Result.Author('Olafs Vandans')], summary='Data collaboration activities typically require systematic or protocol-based\\ncoordination to be scalable. Git, an effective enabler for collaborative\\ncoding, has been attested for its success in countless projects around the\\nworld. Hence, applying the Git philosophy to general data collaboration beyond\\ncoding is motivating. We call it Git for data. However, the original Git design\\nhandles data at the file granule, which is considered too coarse-grained for\\nmany database applications. We argue that Git for data should be co-designed\\nwith database systems. To this end, we developed ForkBase to make Git for data\\npractical. ForkBase is a distributed, immutable storage system designed for\\ndata version management and data collaborative operation. In this\\ndemonstration, we show how ForkBase can greatly facilitate collaborative data\\nmanagement and how its novel data deduplication technique can improve storage\\nefficiency for archiving massive data versions.', comment='In Proceedings of the IEEE International Conference on Data\\n  Engineering (ICDE), 2020 (Demo)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.07585v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.07585v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.07668v3', updated=datetime.datetime(2020, 4, 20, 7, 3, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 16, 14, 5, 24, tzinfo=datetime.timezone.utc), title='Holding a Conference Online and Live due to COVID-19', authors=[arxiv.Result.Author('Angela Bonifati'), arxiv.Result.Author('Giovanna Guerrini'), arxiv.Result.Author('Carsten Lutz'), arxiv.Result.Author('Wim Martens'), arxiv.Result.Author('Lara Mazilu'), arxiv.Result.Author('Norman Paton'), arxiv.Result.Author('Marcos Antonio Vaz Salles'), arxiv.Result.Author('Marc H. Scholl'), arxiv.Result.Author('Yongluan Zhou')], summary='The joint EDBT/ICDT conference (International Conference on Extending\\nDatabase Technology/International Conference on Database Theory) is a well\\nestablished conference series on data management, with annual meetings in the\\nsecond half of March that attract 250 to 300 delegates. Three weeks before\\nEDBT/ICDT 2020 was planned to take place in Copenhagen, the rapidly developing\\nCovid-19 pandemic led to the decision to cancel the face-to-face event. In the\\ninterest of the research community, it was decided to move the conference\\nonline while trying to preserve as much of the real-life experience as\\npossible. As far as we know, we are one of the first conferences that moved to\\na fully synchronous online experience due to the COVID-19 outbreak. With fully\\nsynchronous, we mean that participants jointly listened to presentations, had\\nlive Q&A, and attended other live events associated with the conference. In\\nthis report, we share our decisions, experiences, and lessons learned.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.07668v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.07668v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.07917v1', updated=datetime.datetime(2020, 4, 16, 20, 14, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 16, 20, 14, 20, tzinfo=datetime.timezone.utc), title='Knowledge Scientists: Unlocking the data-driven organization', authors=[arxiv.Result.Author('George Fletcher'), arxiv.Result.Author('Paul Groth'), arxiv.Result.Author('Juan Sequeda')], summary='Organizations across all sectors are increasingly undergoing deep\\ntransformation and restructuring towards data-driven operations. The central\\nrole of data highlights the need for reliable and clean data. Unreliable,\\nerroneous, and incomplete data lead to critical bottlenecks in processing\\npipelines and, ultimately, service failures, which are disastrous for the\\ncompetitive performance of the organization. Given its central importance,\\nthose organizations which recognize and react to the need for reliable data\\nwill have the advantage in the coming decade. We argue that the technologies\\nfor reliable data are driven by distinct concerns and expertise which\\ncomplement those of the data scientist and the data engineer. Those\\norganizations which identify the central importance of meaningful, explainable,\\nreproducible, and maintainable data will be at the forefront of the\\ndemocratization of reliable data. We call the new role which must be developed\\nto fill this critical need the Knowledge Scientist. The organizational\\nstructures, tools, methodologies and techniques to support and make possible\\nthe work of knowledge scientists are still in their infancy. As organizations\\nnot only use data but increasingly rely on data, it is time to empower the\\npeople who are central to this transformation.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CY', 'cs.GL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.07917v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.07917v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.08015v1', updated=datetime.datetime(2020, 4, 17, 1, 22, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 17, 1, 22, 33, tzinfo=datetime.timezone.utc), title='Efficient Constrained Pattern Mining Using Dynamic Item Ordering for Explainable Classification', authors=[arxiv.Result.Author('Hiroaki Iwashita'), arxiv.Result.Author('Takuya Takagi'), arxiv.Result.Author('Hirofumi Suzuki'), arxiv.Result.Author('Keisuke Goto'), arxiv.Result.Author('Kotaro Ohori'), arxiv.Result.Author('Hiroki Arimura')], summary='Learning of interpretable classification models has been attracting much\\nattention for the last few years. Discovery of succinct and contrasting\\npatterns that can highlight the differences between the two classes is very\\nimportant. Such patterns are useful for human experts, and can be used to\\nconstruct powerful classifiers. In this paper, we consider mining of minimal\\nemerging patterns from high-dimensional data sets under a variety of\\nconstraints in a supervised setting. We focus on an extension in which patterns\\ncan contain negative items that designate the absence of an item. In such a\\ncase, a database becomes highly dense, and it makes mining more challenging\\nsince popular pattern mining techniques such as fp-tree and occurrence deliver\\ndo not efficiently work. To cope with this difficulty, we present an efficient\\nalgorithm for mining minimal emerging patterns by combining two techniques:\\ndynamic variable-ordering during pattern search for enhancing pruning effect,\\nand the use of a pointer-based dynamic data structure, called dancing links,\\nfor efficiently maintaining occurrence lists. Experiments on benchmark data\\nsets showed that our algorithm achieves significant speed-ups over emerging\\npattern mining approach based on LCM, a very fast depth-first frequent itemset\\nminer using static variable-ordering.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.08015v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.08015v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.08257v1', updated=datetime.datetime(2020, 4, 17, 14, 12, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 17, 14, 12, 40, tzinfo=datetime.timezone.utc), title='Duplication Detection in Knowledge Graphs: Literature and Tools', authors=[arxiv.Result.Author('Elwin Huaman'), arxiv.Result.Author('Elias Kärle'), arxiv.Result.Author('Dieter Fensel')], summary=\"In recent years, an increasing amount of knowledge graphs (KGs) have been\\ncreated as a means to store cross-domain knowledge and billion of facts, which\\nare the basis of costumers' applications like search engines. However, KGs\\ninevitably have inconsistencies such as duplicates that might generate\\nconflicting property values. Duplication detection (DD) aims to identify\\nduplicated entities and resolve their conflicting property values effectively\\nand efficiently. In this paper, we perform a literature review on DD methods\\nand tools, and an evaluation of them. Our main contributions are a performance\\nevaluation of DD tools in KGs, improvement suggestions, and a DD workflow to\\nsupport future development of DD tools, which are based on desirable features\\ndetected through this study.\", comment='Submitted to EKAW 2020 Conference', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.08257v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.08257v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.08284v3', updated=datetime.datetime(2020, 6, 6, 21, 0, 58, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 17, 14, 53, 59, tzinfo=datetime.timezone.utc), title='Time Series Data Cleaning with Regular and Irregular Time Intervals', authors=[arxiv.Result.Author('Xi Wang'), arxiv.Result.Author('Chen Wang')], summary='Errors are prevalent in time series data, especially in the industrial field.\\nData with errors could not be stored in the database, which results in the loss\\nof data assets. Handling the dirty data in time series is non-trivial, when\\ngiven irregular time intervals. At present, to deal with these time series\\ncontaining errors, besides keeping original erroneous data, discarding\\nerroneous data and manually checking erroneous data, we can also use the\\ncleaning algorithm widely used in the database to automatically clean the time\\nseries data. This survey provides a classification of time series data cleaning\\ntechniques and comprehensively reviews the state-of-the-art methods of each\\ntype. In particular, we have a special focus on the irregular time intervals.\\nBesides we summarize data cleaning tools, systems and evaluation criteria from\\nresearch and industry. Finally, we highlight possible directions time series\\ndata cleaning.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.08284v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.08284v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.08425v1', updated=datetime.datetime(2020, 4, 17, 19, 14, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 17, 19, 14, 24, tzinfo=datetime.timezone.utc), title='Automated System Performance Testing at MongoDB', authors=[arxiv.Result.Author('Henrik Ingo'), arxiv.Result.Author('David Daly')], summary=\"Distributed Systems Infrastructure (DSI) is MongoDB's framework for running\\nfully automated system performance tests in our Continuous Integration (CI)\\nenvironment. To run in CI it needs to automate everything end-to-end:\\nprovisioning and deploying multi-node clusters, executing tests, tuning the\\nsystem for repeatable results, and collecting and analyzing the results. Today\\nDSI is MongoDB's most used and most useful performance testing tool. It runs\\nalmost 200 different benchmarks in daily CI, and we also use it for manual\\nperformance investigations. As we can alert the responsible engineer in a\\ntimely fashion, all but one of the major regressions were fixed before the\\n4.2.0 release. We are also able to catch net new improvements, of which DSI\\ncaught 17. We open sourced DSI in March 2020.\", comment='Author Preprint. Appearing in DBTest.io 2020', journal_ref=None, doi='10.1145/3395032.3395323', primary_category='cs.DB', categories=['cs.DB', 'cs.PF', 'cs.SE'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3395032.3395323', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2004.08425v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.08425v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.09350v1', updated=datetime.datetime(2020, 4, 20, 14, 50, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 20, 14, 50, 50, tzinfo=datetime.timezone.utc), title='MorphStore: Analytical Query Engine with a Holistic Compression-Enabled Processing Model', authors=[arxiv.Result.Author('Patrick Damme'), arxiv.Result.Author('Annett Ungethüm'), arxiv.Result.Author('Johannes Pietrzyk'), arxiv.Result.Author('Alexander Krause'), arxiv.Result.Author('Dirk Habich'), arxiv.Result.Author('Wolfgang Lehner')], summary='In this paper, we present MorphStore, an open-source in-memory columnar\\nanalytical query engine with a novel holistic compression-enabled processing\\nmodel. Basically, compression using lightweight integer compression algorithms\\nalready plays an important role in existing in-memory column-store database\\nsystems, but mainly for base data. In particular, during query processing,\\nthese systems only keep the data compressed until an operator cannot process\\nthe compressed data directly, whereupon the data is decompressed, but not\\nrecompressed. Thus, the full potential of compression during query processing\\nis not exploited. To overcome that, we developed a novel compression-enabled\\nprocessing model as presented in this paper. As we are going to show, the\\ncontinuous usage of compression for all base data and all intermediates is very\\nbeneficial to reduce the overall memory footprint as well as to improve the\\nquery performance.', comment='Submitted to PVLDB', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.09350v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.09350v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.10360v2', updated=datetime.datetime(2020, 7, 14, 23, 59, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 22, 1, 34, 58, tzinfo=datetime.timezone.utc), title='Breaking Down Memory Walls: Adaptive Memory Management in LSM-based Storage Systems (Extended Version)', authors=[arxiv.Result.Author('Chen Luo'), arxiv.Result.Author('Michael J. Carey')], summary='Log-Structured Merge-trees (LSM-trees) have been widely used in modern NoSQL\\nsystems. Due to their out-of-place update design, LSM-trees have introduced\\nmemory walls among the memory components of multiple LSM-trees and between the\\nwrite memory and the buffer cache. Optimal memory allocation among these\\nregions is non-trivial because it is highly workload-dependent. Existing\\nLSM-tree implementations instead adopt static memory allocation schemes due to\\ntheir simplicity and robustness, sacrificing performance. In this paper, we\\nattempt to break down these memory walls in LSM-based storage systems. We first\\npresent a memory management architecture that enables adaptive memory\\nmanagement. We then present a partitioned memory component structure with new\\nflush policies to better exploit the write memory to minimize the write cost.\\nTo break down the memory wall between the write memory and the buffer cache, we\\nfurther introduce a memory tuner that tunes the memory allocation between these\\ntwo regions. We have conducted extensive experiments in the context of Apache\\nAsterixDB using the YCSB and TPC-C benchmarks and we present the results here.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.10360v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.10360v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.12424v1', updated=datetime.datetime(2020, 4, 26, 16, 4, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 26, 16, 4, 53, tzinfo=datetime.timezone.utc), title='An Efficient Index Method for the Optimal Route Query over Multi-Cost Networks', authors=[arxiv.Result.Author('Yajun Yang'), arxiv.Result.Author('Hang Zhang'), arxiv.Result.Author('Hong Gao'), arxiv.Result.Author('Qinghua Hu'), arxiv.Result.Author('Xin Wang')], summary='Smart city has been consider the wave of the future and the route\\nrecommendation in networks is a fundamental problem in it. Most existing\\napproaches for the shortest route problem consider that there is only one kind\\nof cost in networks. However, there always are several kinds of cost in\\nnetworks and users prefer to select an optimal route under the global\\nconsideration of these kinds of cost. In this paper, we study the problem of\\nfinding the optimal route in the multi-cost networks. We prove this problem is\\nNP-hard and the existing index techniques cannot be used to this problem. We\\npropose a novel partition-based index with contour skyline techniques to find\\nthe optimal route. We propose a vertex-filtering algorithm to facilitate the\\nquery processing. We conduct extensive experiments on six real-life networks\\nand the experimental results show that our method has an improvement in\\nefficiency by an order of magnitude compared to the previous heuristic\\nalgorithms.', comment='10 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.12424v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.12424v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.12821v1', updated=datetime.datetime(2020, 4, 27, 14, 2, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 27, 14, 2, 9, tzinfo=datetime.timezone.utc), title='SFTM: Fast Comparison of Web Documents using Similarity-based Flexible Tree Matching', authors=[arxiv.Result.Author('Sacha Brisset'), arxiv.Result.Author('Romain Rouvoy'), arxiv.Result.Author('Renaud Pawlak'), arxiv.Result.Author('Lionel Seinturier')], summary='Tree matching techniques have been investigated in many fields, including web\\ndata mining and extraction, as a key component to analyze the content of web\\ndocuments, existing tree matching approaches, like Tree-Edit Distance (TED) or\\nFlexible Tree Matching (FTM), fail to scale beyond a few hundreds of nodes,\\nwhich is far below the average complexity of existing web online documents and\\napplications. In this paper, we therefore propose a novel Similarity-based\\nFlexible Tree Matching algorithm (SFTM), which is the first algorithm to enable\\ntree matching on real-life web documents with practical computation times. In\\nparticular, we approach tree matching as an optimisation problem and we\\nleverage node labels and local topology similarity in order to avoid any\\ncombinatorial explosion. Our practical evaluation demonstrates that our\\napproach compares to the reference implementation of TED qualitatively, while\\nimproving the computation times by two orders of magnitude.', comment='9 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.SE', 'D.2'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.12821v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.12821v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.12929v1', updated=datetime.datetime(2020, 4, 27, 16, 42, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 27, 16, 42, 40, tzinfo=datetime.timezone.utc), title='Data Engineering for Data Analytics: A Classification of the Issues, and Case Studies', authors=[arxiv.Result.Author('Alfredo Nazabal'), arxiv.Result.Author('Christopher K. I. Williams'), arxiv.Result.Author('Giovanni Colavizza'), arxiv.Result.Author('Camila Rangel Smith'), arxiv.Result.Author('Angus Williams')], summary=\"Consider the situation where a data analyst wishes to carry out an analysis\\non a given dataset. It is widely recognized that most of the analyst's time\\nwill be taken up with \\\\emph{data engineering} tasks such as acquiring,\\nunderstanding, cleaning and preparing the data. In this paper we provide a\\ndescription and classification of such tasks into high-levels groups, namely\\ndata organization, data quality and feature engineering. We also make available\\nfour datasets and example analyses that exhibit a wide variety of these\\nproblems, to help encourage the development of tools and techniques to help\\nreduce this burden and push forward research towards the automation or\\nsemi-automation of the data engineering process.\", comment='24 pages, 1 figure, submitted to IEEE Transactions on Knowledge and\\n  Data Engineering', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.12929v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.12929v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.14478v1', updated=datetime.datetime(2020, 4, 26, 12, 19, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 26, 12, 19, 4, tzinfo=datetime.timezone.utc), title='KGClean: An Embedding Powered Knowledge Graph Cleaning Framework', authors=[arxiv.Result.Author('Congcong Ge'), arxiv.Result.Author('Yunjun Gao'), arxiv.Result.Author('Honghui Weng'), arxiv.Result.Author('Chong Zhang'), arxiv.Result.Author('Xiaoye Miao'), arxiv.Result.Author('Baihua Zheng')], summary='The quality assurance of the knowledge graph is a prerequisite for various\\nknowledge-driven applications. We propose KGClean, a novel cleaning framework\\npowered by knowledge graph embedding, to detect and repair the heterogeneous\\ndirty data. In contrast to previous approaches that either focus on filling\\nmissing data or clean errors violated limited rules, KGClean enables (i)\\ncleaning both missing data and other erroneous values, and (ii) mining\\npotential rules automatically, which expands the coverage of error detecting.\\nKGClean first learns data representations by TransGAT, an effective knowledge\\ngraph embedding model, which gathers the neighborhood information of each data\\nand incorporates the interactions among data for casting data to continuous\\nvector spaces with rich semantics. KGClean integrates an active learning-based\\nclassification model, which identifies errors with a small seed of labels.\\nKGClean utilizes an efficient PRO-repair strategy to repair errors using a\\nnovel concept of propagation power. Extensive experiments on four typical\\nknowledge graphs demonstrate the effectiveness of KGClean in practice.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.14478v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.14478v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.14541v2', updated=datetime.datetime(2020, 5, 22, 21, 1, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 30, 1, 56, 54, tzinfo=datetime.timezone.utc), title='RadixSpline: A Single-Pass Learned Index', authors=[arxiv.Result.Author('Andreas Kipf'), arxiv.Result.Author('Ryan Marcus'), arxiv.Result.Author('Alexander van Renen'), arxiv.Result.Author('Mihail Stoian'), arxiv.Result.Author('Alfons Kemper'), arxiv.Result.Author('Tim Kraska'), arxiv.Result.Author('Thomas Neumann')], summary='Recent research has shown that learned models can outperform state-of-the-art\\nindex structures in size and lookup performance. While this is a very promising\\nresult, existing learned structures are often cumbersome to implement and are\\nslow to build. In fact, most approaches that we are aware of require multiple\\ntraining passes over the data.\\n  We introduce RadixSpline (RS), a learned index that can be built in a single\\npass over the data and is competitive with state-of-the-art learned index\\nmodels, like RMI, in size and lookup performance. We evaluate RS using the SOSD\\nbenchmark and show that it achieves competitive results on all datasets,\\ndespite the fact that it only has two parameters.', comment='Third International Workshop on Exploiting Artificial Intelligence\\n  Techniques for Data Management (aiDM 2020)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.14541v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.14541v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2004.14794v3', updated=datetime.datetime(2020, 5, 12, 14, 32, 7, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 30, 14, 7, 39, tzinfo=datetime.timezone.utc), title='Graph Summarization', authors=[arxiv.Result.Author('Angela Bonifati'), arxiv.Result.Author('Stefania Dumbrava'), arxiv.Result.Author('Haridimos Kondylakis')], summary='The continuous and rapid growth of highly interconnected datasets, which are\\nboth voluminous and complex, calls for the development of adequate processing\\nand analytical techniques. One method for condensing and simplifying such\\ndatasets is graph summarization. It denotes a series of application-specific\\nalgorithms designed to transform graphs into more compact representations while\\npreserving structural patterns, query answers, or specific property\\ndistributions. As this problem is common to several areas studying graph\\ntopologies, different approaches, such as clustering, compression, sampling, or\\ninfluence detection, have been proposed, primarily based on statistical and\\noptimization methods. The focus of our chapter is to pinpoint the main graph\\nsummarization methods, but especially to focus on the most recent approaches\\nand novel research trends on this topic, not yet covered by previous surveys.', comment='To appear in the Encyclopedia of Big Data Technologies', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2004.14794v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2004.14794v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.00044v1', updated=datetime.datetime(2020, 4, 30, 18, 29, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 4, 30, 18, 29, 42, tzinfo=datetime.timezone.utc), title='Efficiently Reclaiming Space in a Log Structured Store', authors=[arxiv.Result.Author('David Lomet'), arxiv.Result.Author('Chen Luo')], summary='A log structured store uses a single write I/O for a number of diverse and\\nnon-contiguous pages within a large buffer instead of using a write I/O for\\neach page separately. This requires that pages be relocated on every write,\\nbecause pages are never updated in place. Instead, pages are dynamically\\nremapped on every write. Log structuring was invented for and used initially in\\nfile systems. Today, a form of log structuring is used in SSD controllers\\nbecause an SSD requires the erasure of a large block of pages before flash\\nstorage can be reused. No update-in-place requires that the storage for\\nout-of-date pages be reclaimed (garbage collected or \"cleaned\"). We analyze\\ncleaning performance and introduce a cleaning strategy that uses a new way to\\nprioritize the order in which stale pages are garbage collected. Our cleaning\\nstrategy approximates an \"optimal cleaning strategy\". Simulation studies\\nconfirm the results of the analysis. This strategy is a significant improvement\\nover previous cleaning strategies.', comment='12 pages, 6 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AR', 'cs.PF'], links=[arxiv.Result.Link('http://arxiv.org/abs/2005.00044v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.00044v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.01511v1', updated=datetime.datetime(2020, 5, 1, 11, 32, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 1, 11, 32, 23, tzinfo=datetime.timezone.utc), title='The ReProVide Query-Sequence Optimization in a Hardware-Accelerated DBMS', authors=[arxiv.Result.Author('Lekshmi B. G.'), arxiv.Result.Author('Andreas Becher'), arxiv.Result.Author('Klaus Meyer-Wegener')], summary='Hardware acceleration of database query processing can be done with the help\\nof FPGAs. In particular, they are partially reconfigurable at runtime, which\\nallows for the runtime adaption of the hardware to a variety of queries.\\nReconfiguration itself, however, takes some time. As the affected area of the\\nFPGA is not available for computations during the reconfiguration, avoiding\\nsome of the reconfigurations can improve overall performance. This paper\\npresents optimizations based on query sequences, which reduces the impact of\\nthe reconfigurations. Knowledge of upcoming queries is used to (I)\\nspeculatively start reconfiguration already when a query is still running and\\n(II) avoid overwriting of reconfigurable regions that will be used again in\\nsubsequent queries. We evaluate our optimizations with a calibrated model and\\nmeasurements for various parameter values. Improvements in execution time of up\\nto 28% can be obtained even with sequences of only two queries.', comment='arXiv admin note: substantial text overlap with arXiv:2001.10719', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2005.01511v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.01511v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.02239v1', updated=datetime.datetime(2020, 5, 3, 22, 35, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 3, 22, 35, 51, tzinfo=datetime.timezone.utc), title='Guided Link-Traversal-Based Query Processing', authors=[arxiv.Result.Author('Ruben Verborgh'), arxiv.Result.Author('Ruben Taelman')], summary='Link-Traversal-Based Query Processing (LTBQP) is a technique for evaluating\\nqueries over a web of data by starting with a set of seed documents that is\\ndynamically expanded through following hyperlinks. Compared to query evaluation\\nover a static set of sources, LTBQP is significantly slower because of the\\nnumber of needed network requests. Furthermore, there are concerns regarding\\nrelevance and trustworthiness of results, given that sources are selected\\ndynamically. To address both issues, we propose guided LTBQP, a technique in\\nwhich information about document linking structure and content policies is\\npassed to a query processor. Thereby, the processor can prune the search tree\\nof documents by only following relevant links, and restrict the result set to\\ndesired results by limiting which documents are considered for what kinds of\\ncontent. In this exploratory paper, we describe the technique at a high level\\nand sketch some of its applications. We argue that such guidance can make LTBQP\\na valuable query strategy in decentralized environments, where data is spread\\nacross documents with varying levels of user trust.', comment='4 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR', 'cs.SI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2005.02239v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.02239v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.03787v1', updated=datetime.datetime(2020, 5, 7, 22, 33, 18, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 7, 22, 33, 18, tzinfo=datetime.timezone.utc), title=\"Détermination Automatique des Fonctions d'Appartenance et Interrogation Flexible et Coopérative des Bases de Données\", authors=[arxiv.Result.Author('Narjes Hachani Gharbi')], summary='Flexible querying of DB allows to extend DBMS in order to support imprecision\\nand flexibility in queries. Flexible queries use vague and imprecise terms\\nwhich have been defined as fuzzy sets. However, there is no consensus on\\nmemberships functions generation. Most of the proposed methods require expert\\nintervention. This thesis is devised in two parts. In the first part, we\\npropose a clustering based approach for automatic and incremental membership\\nfunctions generation. We have proposed the clustering method CLUSTERDB* which\\nevaluates clustering quality underway clusters generation. Moreover, we propose\\nincremental updates of partitions and membership functions after insertion or\\ndeletion of a new object. The second part of this thesis uses these functions\\nand Formal Concepts Analysis in flexible and cooperative querying. In case of\\nempty answers, we formally detect the failure reasons and we generate\\napproximative queries with their answers. These queries help the user to\\nformulate new queries having answers. The different proposed approaches are\\nimplemented and experimented with several databases. The experimentation\\nresults are encouraging.', comment='133 pages, in French. 22 figures PhD thesis', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2005.03787v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.03787v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.05886v3', updated=datetime.datetime(2020, 7, 17, 5, 23, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 12, 16, 1, 9, tzinfo=datetime.timezone.utc), title='Counting Query Answers over a DL-Lite Knowledge Base (extended version)', authors=[arxiv.Result.Author('Diego Calvanese'), arxiv.Result.Author('Julien Corman'), arxiv.Result.Author('Davide Lanti'), arxiv.Result.Author('Simon Razniewski')], summary='Counting answers to a query is an operation supported by virtually all\\ndatabase management systems. In this paper we focus on counting answers over a\\nKnowledge Base (KB), which may be viewed as a database enriched with background\\nknowledge about the domain under consideration. In particular, we place our\\nwork in the context of Ontology-Mediated Query Answering/Ontology-based Data\\nAccess (OMQA/OBDA), where the language used for the ontology is a member of the\\nDL-Lite family and the data is a (usually virtual) set of assertions. We study\\nthe data complexity of query answering, for different members of the DL-Lite\\nfamily that include number restrictions, and for variants of conjunctive\\nqueries with counting that differ with respect to their shape (connected,\\nbranching, rooted). We improve upon existing results by providing a PTIME and\\ncoNP lower bounds, and upper bounds in PTIME and LOGSPACE. For the latter case,\\nwe define a novel query rewriting technique into first-order logic with\\ncounting.', comment='Extended version of an article published at IJCAI 2020', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2005.05886v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.05886v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.06437v1', updated=datetime.datetime(2020, 5, 13, 17, 21, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 13, 17, 21, 27, tzinfo=datetime.timezone.utc), title='On Embeddings in Relational Databases', authors=[arxiv.Result.Author('Siddhant Arora'), arxiv.Result.Author('Srikanta Bedathur')], summary='We address the problem of learning a distributed representation of entities\\nin a relational database using a low-dimensional embedding. Low-dimensional\\nembeddings aim to encapsulate a concise vector representation for an underlying\\ndataset with minimum loss of information. Embeddings across entities in a\\nrelational database have been less explored due to the intricate data relations\\nand representation complexity involved. Relational databases are an\\ninter-weaved collection of relations that not only model relationships between\\nentities but also record complex domain-specific quantitative and temporal\\nattributes of data defining complex relationships among entities. Recent\\nmethods for learning an embedding constitute of a naive approach to consider\\ncomplete denormalization of the database by materializing the full join of all\\ntables and representing as a knowledge graph. This popular approach has certain\\nlimitations as it fails to capture the inter-row relationships and additional\\nsemantics encoded in the relational databases. In this paper we demonstrate; a\\nbetter methodology for learning representations by exploiting the underlying\\nsemantics of columns in a table while using the relation joins and the latent\\ninter-row relationships. Empirical results over a real-world database with\\nevaluations on similarity join and table completion tasks support our\\nproposition.', comment='9 pages, 6 Figures, Proceedings of Knowledge Representation &\\n  Reasoning Meets Machine Learning Workshop, NeurIPS 2019', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2005.06437v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.06437v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.07992v1', updated=datetime.datetime(2020, 5, 16, 13, 46, 37, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 16, 13, 46, 37, tzinfo=datetime.timezone.utc), title='Extending Databases to Support Data Manipulation with Functional Dependencies: a Vision Paper', authors=[arxiv.Result.Author('Nikita Bobrov'), arxiv.Result.Author('Kirill Smirnov'), arxiv.Result.Author('George Chernishev')], summary='In the current paper, we propose to fuse together stored data (tables) and\\ntheir functional dependencies (FDs) inside a DBMS. We aim to make FDs\\nfirst-class citizens: objects which can be queried and used to query data. Our\\nidea is to allow analysts to explore both data and functional dependencies\\nusing the database interface. For example, an analyst may be interested in such\\ntasks as: \"find all rows which prevent a given functional dependency from\\nholding\", \"for a given table, find all functional dependencies that involve a\\ngiven attribute\", \"project all attributes that functionally determine a\\nspecified attribute\".\\n  For this purpose, we propose: (1) an SQL-based query language for querying a\\ncollection of functional dependencies (2) an extension of the SQL SELECT clause\\nfor supporting FD-based predicates, including approximate ones (3) a special\\ndata structure intended for containing mined FDs and acting as a mediator\\nbetween user queries and underlying data. We describe the proposed extensions,\\ndemonstrate their use-cases, and finally, discuss implementation details and\\ntheir impact on query processing.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2.0'], links=[arxiv.Result.Link('http://arxiv.org/abs/2005.07992v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.07992v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.08111v1', updated=datetime.datetime(2020, 5, 16, 21, 43, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 16, 21, 43, 11, tzinfo=datetime.timezone.utc), title='kD-STR: A Method for Spatio-Temporal Data Reduction and Modelling', authors=[arxiv.Result.Author('Liam Steadman'), arxiv.Result.Author('Nathan Griffiths'), arxiv.Result.Author('Stephen Jarvis'), arxiv.Result.Author('Mark Bell'), arxiv.Result.Author('Shaun Helman'), arxiv.Result.Author('Caroline Wallbank')], summary='Analysing and learning from spatio-temporal datasets is an important process\\nin many domains, including transportation, healthcare and meteorology. In\\nparticular, data collected by sensors in the environment allows us to\\nunderstand and model the processes acting within the environment. Recently, the\\nvolume of spatio-temporal data collected has increased significantly,\\npresenting several challenges for data scientists. Methods are therefore needed\\nto reduce the quantity of data that needs to be processed in order to analyse\\nand learn from spatio-temporal datasets. In this paper, we present the\\nk-Dimensional Spatio-Temporal Reduction method (kD-STR) for reducing the\\nquantity of data used to store a dataset whilst enabling multiple types of\\nanalysis on the reduced dataset. kD-STR uses hierarchical partitioning to find\\nspatio-temporal regions of similar instances and models the instances within\\neach region to summarise the dataset. We demonstrate the generality of kD-STR\\nwith 3 datasets exhibiting different spatio-temporal characteristics and\\npresent results for a range of data modelling techniques. Finally, we compare\\nkD-STR with other techniques for reducing the volume of spatio-temporal data.\\nOur results demonstrate that kD-STR is effective in reducing spatio-temporal\\ndata and generalises to datasets that exhibit different properties.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2005.08111v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.08111v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.08540v1', updated=datetime.datetime(2020, 5, 18, 9, 6, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 18, 9, 6, 29, tzinfo=datetime.timezone.utc), title='Approximate Denial Constraints', authors=[arxiv.Result.Author('Ester Livshits'), arxiv.Result.Author('Alireza Heidari'), arxiv.Result.Author('Ihab F. Ilyas'), arxiv.Result.Author('Benny Kimelfeld')], summary='The problem of mining integrity constraints from data has been extensively\\nstudied over the past two decades for commonly used types of constraints\\nincluding the classic Functional Dependencies (FDs) and the more general Denial\\nConstraints (DCs). In this paper, we investigate the problem of mining\\napproximate DCs (i.e., DCs that are \"almost\" satisfied) from data. Considering\\napproximate constraints allows us to discover more accurate constraints in\\ninconsistent databases, detect rules that are generally correct but may have a\\nfew exceptions, as well as avoid overfitting and obtain more general and less\\ncontrived constraints. We introduce the algorithm ADCMiner for mining\\napproximate DCs. An important feature of this algorithm is that it does not\\nassume any specific definition of an approximate DC, but takes the semantics as\\ninput. Since there is more than one way to define an approximate DC and\\ndifferent definitions may produce very different results, we do not focus on\\none definition, but rather on a general family of approximation functions that\\nsatisfies some natural axioms defined in this paper and captures commonly used\\ndefinitions of approximate constraints. We also show how our algorithm can be\\ncombined with sampling to return results with high accuracy while significantly\\nreducing the running time.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2005.08540v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.08540v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.09367v1', updated=datetime.datetime(2020, 5, 19, 11, 24, 36, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 19, 11, 24, 36, tzinfo=datetime.timezone.utc), title='Machine Learning-based Cardinality Estimation in DBMS on Pre-Aggregated Data', authors=[arxiv.Result.Author('Lucas Woltmann'), arxiv.Result.Author('Claudio Hartmann'), arxiv.Result.Author('Dirk Habich'), arxiv.Result.Author('Wolfgang Lehner')], summary='Cardinality estimation is a fundamental task in database query processing and\\noptimization. As shown in recent papers, machine learning (ML)-based approaches\\ncan deliver more accurate cardinality estimations than traditional approaches.\\nHowever, a lot of example queries have to be executed during the model training\\nphase to learn a data-dependent ML model leading to a very time-consuming\\ntraining phase. Many of those example queries use the same base data, have the\\nsame query structure, and only differ in their predicates. Thus, index\\nstructures appear to be an ideal optimization technique at first glance.\\nHowever, their benefit is limited. To speed up this model training phase, our\\ncore idea is to determine a predicate-independent pre-aggregation of the base\\ndata and to execute the example queries over this pre-aggregated data. Based on\\nthis idea, we present a specific aggregate-enabled training phase for ML-based\\ncardinality estimation approaches in this paper. As we are going to show with\\ndifferent workloads in our evaluation, we are able to achieve an average\\nspeedup of 63 with our aggregate-enabled training phase.', comment='10 pages, technical report', journal_ref='Datenbank-Spektrum 22 (2022) 1-13', doi='10.1007/s13222-021-00400-z', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/s13222-021-00400-z', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2005.09367v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.09367v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.09399v1', updated=datetime.datetime(2020, 5, 19, 12, 48, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 19, 12, 48, 53, tzinfo=datetime.timezone.utc), title='Benchmarking Blocking Algorithms for Web Entities', authors=[arxiv.Result.Author('Vasilis Efthymiou'), arxiv.Result.Author('Kostas Stefanidis'), arxiv.Result.Author('Vassilis Christophides')], summary='An increasing number of entities are described by interlinked data rather\\nthan documents on the Web. Entity Resolution (ER) aims to identify descriptions\\nof the same real-world entity within one or across knowledge bases in the Web\\nof data. To reduce the required number of pairwise comparisons among\\ndescriptions, ER methods typically perform a pre-processing step, called\\n\\\\emph{blocking}, which places similar entity descriptions into blocks and thus\\nonly compare descriptions within the same block. We experimentally evaluate\\nseveral blocking methods proposed for the Web of data using real datasets,\\nwhose characteristics significantly impact their effectiveness and efficiency.\\nThe proposed experimental evaluation framework allows us to better understand\\nthe characteristics of the missed matching entity descriptions and contrast\\nthem with ground truth obtained from different kinds of relatedness links.', comment='accepted at IEEE Transactions on Big Data journal', journal_ref=None, doi='10.1109/TBDATA.2016.2576463', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/TBDATA.2016.2576463', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2005.09399v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.09399v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.11264v1', updated=datetime.datetime(2020, 5, 22, 16, 29, 25, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 22, 16, 29, 25, tzinfo=datetime.timezone.utc), title='OBDA for the Web: Creating Virtual RDF Graphs On Top of Web Data Sources', authors=[arxiv.Result.Author('Konstantina Bereta'), arxiv.Result.Author('George Papadakis'), arxiv.Result.Author('Manolis Koubarakis')], summary='Due to Variety, Web data come in many different structures and formats, with\\nHTML tables and REST APIs (e.g., social media APIs) being among the most\\npopular ones. A big subset of Web data is also characterised by Velocity, as\\ndata gets frequently updated so that consumers can obtain the most up-to-date\\nversion of the respective datasets. At the moment, though, these data sources\\nare not effectively supported by Semantic Web tools. To address variety and\\nvelocity, we propose Ontop4theWeb, a system that maps Web data of various\\nformats into virtual RDF triples, thus allowing for querying them on-the-fly\\nwithout materializing them as RDF. We demonstrate how Ontop4theWeb can use\\nSPARQL to uniformly query popular, but heterogeneous Web data sources, like\\nHTML tables and Web APIs. We showcase our approach in a number of use cases,\\nsuch as Twitter, Foursquare, Yelp and HTML tables. We carried out a thorough\\nexperimental evaluation which verifies the high efficiency of our framework,\\nwhich goes beyond the current state-of-the-art in this area, in terms of both\\nfunctionality and performance.', comment='12 pages, 6 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2005.11264v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.11264v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.12375v1', updated=datetime.datetime(2020, 5, 21, 8, 58, 4, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 21, 8, 58, 4, tzinfo=datetime.timezone.utc), title='QuViS -- The Question of Visual Site Selection', authors=[arxiv.Result.Author('Sebastian Baumbach'), arxiv.Result.Author('Jahanzeb Khan'), arxiv.Result.Author('Sheraz Ahmed'), arxiv.Result.Author('Andreas Dengel')], summary='This paper present QuViS, which is an interactive platform for visualization\\nand exploratory data analysis of site selection. The aim of QuViS is to support\\ndecision makers and experts during the process of site selection. In addition\\nto visualization engine for exploratory analysis, QuViS is also integrated with\\nour automatic site selection method (QuIS), which recommend different sites\\nautomatically based on the selected location factors by economists and experts.\\nTo show the potential and highlight the visualization and exploration\\ncapabilities of QuViS, a case study on 1,556 German supermarket site selection\\nis performed. The real publicly available dataset contains 450 location factors\\nfor all 11,162 multiplicities in Germany, covering the last 10-15 years. Case\\nstudy results shows that QuViS provides an easy and intuitive way for\\nexploratory analysis of geospatial multidimensional data.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2005.12375v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.12375v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.13762v3', updated=datetime.datetime(2021, 7, 21, 20, 13, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 28, 3, 26, 2, tzinfo=datetime.timezone.utc), title='CedrusDB: Persistent Key-Value Store with Memory-Mapped Lazy-Trie', authors=[arxiv.Result.Author('Maofan Yin'), arxiv.Result.Author('Hongbo Zhang'), arxiv.Result.Author('Robbert van Renesse'), arxiv.Result.Author('Emin Gün Sirer')], summary='As a result of RAM becoming cheaper, there has been a trend in key-value\\nstore design towards maintaining a fast in-memory index (such as a hash table)\\nwhile logging user operations to disk, allowing high performance under\\nfailure-free conditions while still being able to recover from failures. This\\ndesign, however, comes at the cost of long recovery times or expensive\\ncheckpoint operations. This paper presents a new in-memory index that is also\\nstorage-friendly. A \"lazy-trie\" is a variant of the hash-trie data structure\\nthat achieves near-optimal height, has practical storage overhead, and can be\\nmaintained on-disk with standard write-ahead logging.\\n  We implemented CedrusDB, persistent key-value store based on a lazy-trie. The\\nlazy-trie is kept on disk while made available in memory using standard\\nmemory-mapping. The lazy-trie organization in virtual memory allows CedrusDB to\\nbetter leverage concurrent processing than other on-disk index schemes (LSMs,\\nB+-trees). CedrusDB achieves comparable or superior performance to recent\\nlog-based in-memory key-value stores in mixed workloads while being able to\\nrecover quickly from failures.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2005.13762v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.13762v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.14068v4', updated=datetime.datetime(2021, 9, 7, 4, 7, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 28, 15, 12, 44, tzinfo=datetime.timezone.utc), title='Discovering Domain Orders through Order Dependencies', authors=[arxiv.Result.Author('Reza Karegar'), arxiv.Result.Author('Melicaalsadat Mirsafian'), arxiv.Result.Author('Parke Godfrey'), arxiv.Result.Author('Lukasz Golab'), arxiv.Result.Author('Mehdi Kargar'), arxiv.Result.Author('Divesh Srivastava'), arxiv.Result.Author('Jaroslaw Szlichta')], summary='Much real-world data come with explicitly defined domain orders; e.g.,\\nlexicographic order for strings, numeric for integers, and chronological for\\ntime. Our goal is to discover implicit domain orders that we do not already\\nknow; for instance, that the order of months in the Chinese Lunar calendar is\\nCorner < Apricot < Peach. To do so, we enhance data profiling methods by\\ndiscovering implicit domain orders in data through order dependencies. We\\nenumerate tractable special cases and proceed towards the most general case,\\nwhich we prove is NP-complete. We show that the general case nevertheless can\\nbe effectively handled by a SAT solver. We also devise an interestingness\\nmeasure to rank the discovered implicit domain orders, which we validate with a\\nuser study. Based on an extensive suite of experiments with real-world data, we\\nestablish the efficacy of our algorithms, and the utility of the domain orders\\ndiscovered by demonstrating significant added value in three applications (data\\nprofiling, query optimization, and data mining).', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2005.14068v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.14068v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2005.14213v2', updated=datetime.datetime(2020, 10, 30, 18, 9, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 5, 28, 18, 5, 46, tzinfo=datetime.timezone.utc), title='From WiscKey to Bourbon: A Learned Index for Log-Structured Merge Trees', authors=[arxiv.Result.Author('Yifan Dai'), arxiv.Result.Author('Yien Xu'), arxiv.Result.Author('Aishwarya Ganesan'), arxiv.Result.Author('Ramnatthan Alagappan'), arxiv.Result.Author('Brian Kroth'), arxiv.Result.Author('Andrea C. Arpaci-Dusseau'), arxiv.Result.Author('Remzi H. Arpaci-Dusseau')], summary='We introduce BOURBON, a log-structured merge (LSM) tree that utilizes machine\\nlearning to provide fast lookups. We base the design and implementation of\\nBOURBON on empirically-grounded principles that we derive through careful\\nanalysis of LSM design. BOURBON employs greedy piecewise linear regression to\\nlearn key distributions, enabling fast lookup with minimal computation, and\\napplies a cost-benefit strategy to decide when learning will be worthwhile.\\nThrough a series of experiments on both synthetic and real-world datasets, we\\nshow that BOURBON improves lookup performance by 1.23x-1.78x as compared to\\nstate-of-the-art production LSMs.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2005.14213v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.14213v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.00461v2', updated=datetime.datetime(2020, 12, 31, 7, 38, 17, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 1, 13, 1, 45, tzinfo=datetime.timezone.utc), title='Query Based Access Control for Linked Data', authors=[arxiv.Result.Author('Sabrina Kirrane'), arxiv.Result.Author('Alessandra Mileo'), arxiv.Result.Author('Axel Polleres'), arxiv.Result.Author('Stefan Decker')], summary='In recent years we have seen significant advances in the technology used to\\nboth publish and consume Linked Data. However, in order to support the next\\ngeneration of ebusiness applications on top of interlinked machine readable\\ndata suitable forms of access control need to be put in place. Although a\\nnumber of access control models and frameworks have been put forward, very\\nlittle research has been conducted into the security implications associated\\nwith granting access to partial data or the correctness of the proposed access\\ncontrol mechanisms. Therefore the contributions of this paper are two fold: we\\npropose a query rewriting algorithm which can be used to partially restrict\\naccess to SPARQL 1.1 queries and updates; and we demonstrate how a set of\\ncriteria, which was originally used to verify that an access control policy\\nholds over different database states, can be adapted to verify the correctness\\nof access control via query rewriting.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2007.00461v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.00461v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.01973v1', updated=datetime.datetime(2020, 7, 4, 0, 24, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 4, 0, 24, 2, tzinfo=datetime.timezone.utc), title='Detecting Opportunities for Differential Maintenance of Extracted Views', authors=[arxiv.Result.Author('Besat Kassaie'), arxiv.Result.Author('Frank Wm. Tompa')], summary='Semi-structured and unstructured data management is challenging, but many of\\nthe problems encountered are analogous to problems already addressed in the\\nrelational context. In the area of information extraction, for example, the\\nshift from engineering ad hoc, application-specific extraction rules towards\\nusing expressive languages such as CPSL and AQL creates opportunities to\\npropose solutions that can be applied to a wide range of extraction programs.\\nIn this work, we focus on extracted view maintenance, a problem that is\\nwell-motivated and thoroughly addressed in the relational setting. In\\nparticular, we formalize and address the problem of keeping extracted relations\\nconsistent with source documents that can be arbitrarily updated. We formally\\ncharacterize three classes of document updates, namely those that are\\nirrelevant, autonomously computable, and pseudo-irrelevant with respect to a\\ngiven extractor. Finally, we propose algorithms to detect pseudo-irrelevant\\ndocument updates with respect to extractors that are expressed as document\\nspanners, a model of information extraction inspired by SystemT.', comment='19 pages, 5 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.FL', 'H.0; I.7.0; F.4.3'], links=[arxiv.Result.Link('http://arxiv.org/abs/2007.01973v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.01973v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.03014v2', updated=datetime.datetime(2021, 11, 25, 15, 42, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 6, 19, 0, 36, tzinfo=datetime.timezone.utc), title='Topic-based Community Search over Spatial-Social Networks (Technical Report)', authors=[arxiv.Result.Author('Ahmed Al-Baghdadi'), arxiv.Result.Author('Xiang Lian')], summary='Recently, the community search problem has attracted significant attention,\\ndue to its wide spectrum of real-world applications such as event organization,\\nfriend recommendation, advertisement in e-commence, and so on. Given a query\\nvertex, the community search problem finds dense subgraph that contains the\\nquery vertex. In social networks, users have multiple check-in locations,\\ninfluence score, and profile information (keywords). Most previous studies that\\nsolve the CS problem over social networks usually neglect such information in a\\ncommunity. In this paper, we propose a novel problem, named community search\\nover spatial-social networks (TCS-SSN), which retrieves community with high\\nsocial influence, small traveling time, and covering certain keywords. In order\\nto tackle the TCS-SSN problem over the spatial-social networks, we design\\neffective pruning techniques to reduce the problem search space. We also\\npropose an effective indexing mechanism, namely social-spatial index, to\\nfacilitate the community query, and develop an efficient query answering\\nalgorithm via index traversal. We verify the efficiency and effectiveness of\\nour pruning techniques, indexing mechanism, and query processing algorithm\\nthrough extensive experiments on real-world and synthetic data sets under\\nvarious parameter settings.', comment=\"15 pages, 12 figures, and 3 tables. To appear in the PVLDB'20\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2007.03014v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.03014v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.04450v1', updated=datetime.datetime(2020, 7, 8, 21, 39, 36, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 8, 21, 39, 36, tzinfo=datetime.timezone.utc), title='T-REx: Table Repair Explanations', authors=[arxiv.Result.Author('Daniel Deutch'), arxiv.Result.Author('Nave Frost'), arxiv.Result.Author('Amir Gilad'), arxiv.Result.Author('Oren Sheffer')], summary='Data repair is a common and crucial step in many frameworks today, as\\napplications may use data from different sources and of different levels of\\ncredibility. Thus, this step has been the focus of many works, proposing\\ndiverse approaches. To assist users in understanding the output of such data\\nrepair algorithms, we propose T-REx, a system for providing data repair\\nexplanations through Shapley values. The system is generic and not specific to\\na given repair algorithm or approach: it treats the algorithm as a black box.\\nGiven a specific table cell selected by the user, T-REx employs Shapley values\\nto explain the significance of each constraint and each table cell in the\\nrepair of the cell of interest. T-REx then ranks the constraints and table\\ncells according to their importance in the repair of this cell. This\\nexplanation allows users to understand the repair process, as well as to act\\nbased on this knowledge, to modify the most influencing constraints or the\\noriginal database.', comment=None, journal_ref='In Proceedings of the 2020 ACM SIGMOD. Association for Computing\\n  Machinery, New York, NY, USA, pages: 2765 to 2768 (2020)', doi='10.1145/3318464.3384700', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3318464.3384700', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2007.04450v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.04450v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.04454v1', updated=datetime.datetime(2020, 7, 8, 22, 0, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 8, 22, 0, 1, tzinfo=datetime.timezone.utc), title='Explaining Natural Language Query Results', authors=[arxiv.Result.Author('Daniel Deutch'), arxiv.Result.Author('Nave Frost'), arxiv.Result.Author('Amir Gilad')], summary='Multiple lines of research have developed Natural Language (NL) interfaces\\nfor formulating database queries. We build upon this work, but focus on\\npresenting a highly detailed form of the answers in NL. The answers that we\\npresent are importantly based on the provenance of tuples in the query result,\\ndetailing not only the results but also their explanations. We develop a novel\\nmethod for transforming provenance information to NL, by leveraging the\\noriginal NL query structure. Furthermore, since provenance information is\\ntypically large and complex, we present two solutions for its effective\\npresentation as NL text: one that is based on provenance factorization, with\\nnovel desiderata relevant to the NL case, and one that is based on\\nsummarization. We have implemented our solution in an end-to-end system\\nsupporting questions, answers and provenance, all expressed in NL. Our\\nexperiments, including a user study, indicate the quality of our solution and\\nits scalability.', comment=None, journal_ref='The VLDB Journal 29, pp. 485--508 (2020)', doi='10.1007/s00778-019-00584-7', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/s00778-019-00584-7', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2007.04454v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.04454v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.04697v2', updated=datetime.datetime(2022, 6, 15, 8, 10, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 9, 10, 43, 28, tzinfo=datetime.timezone.utc), title='Open Data Quality Evaluation: A Comparative Analysis of Open Data in Latvia', authors=[arxiv.Result.Author('Anastasija Nikiforova')], summary='Nowadays open data is entering the mainstream - it is free available for\\nevery stakeholder and is often used in business decision-making. It is\\nimportant to be sure data is trustable and error-free as its quality problems\\ncan lead to huge losses. The research discusses how (open) data quality could\\nbe assessed. It also covers main points which should be considered developing a\\ndata quality management solution. One specific approach is applied to several\\nLatvian open data sets. The research provides a step-by-step open data sets\\nanalysis guide and summarizes its results. It is also shown there could exist\\ndifferences in data quality depending on data supplier (centralized and\\ndecentralized data releases) and, unfortunately, trustable data supplier cannot\\nguarantee data quality problems absence. There are also underlined common data\\nquality problems detected not only in Latvian open data but also in open data\\nof 3 European countries.', comment='24 pages, 2 tables, 3 figures, Baltic J. Modern Computing', journal_ref='Baltic J. Modern Computing, Vol. 6(2018), No. 4, 363-386', doi='10.22364/bjmc.2018.6.4.04', primary_category='cs.DB', categories=['cs.DB', 'cs.CY', 'cs.IR', 'stat.AP', 'stat.CO'], links=[arxiv.Result.Link('http://dx.doi.org/10.22364/bjmc.2018.6.4.04', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2007.04697v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.04697v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.05389v1', updated=datetime.datetime(2020, 7, 10, 13, 55, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 10, 13, 55, 9, tzinfo=datetime.timezone.utc), title='COBRA: Compression via Abstraction of Provenance for Hypothetical Reasoning', authors=[arxiv.Result.Author('Daniel Deutch'), arxiv.Result.Author('Yuval Moskovitch'), arxiv.Result.Author('Noam Rinetzky')], summary='Data analytics often involves hypothetical reasoning: repeatedly modifying\\nthe data and observing the induced effect on the computation result of a\\ndata-centric application. Recent work has proposed to leverage ideas from data\\nprovenance tracking towards supporting efficient hypothetical reasoning:\\ninstead of a costly re-execution of the underlying application, one may assign\\nvalues to a pre-computed provenance expression. A prime challenge in leveraging\\nthis approach for large-scale data and complex applications lies in the size of\\nthe provenance. To this end, we present a framework that allows to reduce\\nprovenance size. Our approach is based on reducing the provenance granularity\\nusing abstraction. We propose a demonstration of COBRA, a system that allows\\nexamine the effect of the provenance compression on the anticipated analysis\\nresults. We will demonstrate the usefulness of COBRA in the context of business\\ndata analysis.', comment=None, journal_ref='2019 IEEE 35th International Conference on Data Engineering\\n  (ICDE), Macao, Macao, 2019, pp. 2016--2019', doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2007.05389v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.05389v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.05400v1', updated=datetime.datetime(2020, 7, 10, 14, 5, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 10, 14, 5, 33, tzinfo=datetime.timezone.utc), title='Hypothetical Reasoning via Provenance Abstraction', authors=[arxiv.Result.Author('Daniel Deutch'), arxiv.Result.Author('Yuval Moskovitch'), arxiv.Result.Author('Noam Rinetzky')], summary='Data analytics often involves hypothetical reasoning: repeatedly modifying\\nthe data and observing the induced effect on the computation result of a\\ndata-centric application. Previous work has shown that fine-grained data\\nprovenance can help make such an analysis more efficient: instead of a costly\\nre-execution of the underlying application, hypothetical scenarios are applied\\nto a pre-computed provenance expression. However, storing provenance for\\ncomplex queries and large-scale data leads to a significant overhead, which is\\noften a barrier to the incorporation of provenance-based solutions.\\n  To this end, we present a framework that allows to reduce provenance size.\\nOur approach is based on reducing the provenance granularity using user defined\\nabstraction trees over the provenance variables; the granularity is based on\\nthe anticipated hypothetical scenarios. We formalize the tradeoff between\\nprovenance size and supported granularity of the hypothetical reasoning, and\\nstudy the complexity of the resulting optimization problem, provide efficient\\nalgorithms for tractable cases and heuristics for others. We experimentally\\nstudy the performance of our solution for various queries and abstraction\\ntrees. Our study shows that the algorithms generally lead to substantial\\nspeedup of hypothetical reasoning, with a reasonable loss of accuracy.', comment=None, journal_ref='Proceedings of the 2019 International Conference on Management of\\n  Data, SIGMOD Conference 2019, Amsterdam, The Netherlands, pages 537--554', doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2007.05400v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.05400v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.06540v2', updated=datetime.datetime(2022, 6, 15, 8, 20, 12, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 9, 11, 10, 22, tzinfo=datetime.timezone.utc), title='Open Data Quality', authors=[arxiv.Result.Author('Anastasija Nikiforova')], summary='The research discusses how (open) data quality could be described, what\\nshould be considered developing a data quality management solution and how it\\ncould be applied to open data to check its quality. The proposed approach\\nfocuses on development of data quality specification which can be executed to\\nget data quality evaluation results, find errors in data and possible problems\\nwhich must be solved. The proposed approach is applied to several open data\\nsets to evaluate their quality. Open data is very popular, free available for\\nevery stakeholder - it is often used to make business decisions. It is\\nimportant to be sure that this data is trustable and error-free as its quality\\nproblems can lead to huge losses.', comment='10 pages, 3 figures, 13th International Baltic Conference on\\n  Databases and Information Systems & The Baltic DB&IS 2018 Doctoral Consortium\\n  (Baltic DB&IS 2018) At: Lithuania, Trakai, Volume: 2158. arXiv admin note:\\n  substantial text overlap with arXiv:2007.04697', journal_ref='Baltic DB&IS 2018 Joint Proceedings of the Conference Forum and\\n  Doctoral Consortium', doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2007.06540v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.06540v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.07858v1', updated=datetime.datetime(2020, 7, 15, 17, 23, 48, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 15, 17, 23, 48, tzinfo=datetime.timezone.utc), title='Continuous Prefetch for Interactive Data Applications', authors=[arxiv.Result.Author('Haneen Mohammed'), arxiv.Result.Author('Ziyun Wei'), arxiv.Result.Author('Eugene Wu'), arxiv.Result.Author('Ravi Netravali')], summary=\"Interactive data visualization and exploration (DVE) applications are often\\nnetwork-bottlenecked due to bursty request patterns, large response sizes, and\\nheterogeneous deployments over a range of networks and devices. This makes it\\ndifficult to ensure consistently low response times (< 100ms). Khameleon is a\\nframework for DVE applications that uses a novel combination of prefetching and\\nresponse tuning to dynamically trade-off response quality for low latency.\\nKhameleon exploits DVE's approximation tolerance: immediate lower-quality\\nresponses are preferable to waiting for complete results. To this end,\\nKhameleon progressively encodes responses, and runs a server-side scheduler\\nthat proactively streams portions of responses using available bandwidth to\\nmaximize user's perceived interactivity. The scheduler involves a complex\\noptimization based on available resources, predicted user interactions, and\\nresponse quality levels; yet, decisions must also be real-time. To overcome\\nthis, Khameleon uses a fast greedy approximation which closely mimics the\\noptimal approach. Using image exploration and visualization applications with\\nreal user interaction traces, we show that across a wide range of network and\\nclient resource conditions, Khameleon outperforms classic prefetching\\napproaches that benefit from perfect prediction models: response latencies with\\nKhameleon are never higher, and typically between 2 to 3 orders of magnitude\\nlower while response quality remains within 50%-80%.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2007.07858v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.07858v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.08821v2', updated=datetime.datetime(2020, 8, 7, 13, 33, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 17, 8, 36, 26, tzinfo=datetime.timezone.utc), title='Tackling scalability issues in mining path patterns from knowledge graphs: a preliminary study', authors=[arxiv.Result.Author('Pierre Monnin'), arxiv.Result.Author('Emmanuel Bresso'), arxiv.Result.Author('Miguel Couceiro'), arxiv.Result.Author('Malika Smaïl-Tabbone'), arxiv.Result.Author('Amedeo Napoli'), arxiv.Result.Author('Adrien Coulet')], summary='Features mined from knowledge graphs are widely used within multiple\\nknowledge discovery tasks such as classification or fact-checking. Here, we\\nconsider a given set of vertices, called seed vertices, and focus on mining\\ntheir associated neighboring vertices, paths, and, more generally, path\\npatterns that involve classes of ontologies linked with knowledge graphs. Due\\nto the combinatorial nature and the increasing size of real-world knowledge\\ngraphs, the task of mining these patterns immediately entails scalability\\nissues. In this paper, we address these issues by proposing a pattern mining\\napproach that relies on a set of constraints (e.g., support or degree\\nthresholds) and the monotonicity property. As our motivation comes from the\\nmining of real-world knowledge graphs, we illustrate our approach with PGxLOD,\\na biomedical knowledge graph.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2007.08821v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.08821v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.09141v1', updated=datetime.datetime(2020, 7, 17, 17, 58, 34, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 17, 17, 58, 34, tzinfo=datetime.timezone.utc), title='Diversifying Anonymized Data with Diversity Constraints', authors=[arxiv.Result.Author('Mostafa Milani'), arxiv.Result.Author('Yu Huang'), arxiv.Result.Author('Fei Chiang')], summary='Recently introduced privacy legislation has aimed to restrict and control the\\namount of personal data published by companies and shared to third parties.\\nMuch of this real data is not only sensitive requiring anonymization, but also\\ncontains characteristic details from a variety of individuals. This diversity\\nis desirable in many applications ranging from Web search to drug and product\\ndevelopment. Unfortunately, data anonymization techniques have largely ignored\\ndiversity in its published result. This inadvertently propagates underlying\\nbias in subsequent data analysis. We study the problem of finding a diverse\\nanonymized data instance where diversity is measured via a set of diversity\\nconstraints. We formalize diversity constraints and study their foundations\\nsuch as implication and satisfiability. We show that determining the existence\\nof a diverse, anonymized instance can be done in PTIME, and we present a\\nclustering-based algorithm. We conduct extensive experiments using real and\\nsynthetic data showing the effectiveness of our techniques, and improvement\\nover existing baselines. Our work aligns with recent trends towards responsible\\ndata science by coupling diversity with privacy-preserving data publishing.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2007.09141v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.09141v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.09352v1', updated=datetime.datetime(2020, 7, 18, 7, 15, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 18, 7, 15, 32, tzinfo=datetime.timezone.utc), title='Graph-based process mining', authors=[arxiv.Result.Author('Amin Jalali')], summary=\"Process mining is an area of research that supports discovering information\\nabout business processes from their execution event logs. The increasing amount\\nof event logs in organizations challenges current process mining techniques,\\nwhich tend to load data into the memory of a computer. This issue limits the\\norganizations to apply process mining on a large scale and introduces risks due\\nto the lack of data management capabilities. Therefore, this paper introduces\\nand formalizes a new approach to store and retrieve event logs into/from graph\\ndatabases. It defines an algorithm to compute Directly Follows Graph (DFG)\\ninside the graph database, which shifts the heavy computation parts of process\\nmining into the graph database. Calculating DFG in graph databases enables\\nleveraging the graph databases' horizontal and vertical scaling capabilities in\\nfavor of applying process mining on a large scale. Besides, it removes the\\nrequirement to move data into analysts' computer. Thus, it enables using data\\nmanagement capabilities in graph databases. We implemented this approach in\\nNeo4j and evaluated its performance compared with current techniques using a\\nreal log file. The result shows that our approach enables the calculation of\\nDFG when the data is much bigger than the computational memory. It also shows\\nbetter performance when dicing data into small chunks.\", comment=None, journal_ref=None, doi='10.1007/978-3-030-72693-5_21', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-030-72693-5_21', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2007.09352v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.09352v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.10385v2', updated=datetime.datetime(2023, 3, 6, 7, 37, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 20, 18, 12, 21, tzinfo=datetime.timezone.utc), title='Support Aggregate Analytic Window Function over Large Data by Spilling', authors=[arxiv.Result.Author('Xing Shi'), arxiv.Result.Author('Chao Wang')], summary='Analytic function, also called window function, is to query the aggregation\\nof data over a sliding window. For example, a simple query over the online\\nstock platform is to return the average price of a stock of the last three\\ndays. These functions are commonly used features in SQL databases. They are\\nsupported in most of the commercial databases. With the increasing usage of\\ncloud data infra and machine learning technology, the frequency of queries with\\nanalytic window functions rises. Some analytic functions only require const\\nspace in memory to store the state, such as SUM, AVG, while others require\\nlinear space, such as MIN, MAX. When the window is extremely large, the memory\\nspace to store the state may be too large. In this case, we need to spill the\\nstate to disk, which is a heavy operation. In this paper, we proposed an\\nalgorithm to manipulate the state data in the disk to reduce the disk I/O to\\nmake spill available and efficienct. We analyze the complexity of the algorithm\\nwith different data distribution.', comment='6 pages, conference', journal_ref=None, doi='10.1088/1742-6596/1673/1/012001', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1088/1742-6596/1673/1/012001', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2007.10385v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.10385v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.10568v3', updated=datetime.datetime(2022, 7, 27, 2, 2, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 21, 2, 28, 59, tzinfo=datetime.timezone.utc), title='Buffer Pool Aware Query Scheduling via Deep Reinforcement Learning', authors=[arxiv.Result.Author('Chi Zhang'), arxiv.Result.Author('Ryan Marcus'), arxiv.Result.Author('Anat Kleiman'), arxiv.Result.Author('Olga Papaemmanouil')], summary='In this extended abstract, we propose a new technique for query scheduling\\nwith the explicit goal of reducing disk reads and thus implicitly increasing\\nquery performance. We introduce SmartQueue, a learned scheduler that leverages\\noverlapping data reads among incoming queries and learns a scheduling strategy\\nthat improves cache hits. SmartQueue relies on deep reinforcement learning to\\nproduce workload-specific scheduling strategies that focus on long-term\\nperformance benefits while being adaptive to previously-unseen data access\\npatterns. We present results from a proof-of-concept prototype, demonstrating\\nthat learned schedulers can offer significant performance improvements over\\nhand-crafted scheduling heuristics. Ultimately, we make the case that this is a\\npromising research direction at the intersection of machine learning and\\ndatabases.', comment=None, journal_ref='AIDB@VLDB 2020 Proceedings of the 2nd International Workshop on\\n  Applied AI for Database Systems and Applications', doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2007.10568v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.10568v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.11881v1', updated=datetime.datetime(2020, 7, 23, 9, 40, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 23, 9, 40, 50, tzinfo=datetime.timezone.utc), title='Reachability Queries with Label and Substructure Constraints on Knowledge Graphs', authors=[arxiv.Result.Author('Xiaolong Wan'), arxiv.Result.Author('Hongzhi Wang')], summary='Since knowledge graphs (KGs) describe and model the relationships between\\nentities and concepts in the real world, reasoning on KGs often correspond to\\nthe reachability queries with label and substructure constraints (LSCR).\\nSpecially, for a search path p, LSCR queries not only require that the labels\\nof the edges passed by p are in a certain label set, but also claim that a\\nvertex in p could satisfy a certain substructure constraint. LSCR queries is\\nmuch more complex than the label-constraint reachability (LCR) queries, and\\nthere is no efficient solution for LSCR queries on KGs, to the best of our\\nknowledge. Motivated by this, we introduce two solutions for such queries on\\nKGs, UIS and INS. The former can also be utilized for general edge-labeled\\ngraphs, and is relatively handy for practical implementation. The latter is an\\nefficient local-index-based informed search strategy. An extensive experimental\\nevaluation, on both synthetic and real KGs, illustrates that our solutions can\\nefficiently process LSCR queries on KGs.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2007.11881v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.11881v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.13053v3', updated=datetime.datetime(2022, 9, 21, 13, 39, 44, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 26, 4, 42, 44, tzinfo=datetime.timezone.utc), title='Recursive Rules with Aggregation: A Simple Unified Semantics', authors=[arxiv.Result.Author('Yanhong A. Liu'), arxiv.Result.Author('Scott D. Stoller')], summary='Complex reasoning problems are most clearly and easily specified using\\nlogical rules, but require recursive rules with aggregation such as count and\\nsum for practical applications. Unfortunately, the meaning of such rules has\\nbeen a significant challenge, leading to many disagreeing semantics.\\n  This paper describes a unified semantics for recursive rules with\\naggregation, extending the unified founded semantics and constraint semantics\\nfor recursive rules with negation. The key idea is to support simple expression\\nof the different assumptions underlying different semantics, and orthogonally\\ninterpret aggregation operations using their simple usual meaning. We present a\\nformal definition of the semantics, prove important properties of the\\nsemantics, and compare with prior semantics. In particular, we present an\\nefficient inference over aggregation that gives precise answers to all examples\\nwe have studied from the literature. We also apply our semantics to a wide\\nrange of challenging examples, and show that our semantics is simple and\\nmatches the desired results in all cases. Finally, we describe experiments on\\nthe most challenging examples, exhibiting unexpectedly superior performance\\nover well-known systems when they can compute correct answers.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LO'], links=[arxiv.Result.Link('http://arxiv.org/abs/2007.13053v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.13053v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.14009v5', updated=datetime.datetime(2022, 7, 28, 6, 25, 49, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 28, 6, 24, 39, tzinfo=datetime.timezone.utc), title='The Minimum Description Length Principle for Pattern Mining: A Survey', authors=[arxiv.Result.Author('Esther Galbrun')], summary='This is about the Minimum Description Length (MDL) principle applied to\\npattern mining. The length of this description is kept to the minimum.\\n  Mining patterns is a core task in data analysis and, beyond issues of\\nefficient enumeration, the selection of patterns constitutes a major challenge.\\nThe MDL principle, a model selection method grounded in information theory, has\\nbeen applied to pattern mining with the aim to obtain compact high-quality sets\\nof patterns. After giving an outline of relevant concepts from information\\ntheory and coding, as well as of work on the theory behind the MDL and similar\\nprinciples, we review MDL-based methods for mining various types of data and\\npatterns. Finally, we open a discussion on some issues regarding these methods,\\nand highlight currently active related data analysis problems.', comment='Data Min Knowl Disc (2022)', journal_ref=None, doi='10.1007/s10618-022-00846-z', primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.IT', 'math.IT'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/s10618-022-00846-z', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2007.14009v5', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.14009v5', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.14244v1', updated=datetime.datetime(2020, 7, 25, 14, 36, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 25, 14, 36, 55, tzinfo=datetime.timezone.utc), title='Automated Database Indexing using Model-free Reinforcement Learning', authors=[arxiv.Result.Author('Gabriel Paludo Licks'), arxiv.Result.Author('Felipe Meneguzzi')], summary='Configuring databases for efficient querying is a complex task, often carried\\nout by a database administrator. Solving the problem of building indexes that\\ntruly optimize database access requires a substantial amount of database and\\ndomain knowledge, the lack of which often results in wasted space and memory\\nfor irrelevant indexes, possibly jeopardizing database performance for querying\\nand certainly degrading performance for updating. We develop an architecture to\\nsolve the problem of automatically indexing a database by using reinforcement\\nlearning to optimize queries by indexing data throughout the lifetime of a\\ndatabase. In our experimental evaluation, our architecture shows superior\\nperformance compared to related work on reinforcement learning and genetic\\nalgorithms, maintaining near-optimal index configurations and efficiently\\nscaling to large databases.', comment='8 pages, 5 figures (some have subfigures), 1 table', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'I.2.6; H.2.4'], links=[arxiv.Result.Link('http://arxiv.org/abs/2007.14244v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.14244v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2007.14997v2', updated=datetime.datetime(2023, 3, 6, 7, 44, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 29, 9, 47, 54, tzinfo=datetime.timezone.utc), title='Aggregate Analytic Window Query over Spatial Data', authors=[arxiv.Result.Author('Xing Shi'), arxiv.Result.Author('Chao Wang')], summary='Analytic window query is a commonly used query in the relational databases.\\nIt answers the aggregations of data over a sliding window. For example, to get\\nthe average prices of a stock for each day. However, it is not supported in the\\nspatial databases. Because the spatial data are not in a one-dimension space,\\nthere is no straightforward way to extend the original analytic window query to\\nspatial databases. But these queries are useful and meaningful. For example, to\\nfind the average number of visits for all the POIs in the circle with a fixed\\nradius for each POI as the centre. In this paper, we define the aggregate\\nanalytic window query over spatial data and propose algorithms for grid index\\nand tree-index. We also analyze the complexity of the algorithms to prove they\\nare efficient and practical.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2007.14997v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.14997v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.00035v1', updated=datetime.datetime(2020, 8, 31, 18, 4, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 31, 18, 4, 20, tzinfo=datetime.timezone.utc), title='The Data Station: Combining Data, Compute, and Market Forces', authors=[arxiv.Result.Author('Raul Castro Fernandez'), arxiv.Result.Author('Kyle Chard'), arxiv.Result.Author('Ben Blaiszik'), arxiv.Result.Author('Sanjay Krishnan'), arxiv.Result.Author('Aaron Elmore'), arxiv.Result.Author('Ziad Obermeyer'), arxiv.Result.Author('Josh Risley'), arxiv.Result.Author('Sendhil Mullainathan'), arxiv.Result.Author('Michael Franklin'), arxiv.Result.Author('Ian Foster')], summary='This paper introduces Data Stations, a new data architecture that we are\\ndesigning to tackle some of the most challenging data problems that we face\\ntoday: access to sensitive data; data discovery and integration; and governance\\nand compliance. Data Stations depart from modern data lakes in that both data\\nand derived data products, such as machine learning models, are sealed and\\ncannot be directly seen, accessed, or downloaded by anyone. Data Stations do\\nnot deliver data to users; instead, users bring questions to data. This\\ninversion of the usual relationship between data and compute mitigates many of\\nthe security risks that are otherwise associated with sharing and working with\\nsensitive data.\\n  Data Stations are designed following the principle that many data problems\\nrequire human involvement, and that incentives are the key to obtaining such\\ninvolvement. To that end, Data Stations implement market designs to create,\\nmanage, and coordinate the use of incentives. We explain the motivation for\\nthis new kind of platform and its design.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.00035v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.00035v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.00061v1', updated=datetime.datetime(2020, 8, 31, 18, 59, 58, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 31, 18, 59, 58, tzinfo=datetime.timezone.utc), title='SparkGOR: A unified framework for genomic data analysis', authors=[arxiv.Result.Author('Sigmar K. Stefánsson'), arxiv.Result.Author('Hákon Guðbjartsson')], summary='Motivation: Our goal was to combine the capabilities of Spark and GOR into a\\nsingle computing framework for use in analysis of large scale genome data.\\n  Results: We have created a relational query engine that unites SparkSQL and\\nGORpipe into a single declarative query framework. This has been achieved by\\nallowing embedding of SQL expressions into the high-level relational statement\\nsyntax in GOR and by supporting virtual relations and nested GORpipe\\nexpressions within SQL. Furthermore, we have built drivers to enable Spark and\\nGOR to use and leverage their preferred file formats, Parquet and GORZ\\nrespectively, and introduced APIs to allow the use of GOR with Spark\\ndataframes.\\n  Availability: The SparkGOR version of the GORpipe software is open-source and\\nfreely available at https://gorpipe-website.now.sh and\\nhttps://github.com/gorpipe.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.00061v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.00061v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.00099v1', updated=datetime.datetime(2020, 8, 31, 21, 5, 21, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 8, 31, 21, 5, 21, tzinfo=datetime.timezone.utc), title='Interactive and Explainable Point-of-Interest Recommendation using Look-alike Groups', authors=[arxiv.Result.Author('Behrooz Omidvar-Tehrani'), arxiv.Result.Author('Sruthi Viswanathan'), arxiv.Result.Author('Jean-Michel Renders')], summary='Recommending Points-of-Interest (POIs) is surfacing in many location-based\\napplications. The literature contains personalized and socialized POI\\nrecommendation approaches which employ historical check-ins and social links to\\nmake recommendations. However these systems still lack customizability\\n(incorporating session-based user interactions with the system) and\\ncontextuality (incorporating the situational context of the user), particularly\\nin cold start situations, where nearly no user information is available. In\\nthis paper, we propose LikeMind, a POI recommendation system which tackles the\\nchallenges of cold start, customizability, contextuality, and explainability by\\nexploiting look-alike groups mined in public POI datasets. LikeMind\\nreformulates the problem of POI recommendation, as recommending explainable\\nlook-alike groups (and their POIs) which are in line with user\\'s interests.\\nLikeMind frames the task of POI recommendation as an exploratory process where\\nusers interact with the system by expressing their favorite POIs, and their\\ninteractions impact the way look-alike groups are selected out. Moreover,\\nLikeMind employs \"mindsets\", which capture actual situation and intent of the\\nuser, and enforce the semantics of POI interestingness. In an extensive set of\\nexperiments, we show the quality of our approach in recommending relevant\\nlook-alike groups and their POIs, in terms of efficiency and effectiveness.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.00099v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.00099v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.00166v1', updated=datetime.datetime(2020, 9, 1, 1, 25, 37, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 1, 1, 25, 37, tzinfo=datetime.timezone.utc), title='ParIS+: Data Series Indexing on Multi-Core Architectures', authors=[arxiv.Result.Author('Botao Peng'), arxiv.Result.Author('Panagiota Fatourou'), arxiv.Result.Author('Themis Palpanas')], summary='Data series similarity search is a core operation for several data series\\nanalysis applications across many different domains. Nevertheless, even\\nstate-of-the-art techniques cannot provide the time performance required for\\nlarge data series collections. We propose ParIS and ParIS+, the first\\ndisk-based data series indices carefully designed to inherently take advantage\\nof multi-core architectures, in order to accelerate similarity search\\nprocessing times. Our experiments demonstrate that ParIS+ completely removes\\nthe CPU latency during index construction for disk-resident data, and for exact\\nquery answering is up to 1 order of magnitude faster than the current state of\\nthe art index scan method, and up to 3 orders of magnitude faster than the\\noptimized serial scan method. ParIS+ (which is an evolution of the ADS+ index)\\nowes its efficiency to the effective use of multi-core and multi-socket\\narchitectures, in order to distribute and execute in parallel both index\\nconstruction and query answering, and to the exploitation of the Single\\nInstruction Multiple Data (SIMD) capabilities of modern CPUs, in order to\\nfurther parallelize the execution of instructions inside each core.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.00166v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.00166v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.00524v3', updated=datetime.datetime(2021, 8, 9, 8, 35, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 1, 15, 51, 24, tzinfo=datetime.timezone.utc), title='Tensor Relational Algebra for Machine Learning System Design', authors=[arxiv.Result.Author('Binhang Yuan'), arxiv.Result.Author('Dimitrije Jankov'), arxiv.Result.Author('Jia Zou'), arxiv.Result.Author('Yuxin Tang'), arxiv.Result.Author('Daniel Bourgeois'), arxiv.Result.Author('Chris Jermaine')], summary='We consider the question: what is the abstraction that should be implemented\\nby the computational engine of a machine learning system? Current machine\\nlearning systems typically push whole tensors through a series of compute\\nkernels such as matrix multiplications or activation functions, where each\\nkernel runs on an AI accelerator (ASIC) such as a GPU. This implementation\\nabstraction provides little built-in support for ML systems to scale past a\\nsingle machine, or for handling large models with matrices or tensors that do\\nnot easily fit into the RAM of an ASIC. In this paper, we present an\\nalternative implementation abstraction called the tensor relational algebra\\n(TRA). The TRA is a set-based algebra based on the relational algebra.\\nExpressions in the TRA operate over binary tensor relations, where keys are\\nmulti-dimensional arrays and values are tensors. The TRA is easily executed\\nwith high efficiency in a parallel or distributed environment, and amenable to\\nautomatic optimization. Our empirical study shows that the optimized TRA-based\\nback-end can significantly outperform alternatives for running ML workflows in\\ndistributed clusters.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.00524v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.00524v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.00786v1', updated=datetime.datetime(2020, 9, 2, 2, 10, 18, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 2, 2, 10, 18, tzinfo=datetime.timezone.utc), title='MESSI: In-Memory Data Series Indexing', authors=[arxiv.Result.Author('Botao Peng'), arxiv.Result.Author('Panagiota Fatourou'), arxiv.Result.Author('Themis Palpanas')], summary='Data series similarity search is a core operation for several data series\\nanalysis applications across many different domains. However, the\\nstate-of-the-art techniques fail to deliver the time performance required for\\ninteractive exploration, or analysis of large data series collections. In this\\nwork, we propose MESSI, the first data series index designed for in-memory\\noperation on modern hardware. Our index takes advantage of the modern hardware\\nparallelization opportunities (i.e., SIMD instructions, multi-core and\\nmulti-socket architectures), in order to accelerate both index construction and\\nsimilarity search processing times. Moreover, it benefits from a careful design\\nin the setup and coordination of the parallel workers and data structures, so\\nthat it maximizes its performance for in-memory operations. Our experiments\\nwith synthetic and real datasets demonstrate that overall MESSI is up to 4x\\nfaster at index construction, and up to 11x faster at query answering than the\\nstate-of-the-art parallel approach. MESSI is the first to answer exact\\nsimilarity search queries on 100GB datasets in _50msec (30-75msec across\\ndiverse datasets), which enables real-time, interactive data exploration on\\nvery large data series collections.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.00786v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.00786v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.01614v1', updated=datetime.datetime(2020, 9, 2, 2, 26, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 2, 2, 26, 19, tzinfo=datetime.timezone.utc), title='Data Series Indexing Gone Parallel', authors=[arxiv.Result.Author('Botao Peng')], summary='Data series similarity search is a core operation for several data series\\nanalysis applications across many different domains. However, the\\nstate-of-the-art techniques fail to deliver the time performance required for\\ninteractive exploration, or analysis of large data series collections. In this\\nPh.D. work, we present the first data series indexing solutions, for both\\non-disk and in-memory data, that are designed to inherently take advantage of\\nmulti-core architectures, in order to accelerate similarity search processing\\ntimes. Our experiments on a variety of synthetic and real data demonstrate that\\nour approaches are up to orders of magnitude faster than the alternatives. More\\nspecifically, our on-disk solution can answer exact similarity search queries\\non 100GB datasets in a few seconds, and our in-memory solution in a few\\nmilliseconds, which enables real-time, interactive data exploration on very\\nlarge data series collections.', comment='arXiv admin note: substantial text overlap with arXiv:2009.00166,\\n  arXiv: 2009.00786', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.01614v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.01614v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.01769v1', updated=datetime.datetime(2020, 9, 2, 13, 8, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 2, 13, 8, 55, tzinfo=datetime.timezone.utc), title='HyperBench: A Benchmark and Tool for Hypergraphs and Empirical Findings', authors=[arxiv.Result.Author('Wolfgang Fischl'), arxiv.Result.Author('Georg Gottlob'), arxiv.Result.Author('Davide Mario Longo'), arxiv.Result.Author('Reinhard Pichler')], summary='To cope with the intractability of answering Conjunctive Queries (CQs) and\\nsolving Constraint Satisfaction Problems (CSPs), several notions of hypergraph\\ndecompositions have been proposed -- giving rise to different notions of width,\\nnoticeably, plain, generalized, and fractional hypertree width (hw, ghw, and\\nfhw). Given the increasing interest in using such decomposition methods in\\npractice, a publicly accessible repository of decomposition software, as well\\nas a large set of benchmarks, and a web-accessible workbench for inserting,\\nanalyzing, and retrieving hypergraphs are called for.\\n  We address this need by providing (i) concrete implementations of hypergraph\\ndecompositions (including new practical algorithms), (ii) a new, comprehensive\\nbenchmark of hypergraphs stemming from disparate CQ and CSP collections, and\\n(iii) HyperBench, our new web-inter\\\\-face for accessing the benchmark and the\\nresults of our analyses. In addition, we describe a number of actual\\nexperiments we carried out with this new infrastructure.', comment='arXiv admin note: substantial text overlap with arXiv:1811.08181', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.01769v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.01769v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.02258v1', updated=datetime.datetime(2020, 9, 4, 15, 38, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 4, 15, 38, 27, tzinfo=datetime.timezone.utc), title='AnyDB: An Architecture-less DBMS for Any Workload', authors=[arxiv.Result.Author('Tiemo Bang'), arxiv.Result.Author('Norman May'), arxiv.Result.Author('Ilia Petrov'), arxiv.Result.Author('Carsten Binnig')], summary='In this paper, we propose a radical new approach for scale-out distributed\\nDBMSs. Instead of hard-baking an architectural model, such as a shared-nothing\\narchitecture, into the distributed DBMS design, we aim for a new class of\\nso-called architecture-less DBMSs. The main idea is that an architecture-less\\nDBMS can mimic any architecture on a per-query basis on-the-fly without any\\nadditional overhead for reconfiguration. Our initial results show that our\\narchitecture-less DBMS AnyDB can provide significant speed-ups across varying\\nworkloads compared to a traditional DBMS implementing a static architecture.', comment='Submitted to 11th Annual Conference on Innovative Data Systems\\n  Research (CIDR 21)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.02258v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.02258v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.02678v2', updated=datetime.datetime(2020, 9, 8, 10, 9, 25, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 6, 9, 6, 13, tzinfo=datetime.timezone.utc), title='Universal Layout Emulation for Long-Term Database Archival', authors=[arxiv.Result.Author('Raja Appuswamy'), arxiv.Result.Author('Vincent Joguin')], summary=\"Research on alternate media technologies, like film, synthetic DNA, and\\nglass, for long-term data archival has received a lot of attention recently due\\nto the media obsolescence issues faced by contemporary storage media like tape,\\nHard Disk Drives (HDD), and Solid State Disks (SSD). While researchers have\\ndeveloped novel layout and encoding techniques for archiving databases on these\\nnew media types, one key question remains unaddressed: How do we ensure that\\nthe decoders developed today will be available and executable by a user who is\\nrestoring an archived database several decades later in the future, on a\\ncomputing platform that potentially does not even exist today?\\n  In this paper, we make the case for Universal Layout Emulation (ULE), a new\\napproach for future-proof, long-term database archival that advocates archiving\\ndecoders together with the data to ensure successful recovery. In order to do\\nso, ULE brings together concepts from Data Management and Digital Preservation\\ncommunities by using emulation for archiving decoders. In order to show that\\nULE can be implemented in practice, we present the design and evaluation of\\nMicr'Olonys, an end-to-end long-term database archival system that can be used\\nto archive databases using visual analog media like film, microform, and\\narchival paper.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.02678v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.02678v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.03776v1', updated=datetime.datetime(2020, 9, 8, 14, 8, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 8, 14, 8, 14, tzinfo=datetime.timezone.utc), title='Sequenced Route Query with Semantic Hierarchy', authors=[arxiv.Result.Author('Yuya Sasaki'), arxiv.Result.Author('Yoshiharu Ishikawa'), arxiv.Result.Author('Yasuhiro Fujiwara'), arxiv.Result.Author('Makoto Onizuka')], summary='The trip planning query searches for preferred routes starting from a given\\npoint through multiple Point-of-Interests (PoI) that match user requirements.\\nAlthough previous studies have investigated trip planning queries, they lack\\nflexibility for finding routes because all of them output routes that strictly\\nmatch user requirements. We study trip planning queries that output multiple\\nroutes in a flexible manner. We propose a new type of query called skyline\\nsequenced route (SkySR) query, which searches for all preferred sequenced\\nroutes to users by extending the shortest route search with the semantic\\nsimilarity of PoIs in the route. Flexibility is achieved by the {\\\\it semantic\\nhierarchy} of the PoI category. We propose an efficient algorithm for the SkySR\\nquery, bulk SkySR algorithm that simultaneously searches for sequenced routes\\nand prunes unnecessary routes effectively. Experimental evaluations show that\\nthe proposed approach significantly outperforms the existing approaches in\\nterms of response time (up to four orders of magnitude). Moreover, we develop a\\nprototype service that uses the SkySR query, and conduct a user test to\\nevaluate its usefulness.', comment=None, journal_ref='EDBT2018', doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.03776v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.03776v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.04283v2', updated=datetime.datetime(2021, 5, 5, 14, 23, 46, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 9, 13, 5, 32, tzinfo=datetime.timezone.utc), title='Graph-based keyword search in heterogeneous data sources', authors=[arxiv.Result.Author('Mhd Yamen Haddad'), arxiv.Result.Author('Angelos Anadiotis'), arxiv.Result.Author('Yamen Mhd'), arxiv.Result.Author('Ioana Manolescu')], summary='Data journalism is the field of investigative journalism which focuses on\\ndigital data by treating them as first-class citizens. Following the trends in\\nhuman activity, which leaves strong digital traces, data journalism becomes\\nincreasingly important. However, as the number and the diversity of data\\nsources increase, heterogeneous data models with different structure, or even\\nno structure at all, need to be considered in query answering. Inspired by our\\ncollaboration with Le Monde, a leading French newspaper, we designed a novel\\nquery algorithm for exploiting such heterogeneous corpora through keyword\\nsearch. We model our underlying data as graphs and, given a set of search\\nterms, our algorithm nds links between them within and across the heterogeneous\\ndatasets included in the graph. We draw inspiration from prior work on keyword\\nsearch in structured and unstructured data, which we extend with the data\\nheterogeneity dimension, which makes the keyword search problem computationally\\nharder. We implement our algorithm and we evaluate its performance using\\nsynthetic and real-world datasets.', comment='Informal publication only', journal_ref=\"36{\\\\`e}me Conf{\\\\'e}rence sur la Gestion de Donn{\\\\'e}es --\\n  Principes, Technologies et Applications (BDA 2020), Oct 2020, Online, France\", doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.04283v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.04283v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.04611v1', updated=datetime.datetime(2020, 9, 10, 0, 4, 53, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 10, 0, 4, 53, tzinfo=datetime.timezone.utc), title='Subscribing to Big Data at Scale', authors=[arxiv.Result.Author('Xikui Wang'), arxiv.Result.Author('Michael J. Carey'), arxiv.Result.Author('Vassilis J. Tsotras')], summary='Today, data is being actively generated by a variety of devices, services,\\nand applications. Such data is important not only for the information that it\\ncontains, but also for its relationships to other data and to interested users.\\nMost existing Big Data systems focus on passively answering queries from users,\\nrather than actively collecting data, processing it, and serving it to users.\\nTo satisfy both passive and active requests at scale, users need either to\\nheavily customize an existing passive Big Data system or to glue multiple\\nsystems together. Either choice would require significant effort from users and\\nincur additional overhead. In this paper, we present the BAD (Big Active Data)\\nsystem, which is designed to preserve the merits of passive Big Data systems\\nand introduce new features for actively serving Big Data to users at scale. We\\nshow the design and implementation of the BAD system, demonstrate how BAD\\nfacilitates providing both passive and active data services, investigate the\\nBAD system\\'s performance at scale, and illustrate the complexities that would\\nresult from instead providing BAD-like services with a \"glued\" system.', comment='36 pages, 47 figures, submitted to TOCS', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.04611v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.04611v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.06194v1', updated=datetime.datetime(2020, 9, 14, 4, 44, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 14, 4, 44, 59, tzinfo=datetime.timezone.utc), title='SPARQL with XQuery-based Filtering', authors=[arxiv.Result.Author('Takahiro Komamizu')], summary='Linked Open Data (LOD) has been proliferated over various domains, however,\\nthere are still lots of open data in various format other than RDF, a standard\\ndata description framework in LOD. These open data can also be connected to\\nentities in LOD when they are associated with URIs. Document-centric XML data\\nare such open data that are connected with entities in LOD as supplemental\\ndocuments for these entities, and to convert these XML data into RDF requires\\nvarious techniques such as information extraction, ontology design and ontology\\nmapping with human prior knowledge. To utilize document-centric XML data linked\\nfrom entities in LOD, in this paper, a SPARQL-based seamless access method on\\nRDF and XML data is proposed. In particular, an extension to SPARQL,\\nXQueryFILTER, which enables XQuery as a filter in SPARQL is proposed. For\\nefficient query processing of the combination of SPARQL and XQuery, a database\\ntheory-based query optimization is proposed. Real-world scenario-based\\nexperiments in this paper showcase that effectiveness of XQueryFILTER and\\nefficiency of the optimization.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.06194v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.06194v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.06625v2', updated=datetime.datetime(2020, 11, 2, 3, 39, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 13, 16, 0, 21, tzinfo=datetime.timezone.utc), title='Revealing Secrets in SPARQL Session Level', authors=[arxiv.Result.Author('Xinyue Zhang'), arxiv.Result.Author('Meng Wang'), arxiv.Result.Author('Muhammad Saleem'), arxiv.Result.Author('Axel-Cyrille Ngonga Ngomo'), arxiv.Result.Author('Guilin Qi'), arxiv.Result.Author('Haofen Wang')], summary='Based on Semantic Web technologies, knowledge graphs help users to discover\\ninformation of interest by using live SPARQL services. Answer-seekers often\\nexamine intermediate results iteratively and modify SPARQL queries repeatedly\\nin a search session. In this context, understanding user behaviors is critical\\nfor effective intention prediction and query optimization. However, these\\nbehaviors have not yet been researched systematically at the SPARQL session\\nlevel. This paper reveals secrets of session-level user search behaviors by\\nconducting a comprehensive investigation over massive real-world SPARQL query\\nlogs. In particular, we thoroughly assess query changes made by users w.r.t.\\nstructural and data-driven features of SPARQL queries. To illustrate the\\npotentiality of our findings, we employ an application example of how to use\\nour findings, which might be valuable to devise efficient SPARQL caching,\\nauto-completion, query suggestion, approximation, and relaxation techniques in\\nthe future.', comment='18 pages. Accepted by ISWC 2020', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.06625v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.06625v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.08150v2', updated=datetime.datetime(2020, 11, 15, 14, 16, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 17, 8, 34, 51, tzinfo=datetime.timezone.utc), title='Extensible Data Skipping', authors=[arxiv.Result.Author('Paula Ta-Shma'), arxiv.Result.Author('Guy Khazma'), arxiv.Result.Author('Gal Lushi'), arxiv.Result.Author('Oshrit Feder')], summary='Data skipping reduces I/O for SQL queries by skipping over irrelevant data\\nobjects (files) based on their metadata. We extend this notion by allowing\\ndevelopers to define their own data skipping metadata types and indexes using a\\nflexible API. Our framework is the first to natively support data skipping for\\narbitrary data types (e.g. geospatial, logs) and queries with User Defined\\nFunctions (UDFs). We integrated our framework with Apache Spark and it is now\\ndeployed across multiple products/services at IBM. We present our extensible\\ndata skipping APIs, discuss index design, and implement various metadata\\nindexes, requiring only around 30 lines of additional code per index. In\\nparticular we implement data skipping for a third party library with geospatial\\nUDFs and demonstrate speedups of two orders of magnitude. Our centralized\\nmetadata approach provides a x3.6 speed up even when compared to queries which\\nare rewritten to exploit Parquet min/max metadata. We demonstrate that\\nextensible data skipping is applicable to broad class of applications, where\\nuser defined indexes achieve significant speedups and cost savings with very\\nlow development cost.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.08150v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.08150v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.08791v1', updated=datetime.datetime(2020, 9, 17, 15, 39, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 17, 15, 39, 45, tzinfo=datetime.timezone.utc), title='Multi-source Data Mining for e-Learning', authors=[arxiv.Result.Author('Julie Bu Daher'), arxiv.Result.Author('Armelle Brun'), arxiv.Result.Author('Anne Boyer')], summary='Data mining is the task of discovering interesting, unexpected or valuable\\nstructures in large datasets and transforming them into an understandable\\nstructure for further use . Different approaches in the domain of data mining\\nhave been proposed, among which pattern mining is the most important one.\\nPattern mining mining involves extracting interesting frequent patterns from\\ndata. Pattern mining has grown to be a topic of high interest where it is used\\nfor different purposes, for example, recommendations. Some of the most common\\nchallenges in this domain include reducing the complexity of the process and\\navoiding the redundancy within the patterns. So far, pattern mining has mainly\\nfocused on the mining of a single data source. However, with the increase in\\nthe amount of data, in terms of volume, diversity of sources and nature of\\ndata, mining multi-source and heterogeneous data has become an emerging\\nchallenge in this domain. This challenge is the main focus of our work where we\\npropose to mine multi-source data in order to extract interesting frequent\\npatterns.', comment=None, journal_ref='7th International Symposium \"From Data to Models and Back\\n  (DataMod)\" 2018 Jun 25', doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.08791v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.08791v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.09822v3', updated=datetime.datetime(2021, 1, 8, 0, 0, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 18, 15, 36, 43, tzinfo=datetime.timezone.utc), title='TODS: An Automated Time Series Outlier Detection System', authors=[arxiv.Result.Author('Kwei-Herng Lai'), arxiv.Result.Author('Daochen Zha'), arxiv.Result.Author('Guanchu Wang'), arxiv.Result.Author('Junjie Xu'), arxiv.Result.Author('Yue Zhao'), arxiv.Result.Author('Devesh Kumar'), arxiv.Result.Author('Yile Chen'), arxiv.Result.Author('Purav Zumkhawaka'), arxiv.Result.Author('Minyang Wan'), arxiv.Result.Author('Diego Martinez'), arxiv.Result.Author('Xia Hu')], summary='We present TODS, an automated Time Series Outlier Detection System for\\nresearch and industrial applications. TODS is a highly modular system that\\nsupports easy pipeline construction. The basic building block of TODS is\\nprimitive, which is an implementation of a function with hyperparameters. TODS\\ncurrently supports 70 primitives, including data processing, time series\\nprocessing, feature analysis, detection algorithms, and a reinforcement module.\\nUsers can freely construct a pipeline using these primitives and perform end-\\nto-end outlier detection with the constructed pipeline. TODS provides a\\nGraphical User Interface (GUI), where users can flexibly design a pipeline with\\ndrag-and-drop. Moreover, a data-driven searcher is provided to automatically\\ndiscover the most suitable pipelines given a dataset. TODS is released under\\nApache 2.0 license at https://github.com/datamllab/tods.', comment=\"Accepted by AAAI'21 demo track\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG', 'stat.ML'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.09822v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.09822v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.09883v1', updated=datetime.datetime(2020, 9, 21, 14, 5, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 21, 14, 5, 5, tzinfo=datetime.timezone.utc), title='Selectivity Estimation with Attribute Value Dependencies using Linked Bayesian Networks', authors=[arxiv.Result.Author('Max Halford'), arxiv.Result.Author('Philippe Saint-Pierre'), arxiv.Result.Author('Franck Morvan')], summary='Relational query optimisers rely on cost models to choose between different\\nquery execution plans. Selectivity estimates are known to be a crucial input to\\nthe cost model. In practice, standard selectivity estimation procedures are\\nprone to large errors. This is mostly because they rely on the so-called\\nattribute value independence and join uniformity assumptions. Therefore,\\nmultidimensional methods have been proposed to capture dependencies between two\\nor more attributes both within and across relations. However, these methods\\nrequire a large computational cost which makes them unusable in practice. We\\npropose a method based on Bayesian networks that is able to capture\\ncross-relation attribute value dependencies with little overhead. Our proposal\\nis based on the assumption that dependencies between attributes are preserved\\nwhen joins are involved. Furthermore, we introduce a parameter for trading\\nbetween estimation accuracy and computational cost. We validate our work by\\ncomparing it with other relevant methods on a large workload derived from the\\nJOB and TPC-DS benchmarks. Our results show that our method is an order of\\nmagnitude more efficient than existing methods, whilst maintaining a high level\\nof accuracy.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.09883v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.09883v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.09884v1', updated=datetime.datetime(2020, 9, 21, 14, 5, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 21, 14, 5, 29, tzinfo=datetime.timezone.utc), title='Selectivity correction with online machine learning', authors=[arxiv.Result.Author('Max Halford'), arxiv.Result.Author('Philippe Saint-Pierre'), arxiv.Result.Author('Franck Morvan')], summary=\"Computer systems are full of heuristic rules which drive the decisions they\\nmake. These rules of thumb are designed to work well on average, but ignore\\nspecific information about the available context, and are thus sub-optimal. The\\nemerging field of machine learning for systems attempts to learn decision rules\\nwith machine learning algorithms. In the database community, many recent\\nproposals have been made to improve selectivity estimation with batch machine\\nlearning methods. Such methods are all batch methods which require retraining\\nand cannot handle concept drift, such as workload changes and schema\\nmodifications. We present online machine learning as an alternative approach.\\nOnline models learn on the fly and do not require storing data, they are more\\nlightweight than batch models, and finally may adapt to concept drift. As an\\nexperiment, we teach models to improve the selectivity estimates made by\\nPostgreSQL's cost model. Our experiments make the case that simple online\\nmodels are able to compete with a recently proposed deep learning method.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.09884v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.09884v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.10373v1', updated=datetime.datetime(2020, 9, 22, 8, 4, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 22, 8, 4, 20, tzinfo=datetime.timezone.utc), title='Scalable Data Series Subsequence Matching with ULISSE', authors=[arxiv.Result.Author('Michele Linardi'), arxiv.Result.Author('Themis Palpanas')], summary='Data series similarity search is an important operation and at the core of\\nseveral analysis tasks and applications related to data series collections.\\nDespite the fact that data series indexes enable fast similarity search, all\\nexisting indexes can only answer queries of a single length (fixed at index\\nconstruction time), which is a severe limitation. In this work, we propose\\nULISSE, the first data series index structure designed for answering similarity\\nsearch queries of variable length (within some range). Our contribution is\\ntwo-fold. First, we introduce a novel representation technique, which\\neffectively and succinctly summarizes multiple sequences of different length.\\nBased on the proposed index, we describe efficient algorithms for approximate\\nand exact similarity search, combining disk based index visits and in-memory\\nsequential scans. Our approach supports non Z-normalized and Z-normalized\\nsequences, and can be used with no changes with both Euclidean Distance and\\nDynamic Time Warping, for answering both k-NN and epsilon-range queries. We\\nexperimentally evaluate our approach using several synthetic and real datasets.\\nThe results show that ULISSE is several times, and up to orders of magnitude\\nmore efficient in terms of both space and time cost, when compared to competing\\napproaches. (Paper published in VLDBJ 2020)', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.10373v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.10373v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.11013v1', updated=datetime.datetime(2020, 9, 23, 9, 17, 57, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 23, 9, 17, 57, tzinfo=datetime.timezone.utc), title='Segmented Pairwise Distance for Time Series with Large Discontinuities', authors=[arxiv.Result.Author('Jiabo He'), arxiv.Result.Author('Sarah Erfani'), arxiv.Result.Author('Sudanthi Wijewickrema'), arxiv.Result.Author(\"Stephen O'Leary\"), arxiv.Result.Author('Kotagiri Ramamohanarao')], summary='Time series with large discontinuities are common in many scenarios. However,\\nexisting distance-based algorithms (e.g., DTW and its derivative algorithms)\\nmay perform poorly in measuring distances between these time series pairs. In\\nthis paper, we propose the segmented pairwise distance (SPD) algorithm to\\nmeasure distances between time series with large discontinuities. SPD is\\northogonal to distance-based algorithms and can be embedded in them. We\\nvalidate advantages of SPD-embedded algorithms over corresponding\\ndistance-based ones on both open datasets and a proprietary dataset of surgical\\ntime series (of surgeons performing a temporal bone surgery in a virtual\\nreality surgery simulator). Experimental results demonstrate that SPD-embedded\\nalgorithms outperform corresponding distance-based ones in distance measurement\\nbetween time series with large discontinuities, measured by the Silhouette\\nindex (SI).', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.11013v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.11013v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.11648v1', updated=datetime.datetime(2020, 9, 22, 9, 59, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 22, 9, 59, 14, tzinfo=datetime.timezone.utc), title='Effective and Efficient Variable-Length Data Series Analytics', authors=[arxiv.Result.Author('Michele Linardi')], summary='In the last twenty years, data series similarity search has emerged as a\\nfundamental operation at the core of several analysis tasks and applications\\nrelated to data series collections. Many solutions to different mining problems\\nwork by means of similarity search. In this regard, all the proposed solutions\\nrequire the prior knowledge of the series length on which similarity search is\\nperformed. In several cases, the choice of the length is critical and sensibly\\ninfluences the quality of the expected outcome. Unfortunately, the obvious\\nbrute-force solution, which provides an outcome for all lengths within a given\\nrange is computationally untenable. In this Ph.D. work, we present the first\\nsolutions that inherently support scalable and variable-length similarity\\nsearch in data series, applied to sequence/subsequences matching, motif and\\ndiscord discovery problems.The experimental results show that our approaches\\nare up to orders of magnitude faster than the alternatives. They also\\ndemonstrate that we can remove the unrealistic constraint of performing\\nanalytics using a predefined length, leading to more intuitive and actionable\\nresults, which would have otherwise been missed.', comment='The author was supervised by Themis Palpanas. arXiv admin note:\\n  substantial text overlap with arXiv:2009.10373', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.11648v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.11648v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.13768v1', updated=datetime.datetime(2020, 9, 29, 4, 11, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 29, 4, 11, 13, tzinfo=datetime.timezone.utc), title='In-Order Sliding-Window Aggregation in Worst-Case Constant Time', authors=[arxiv.Result.Author('Kanat Tangwongsan'), arxiv.Result.Author('Martin Hirzel'), arxiv.Result.Author('Scott Schneider')], summary='Sliding-window aggregation is a widely-used approach for extracting insights\\nfrom the most recent portion of a data stream. The aggregations of interest can\\nusually be expressed as binary operators that are associative but not\\nnecessarily commutative nor invertible. Non-invertible operators, however, are\\ndifficult to support efficiently. In a 2017 conference paper, we introduced\\nDABA, the first algorithm for sliding-window aggregation with worst-case\\nconstant time. Before DABA, if a window had size $n$, the best published\\nalgorithms would require $O(\\\\log n)$ aggregation steps per window\\noperation---and while for strictly in-order streams, this bound could be\\nimproved to $O(1)$ aggregation steps on average, it was not known how to\\nachieve an $O(1)$ bound for the worst-case, which is critical for\\nlatency-sensitive applications.\\n  This article is an extended version of our 2017 paper. Besides describing\\nDABA in more detail, this article introduces a new variant, DABA Lite, which\\nachieves the same time bounds in less memory. Whereas DABA requires space for\\nstoring $2n$ partial aggregates, DABA Lite only requires space for $n+2$\\npartial aggregates. Our experiments on synthetic and real data support the\\ntheoretical findings.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.13768v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.13768v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.13819v4', updated=datetime.datetime(2022, 6, 14, 14, 47, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 29, 7, 10, 36, tzinfo=datetime.timezone.utc), title='The Shapley Value of Inconsistency Measures for Functional Dependencies', authors=[arxiv.Result.Author('Ester Livshits'), arxiv.Result.Author('Benny Kimelfeld')], summary='Quantifying the inconsistency of a database is motivated by various goals\\nincluding reliability estimation for new datasets and progress indication in\\ndata cleaning. Another goal is to attribute to individual tuples a level of\\nresponsibility to the overall inconsistency, and thereby prioritize tuples in\\nthe explanation or inspection of dirt. Therefore, inconsistency quantification\\nand attribution have been a subject of much research in Knowledge\\nRepresentation and, more recently, in Databases. As in many other fields, a\\nconventional responsibility sharing mechanism is the Shapley value from\\ncooperative game theory. In this paper, we carry out a systematic investigation\\nof the complexity of the Shapley value in common inconsistency measures for\\nfunctional-dependency (FD) violations. For several measures we establish a full\\nclassification of the FD sets into tractable and intractable classes with\\nrespect to Shapley-value computation. We also study the complexity of\\napproximation in intractable cases.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.13819v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.13819v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.13821v1', updated=datetime.datetime(2020, 9, 29, 7, 17, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 29, 7, 17, 16, tzinfo=datetime.timezone.utc), title='Database Repairing with Soft Functional Dependencies', authors=[arxiv.Result.Author('Nofar Carmeli'), arxiv.Result.Author('Martin Grohe'), arxiv.Result.Author('Benny Kimelfeld'), arxiv.Result.Author('Ester Livshits'), arxiv.Result.Author('Muhammad Tibi')], summary='A common interpretation of soft constraints penalizes the database for every\\nviolation of every constraint, where the penalty is the cost (weight) of the\\nconstraint. A computational challenge is that of finding an optimal subset: a\\ncollection of database tuples that minimizes the total penalty when each tuple\\nhas a cost of being excluded. When the constraints are strict (i.e., have an\\ninfinite cost), this subset is a \"cardinality repair\" of an inconsistent\\ndatabase; in soft interpretations, this subset corresponds to a \"most probable\\nworld\" of a probabilistic database, a \"most likely intention\" of a\\nprobabilistic unclean database, and so on. Within the class of functional\\ndependencies, the complexity of finding a cardinality repair is thoroughly\\nunderstood. Yet, very little is known about the complexity of this problem in\\nthe more general soft semantics. This paper makes a significant progress in\\nthis direction. In addition to general insights about the hardness and\\napproximability of the problem, we present algorithms for two special cases: a\\nsingle functional dependency, and a bipartite matching. The latter is the\\nproblem of finding an optimal \"almost matching\" of a bipartite graph where a\\npenalty is paid for every lost edge and every violation of monogamy.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2009.13821v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.13821v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2009.14094v2', updated=datetime.datetime(2020, 10, 5, 15, 40, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 9, 29, 15, 24, 42, tzinfo=datetime.timezone.utc), title='Alignment Approximation for Process Trees', authors=[arxiv.Result.Author('Daniel Schuster'), arxiv.Result.Author('Sebastiaan van Zelst'), arxiv.Result.Author('Wil M. P. van der Aalst')], summary='Comparing observed behavior (event data generated during process executions)\\nwith modeled behavior (process models), is an essential step in process mining\\nanalyses. Alignments are the de-facto standard technique for calculating\\nconformance checking statistics. However, the calculation of alignments is\\ncomputationally complex since a shortest path problem must be solved on a state\\nspace which grows non-linearly with the size of the model and the observed\\nbehavior, leading to the well-known state space explosion problem. In this\\npaper, we present a novel framework to approximate alignments on process trees\\nby exploiting their hierarchical structure. Process trees are an important\\nprocess model formalism used by state-of-the-art process mining techniques such\\nas the inductive mining approaches. Our approach exploits structural properties\\nof a given process tree and splits the alignment computation problem into\\nsmaller sub-problems. Finally, sub-results are composed to obtain an alignment.\\nOur experiments show that our approach provides a good balance between accuracy\\nand computation time.', comment=None, journal_ref=None, doi='10.1007/978-3-030-72693-5_19', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-030-72693-5_19', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2009.14094v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2009.14094v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.00843v1', updated=datetime.datetime(2020, 10, 2, 8, 10, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 2, 8, 10, 32, tzinfo=datetime.timezone.utc), title='All You Need Is CONSTRUCT', authors=[arxiv.Result.Author('Dominique Duval'), arxiv.Result.Author('Rachid Echahed'), arxiv.Result.Author('Frederic Prost')], summary='In SPARQL, the query forms SELECT and CONSTRUCT have been the subject of\\nseveral studies, both theoretical and practical. However, the composition of\\nsuch queries and their interweaving when forming involved nested queries has\\nnot yet received much interest in the literature. We mainly tackle the problem\\nof composing such queries. For this purpose, we introduce a language close to\\nSPARQL where queries can be nested at will, involving either CONSTRUCT or\\nSELECT query forms and provide a formal semantics for it. This semantics is\\nbased on a uniform interpretation of queries. This uniformity is due to an\\nextension of the notion of RDF graphs to include isolated items such as\\nvariables. As a key feature of this work, we show how classical SELECT queries\\ncan be easily encoded as a particular case of CONSTRUCT queries.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.PL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.00843v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.00843v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.01516v1', updated=datetime.datetime(2020, 10, 4, 8, 45, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 4, 8, 45, 14, tzinfo=datetime.timezone.utc), title='Trajectory-Based Spatiotemporal Entity Linking', authors=[arxiv.Result.Author('Fengmei Jin'), arxiv.Result.Author('Wen Hua'), arxiv.Result.Author('Thomas Zhou'), arxiv.Result.Author('Jiajie Xu'), arxiv.Result.Author('Matteo Francia'), arxiv.Result.Author('Maria E Orlowska'), arxiv.Result.Author('Xiaofang Zhou')], summary='Trajectory-based spatiotemporal entity linking is to match the same moving\\nobject in different datasets based on their movement traces. It is a\\nfundamental step to support spatiotemporal data integration and analysis. In\\nthis paper, we study the problem of spatiotemporal entity linking using\\neffective and concise signatures extracted from their trajectories. This\\nlinking problem is formalized as a k-nearest neighbor (k-NN) query on the\\nsignatures. Four representation strategies (sequential, temporal, spatial, and\\nspatiotemporal) and two quantitative criteria (commonality and unicity) are\\ninvestigated for signature construction. A simple yet effective dimension\\nreduction strategy is developed together with a novel indexing structure called\\nthe WR-tree to speed up the search. A number of optimization methods are\\nproposed to improve the accuracy and robustness of the linking. Our extensive\\nexperiments on real-world datasets verify the superiority of our approach over\\nthe state-of-the-art solutions in terms of both accuracy and efficiency.', comment='15 pages, 3 figures, 15 tables', journal_ref='IEEE Transactions on Knowledge and Data Engineering 2020', doi='10.1109/TKDE.2020.3036633', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/TKDE.2020.3036633', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2010.01516v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.01516v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.01951v1', updated=datetime.datetime(2020, 10, 5, 12, 42, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 5, 12, 42, 39, tzinfo=datetime.timezone.utc), title='LEAPME: Learning-based Property Matching with Embeddings', authors=[arxiv.Result.Author('Daniel Ayala'), arxiv.Result.Author('Inma Hernández'), arxiv.Result.Author('David Ruiz'), arxiv.Result.Author('Erhard Rahm')], summary='Data integration tasks such as the creation and extension of knowledge graphs\\ninvolve the fusion of heterogeneous entities from many sources. Matching and\\nfusion of such entities require to also match and combine their properties\\n(attributes). However, previous schema matching approaches mostly focus on two\\nsources only and often rely on simple similarity measurements. They thus face\\nproblems in challenging use cases such as the integration of heterogeneous\\nproduct entities from many sources.\\n  We therefore present a new machine learning-based property matching approach\\ncalled LEAPME (LEArning-based Property Matching with Embeddings) that utilizes\\nnumerous features of both property names and instance values. The approach\\nheavily makes use of word embeddings to better utilize the domain-specific\\nsemantics of both property names and instance values. The use of supervised\\nmachine learning helps exploit the predictive power of word embeddings.\\n  Our comparative evaluation against five baselines for several multi-source\\ndatasets with real-world data shows the high effectiveness of LEAPME. We also\\nshow that our approach is even effective when training data from another domain\\n(transfer learning) is used.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG', '68U35'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.01951v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.01951v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.02987v1', updated=datetime.datetime(2020, 10, 6, 19, 26, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 6, 19, 26, 9, tzinfo=datetime.timezone.utc), title='Event Trend Aggregation Under Rich Event Matching Semantics', authors=[arxiv.Result.Author('Olga Poppe'), arxiv.Result.Author('Chuan Lei'), arxiv.Result.Author('Elke A. Rundensteiner'), arxiv.Result.Author('David Maier')], summary='Streaming applications from health care analytics to algorithmic trading\\ndeploy Kleene queries to detect and aggregate event trends. Rich event matching\\nsemantics determine how to compose events into trends. The expressive power of\\nstate-of-the-art systems remains limited in that they do not support the rich\\nvariety of these semantics. Worse yet, they suffer from long delays and high\\nmemory costs because they opt to maintain aggregates at a fine granularity. To\\novercome these limitations, our Coarse-Grained Event Trend Aggregation (Cogra)\\napproach supports this rich diversity of event matching semantics within one\\nsystem. Better yet, Cogra incrementally maintains aggregates at the coarsest\\ngranularity possible for each of these semantics. In this way, Cogra minimizes\\nthe number of aggregates -- reducing both time and space complexity. Our\\nexperiments demonstrate that Cogra achieves up to four orders of magnitude\\nspeed-up and up to eight orders of magnitude memory reduction compared to\\nstate-of-the-art approaches.', comment='Technical report for the paper in SIGMOD 2019', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.PF'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.02987v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.02987v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.02989v1', updated=datetime.datetime(2020, 10, 6, 19, 27, 26, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 6, 19, 27, 26, tzinfo=datetime.timezone.utc), title='Sharon: Shared Online Event Sequence Aggregation', authors=[arxiv.Result.Author('Olga Poppe'), arxiv.Result.Author('Allison Rozet'), arxiv.Result.Author('Chuan Lei'), arxiv.Result.Author('Elke A. Rundensteiner'), arxiv.Result.Author('David Maier')], summary='Streaming systems evaluate massive workloads of event sequence aggregation\\nqueries. State-of-the-art approaches suffer from long delays caused by not\\nsharing intermediate results of similar queries and by constructing event\\nsequences prior to their aggregation. To overcome these limitations, our Shared\\nOnline Event Sequence Aggregation (Sharon) approach shares intermediate\\naggregates among multiple queries while avoiding the expensive construction of\\nevent sequences. Our Sharon optimizer faces two challenges. One, a sharing\\ndecision is not always beneficial. Two, a sharing decision may exclude other\\nsharing opportunities. To guide our Sharon optimizer, we compactly encode\\nsharing candidates, their benefits, and conflicts among candidates into the\\nSharon graph. Based on the graph, we map our problem of finding an optimal\\nsharing plan to the Maximum Weight Independent Set (MWIS) problem. We then use\\nthe guaranteed weight of a greedy algorithm for the MWIS problem to prune the\\nsearch of our sharing plan finder without sacrificing its optimality. The\\nSharon optimizer is shown to produce sharing plans that achieve up to an\\n18-fold speed-up compared to state-of-the-art approaches.', comment='Technical report for the paper in ICDE 2018', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.PF'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.02989v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.02989v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.03527v1', updated=datetime.datetime(2020, 10, 7, 17, 12, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 7, 17, 12, 43, tzinfo=datetime.timezone.utc), title='Query Rewriting On Path Views Without Integrity Constraints', authors=[arxiv.Result.Author('Julien Romero'), arxiv.Result.Author('Nicoleta Preda'), arxiv.Result.Author('Fabian Suchanek')], summary='A view with a binding pattern is a parameterised query on a database. Such\\nviews are used, e.g., to model Web services. To answer a query on such views,\\none has to orchestrate the views together in execution plans. The goal is\\nusually to find equivalent rewritings, which deliver precisely the same results\\nas the query on all databases. However, such rewritings are usually possible\\nonly in the presence of integrity constraints - and not all databases have such\\nconstraints. In this paper, we describe a class of plans that give practical\\nguarantees about their result even if there are no integrity constraints. We\\nprovide a characterisation of such plans and a complete and correct algorithm\\nto enumerate them. Finally, we show that our method can find plans on\\nreal-world Web Services.', comment=\"This is the full version of the Datamod'2020 article, which\\n  integrates all reviewer feedback, with the same text as the publisher version\\n  except minor changes\", journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.03527v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.03527v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.03600v2', updated=datetime.datetime(2022, 5, 2, 1, 36, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 7, 18, 41, 33, tzinfo=datetime.timezone.utc), title='Anomaly Detection in Large Labeled Multi-Graph Databases', authors=[arxiv.Result.Author('Hung T. Nguyen'), arxiv.Result.Author('Pierre J. Liang'), arxiv.Result.Author('Leman Akoglu')], summary='Within a large database G containing graphs with labeled nodes and directed,\\nmulti-edges; how can we detect the anomalous graphs? Most existing work are\\ndesigned for plain (unlabeled) and/or simple (unweighted) graphs. We introduce\\nCODETECT, the first approach that addresses the anomaly detection task for\\ngraph databases with such complex nature. To this end, it identifies a small\\nrepresentative set S of structural patterns (i.e., node-labeled network motifs)\\nthat losslessly compress database G as concisely as possible. Graphs that do\\nnot compress well are flagged as anomalous. CODETECT exhibits two novel\\nbuilding blocks: (i) a motif-based lossless graph encoding scheme, and (ii)\\nfast memory-efficient search algorithms for S. We show the effectiveness of\\nCODETECT on transaction graph databases from three different corporations,\\nwhere existing baselines adjusted for the task fall behind significantly,\\nacross different types of anomalies and performance metrics.', comment='24 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.03600v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.03600v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.03910v1', updated=datetime.datetime(2020, 10, 8, 11, 48, 35, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 8, 11, 48, 35, tzinfo=datetime.timezone.utc), title='An Experimental Analysis of Indoor Spatial Queries: Modeling, Indexing, and Processing', authors=[arxiv.Result.Author('Tiantian Liu'), arxiv.Result.Author('Huan Li'), arxiv.Result.Author('Hua Lu'), arxiv.Result.Author('Muhammad Aamir Cheema'), arxiv.Result.Author('Lidan Shou')], summary='Indoor location-based services (LBS), such as POI search and routing, are\\noften built on top of typical indoor spatial queries. To support such queries\\nand indoor LBS, multiple techniques including model/indexes and search\\nalgorithms have been proposed. In this work, we conduct an extensive\\nexperimental study on existing proposals for indoor spatial queries. We survey\\nfive model/indexes, compare their algorithmic characteristics, and analyze\\ntheir space and time complexities. We also design an in-depth benchmark with\\nreal and synthetic datasets, evaluation tasks and performance metrics. Enabled\\nby the benchmark, we obtain and report the performance results of all\\nmodel/indexes under investigation. By analyzing the results, we summarize the\\npros and cons of all techniques and suggest the best choice for typical\\nscenarios.', comment='An Experiment and Analysis Paper', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.03910v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.03910v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.05529v2', updated=datetime.datetime(2021, 2, 10, 7, 8, 38, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 12, 8, 37, 24, tzinfo=datetime.timezone.utc), title='PolyFrame: A Retargetable Query-based Approach to Scaling DataFrames (Extended Version)', authors=[arxiv.Result.Author('Phanwadee Sinthong'), arxiv.Result.Author('Michael J. Carey')], summary=\"In the last few years, the field of data science has been growing rapidly as\\nvarious businesses have adopted statistical and machine learning techniques to\\nempower their decision making and applications. Scaling data analysis, possibly\\nincluding the application of custom machine learning models, to large volumes\\nof data requires the utilization of distributed frameworks. This can lead to\\nserious technical challenges for data analysts and reduce their productivity.\\nAFrame, a Python data analytics library, is implemented as a layer on top of\\nApache AsterixDB, addressing these issues by incorporating the data scientists'\\ndevelopment environment and transparently scaling out the evaluation of\\nanalytical operations through a Big Data management system. While AFrame is\\nable to leverage data management facilities (e.g., indexes and query\\noptimization) and allows users to interact with a very large volume of data,\\nthe initial version only generated SQL++ queries and only operated against\\nApache AsterixDB. In this work, we describe a new design that retargets\\nAFrame's incremental query formation to other query-based database systems as\\nwell, making it more flexible for deployment against other data management\\nsystems with composable query languages.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.05529v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.05529v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.06037v3', updated=datetime.datetime(2022, 1, 7, 20, 1, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 12, 21, 26, 31, tzinfo=datetime.timezone.utc), title='Streaming enumeration on nested documents', authors=[arxiv.Result.Author('Martín Muñoz'), arxiv.Result.Author('Cristian Riveros')], summary='Some of the most relevant document schemas used online, such as XML and JSON,\\nhave a nested format. In the last decade, the task of extracting data from\\nnested documents over streams has become especially relevant. We focus on the\\nstreaming evaluation of queries with outputs of varied sizes over nested\\ndocuments. We model queries of this kind as Visibly Pushdown Transducers (VPT),\\na computational model that extends visibly pushdown automata with outputs and\\nhas the same expressive power as MSO over nested documents. Since processing a\\ndocument through a VPT can generate a massive number of results, we are\\ninterested in reading the input in a streaming fashion and enumerating the\\noutputs one after another as efficiently as possible, namely, with\\nconstant-delay. This paper presents an algorithm that enumerates these elements\\nwith constant-delay after processing the document stream in a single pass.\\nFurthermore, we show that this algorithm is worst-case optimal in terms of\\nupdate-time per symbol and memory usage.', comment='39 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS', 'cs.FL'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.06037v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.06037v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.06641v1', updated=datetime.datetime(2020, 10, 13, 19, 15, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 13, 19, 15, 9, tzinfo=datetime.timezone.utc), title='Raptor Zonal Statistics: Fully Distributed Zonal Statistics of Big Raster + Vector Data [Pre-Print]', authors=[arxiv.Result.Author('Samriddhi Singla'), arxiv.Result.Author('Ahmed Eldawy')], summary='Recent advancements in remote sensing technology have resulted in petabytes\\nof data in raster format. This data is often processed in combination with high\\nresolution vector data that represents, for example, city boundaries. One of\\nthe common operations that combine big raster and vector data is the zonal\\nstatistics which computes some statistics for each polygon in the vector\\ndataset. This paper models the zonal statistics problem as a join problem and\\nproposes a novel distributed system that can scale to petabytes of raster and\\nvector data. The proposed method does not require any preprocessing or indexing\\nwhich makes it perfect for ad-hoc queries that scientists usually want to run.\\nWe devise a theoretical cost model that proves the efficiency of our algorithm\\nover the baseline method. Furthermore, we run an extensive experimental\\nevaluation on large scale satellite data with up-to a trillion pixels, and big\\nvector data with up-to hundreds of millions of edges, and we show that our\\nmethod can perfectly scale to big data with up-to two orders of magnitude\\nperformance gain over Rasdaman and Google Earth Engine.', comment='17 pages', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2.4'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.06641v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.06641v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.06654v2', updated=datetime.datetime(2020, 10, 27, 2, 15, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 13, 19, 45, 30, tzinfo=datetime.timezone.utc), title='On the Efficiency of K-Means Clustering: Evaluation, Optimization, and Algorithm Selection', authors=[arxiv.Result.Author('Sheng Wang'), arxiv.Result.Author('Yuan Sun'), arxiv.Result.Author('Zhifeng Bao')], summary=\"This paper presents a thorough evaluation of the existing methods that\\naccelerate Lloyd's algorithm for fast k-means clustering. To do so, we analyze\\nthe pruning mechanisms of existing methods, and summarize their common pipeline\\ninto a unified evaluation framework UniK. UniK embraces a class of well-known\\nmethods and enables a fine-grained performance breakdown. Within UniK, we\\nthoroughly evaluate the pros and cons of existing methods using multiple\\nperformance metrics on a number of datasets. Furthermore, we derive an\\noptimized algorithm over UniK, which effectively hybridizes multiple existing\\nmethods for more aggressive pruning. To take this further, we investigate\\nwhether the most efficient method for a given clustering task can be\\nautomatically selected by machine learning, to benefit practitioners and\\nresearchers.\", comment='accepted to VLDB 2021; this is a technical report with five-page\\n  appendix', journal_ref='PVLDB, 14(2): 163 - 175, 2021', doi='10.14778/3425879.3425887', primary_category='cs.DB', categories=['cs.DB', 'cs.LG', 'H.2.8'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3425879.3425887', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2010.06654v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.06654v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.06760v1', updated=datetime.datetime(2020, 10, 14, 1, 20, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 14, 1, 20, 54, tzinfo=datetime.timezone.utc), title='Taurus: Lightweight Parallel Logging for In-Memory Database Management Systems (Extended Version)', authors=[arxiv.Result.Author('Yu Xia'), arxiv.Result.Author('Xiangyao Yu'), arxiv.Result.Author('Andrew Pavlo'), arxiv.Result.Author('Srinivas Devadas')], summary=\"Existing single-stream logging schemes are unsuitable for in-memory database\\nmanagement systems (DBMSs) as the single log is often a performance bottleneck.\\nTo overcome this problem, we present Taurus, an efficient parallel logging\\nscheme that uses multiple log streams, and is compatible with both data and\\ncommand logging. Taurus tracks and encodes transaction dependencies using a\\nvector of log sequence numbers (LSNs). These vectors ensure that the\\ndependencies are fully captured in logging and correctly enforced in recovery.\\nOur experimental evaluation with an in-memory DBMS shows that Taurus's parallel\\nlogging achieves up to 9.9x and 2.9x speedups over single-streamed data logging\\nand command logging, respectively. It also enables the DBMS to recover up to\\n22.9x and 75.6x faster than these baselines for data and command logging,\\nrespectively. We also compare Taurus with two state-of-the-art parallel logging\\nschemes and show that the DBMS achieves up to 2.8x better performance on NVMe\\ndrives and 9.2x on HDDs.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.06760v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.06760v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.07213v2', updated=datetime.datetime(2020, 10, 15, 13, 30, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 14, 16, 26, 29, tzinfo=datetime.timezone.utc), title='Data Readiness Report', authors=[arxiv.Result.Author('Shazia Afzal'), arxiv.Result.Author('Rajmohan C'), arxiv.Result.Author('Manish Kesarwani'), arxiv.Result.Author('Sameep Mehta'), arxiv.Result.Author('Hima Patel')], summary='Data exploration and quality analysis is an important yet tedious process in\\nthe AI pipeline. Current practices of data cleaning and data readiness\\nassessment for machine learning tasks are mostly conducted in an arbitrary\\nmanner which limits their reuse and results in loss of productivity. We\\nintroduce the concept of a Data Readiness Report as an accompanying\\ndocumentation to a dataset that allows data consumers to get detailed insights\\ninto the quality of input data. Data characteristics and challenges on various\\nquality dimensions are identified and documented keeping in mind the principles\\nof transparency and explainability. The Data Readiness Report also serves as a\\nrecord of all data assessment operations including applied transformations.\\nThis provides a detailed lineage for the purpose of data governance and\\nmanagement. In effect, the report captures and documents the actions taken by\\nvarious personas in a data readiness and assessment workflow. Overtime this\\nbecomes a repository of best practices and can potentially drive a\\nrecommendation system for building automated data readiness workflows on the\\nlines of AutoML [8]. We anticipate that together with the Datasheets [9],\\nDataset Nutrition Label [11], FactSheets [1] and Model Cards [15], the Data\\nReadiness Report makes significant progress towards Data and AI lifecycle\\ndocumentation.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.07213v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.07213v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.07586v1', updated=datetime.datetime(2020, 10, 15, 8, 10, 37, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 15, 8, 10, 37, tzinfo=datetime.timezone.utc), title='Survive the Schema Changes: Integration of Unmanaged Data Using Deep Learning', authors=[arxiv.Result.Author('Zijie Wang'), arxiv.Result.Author('Lixi Zhou'), arxiv.Result.Author('Amitabh Das'), arxiv.Result.Author('Valay Dave'), arxiv.Result.Author('Zhanpeng Jin'), arxiv.Result.Author('Jia Zou')], summary='Data is the king in the age of AI. However data integration is often a\\nlaborious task that is hard to automate. Schema change is one significant\\nobstacle to the automation of the end-to-end data integration process. Although\\nthere exist mechanisms such as query discovery and schema modification language\\nto handle the problem, these approaches can only work with the assumption that\\nthe schema is maintained by a database. However, we observe diversified schema\\nchanges in heterogeneous data and open data, most of which has no schema\\ndefined. In this work, we propose to use deep learning to automatically deal\\nwith schema changes through a super cell representation and automatic injection\\nof perturbations to the training data to make the model robust to schema\\nchanges. Our experimental results demonstrate that our proposed approach is\\neffective for two real-world data integration scenarios: coronavirus data\\nintegration, and machine log integration.', comment='In submission', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.07586v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.07586v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.08302v1', updated=datetime.datetime(2020, 10, 16, 10, 50, 41, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 16, 10, 50, 41, tzinfo=datetime.timezone.utc), title='Discovering Hierarchical Processes Using Flexible Activity Trees for Event Abstraction', authors=[arxiv.Result.Author('Xixi Lu'), arxiv.Result.Author('Avigdor Gal'), arxiv.Result.Author('Hajo A. Reijers')], summary='Processes, such as patient pathways, can be very complex, comprising of\\nhundreds of activities and dozens of interleaved subprocesses. While existing\\nprocess discovery algorithms have proven to construct models of high quality on\\nclean logs of structured processes, it still remains a challenge when the\\nalgorithms are being applied to logs of complex processes. The creation of a\\nmulti-level, hierarchical representation of a process can help to manage this\\ncomplexity. However, current approaches that pursue this idea suffer from a\\nvariety of weaknesses. In particular, they do not deal well with interleaving\\nsubprocesses. In this paper, we propose FlexHMiner, a three-step approach to\\ndiscover processes with multi-level interleaved subprocesses. We implemented\\nFlexHMiner in the open source Process Mining toolkit ProM. We used seven\\nreal-life logs to compare the qualities of hierarchical models discovered using\\ndomain knowledge, random clustering, and flat approaches. Our results indicate\\nthat the hierarchical process models that the FlexHMiner generates compare\\nfavorably to approaches that do not exploit hierarchy.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.08302v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.08302v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.08382v4', updated=datetime.datetime(2022, 5, 9, 8, 0, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 16, 13, 33, 34, tzinfo=datetime.timezone.utc), title='Enumerating Answers to First-Order Queries over Databases of Low Degree', authors=[arxiv.Result.Author('Arnaud Durand'), arxiv.Result.Author('Nicole Schweikardt'), arxiv.Result.Author('Luc Segoufin')], summary='A class of relational databases has low degree if for all $\\\\delta>0$, all but\\nfinitely many databases in the class have degree at most $n^{\\\\delta}$, where\\n$n$ is the size of the database. Typical examples are databases of bounded\\ndegree or of degree bounded by $\\\\log n$.\\n  It is known that over a class of databases having low degree, first-order\\nboolean queries can be checked in pseudo-linear time, i.e.\\\\ for all\\n$\\\\epsilon>0$ in time bounded by $n^{1+\\\\epsilon}$. We generalize this result by\\nconsidering query evaluation.\\n  We show that counting the number of answers to a query can be done in\\npseudo-linear time and that after a pseudo-linear time preprocessing we can\\ntest in constant time whether a given tuple is a solution to a query or\\nenumerate the answers to a query with constant delay.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LO'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.08382v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.08382v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.08622v1', updated=datetime.datetime(2020, 10, 16, 20, 46, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 16, 20, 46, 51, tzinfo=datetime.timezone.utc), title='LiMITS: An Effective Approach for Trajectory Simplification', authors=[arxiv.Result.Author('Yunheng Han'), arxiv.Result.Author('Hanan Samet')], summary='Trajectories represent the mobility of moving objects and thus is of great\\nvalue in data mining applications. However, trajectory data is enormous in\\nvolume, so it is expensive to store and process the raw data directly.\\nTrajectories are also redundant so data compression techniques can be applied.\\nIn this paper, we propose effective algorithms to simplify trajectories. We\\nfirst extend existing algorithms by replacing the commonly used $L_2$ metric\\nwith the $L_\\\\infty$ metric so that they can be generalized to high dimensional\\nspace (e.g., 3-space in practice). Next, we propose a novel approach, namely\\nL-infinity Multidimensional Interpolation Trajectory Simplification (LiMITS).\\nLiMITS belongs to weak simplification and takes advantage of the $L_\\\\infty$\\nmetric. It generates simplified trajectories by multidimensional interpolation.\\nIt also allows a new format called compact representation to further improve\\nthe compression ratio. Finally, We demonstrate the performance of LiMITS\\nthrough experiments on real-world datasets, which show that it is more\\neffective than other existing methods.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.08622v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.08622v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.08694v1', updated=datetime.datetime(2020, 10, 17, 1, 50, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 17, 1, 50, 27, tzinfo=datetime.timezone.utc), title='Aggregated Deletion Propagation for Counting Conjunctive Query Answers', authors=[arxiv.Result.Author('Xiao Hu'), arxiv.Result.Author('Shouzhuo Sun'), arxiv.Result.Author('Shweta Patwa'), arxiv.Result.Author('Debmalya Panigrahi'), arxiv.Result.Author('Sudeepa Roy')], summary='We investigate the computational complexity of minimizing the source\\nside-effect in order to remove a given number of tuples from the output of a\\nconjunctive query. This is a variant of the well-studied {\\\\em deletion\\npropagation} problem, the difference being that we are interested in removing\\nthe smallest subset of input tuples to remove a given number of output tuples}\\nwhile deletion propagation focuses on removing a specific output tuple. We call\\nthis the {\\\\em Aggregated Deletion Propagation} problem. We completely\\ncharacterize the poly-time solvability of this problem for arbitrary\\nconjunctive queries without self-joins. This includes a poly-time algorithm to\\ndecide solvability, as well as an exact structural characterization of NP-hard\\ninstances. We also provide a practical algorithm for this problem (a heuristic\\nfor NP-hard instances) and evaluate its experimental performance on real and\\nsynthetic datasets.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.08694v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.08694v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.08995v1', updated=datetime.datetime(2020, 10, 18, 14, 26, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 18, 14, 26, 10, tzinfo=datetime.timezone.utc), title='Construction and Application of Teaching System Based on Crowdsourcing Knowledge Graph', authors=[arxiv.Result.Author('Jinta Weng'), arxiv.Result.Author('Ying Gao'), arxiv.Result.Author('Jing Qiu'), arxiv.Result.Author('Guozhu Ding'), arxiv.Result.Author('Huanqin Zheng')], summary=\"Through the combination of crowdsourcing knowledge graph and teaching system,\\nresearch methods to generate knowledge graph and its applications. Using two\\ncrowdsourcing approaches, crowdsourcing task distribution and reverse captcha\\ngeneration, to construct knowledge graph in the field of teaching system.\\nGenerating a complete hierarchical knowledge graph of the teaching domain by\\nnodes of school, student, teacher, course, knowledge point and exercise type.\\nThe knowledge graph constructed in a crowdsourcing manner requires many users\\nto participate collaboratively with fully consideration of teachers' guidance\\nand users' mobilization issues. Based on the three subgraphs of knowledge\\ngraph, prominent teacher, student learning situation and suitable learning\\nroute could be visualized. Personalized exercises recommendation model is used\\nto formulate the personalized exercise by algorithm based on the knowledge\\ngraph. Collaborative creation model is developed to realize the crowdsourcing\\nconstruction mechanism. Though unfamiliarity with the learning mode of\\nknowledge graph and learners' less attention to the knowledge structure, system\\nbased on Crowdsourcing Knowledge Graph can still get high acceptance around\\nstudents and teachers\", comment='Number of references:15 Classification code:903.3 Information\\n  Retrieval and Use Conference code: 235759', journal_ref='4th China Conference on Knowledge Graph and Semantic Computing,\\n  CCKS 2019', doi='10.1007/978-981-15-1956-7_3', primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'cs.CL'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-981-15-1956-7_3', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2010.08995v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.08995v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.10884v1', updated=datetime.datetime(2020, 10, 21, 10, 36, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 21, 10, 36, 31, tzinfo=datetime.timezone.utc), title='uARMSolver: A framework for Association Rule Mining', authors=[arxiv.Result.Author('Iztok Fister'), arxiv.Result.Author('Iztok Fister Jr')], summary='The paper presents a novel software framework for Association Rule Mining\\nnamed uARMSolver. The framework is written fully in C++ and runs on all\\nplatforms. It allows users to preprocess their data in a transaction database,\\nto make discretization of data, to search for association rules and to guide a\\npresentation/visualization of the best rules found using external tools. As\\nopposed to the existing software packages or frameworks, this also supports\\nnumerical and real-valued types of attributes besides the categorical ones.\\nMining the association rules is defined as an optimization and solved using the\\nnature-inspired algorithms that can be incorporated easily. Because the\\nalgorithms normally discover a huge amount of association rules, the framework\\nenables a modular inclusion of so-called visual guiders for extracting the\\nknowledge hidden in data, and visualize these using external tools.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.NE'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.10884v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.10884v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.11075v2', updated=datetime.datetime(2021, 5, 31, 21, 51, 58, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 21, 15, 36, 3, tzinfo=datetime.timezone.utc), title='Neural Networks for Entity Matching: A Survey', authors=[arxiv.Result.Author('Nils Barlaug'), arxiv.Result.Author('Jon Atle Gulla')], summary='Entity matching is the problem of identifying which records refer to the same\\nreal-world entity. It has been actively researched for decades, and a variety\\nof different approaches have been developed. Even today, it remains a\\nchallenging problem, and there is still generous room for improvement. In\\nrecent years we have seen new methods based upon deep learning techniques for\\nnatural language processing emerge.\\n  In this survey, we present how neural networks have been used for entity\\nmatching. Specifically, we identify which steps of the entity matching process\\nexisting work have targeted using neural networks, and provide an overview of\\nthe different techniques used at each step. We also discuss contributions from\\ndeep learning in entity matching compared to traditional methods, and propose a\\ntaxonomy of deep neural networks for entity matching.', comment='Published in ACM Transactions on Knowledge Discovery from Data (TKDD)', journal_ref='ACM Transactions on Knowledge Discovery from Data, Volume 15,\\n  Issue 3, April 2021', doi='10.1145/3442200', primary_category='cs.DB', categories=['cs.DB', 'cs.CL', 'cs.LG'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3442200', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2010.11075v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.11075v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.11497v1', updated=datetime.datetime(2020, 10, 22, 7, 31, 12, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 22, 7, 31, 12, tzinfo=datetime.timezone.utc), title='Cluster-and-Conquer: When Randomness Meets Graph Locality', authors=[arxiv.Result.Author('George Giakkoupis'), arxiv.Result.Author('Anne-Marie Kermarrec'), arxiv.Result.Author('Olivier Ruas'), arxiv.Result.Author('François Taïani')], summary='K-Nearest-Neighbors (KNN) graphs are central to many emblematic data mining\\nand machine-learning applications. Some of the most efficient KNN graph\\nalgorithms are incremental and local: they start from a random graph, which\\nthey incrementally improve by traversing neighbors-of-neighbors links.\\nParadoxically, this random start is also one of the key weaknesses of these\\nalgorithms: nodes are initially connected to dissimilar neighbors, that lie far\\naway according to the similarity metric. As a result, incremental algorithms\\nmust first laboriously explore spurious potential neighbors before they can\\nidentify similar nodes, and start converging. In this paper, we remove this\\ndrawback with Cluster-and-Conquer (C 2 for short). Cluster-and-Conquer boosts\\nthe starting configuration of greedy algorithms thanks to a novel lightweight\\nclustering mechanism, dubbed FastRandomHash. FastRandomHash leverages\\nrandom-ness and recursion to pre-cluster similar nodes at a very low cost. Our\\nextensive evaluation on real datasets shows that Cluster-and-Conquer\\nsignificantly outperforms existing approaches, including LSH, yielding\\nspeed-ups of up to x4.42 while incurring only a negligible loss in terms of KNN\\nquality.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC', 'cs.DS', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.11497v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.11497v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.11538v1', updated=datetime.datetime(2020, 10, 22, 8, 57, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 22, 8, 57, 24, tzinfo=datetime.timezone.utc), title='Efficient RDF Graph Storage based on Reinforcement Learning', authors=[arxiv.Result.Author('Lei Zheng'), arxiv.Result.Author('Ziming Shen'), arxiv.Result.Author('Hongzhi Wang')], summary='Knowledge graph is an important cornerstone of artificial intelligence. The\\nconstruction and release of large-scale knowledge graphs in various fields pose\\nnew challenges to knowledge graph data management. Due to the maturity and\\nstability, relational database is also suitable for RDF data storage. However,\\nthe complex structure of RDF graph brings challenges to storage structure\\ndesign for RDF graph in the relational database. To address the difficult\\nproblem, this paper adopts reinforcement learning (RL) to optimize the storage\\npartition method of RDF graph based on the relational database. We transform\\nthe graph storage into a Markov decision process, and develop the reinforcement\\nlearning algorithm for graph storage design. For effective RL-based storage\\ndesign, we propose the data feature extraction method of RDF tables and the\\nquery rewriting priority policy during model training. The extensive\\nexperimental results demonstrate that our approach outperforms existing RDF\\nstorage design methods.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.11538v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.11538v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.11827v2', updated=datetime.datetime(2020, 12, 1, 16, 23, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 17, 2, 14, 15, tzinfo=datetime.timezone.utc), title='Automated Metadata Harmonization Using Entity Resolution & Contextual Embedding', authors=[arxiv.Result.Author('Kunal Sawarkar'), arxiv.Result.Author('Meenkakshi Kodati')], summary=\"ML Data Curation process typically consist of heterogeneous & federated\\nsource systems with varied schema structures; requiring curation process to\\nstandardize metadata from different schemas to an inter-operable schema. This\\nmanual process of Metadata Harmonization & cataloging slows efficiency of\\nML-Ops lifecycle. We demonstrate automation of this step with the help of\\nentity resolution methods & also by using Cogntive Database's Db2Vec embedding\\napproach to capture hidden inter-column & intra-column relationships which\\ndetect similarity of metadata and then predict metadata columns from source\\nschemas to any standardized schemas. Apart from matching schemas, we\\ndemonstrate that it can also infer the correct ontological structure of the\\ntarget data model.\", comment='Paper Accepted at Computing Conference, 2021 (Research Conference\\n  formerly called Science and Information (SAI) Conference). This is a\\n  replacement with change edit on conference status updated to \"Accepted\"', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.11827v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.11827v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.12734v2', updated=datetime.datetime(2021, 1, 25, 21, 34, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 24, 1, 21, 21, tzinfo=datetime.timezone.utc), title='REMIX: Efficient Range Query for LSM-trees', authors=[arxiv.Result.Author('Wenshao Zhong'), arxiv.Result.Author('Chen Chen'), arxiv.Result.Author('Xingbo Wu'), arxiv.Result.Author('Song Jiang')], summary='LSM-tree based key-value (KV) stores organize data in a multi-level structure\\nfor high-speed writes. Range queries on traditional LSM-trees must seek and\\nsort-merge data from multiple table files on the fly, which is expensive and\\noften leads to mediocre read performance. To improve range query efficiency on\\nLSM-trees, we introduce a space-efficient KV index data structure, named REMIX,\\nthat records a globally sorted view of KV data spanning multiple table files. A\\nrange query on multiple REMIX-indexed data files can quickly locate the target\\nkey using a binary search, and retrieve subsequent keys in sorted order without\\nkey comparisons. We build RemixDB, an LSM-tree based KV-store that adopts a\\nwrite-efficient compaction strategy and employs REMIXes for fast point and\\nrange queries. Experimental results show that REMIXes can substantially improve\\nrange query performance in a write-optimized LSM-tree based KV-store.', comment='19th USENIX Conference on File and Storage Technologies', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.12734v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.12734v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.13442v2', updated=datetime.datetime(2021, 10, 27, 14, 37, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 26, 9, 27, 39, tzinfo=datetime.timezone.utc), title='A Purely Regular Approach to Non-Regular Core Spanners', authors=[arxiv.Result.Author('Markus L. Schmid'), arxiv.Result.Author('Nicole Schweikardt')], summary=\"The regular spanners (characterised by vset-automata) are closed under the\\nalgebraic operations of union, join and projection, and have desirable\\nalgorithmic properties. The core spanners (introduced by Fagin, Kimelfeld,\\nReiss, and Vansummeren (PODS 2013, JACM 2015) as a formalisation of the core\\nfunctionality of the query language AQL used in IBM's SystemT) additionally\\nneed string equality selections and it has been shown by Freydenberger and\\nHolldack (ICDT 2016, Theory of Computing Systems 2018) that this leads to high\\ncomplexity and even undecidability of the typical problems in static analysis\\nand query evaluation. We propose an alternative approach to core spanners: by\\nincorporating the string-equality selections directly into the regular language\\nthat represents the underlying regular spanner (instead of treating it as an\\nalgebraic operation on the table extracted by the regular spanner), we obtain a\\nfragment of core spanners that, while having slightly weaker expressive power\\nthan the full class of core spanners, arguably still covers the intuitive\\napplications of string equality selections for information extraction and has\\nmuch better upper complexity bounds of the typical problems in static analysis\\nand query evaluation.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.FL', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.13442v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.13442v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.13619v3', updated=datetime.datetime(2021, 2, 7, 20, 17, 31, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 26, 14, 34, 39, tzinfo=datetime.timezone.utc), title='Exploring Memory Access Patterns for Graph Processing Accelerators', authors=[arxiv.Result.Author('Jonas Dann'), arxiv.Result.Author('Daniel Ritter'), arxiv.Result.Author('Holger Fröning')], summary='Recent trends in business and technology (e.g., machine learning, social\\nnetwork analysis) benefit from storing and processing growing amounts of\\ngraph-structured data in databases and data science platforms. FPGAs as\\naccelerators for graph processing with a customizable memory hierarchy promise\\nsolving performance problems caused by inherent irregular memory access\\npatterns on traditional hardware (e.g., CPU). However, developing such hardware\\naccelerators is yet time-consuming and difficult and benchmarking is\\nnon-standardized, hindering comprehension of the impact of memory access\\npattern changes and systematic engineering of graph processing accelerators.\\n  In this work, we propose a simulation environment for the analysis of graph\\nprocessing accelerators based on simulating their memory access patterns.\\nFurther, we evaluate our approach on two state-of-the-art FPGA graph processing\\naccelerators and show reproducibility, comparablity, as well as the shortened\\ndevelopment process by an example. Not implementing the cycle-accurate internal\\ndata flow on accelerator hardware like FPGAs significantly reduces the\\nimplementation time, increases the benchmark parameter transparency, and allows\\ncomparison of graph processing approaches.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.13619v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.13619v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.13721v1', updated=datetime.datetime(2020, 10, 26, 17, 2, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 26, 17, 2, 52, tzinfo=datetime.timezone.utc), title='PPQ-Trajectory: Spatio-temporal Quantization for Querying in Large Trajectory Repositories', authors=[arxiv.Result.Author('Shuang Wang'), arxiv.Result.Author('Hakan Ferhatosmanoglu')], summary='We present PPQ-trajectory, a spatio-temporal quantization based solution for\\nquerying large dynamic trajectory data. PPQ-trajectory includes a\\npartition-wise predictive quantizer (PPQ) that generates an error-bounded\\ncodebook with autocorrelation and spatial proximity-based partitions. The\\ncodebook is indexed to run approximate and exact spatio-temporal queries over\\ncompressed trajectories. PPQ-trajectory includes a coordinate quadtree coding\\nfor the codebook with support for exact queries. An incremental temporal\\npartition-based index is utilised to avoid full reconstruction of trajectories\\nduring queries. An extensive set of experimental results for spatio-temporal\\nqueries on real trajectory datasets is presented. PPQ-trajectory shows\\nsignificant improvements over the alternatives with respect to several\\nperformance measures, including the accuracy of results when the summary is\\nused directly to provide approximate query results, the spatial deviation with\\nwhich spatio-temporal path queries can be answered when the summary is used as\\nan index, and the time taken to construct the summary. Superior results on the\\nquality of the summary and the compression ratio are also demonstrated.', comment='To appear at VLDB 2021', journal_ref=None, doi='10.14778/3425879.3425891', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3425879.3425891', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2010.13721v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.13721v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.15981v1', updated=datetime.datetime(2020, 10, 29, 22, 54, 52, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 29, 22, 54, 52, tzinfo=datetime.timezone.utc), title='CoroBase: Coroutine-Oriented Main-Memory Database Engine', authors=[arxiv.Result.Author('Yongjun He'), arxiv.Result.Author('Jiacheng Lu'), arxiv.Result.Author('Tianzheng Wang')], summary='Data stalls are a major overhead in main-memory database engines due to the\\nuse of pointer-rich data structures. Lightweight coroutines ease the\\nimplementation of software prefetching to hide data stalls by overlapping\\ncomputation and asynchronous data prefetching. Prior solutions, however, mainly\\nfocused on (1) individual components and operations and (2) intra-transaction\\nbatching that requires interface changes, breaking backward compatibility. It\\nwas not clear how they apply to a full database engine and how much end-to-end\\nbenefit they bring under various workloads.\\n  This paper presents \\\\corobase, a main-memory database engine that tackles\\nthese challenges with a new coroutine-to-transaction paradigm.\\nCoroutine-to-transaction models transactions as coroutines and thus enables\\ninter-transaction batching, avoiding application changes but retaining the\\nbenefits of prefetching. We show that on a 48-core server, CoroBase can perform\\nclose to 2x better for read-intensive workloads and remain competitive for\\nworkloads that inherently do not benefit from software prefetching.', comment='To appear in VLDB 2021', journal_ref=None, doi='10.14778/3430915.3430932', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3430915.3430932', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2010.15981v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.15981v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2010.16340v2', updated=datetime.datetime(2020, 11, 7, 15, 10, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 30, 16, 0, 34, tzinfo=datetime.timezone.utc), title='Patterns Count-Based Labels for Datasets', authors=[arxiv.Result.Author('Yuval Moskovitch'), arxiv.Result.Author('H. V. Jagadish')], summary='Counts of attribute-value combinations are central to the profiling of a\\ndataset, particularly in determining fitness for use and in eliminating bias\\nand unfairness. While counts of individual attribute values may be stored in\\nsome dataset profiles, there are too many combinations of attributes for it to\\nbe practical to store counts for each combination. In this paper, we develop\\nthe notion of storing a \"label\" of limited size that can be used to obtain good\\nestimates for these counts. A label, in this paper, contains information\\nregarding the count of selected patterns--attributes values combinations--in\\nthe data. We define an estimation function, that uses this label to estimate\\nthe count of every pattern. We present the problem of finding the optimal label\\ngiven a bound on its size and propose a heuristic algorithm for generating\\noptimal labels. We experimentally show the accuracy of count estimates derived\\nfrom the resulting labels and the efficiency of our algorithm.', comment='ICDE2021', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2010.16340v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.16340v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.00096v3', updated=datetime.datetime(2022, 5, 31, 9, 52, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 30, 20, 34, 39, tzinfo=datetime.timezone.utc), title='Independence in Infinite Probabilistic Databases', authors=[arxiv.Result.Author('Martin Grohe'), arxiv.Result.Author('Peter Lindner')], summary='Probabilistic databases (PDBs) model uncertainty in data. The current\\nstandard is to view PDBs as finite probability spaces over relational database\\ninstances. Since many attributes in typical databases have infinite domains,\\nsuch as integers, strings, or real numbers, it is often more natural to view\\nPDBs as infinite probability spaces over database instances. In this paper, we\\nlay the mathematical foundations of infinite probabilistic databases. Our focus\\nthen is on independence assumptions. Tuple-independent PDBs play a central role\\nin theory and practice of PDBs. Here, we study infinite tuple-independent PDBs\\nas well as related models such as infinite block-independent disjoint PDBs.\\nWhile the standard model of PDBs focuses on a set-based semantics, we also\\nstudy tuple-independent PDBs with a bag semantics and independence in PDBs over\\nuncountable fact spaces.\\n  We also propose a new approach to PDBs with an open-world assumption,\\naddressing issues raised by Ceylan et al. (Proc. KR 2016) and generalizing\\ntheir work, which is still rooted in finite tuple-independent PDBs.\\n  Moreover, for countable PDBs we propose an approximate query answering\\nalgorithm.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2011.00096v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.00096v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.00235v3', updated=datetime.datetime(2021, 3, 3, 13, 25, 56, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 10, 31, 10, 39, 53, tzinfo=datetime.timezone.utc), title='Domain-specific Knowledge Graphs: A survey', authors=[arxiv.Result.Author('Bilal Abu-Salih')], summary='Knowledge Graphs (KGs) have made a qualitative leap and effected a real\\nrevolution in knowledge representation. This is leveraged by the underlying\\nstructure of the KG which underpins a better comprehension, reasoning and\\ninterpretation of knowledge for both human and machine. Therefore, KGs continue\\nto be used as the main means of tackling a plethora of real-life problems in\\nvarious domains. However, there is no consensus in regard to a plausible and\\ninclusive definition of a domain-specific KG. Further, in conjunction with\\nseveral limitations and deficiencies, various domain-specific KG construction\\napproaches are far from perfect. This survey is the first to offer a\\ncomprehensive definition of a domain-specific KG. Also, the paper presents a\\nthorough review of the state-of-the-art approaches drawn from academic works\\nrelevant to seven domains of knowledge. An examination of current approaches\\nreveals a range of limitations and deficiencies. At the same time, uncharted\\nterritories on the research map are highlighted to tackle extant issues in the\\nliterature and point to directions for future research.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2011.00235v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.00235v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.01773v1', updated=datetime.datetime(2020, 11, 3, 15, 13, 5, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 3, 15, 13, 5, tzinfo=datetime.timezone.utc), title='Memory-Efficient RkNN Retrieval by Nonlinear k-Distance Approximation', authors=[arxiv.Result.Author('Sandra Obermeier'), arxiv.Result.Author('Max Berrendorf'), arxiv.Result.Author('Peer Kröger')], summary='The reverse k-nearest neighbor (RkNN) query is an established query type with\\nvarious applications reaching from identifying highly influential objects over\\nincrementally updating kNN graphs to optimizing sensor communication and\\noutlier detection. State-of-the-art solutions exploit that the k-distances in\\nreal-world datasets often follow the power-law distribution, and bound them\\nwith linear lines in log-log space. In this work, we investigate this\\nassumption and uncover that it is violated in regions of changing density,\\nwhich we show are typical for real-life datasets. Towards a generic solution,\\nwe pose the estimation of k-distances as a regression problem. Thereby, we\\nenable harnessing the power of the abundance of available Machine Learning\\nmodels and profiting from their advancement. We propose a flexible approach\\nwhich allows steering the performance-memory consumption trade-off, and in\\nparticular to find good solutions with a fixed memory budget crucial in the\\ncontext of edge computing. Moreover, we show how to obtain and improve\\nguaranteed bounds essential to exact query processing. In experiments on\\nreal-world datasets, we demonstrate how this framework can significantly reduce\\nthe index memory consumption, and strongly reduce the candidate set size. We\\npublish our code at https://github.com/sobermeier/nonlinear-kdist.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2011.01773v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.01773v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.02556v1', updated=datetime.datetime(2020, 11, 4, 22, 3, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 4, 22, 3, 9, tzinfo=datetime.timezone.utc), title='Predict and Write: Using K-Means Clustering to Extend the Lifetime of NVM Storage', authors=[arxiv.Result.Author('Saeed Kargar'), arxiv.Result.Author('Heiner Litz'), arxiv.Result.Author('Faisal Nawab')], summary='Non-volatile memory (NVM) technologies suffer from limited write endurance.\\nTo address this challenge, we propose Predict and Write (PNW), a K/V-store that\\nuses a clustering-based machine learning approach to extend the lifetime of\\nNVMs. PNW decreases the number of bit flips for PUT/UPDATE operations by\\ndetermining the best memory location an updated value should be written to. PNW\\nleverages the indirection level of K/V-stores to freely choose the target\\nmemory location for any given write based on its value. PNW organizes NVM\\naddresses in a dynamic address pool clustered by the similarity of the data\\nvalues they refer to. We show that, by choosing the right target memory\\nlocation for a given PUT/UPDATE operation, the number of total bit flips and\\ncache lines can be reduced by up to 85% and 56% over the state of the art.', comment='ICDE2021', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AR', 'cs.LG', 'H.2.4; B.7.1; I.2'], links=[arxiv.Result.Link('http://arxiv.org/abs/2011.02556v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.02556v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.04378v1', updated=datetime.datetime(2020, 11, 9, 12, 26, 14, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 9, 12, 26, 14, tzinfo=datetime.timezone.utc), title='Characterizing Transactional Databases for Frequent Itemset Mining', authors=[arxiv.Result.Author('Christian Lezcano'), arxiv.Result.Author('Marta Arias')], summary=\"This paper presents a study of the characteristics of transactional databases\\nused in frequent itemset mining. Such characterizations have typically been\\nused to benchmark and understand the data mining algorithms working on these\\ndatabases. The aim of our study is to give a picture of how diverse and\\nrepresentative these benchmarking databases are, both in general but also in\\nthe context of particular empirical studies found in the literature. Our\\nproposed list of metrics contains many of the existing metrics found in the\\nliterature, as well as new ones. Our study shows that our list of metrics is\\nable to capture much of the datasets' inner complexity and thus provides a good\\nbasis for the characterization of transactional datasets. Finally, we provide a\\nset of representative datasets based on our characterization that may be used\\nas a benchmark safely.\", comment='Workshop on Evaluation and Experimental Design in Data Mining and\\n  Machine Learning (EDML@SDM 2019), May 2019', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2011.04378v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.04378v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.04730v1', updated=datetime.datetime(2020, 11, 9, 20, 15, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 9, 20, 15, 2, tzinfo=datetime.timezone.utc), title='Batchwise Probabilistic Incremental Data Cleaning', authors=[arxiv.Result.Author('Paulo H. Oliveira'), arxiv.Result.Author('Daniel S. Kaster'), arxiv.Result.Author('Caetano Traina-Jr.'), arxiv.Result.Author('Ihab F. Ilyas')], summary='Lack of data and data quality issues are among the main bottlenecks that\\nprevent further artificial intelligence adoption within many organizations,\\npushing data scientists to spend most of their time cleaning data before being\\nable to answer analytical questions. Hence, there is a need for more effective\\nand efficient data cleaning solutions, which, not surprisingly, is rife with\\ntheoretical and engineering problems. This report addresses the problem of\\nperforming holistic data cleaning incrementally, given a fixed rule set and an\\nevolving categorical relational dataset acquired in sequential batches. To the\\nbest of our knowledge, our contributions compose the first incremental\\nframework that cleans data (i) independently of user interventions, (ii)\\nwithout requiring knowledge about the incoming dataset, such as the number of\\nclasses per attribute, and (iii) holistically, enabling multiple error types to\\nbe repaired simultaneously, and thus avoiding conflicting repairs. Extensive\\nexperiments show that our approach outperforms the competitors with respect to\\nrepair quality, execution time, and memory consumption.', comment='29 pages, 13 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2011.04730v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.04730v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.05549v2', updated=datetime.datetime(2021, 2, 3, 5, 15, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 11, 5, 16, 30, tzinfo=datetime.timezone.utc), title='Comprehensive and Efficient Workload Compression', authors=[arxiv.Result.Author('Shaleen Deep'), arxiv.Result.Author('Anja Gruenheid'), arxiv.Result.Author('Paraschos Koutris'), arxiv.Result.Author('Jeffrey Naughton'), arxiv.Result.Author('Stratis Viglas')], summary='This work studies the problem of constructing a representative workload from\\na given input analytical query workload where the former serves as an\\napproximation with guarantees of the latter. We discuss our work in the context\\nof workload analysis and monitoring. As an example, evolving system usage\\npatterns in a database system can cause load imbalance and performance\\nregressions which can be controlled by monitoring system usage patterns,\\ni.e.,~a representative workload, over time. To construct such a workload in a\\nprincipled manner, we formalize the notions of workload {\\\\em representativity}\\nand {\\\\em coverage}. These metrics capture the intuition that the distribution\\nof features in a compressed workload should match a target distribution,\\nincreasing representativity, and include common queries as well as outliers,\\nincreasing coverage. We show that solving this problem optimally is NP-hard and\\npresent a novel greedy algorithm that provides approximation guarantees. We\\ncompare our techniques to established algorithms in this problem space such as\\nsampling and clustering, and demonstrate advantages and key trade-offs of our\\ntechniques.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2011.05549v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.05549v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.06381v1', updated=datetime.datetime(2020, 11, 12, 13, 45, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 12, 13, 45, 8, tzinfo=datetime.timezone.utc), title='Scalable Querying of Nested Data', authors=[arxiv.Result.Author('Jaclyn Smith'), arxiv.Result.Author('Michael Benedikt'), arxiv.Result.Author('Milos Nikolic'), arxiv.Result.Author('Amir Shaikhha')], summary='While large-scale distributed data processing platforms have become an\\nattractive target for query processing, these systems are problematic for\\napplications that deal with nested collections. Programmers are forced either\\nto perform non-trivial translations of collection programs or to employ\\nautomated flattening procedures, both of which lead to performance problems.\\nThese challenges only worsen for nested collections with skewed cardinalities,\\nwhere both handcrafted rewriting and automated flattening are unable to enforce\\nload balancing across partitions.\\n  In this work, we propose a framework that translates a program manipulating\\nnested collections into a set of semantically equivalent shredded queries that\\ncan be efficiently evaluated. The framework employs a combination of query\\ncompilation techniques, an efficient data representation for nested\\ncollections, and automated skew-handling. We provide an extensive experimental\\nevaluation, demonstrating significant improvements provided by the framework in\\ndiverse scenarios for nested collection programs.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'H.2.3; H.2.4'], links=[arxiv.Result.Link('http://arxiv.org/abs/2011.06381v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.06381v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.06423v1', updated=datetime.datetime(2020, 11, 12, 14, 56, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 12, 14, 56, 15, tzinfo=datetime.timezone.utc), title='Turning Transport Data to Comply with EU Standards while Enabling a Multimodal Transport Knowledge Graph', authors=[arxiv.Result.Author('Mario Scrocca'), arxiv.Result.Author('Marco Comerio'), arxiv.Result.Author('Alessio Carenini'), arxiv.Result.Author('Irene Celino')], summary='Complying with the EU Regulation on multimodal transportation services\\nrequires sharing data on the National Access Points in one of the standards\\n(e.g., NeTEx and SIRI) indicated by the European Commission. These standards\\nare complex and of limited practical adoption. This means that datasets are\\nnatively expressed in other formats and require a data translation process for\\nfull compliance.\\n  This paper describes the solution to turn the authoritative data of three\\ndifferent transport stakeholders from Italy and Spain into a format compliant\\nwith EU standards by means of Semantic Web technologies. Our solution addresses\\nthe challenge and also contributes to build a multi-modal transport Knowledge\\nGraph of interlinked and interoperable information that enables intelligent\\nquerying and exploration, as well as facilitates the design of added-value\\nservices.', comment='International Semantic Web Conference (ISWC 2020) - In Use Track', journal_ref=None, doi='10.1007/978-3-030-62466-8_26', primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-030-62466-8_26', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2011.06423v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.06423v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.07957v1', updated=datetime.datetime(2020, 11, 16, 13, 50, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 16, 13, 50, 43, tzinfo=datetime.timezone.utc), title='Bridging the Technology Gap Between Industry and Semantic Web: Generating Databases and Server Code From RDF', authors=[arxiv.Result.Author('Markus Schröder'), arxiv.Result.Author('Michael Schulze'), arxiv.Result.Author('Christian Jilek'), arxiv.Result.Author('Andreas Dengel')], summary='Despite great advances in the area of Semantic Web, industry rather seldom\\nadopts Semantic Web technologies and their storage and query concepts. Instead,\\nrelational databases (RDB) are often deployed to store business-critical data,\\nwhich are accessed via REST interfaces. Yet, some enterprises would greatly\\nbenefit from Semantic Web related datasets which are usually represented with\\nthe Resource Description Framework (RDF). To bridge this technology gap, we\\npropose a fully automatic approach that generates suitable RDB models with REST\\nAPIs to access them. In our evaluation, generated databases from different RDF\\ndatasets are examined and compared. Our findings show that the databases\\nsufficiently reflect their counterparts while the API is able to reproduce\\nrather simple SPARQL queries. Potentials for improvements are identified, for\\nexample, the reduction of data redundancies in generated databases.', comment='9 pages, accepted at ICAART 2021', journal_ref='Proceedings of the 13th International Conference on Agents and\\n  Artificial Intelligence (ICAART 2021), Online Streaming, February 4-6, Vol.\\n  2, pp. 507-514, SciTePress, 2021', doi='10.5220/0010186005070514', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.5220/0010186005070514', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2011.07957v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.07957v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.08077v2', updated=datetime.datetime(2022, 6, 7, 6, 57, 16, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 13, 14, 27, 50, tzinfo=datetime.timezone.utc), title='The Problem with XSD Binary Floating Point Datatypes in RDF', authors=[arxiv.Result.Author('Jan Martin Keil'), arxiv.Result.Author('Merle Gänßinger')], summary='The XSD binary floating point datatypes are regularly used for precise\\nnumeric values in RDF. However, the use of these datatypes for knowledge\\nrepresentation can systematically impair the quality of data and, compared to\\nthe XSD decimal datatype, increases the probability of data processing\\nproducing false results. We argue why in most cases the XSD decimal datatype is\\nbetter suited to represent numeric values in RDF. A survey of the actual usage\\nof datatypes on the relevant subset of the December 2020 Web Data Commons\\ndataset, containing 19453060341 literals from real web data, substantiates the\\npractical relevancy of the described problem: 29 %-68 % of binary floating\\npoint values are distorted due to the datatype.', comment='17 pages, 3 figures', journal_ref='The Semantic Web. ESWC 2022. Lecture Notes in Computer Science,\\n  vol 13261. 2022. 165-182', doi='10.1007/978-3-031-06981-9_10', primary_category='cs.DB', categories=['cs.DB', '68T30', 'I.2.4; H.3.2'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-031-06981-9_10', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2011.08077v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.08077v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.08724v2', updated=datetime.datetime(2021, 5, 16, 7, 36, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 17, 15, 54, 18, tzinfo=datetime.timezone.utc), title='Multi-SQL: An extensible multi-model data query language', authors=[arxiv.Result.Author('Yu Yan'), arxiv.Result.Author('Nan Jiang'), arxiv.Result.Author('Hongzhi Wang'), arxiv.Result.Author('Yutong Wang'), arxiv.Result.Author('Chang Liu'), arxiv.Result.Author('Yuzhuo Wang')], summary='Big data management aims to establish data hubs that support data in multiple\\nmodels and types in an all-around way. Thus, the multi-model database system is\\na promising architecture for building such a multi-model data store. For an\\nintegrated data hub, a unified and flexible query language is incredibly\\nnecessary. In this paper, an extensible and practical query language--Multi-SQL\\nis proposed to realize the unified management of multi-model data considering\\nthe co-processing of multi-model data. To the best of our knowledge, Multi-SQL\\nis the first query language based on various data models. Multi-SQL can also be\\nexpanded to suit more complicated scenarios as it is flexible to support other\\ndata models. Moreover, we provide a formal semantic definition of the core\\nfeatures of Multi-SQL, including the multi-model definition, multi-model\\nfilters, multi-model joins, etc. Furthermore, we propose a two-level query\\nimplementation method to totally exploit the existing query optimization\\ncapabilities of the underlying engines which could largely improve the query\\nexcution efficiency.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2011.08724v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.08724v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.09176v1', updated=datetime.datetime(2020, 11, 18, 9, 50, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 18, 9, 50, 51, tzinfo=datetime.timezone.utc), title='Query Expressibility and Verification in Ontology-Based Data Access', authors=[arxiv.Result.Author('Carsten Lutz'), arxiv.Result.Author('Johannes Marti'), arxiv.Result.Author('Leif Sabellek')], summary=\"In ontology-based data access, multiple data sources are integrated using an\\nontology and mappings. In practice, this is often achieved by a bootstrapping\\nprocess, that is, the ontology and mappings are first designed to support only\\nthe most important queries over the sources and then gradually extended to\\nenable additional queries. In this paper, we study two reasoning problems that\\nsupport such an approach. The expressibility problem asks whether a given\\nsource query $q_s$ is expressible as a target query (that is, over the\\nontology's vocabulary) and the verification problem asks, additionally given a\\ncandidate target query $q_t$, whether $q_t$ expresses $q_s$. We consider (U)CQs\\nas source and target queries and GAV mappings, showing that both problems are\\n$\\\\Pi^p_2$-complete in DL-Lite, coNExpTime-complete between EL and ELHI when\\nsource queries are rooted, and 2ExpTime-complete for unrestricted source\\nqueries.\", comment=None, journal_ref='Principles of Knowledge Representation and Reasoning: Proceedings\\n  of the Sixteenth International Conference, KR 2018, Tempe, Arizona, 30\\n  October - 2 November 2018, pages 389--398, AAAI Press, 2018', doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2011.09176v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.09176v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.10180v1', updated=datetime.datetime(2020, 11, 20, 2, 35, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 20, 2, 35, 47, tzinfo=datetime.timezone.utc), title='Survey and Open Problems in Privacy Preserving Knowledge Graph: Merging, Query, Representation, Completion and Applications', authors=[arxiv.Result.Author('Chaochao Chen'), arxiv.Result.Author('Jamie Cui'), arxiv.Result.Author('Guanfeng Liu'), arxiv.Result.Author('Jia Wu'), arxiv.Result.Author('Li Wang')], summary=\"Knowledge Graph (KG) has attracted more and more companies' attention for its\\nability to connect different types of data in meaningful ways and support rich\\ndata services. However, the data isolation problem limits the performance of KG\\nand prevents its further development. That is, multiple parties have their own\\nKGs but they cannot share with each other due to regulation or competition\\nreasons. Therefore, how to conduct privacy preserving KG becomes an important\\nresearch question to answer. That is, multiple parties conduct KG related tasks\\ncollaboratively on the basis of protecting the privacy of multiple KGs. To\\ndate, there is few work on solving the above KG isolation problem. In this\\npaper, to fill this gap, we summarize the open problems for privacy preserving\\nKG in data isolation setting and propose possible solutions for them.\\nSpecifically, we summarize the open problems in privacy preserving KG from four\\naspects, i.e., merging, query, representation, and completion. We present these\\nproblems in details and propose possible technical solutions for them.\\nMoreover, we present three privacy preserving KG-aware applications and simply\\ndescribe how can our proposed techniques be applied into these applications.\", comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CR', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2011.10180v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.10180v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.11442v1', updated=datetime.datetime(2020, 11, 20, 14, 16, 12, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 20, 14, 16, 12, tzinfo=datetime.timezone.utc), title='OAK: Ontology-Based Knowledge Map Model for Digital Agriculture', authors=[arxiv.Result.Author('Quoc Hung Ngo'), arxiv.Result.Author('Tahar Kechadi'), arxiv.Result.Author('Nhien-An Le-Khac')], summary='Nowadays, a huge amount of knowledge has been amassed in digital agriculture.\\nThis knowledge and know-how information are collected from various sources,\\nhence the question is how to organise this knowledge so that it can be\\nefficiently exploited. Although this knowledge about agriculture practices can\\nbe represented using ontology, rule-based expert systems, or knowledge model\\nbuilt from data mining processes, the scalability still remains an open issue.\\nIn this study, we propose a knowledge representation model, called an\\nontology-based knowledge map, which can collect knowledge from different\\nsources, store it, and exploit either directly by stakeholders or as an input\\nto the knowledge discovery process (Data Mining). The proposed model consists\\nof two stages, 1) build an ontology as a knowledge base for a specific domain\\nand data mining concepts, and 2) build the ontology-based knowledge map model\\nfor representing and storing the knowledge mined on the crop datasets. A\\nframework of the proposed model has been implemented in agriculture domain. It\\nis an efficient and scalable model, and it can be used as knowledge repository\\na digital agriculture.', comment=None, journal_ref=None, doi='10.1007/978-3-030-63924-2_14', primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-030-63924-2_14', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2011.11442v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.11442v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.11487v1', updated=datetime.datetime(2020, 11, 19, 14, 9, 12, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 19, 14, 9, 12, tzinfo=datetime.timezone.utc), title='Verifying the Correctness of Analytic Query Results', authors=[arxiv.Result.Author('Masoud Nosrati'), arxiv.Result.Author('Ying Cai')], summary='Data outsourcing is a cost-effective solution for data owners to tackle\\nissues such as large volumes of data, huge number of users, and intensive\\ncomputation needed for data analysis. They can simply upload their databases to\\na cloud and let it perform all management works, including query processing.\\nOne problem with this service model is how query issuers can verify the query\\nresults they receive are indeed correct. This concern is legitimate because, as\\na third party, clouds may not be fully trustworthy, and as a large data center,\\nclouds are ideal targets for hackers. There has been significant work on query\\nresult verification, but most consider only simple queries where query results\\ncan be attained by checking the raw data against the query conditions directly.\\nIn this paper, we consider the problem of enabling users to verify the\\ncorrectness of the results of analytic queries. Unlike simple queries, analytic\\nqueries involve ranking functions to score a database, which makes it difficult\\nto build data structures for verification purposes. We propose two approaches,\\nnamely one-signature and multi-signature, and show that they work well on three\\nrepresentative types of analytic queries, including top-k, range, and KNN\\nqueries, through both analysis and experiments.', comment='IEEE Transactions on Knowledge and Data Engineering (12 Pages)', journal_ref=None, doi='10.1109/TKDE.2020.3037313', primary_category='cs.DB', categories=['cs.DB', 'cs.CR'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/TKDE.2020.3037313', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2011.11487v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.11487v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.11907v1', updated=datetime.datetime(2020, 11, 24, 5, 56, 48, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 24, 5, 56, 48, tzinfo=datetime.timezone.utc), title='Efficient Approximate Nearest Neighbor Search for Multiple Weighted $l_{p\\\\leq2}$ Distance Functions', authors=[arxiv.Result.Author('Huan Hu'), arxiv.Result.Author('Jianzhong Li')], summary='Nearest neighbor search is fundamental to a wide range of applications. Since\\nthe exact nearest neighbor search suffers from the \"curse of dimensionality\",\\napproximate approaches, such as Locality-Sensitive Hashing (LSH), are widely\\nused to trade a little query accuracy for a much higher query efficiency. In\\nmany scenarios, it is necessary to perform nearest neighbor search under\\nmultiple weighted distance functions in high-dimensional spaces. This paper\\nconsiders the important problem of supporting efficient approximate nearest\\nneighbor search for multiple weighted distance functions in high-dimensional\\nspaces. To the best of our knowledge, prior work can only solve the problem for\\nthe $l_2$ distance. However, numerous studies have shown that the $l_p$\\ndistance with $p\\\\in(0,2)$ could be more effective than the $l_2$ distance in\\nhigh-dimensional spaces. We propose a novel method, WLSH, to address the\\nproblem for the $l_p$ distance for $p\\\\in(0,2]$. WLSH takes the LSH approach and\\ncan theoretically guarantee both the efficiency of processing queries and the\\naccuracy of query results while minimizing the required total number of hash\\ntables. We conduct extensive experiments on synthetic and real data sets, and\\nthe results show that WLSH achieves high performance in terms of query\\nefficiency, query accuracy and space consumption.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DS'], links=[arxiv.Result.Link('http://arxiv.org/abs/2011.11907v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.11907v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.13454v1', updated=datetime.datetime(2020, 11, 26, 19, 36, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 26, 19, 36, 20, tzinfo=datetime.timezone.utc), title='TKUS: Mining Top-K High-Utility Sequential Patterns', authors=[arxiv.Result.Author('Chunkai Zhang'), arxiv.Result.Author('Zilin Du'), arxiv.Result.Author('Wensheng Gan'), arxiv.Result.Author('Philip S. Yu')], summary='High-utility sequential pattern mining (HUSPM) has recently emerged as a\\nfocus of intense research interest. The main task of HUSPM is to find all\\nsubsequences, within a quantitative sequential database, that have high utility\\nwith respect to a user-defined minimum utility threshold. However, it is\\ndifficult to specify the minimum utility threshold, especially when database\\nfeatures, which are invisible in most cases, are not understood. To handle this\\nproblem, top-k HUSPM was proposed. Up to now, only very preliminary work has\\nbeen conducted to capture top-k HUSPs, and existing strategies require\\nimprovement in terms of running time, memory consumption, unpromising candidate\\nfiltering, and scalability. Moreover, no systematic problem statement has been\\ndefined. In this paper, we formulate the problem of top-k HUSPM and propose a\\nnovel algorithm called TKUS. To improve efficiency, TKUS adopts a projection\\nand local search mechanism and employs several schemes, including the Sequence\\nUtility Raising, Terminate Descendants Early, and Eliminate Unpromising Items\\nstrategies, which allow it to greatly reduce the search space. Finally,\\nexperimental results demonstrate that TKUS can achieve sufficiently good top-k\\nHUSPM performance compared to state-of-the-art algorithm TKHUS-Span.', comment='Preprint, 22 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2011.13454v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.13454v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.14195v3', updated=datetime.datetime(2022, 2, 9, 11, 2, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 28, 18, 53, 3, tzinfo=datetime.timezone.utc), title='A Recursive Algorithm for Mining Association Rules', authors=[arxiv.Result.Author('Abdelkader Mokkadem'), arxiv.Result.Author('Mariane Pelletier'), arxiv.Result.Author('Louis Raimbault')], summary='Mining frequent itemsets and association rules is an essential task within\\ndata mining and data analysis. In this paper, we introduce PrefRec, a recursive\\nalgorithm for finding frequent itemsets and association rules. Its main\\nadvantage is its recursiveness with respect to the items. It is particularly\\nefficient for updating the mining process when new items are added to the\\ndatabase or when some are excluded. We present in a complete way the logic of\\nthe algorithm, and give some of its applications. After that, we carry out an\\nexperimental study on the effectiveness of PrefRec. We first compare the\\nexecution times with some very popular frequent itemset mining algorithms.\\nThen, we do experiments to test the updating capabilities of our algorithm.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'math.ST', 'stat.TH'], links=[arxiv.Result.Link('http://arxiv.org/abs/2011.14195v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.14195v3', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.14482v4', updated=datetime.datetime(2022, 5, 4, 16, 32, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 30, 0, 48, 35, tzinfo=datetime.timezone.utc), title='A Near-Optimal Parallel Algorithm for Joining Binary Relations', authors=[arxiv.Result.Author('Bas Ketsman'), arxiv.Result.Author('Dan Suciu'), arxiv.Result.Author('Yufei Tao')], summary='We present a constant-round algorithm in the massively parallel computation\\n(MPC) model for evaluating a natural join where every input relation has two\\nattributes. Our algorithm achieves a load of $\\\\tilde{O}(m/p^{1/\\\\rho})$ where\\n$m$ is the total size of the input relations, $p$ is the number of machines,\\n$\\\\rho$ is the join\\'s fractional edge covering number, and $\\\\tilde{O}(.)$ hides\\na polylogarithmic factor. The load matches a known lower bound up to a\\npolylogarithmic factor. At the core of the proposed algorithm is a new theorem\\n(which we name the \"isolated cartesian product theorem\") that provides fresh\\ninsight into the problem\\'s mathematical structure. Our result implies that the\\nsubgraph enumeration problem, where the goal is to report all the occurrences\\nof a constant-sized subgraph pattern, can be settled optimally (up to a\\npolylogarithmic factor) in the MPC model.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2011.14482v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.14482v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2011.14843v1', updated=datetime.datetime(2020, 11, 30, 14, 36, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 30, 14, 36, 29, tzinfo=datetime.timezone.utc), title='Mint: MDL-based approach for Mining INTeresting Numerical Pattern Sets', authors=[arxiv.Result.Author('Tatiana Makhalova'), arxiv.Result.Author('Sergei O. Kuznetsov'), arxiv.Result.Author('Amedeo Napoli')], summary='Pattern mining is well established in data mining research, especially for\\nmining binary datasets. Surprisingly, there is much less work about numerical\\npattern mining and this research area remains under-explored. In this paper, we\\npropose Mint, an efficient MDL-based algorithm for mining numerical datasets.\\nThe MDL principle is a robust and reliable framework widely used in pattern\\nmining, and as well in subgroup discovery. In Mint we reuse MDL for discovering\\nuseful patterns and returning a set of non-redundant overlapping patterns with\\nwell-defined boundaries and covering meaningful groups of objects. Mint is not\\nalone in the category of numerical pattern miners based on MDL. In the\\nexperiments presented in the paper we show that Mint outperforms competitors\\namong which Slim and RealKrimp.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2011.14843v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2011.14843v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.00135v6', updated=datetime.datetime(2021, 6, 14, 17, 30, 36, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 11, 30, 22, 10, 21, tzinfo=datetime.timezone.utc), title='Optimizing Fitness-For-Use of Differentially Private Linear Queries', authors=[arxiv.Result.Author('Yingtai Xiao'), arxiv.Result.Author('Zeyu Ding'), arxiv.Result.Author('Yuxin Wang'), arxiv.Result.Author('Danfeng Zhang'), arxiv.Result.Author('Daniel Kifer')], summary='In practice, differentially private data releases are designed to support a\\nvariety of applications. A data release is fit for use if it meets target\\naccuracy requirements for each application. In this paper, we consider the\\nproblem of answering linear queries under differential privacy subject to\\nper-query accuracy constraints. Existing practical frameworks like the matrix\\nmechanism do not provide such fine-grained control (they optimize total error,\\nwhich allows some query answers to be more accurate than necessary, at the\\nexpense of other queries that become no longer useful). Thus, we design a\\nfitness-for-use strategy that adds privacy-preserving Gaussian noise to query\\nanswers. The covariance structure of the noise is optimized to meet the\\nfine-grained accuracy requirements while minimizing the cost to privacy.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.00135v6', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.00135v6', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.01229v1', updated=datetime.datetime(2020, 12, 2, 14, 16, 38, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 2, 14, 16, 38, tzinfo=datetime.timezone.utc), title='Learning to Characterize Matching Experts', authors=[arxiv.Result.Author('Roee Shraga'), arxiv.Result.Author('Ofra Amir'), arxiv.Result.Author('Avigdor Gal')], summary='Matching is a task at the heart of any data integration process, aimed at\\nidentifying correspondences among data elements. Matching problems were\\ntraditionally solved in a semi-automatic manner, with correspondences being\\ngenerated by matching algorithms and outcomes subsequently validated by human\\nexperts. Human-in-the-loop data integration has been recently challenged by the\\nintroduction of big data and recent studies have analyzed obstacles to\\neffective human matching and validation. In this work we characterize human\\nmatching experts, those humans whose proposed correspondences can mostly be\\ntrusted to be valid. We provide a novel framework for characterizing matching\\nexperts that, accompanied with a novel set of features, can be used to identify\\nreliable and valuable human experts. We demonstrate the usefulness of our\\napproach using an extensive empirical evaluation. In particular, we show that\\nour approach can improve matching results by filtering out inexpert matchers.', comment='Accepted by the 37th IEEE International Conference on Data\\n  Engineering (ICDE 2021)', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.HC', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.01229v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.01229v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.01592v1', updated=datetime.datetime(2020, 12, 2, 23, 28, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 2, 23, 28, 27, tzinfo=datetime.timezone.utc), title='Free Gap Estimates from the Exponential Mechanism, Sparse Vector, Noisy Max and Related Algorithms', authors=[arxiv.Result.Author('Zeyu Ding'), arxiv.Result.Author('Yuxin Wang'), arxiv.Result.Author('Yingtai Xiao'), arxiv.Result.Author('Guanhong Wang'), arxiv.Result.Author('Danfeng Zhang'), arxiv.Result.Author('Daniel Kifer')], summary='Private selection algorithms, such as the Exponential Mechanism, Noisy Max\\nand Sparse Vector, are used to select items (such as queries with large\\nanswers) from a set of candidates, while controlling privacy leakage in the\\nunderlying data. Such algorithms serve as building blocks for more complex\\ndifferentially private algorithms. In this paper we show that these algorithms\\ncan release additional information related to the gaps between the selected\\nitems and the other candidates for free (i.e., at no additional privacy cost).\\nThis free gap information can improve the accuracy of certain follow-up\\ncounting queries by up to 66%. We obtain these results from a careful privacy\\nanalysis of these algorithms. Based on this analysis, we further propose novel\\nhybrid algorithms that can dynamically save additional privacy budget.', comment='arXiv admin note: substantial text overlap with arXiv:1904.12773', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.01592v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.01592v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.01658v1', updated=datetime.datetime(2020, 12, 3, 2, 27, 57, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 3, 2, 27, 57, tzinfo=datetime.timezone.utc), title='An Algebraic Graph Transformation Approach for RDF and SPARQL', authors=[arxiv.Result.Author('Dominique Duval'), arxiv.Result.Author('Rachid Echahed'), arxiv.Result.Author('Frédéric Prost')], summary='We consider the recommendations of the World Wide Web Consortium (W3C) about\\nRDF framework and its associated query language SPARQL. We propose a new formal\\nframework based on category theory which provides clear and concise formal\\ndefinitions of the main basic features of RDF and SPARQL. We define RDF graphs\\nas well as SPARQL basic graph patterns as objects of some nested categories.\\nThis allows one to clarify, in particular, the role of blank nodes.\\nFurthermore, we consider basic SPARQL CONSTRUCT and SELECT queries and\\nformalize their operational semantics following a novel algebraic graph\\ntransformation approach called POIM.', comment='In Proceedings GCM 2020, arXiv:2012.01181. arXiv admin note:\\n  substantial text overlap with arXiv:1910.07519', journal_ref='EPTCS 330, 2020, pp. 55-70', doi='10.4204/EPTCS.330.4', primary_category='cs.DB', categories=['cs.DB', 'cs.PL', 'cs.SC', 'H.2.3;D.3.2;F.4.2'], links=[arxiv.Result.Link('http://dx.doi.org/10.4204/EPTCS.330.4', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2012.01658v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.01658v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.02258v1', updated=datetime.datetime(2020, 12, 3, 20, 50, 22, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 3, 20, 50, 22, tzinfo=datetime.timezone.utc), title='WedgeChain: A Trusted Edge-Cloud Store With Asynchronous (Lazy) Trust', authors=[arxiv.Result.Author('Faisal Nawab')], summary='We propose WedgeChain, a data store that spans both edge and cloud nodes (an\\nedge-cloud system). WedgeChain consists of a logging layer and a data indexing\\nlayer. In this study, we encounter two challenges: (1) edge nodes are untrusted\\nand potentially malicious, and (2) edge-cloud coordination is expensive.\\nWedgeChain tackles these challenges by the following proposals: (1) Lazy\\n(asynchronous) certification: where data is committed at the untrusted edge and\\nthen lazily certified at the cloud node. This lazy certification method takes\\nadvantage of the observation that an untrusted edge node is unlikely to act\\nmaliciously if it knows it will be detected (and punished) eventually. Our lazy\\ncertification method guarantees that malicious acts (i.e., lying) are\\neventually detected. (2) Data-free certification: our lazy certification method\\nonly needs to send digests of data to the cloud, instead of sending all data to\\nthe cloud, which enables saving network and cloud resources and reduce costs.\\n(3) LSMerkle: we extend a trusted index (mLSM) to enable indexing data at the\\nedge while utilizing lazy and data-free certification.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.02258v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.02258v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.02710v1', updated=datetime.datetime(2020, 12, 4, 16, 38, 1, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 4, 16, 38, 1, tzinfo=datetime.timezone.utc), title='Hiperfact: In-Memory High Performance Fact Processing -- Rethinking the Rete Inference Algorithm', authors=[arxiv.Result.Author('Conrad Indiono'), arxiv.Result.Author('Stefanie Rinderle-Ma')], summary='The Rete forward inference algorithm forms the basis for many rule engines\\ndeployed today, but it exhibits the following problems: (1) the caching of all\\nintermediate join results, (2) the processing of all rules regardless of the\\nnecessity to do so (stemming from the underlying forward inference approach),\\n(3) not defining the join order of rules and its conditions, significantly\\naffecting the final run-time performance, and finally (4) pointer chasing due\\nto the overall network structure, leading to inefficient usage of the CPU\\ncaches caused by random access patterns. The Hiperfact approach aims to\\novercome these shortcomings by (1) choosing cache efficient data structures on\\nthe primary rank 1 fact index storage and intermediate join result storage\\nlevels, (2) introducing island fact processing for determining the join order\\nby ensuring minimal intermediate join result construction, and (3) introducing\\nderivation trees to allow for parallel read/write access and lazy rule\\nevaluation. The experimental evaluations show that the Hiperfact prototype\\nengine implementing the approach achieves significant improvements in respect\\nto both inference and query performance. Moreover, the proposed Hiperfact\\nengine is compared to existing engines in the context of a comprehensive\\nbenchmark.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.02710v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.02710v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.04361v1', updated=datetime.datetime(2020, 12, 8, 11, 15, 48, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 8, 11, 15, 48, tzinfo=datetime.timezone.utc), title='From Data Harvesting to Querying for Making Urban Territories Smart', authors=[arxiv.Result.Author('Genoveva Vargas-Solar'), arxiv.Result.Author('Ana-Sagrario Castillo-Camporro'), arxiv.Result.Author('José Zechinelli-Martini'), arxiv.Result.Author('Javier Espinosa-Oviedo')], summary='This chapter provides a summarized, critical and analytical point of view of\\nthe data-centric solutions that are currently applied for addressing urban\\nproblems in cities. These solutions lead to the use of urban computing\\ntechniques to address their daily life issues. Data-centric solutions have\\nbecome popular due to the emergence of data science. The chapter describes and\\ndiscusses the type of urban challenges and how data science in urban computing\\ncan face them. Current solutions address a spectrum that goes from data\\nharvesting techniques to decision making support. Finally, the chapter also\\nputs in perspective families of strategies developed in the state of the art\\nfor addressing urban problems and exhibits guidelines that can lead to a\\nmethodological understanding of these strategies.', comment=None, journal_ref='Carlos Alberto Ochoa. Innovative Applications in Smart Cities,\\n  Taylor and Francis, In press', doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.04361v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.04361v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.06446v1', updated=datetime.datetime(2020, 12, 11, 16, 13, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 11, 16, 13, 24, tzinfo=datetime.timezone.utc), title='SEGSys: A mapping system for segmentation analysis in energy', authors=[arxiv.Result.Author('Xiufeng Liu'), arxiv.Result.Author('Rongling Li'), arxiv.Result.Author('Yi Wang'), arxiv.Result.Author('Per Sieverts Nielsen')], summary='Customer segmentation analysis can give valuable insights into the energy\\nefficiency of residential buildings. This paper presents a mapping system,\\nSEGSys that enables segmentation analysis at the individual and the\\nneighborhood levels. SEGSys supports the online and offline classification of\\ncustomers based on their daily consumption patterns and consumption intensity.\\nIt also supports the segmentation analysis according to the social\\ncharacteristics of customers of individual households or neighborhoods, as well\\nas spatial geometries. SEGSys uses a three-layer architecture to model the\\nsegmentation system, including the data layer, the service layer, and the\\npresentation layer. The data layer models data into a star schema within a data\\nwarehouse, the service layer provides data service through a RESTful interface,\\nand the presentation layer interacts with users through a visual map. This\\npaper showcases the system on the segmentation analysis using an electricity\\nconsumption data set and validates the effectiveness of the system.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.06446v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.06446v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.06683v1', updated=datetime.datetime(2020, 12, 12, 0, 22, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 12, 0, 22, 51, tzinfo=datetime.timezone.utc), title='Cortex: Harnessing Correlations to Boost Query Performance', authors=[arxiv.Result.Author('Vikram Nathan'), arxiv.Result.Author('Jialin Ding'), arxiv.Result.Author('Tim Kraska'), arxiv.Result.Author('Mohammad Alizadeh')], summary='Databases employ indexes to filter out irrelevant records, which reduces scan\\noverhead and speeds up query execution. However, this optimization is only\\navailable to queries that filter on the indexed attribute. To extend these\\nspeedups to queries on other attributes, database systems have turned to\\nsecondary and multi-dimensional indexes. Unfortunately, these approaches are\\nrestrictive: secondary indexes have a large memory footprint and can only speed\\nup queries that access a small number of records, and multi-dimensional indexes\\ncannot scale to more than a handful of columns. We present Cortex, an approach\\nthat takes advantage of correlations to extend the reach of primary indexes to\\nmore attributes. Unlike prior work, Cortex can adapt itself to any existing\\nprimary index, whether single or multi-dimensional, to harness a broad variety\\nof correlations, such as those that exist between more than two attributes or\\nhave a large number of outliers. We demonstrate that on real datasets\\nexhibiting these diverse types of correlations, Cortex matches or outperforms\\ntraditional secondary indexes with $5\\\\times$ less space, and it is $2-8\\\\times$\\nfaster than existing approaches to indexing correlations.', comment='13 pages, including references. Under submission', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.06683v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.06683v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.06802v2', updated=datetime.datetime(2021, 4, 20, 15, 28, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 12, 12, 29, 38, tzinfo=datetime.timezone.utc), title='Deep Analysis on Subgraph Isomorphism', authors=[arxiv.Result.Author('Li Zeng'), arxiv.Result.Author('Yan Jiang'), arxiv.Result.Author('Weixin Lu'), arxiv.Result.Author('Lei Zou')], summary='Subgraph isomorphism is a well-known NP-hard problem which is widely used in\\nmany applications, such as social network analysis and knowledge graph query.\\nIts performance is often limited by the inherent hardness. Several insightful\\nworks have been done since 2012, mainly optimizing pruning rules and matching\\norders to accelerate enumerating all isomorphic subgraphs. Nevertheless, their\\ncorrectness and performance are not well studied. First, different languages\\nare used in implementation with different compilation flags. Second,\\nexperiments are not done on the same platform and the same datasets. Third,\\nsome ideas of different works are even complementary. Last but not least, there\\nexist errors when applying some algorithms. In this paper, we address these\\nproblems by re-implementing seven representative subgraph isomorphism\\nalgorithms as well as their improved versions, and conducting comprehensive\\nexperiments on various graphs. The results show pros and cons of\\nstate-of-the-art solutions and explore new approaches to optimization.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.06802v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.06802v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.06966v1', updated=datetime.datetime(2020, 12, 13, 5, 33, 19, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 13, 5, 33, 19, tzinfo=datetime.timezone.utc), title='A Dual-Store Structure for Knowledge Graphs', authors=[arxiv.Result.Author('Zhixin Qi'), arxiv.Result.Author('Hongzhi Wang'), arxiv.Result.Author('Haoran Zhang')], summary='To effectively manage increasing knowledge graphs in various domains, a hot\\nresearch topic, knowledge graph storage management, has emerged. Existing\\nmethods are classified to relational stores and native graph stores. Relational\\nstores are able to store large-scale knowledge graphs and convenient in\\nupdating knowledge, but the query performance weakens obviously when the\\nselectivity of a knowledge graph query is large. Native graph stores are\\nefficient in processing complex knowledge graph queries due to its index-free\\nadjacent property, but they are inapplicable to manage a large-scale knowledge\\ngraph due to limited storage budgets or inflexible updating process. Motivated\\nby this, we propose a dual-store structure which leverages a graph store to\\naccelerate the complex query process in the relational store. However, it is\\nchallenging to determine what data to transfer from relational store to graph\\nstore at what time. To address this problem, we formulate it as a Markov\\nDecision Process and derive a physical design tuner DOTIL based on\\nreinforcement learning. With DOTIL, the dual-store structure is adaptive to\\ndynamic changing workloads. Experimental results on real knowledge graphs\\ndemonstrate that our proposed dual-store structure improves query performance\\nup to average 43.72% compared with the most commonly used relational stores.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.06966v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.06966v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.08105v2', updated=datetime.datetime(2021, 10, 18, 6, 6, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 15, 5, 57, 41, tzinfo=datetime.timezone.utc), title='Schema Extraction on Semi-structured Data', authors=[arxiv.Result.Author('Panpan Li'), arxiv.Result.Author('Yikun Gong'), arxiv.Result.Author('Chen Wang')], summary=\"With the continuous development of NoSQL databases, more and more developers\\nchoose to use semi-structured data for development and data management, which\\nputs forward requirements for schema management of semi-structured data stored\\nin NoSQL databases. Schema extraction plays an important role in understanding\\nschemas, optimizing queries, and validating data consistency. Therefore, in\\nthis survey we investigate structural methods based on tree and graph and\\nstatistical methods based on distributed architecture and machine learning to\\nextract schemas. The schemas obtained by the structural methods are more\\ninterpretable, and the statistical methods have better applicability and\\ngeneralization ability. Moreover, we also investigate tools and systems for\\nschemas extraction. Schema extraction tools are mainly used for spark or NoSQL\\ndatabases, and are suitable for small datasets or simple application\\nenvironments. The system mainly focuses on the extraction and management of\\nschemas in large data sets and complex application scenarios. Furthermore, we\\nalso compare these techniques to facilitate data managers' choice.\", comment='More schema extraction methods will be investigated to enrich the\\n  content of the paper, so the manuscript is temporarily retracted', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.08105v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.08105v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.08830v1', updated=datetime.datetime(2020, 12, 16, 9, 59, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 16, 9, 59, 27, tzinfo=datetime.timezone.utc), title='Graph integration of structured, semistructured and unstructured data for data journalism', authors=[arxiv.Result.Author('Angelos-Christos Anadiotis'), arxiv.Result.Author('Oana Balalau'), arxiv.Result.Author('Catarina Conceicao'), arxiv.Result.Author('Helena Galhardas'), arxiv.Result.Author('Mhd Yamen Haddad'), arxiv.Result.Author('Ioana Manolescu'), arxiv.Result.Author('Tayeb Merabti'), arxiv.Result.Author('Jingmao You')], summary='Digital data is a gold mine for modern journalism. However, datasets which\\ninterest journalists are extremely heterogeneous, ranging from highly\\nstructured (relational databases), semi-structured (JSON, XML, HTML), graphs\\n(e.g., RDF), and text. Journalists (and other classes of users lacking advanced\\nIT expertise, such as most non-governmental-organizations, or small public\\nadministrations) need to be able to make sense of such heterogeneous corpora,\\neven if they lack the ability to define and deploy custom\\nextract-transform-load workflows, especially for dynamically varying sets of\\ndata sources.\\n  We describe a complete approach for integrating dynamic sets of heterogeneous\\ndatasets along the lines described above: the challenges we faced to make such\\ngraphs useful, allow their integration to scale, and the solutions we proposed\\nfor these problems. Our approach is implemented within the ConnectionLens\\nsystem; we validate it through a set of experiments.', comment='40 pages, 9 figures. arXiv admin note: substantial text overlap with\\n  arXiv:2007.12488, arXiv:2009.04283', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.08830v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.08830v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.09329v1', updated=datetime.datetime(2020, 12, 17, 0, 5, 50, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 17, 0, 5, 50, tzinfo=datetime.timezone.utc), title='Clique: Spatiotemporal Object Re-identification at the City Scale', authors=[arxiv.Result.Author('Tiantu Xu'), arxiv.Result.Author('Kaiwen Shen'), arxiv.Result.Author('Yang Fu'), arxiv.Result.Author('Humphrey Shi'), arxiv.Result.Author('Felix Xiaozhu Lin')], summary='Object re-identification (ReID) is a key application of city-scale cameras.\\nWhile classic ReID tasks are often considered as image retrieval, we treat them\\nas spatiotemporal queries for locations and times in which the target object\\nappeared. Spatiotemporal reID is challenged by the accuracy limitation in\\ncomputer vision algorithms and the colossal videos from city cameras. We\\npresent Clique, a practical ReID engine that builds upon two new techniques:\\n(1) Clique assesses target occurrences by clustering fuzzy object features\\nextracted by ReID algorithms, with each cluster representing the general\\nimpression of a distinct object to be matched against the input; (2) to search\\nin videos, Clique samples cameras to maximize the spatiotemporal coverage and\\nincrementally adds cameras for processing on demand. Through evaluation on 25\\nhours of videos from 25 cameras, Clique reached a high accuracy of 0.87 (recall\\nat 5) across 70 queries and runs at 830x of video realtime in achieving high\\naccuracy.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.09329v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.09329v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.12013v1', updated=datetime.datetime(2020, 12, 21, 16, 41, 59, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 21, 16, 41, 59, tzinfo=datetime.timezone.utc), title='SARS-CoV-2 Coronavirus Data Compression Benchmark', authors=[arxiv.Result.Author('Innar Liiv')], summary=\"This paper introduces a lossless data compression competition that benchmarks\\nsolutions (computer programs) by the compressed size of the 44,981 concatenated\\nSARS-CoV-2 sequences, with a total uncompressed size of 1,339,868,341 bytes.\\nThe data, downloaded on 13 December 2020, from the severe acute respiratory\\nsyndrome coronavirus 2 data hub of ncbi.nlm.nih.gov is presented in FASTA and\\n2Bit format. The aim of this competition is to encourage multidisciplinary\\nresearch to find the shortest lossless description for the sequences and to\\ndemonstrate that data compression can serve as an objective and repeatable\\nmeasure to align scientific breakthroughs across disciplines. The shortest\\ndescription of the data is the best model; therefore, further reducing the size\\nof this description requires a fundamental understanding of the underlying\\ncontext and data. This paper presents preliminary results with multiple\\nwell-known compression algorithms for baseline measurements, and insights\\nregarding promising research avenues. The competition's progress will be\\nreported at \\\\url{https://coronavirus.innar.com}, and the benchmark is open for\\nall to participate and contribute.\", comment='6 pages, 2 tables', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CY'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.12013v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.12013v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.12028v1', updated=datetime.datetime(2020, 12, 21, 12, 58, 13, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 21, 12, 58, 13, tzinfo=datetime.timezone.utc), title='Data Validation', authors=[arxiv.Result.Author('Mark P. J. van der Loo'), arxiv.Result.Author('Edwin de Jonge')], summary='Data validation is the activity where one decides whether or not a particular\\ndata set is fit for a given purpose. Formalizing the requirements that drive\\nthis decision process allows for unambiguous communication of the requirements,\\nautomation of the decision process, and opens up ways to maintain and\\ninvestigate the decision process itself. The purpose of this article is to\\nformalize the definition of data validation and to demonstrate some of the\\nproperties that can be derived from this definition. In particular, it is shown\\nhow a formal view of the concept permits a classification of data quality\\nrequirements, allowing them to be ordered in increasing levels of complexity.\\nSome subtleties arising from combining possibly many such requirements are\\npointed out as well.', comment='7 pages, 1 figure. In Wiley StatsRef: Statistics Reference Online\\n  (2020)', journal_ref=None, doi='10.1002/9781118445112', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1002/9781118445112', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2012.12028v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.12028v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.12031v1', updated=datetime.datetime(2020, 12, 21, 10, 4, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 21, 10, 4, 54, tzinfo=datetime.timezone.utc), title='Towards Quantifying Privacy in Process Mining', authors=[arxiv.Result.Author('Majid Rafiei'), arxiv.Result.Author('Wil M. P. van der Aalst')], summary='Process mining employs event logs to provide insights into the actual\\nprocesses. Event logs are recorded by information systems and contain valuable\\ninformation helping organizations to improve their processes. However, these\\ndata also include highly sensitive private information which is a major concern\\nwhen applying process mining. Therefore, privacy preservation in process mining\\nis growing in importance, and new techniques are being introduced. The\\neffectiveness of the proposed privacy preservation techniques needs to be\\nevaluated. It is important to measure both sensitive data protection and data\\nutility preservation. In this paper, we propose an approach to quantify the\\neffectiveness of privacy preservation techniques. We introduce two measures for\\nquantifying disclosure risks to evaluate the sensitive data protection aspect.\\nMoreover, a measure is proposed to quantify data utility preservation for the\\nmain process mining activities. The proposed measures have been tested using\\nvarious real-life event logs.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.CR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.12031v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.12031v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.12501v1', updated=datetime.datetime(2020, 12, 23, 5, 56, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 23, 5, 56, 45, tzinfo=datetime.timezone.utc), title='Learned Indexes for a Google-scale Disk-based Database', authors=[arxiv.Result.Author('Hussam Abu-Libdeh'), arxiv.Result.Author('Deniz Altınbüken'), arxiv.Result.Author('Alex Beutel'), arxiv.Result.Author('Ed H. Chi'), arxiv.Result.Author('Lyric Doshi'), arxiv.Result.Author('Tim Kraska'), arxiv.Result.Author('Xiaozhou'), arxiv.Result.Author('Li'), arxiv.Result.Author('Andy Ly'), arxiv.Result.Author('Christopher Olston')], summary=\"There is great excitement about learned index structures, but understandable\\nskepticism about the practicality of a new method uprooting decades of research\\non B-Trees. In this paper, we work to remove some of that uncertainty by\\ndemonstrating how a learned index can be integrated in a distributed,\\ndisk-based database system: Google's Bigtable. We detail several design\\ndecisions we made to integrate learned indexes in Bigtable. Our results show\\nthat integrating learned index significantly improves the end-to-end read\\nlatency and throughput for Bigtable.\", comment='4 pages, Presented at Workshop on ML for Systems at NeurIPS 2020', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.DC', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.12501v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.12501v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.13685v1', updated=datetime.datetime(2020, 12, 26, 7, 8, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 26, 7, 8, 42, tzinfo=datetime.timezone.utc), title='Discovering Closed and Maximal Embedded Patterns from Large Tree Data', authors=[arxiv.Result.Author('Xiaoying Wu'), arxiv.Result.Author('Dimitri Theodoratos'), arxiv.Result.Author('Nikos Mamoulis')], summary='We address the problem of summarizing embedded tree patterns extracted from\\nlarge data trees. We do so by defining and mining closed and maximal embedded\\nunordered tree patterns from a single large data tree. We design an embedded\\nfrequent pattern mining algorithm extended with a local closedness checking\\ntechnique. This algorithm is called {\\\\em closedEmbTM-prune} as it eagerly\\neliminates non-closed patterns. To mitigate the generation of intermediate\\npatterns, we devise pattern search space pruning rules to proactively detect\\nand prune branches in the pattern search space which do not correspond to\\nclosed patterns. The pruning rules are accommodated into the extended embedded\\npattern miner to produce a new algorithm, called {\\\\em closedEmbTM-prune}, for\\nmining all the closed and maximal embedded frequent patterns from large data\\ntrees. Our extensive experiments on synthetic and real large-tree datasets\\ndemonstrate that, on dense datasets, {\\\\em closedEmbTM-prune} not only generates\\na complete closed and maximal pattern set which is substantially smaller than\\nthat generated by the embedded pattern miner, but also runs much faster with\\nnegligible overhead on pattern pruning.', comment=None, journal_ref=None, doi='10.1016/j.datak.2021.101890', primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://dx.doi.org/10.1016/j.datak.2021.101890', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2012.13685v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.13685v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.14234v1', updated=datetime.datetime(2020, 12, 28, 14, 3, 18, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 28, 14, 3, 18, tzinfo=datetime.timezone.utc), title='Recommending Courses in MOOCs for Jobs: An Auto Weak Supervision Approach', authors=[arxiv.Result.Author('Bowen Hao'), arxiv.Result.Author('Jing Zhang'), arxiv.Result.Author('Cuiping Li'), arxiv.Result.Author('Hong Chen'), arxiv.Result.Author('Hongzhi Yin')], summary='The proliferation of massive open online courses (MOOCs) demands an effective\\nway of course recommendation for jobs posted in recruitment websites,\\nespecially for the people who take MOOCs to find new jobs. Despite the advances\\nof supervised ranking models, the lack of enough supervised signals prevents us\\nfrom directly learning a supervised ranking model. This paper proposes a\\ngeneral automated weak supervision framework AutoWeakS via reinforcement\\nlearning to solve the problem. On the one hand, the framework enables training\\nmultiple supervised ranking models upon the pseudo labels produced by multiple\\nunsupervised ranking models. On the other hand, the framework enables\\nautomatically searching the optimal combination of these supervised and\\nunsupervised models. Systematically, we evaluate the proposed model on several\\ndatasets of jobs from different recruitment websites and courses from a MOOCs\\nplatform. Experiments show that our model significantly outperforms the\\nclassical unsupervised, supervised and weak supervision baselines.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.IR'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.14234v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.14234v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.14555v2', updated=datetime.datetime(2021, 1, 6, 0, 9, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 29, 1, 26, 26, tzinfo=datetime.timezone.utc), title='Misplaced Subsequences Repairing with Application to Multivariate Industrial Time Series Data', authors=[arxiv.Result.Author('Xiaoou Ding'), arxiv.Result.Author('Hongzhi Wang'), arxiv.Result.Author('Jiaxuan Su'), arxiv.Result.Author('Chen Wang')], summary='Both the volume and the collection velocity of time series generated by\\nmonitoring sensors are increasing in the Internet of Things (IoT). Data\\nmanagement and analysis requires high quality and applicability of the IoT\\ndata. However, errors are prevalent in original time series data. Inconsistency\\nin time series is a serious data quality problem existing widely in IoT. Such\\nproblem could be hardly solved by existing techniques. Motivated by this, we\\ndefine an inconsistent subsequences problem in multivariate time series, and\\npropose an integrity data repair approach to solve inconsistent problems. Our\\nproposed repairing method consists of two parts: (1) we design effective\\nanomaly detection method to discover latent inconsistent subsequences in the\\nIoT time series; and (2) we develop repair algorithms to precisely locate the\\nstart and finish time of inconsistent intervals, and provide reliable repairing\\nstrategies. A thorough experiment on two real-life datasets verifies the\\nsuperiority of our method compared to other practical approaches. Experimental\\nresults also show that our method captures and repairs inconsistency problems\\neffectively in industrial time series in complex IIoT scenarios.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.14555v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.14555v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.15570v1', updated=datetime.datetime(2020, 12, 31, 12, 9, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 31, 12, 9, 54, tzinfo=datetime.timezone.utc), title='On the importance of functions in data modeling', authors=[arxiv.Result.Author('Alexandr Savinov')], summary='In this paper we argue that representing entity properties by tuple\\nattributes, as evangelized in most set-oriented data models, is a controversial\\nmethod conflicting with the principle of tuple immutability. As a principled\\nsolution to this problem of tuple immutability on one hand and the need to\\nmodify tuple attributes on the other hand, we propose to use mathematical\\nfunctions for representing entity properties. In this approach, immutable\\ntuples are intended for representing the existence of entities while mutable\\nfunctions (mappings between sets) are used for representing entity properties.\\nIn this model, called the concept-oriented model (COM), functions are made\\nfirst-class elements along with sets, and both functions and sets are used to\\nrepresent and process data in a simpler and more natural way in comparison to\\npurely set-oriented models.', comment='9 pages, 4 figures', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.15570v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.15570v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2012.15596v1', updated=datetime.datetime(2020, 12, 31, 13, 17, 37, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 12, 31, 13, 17, 37, tzinfo=datetime.timezone.utc), title='bloomRF: On Performing Range-Queries with Bloom-Filters based on Piecewise-Monotone Hash Functions and Dyadic Trace-Trees', authors=[arxiv.Result.Author('Christian Riegger'), arxiv.Result.Author('Arthur Bernhardt'), arxiv.Result.Author('Bernhard Moessner'), arxiv.Result.Author('Ilia Petrov')], summary='We introduce bloomRF as a unified method for approximate membership testing\\nthat supports both point- and range-queries on a single data structure. bloomRF\\nextends Bloom-Filters with range query support and may replace them. The core\\nidea is to employ a dyadic interval scheme to determine the set of dyadic\\nintervals covering a data point, which are then encoded and inserted. bloomRF\\nintroduces Dyadic Trace-Trees as novel data structure that represents those\\ncovering intervals implicitly. A Trace-Tree encoding scheme represents the set\\nof covering intervals efficiently, in a compact bit representation.\\nFurthermore, bloomRF introduces novel piecewise-monotone hash functions that\\nare locally order-preserving and thus support range querying. We present an\\nefficient membership computation method for range-queries. Although, bloomRF is\\ndesigned for integers it also supports string and floating-point data types. It\\ncan also handle multiple attributes and serve as multi-attribute filter.\\n  We evaluate bloomRF in RocksDB and in a standalone library. bloomRF is more\\nefficient and outperforms existing point-range-filters by up to 4x across a\\nrange of settings.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2012.15596v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2012.15596v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2101.00361v2', updated=datetime.datetime(2021, 3, 3, 6, 18, 38, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 1, 2, 3, 21, 32, tzinfo=datetime.timezone.utc), title='To Share, or not to Share Online Event Trend Aggregation Over Bursty Event Streams', authors=[arxiv.Result.Author('Olga Poppe'), arxiv.Result.Author('Chuan Lei'), arxiv.Result.Author('Lei Ma'), arxiv.Result.Author('Allison Rozet'), arxiv.Result.Author('Elke A. Rundensteiner')], summary='Complex event processing (CEP) systems continuously evaluate large workloads\\nof pattern queries under tight time constraints. Event trend aggregation\\nqueries with Kleene patterns are commonly used to retrieve summarized insights\\nabout the recent trends in event streams. State-of-art methods are limited\\neither due to repetitive computations or unnecessary trend construction.\\nExisting shared approaches are guided by statically selected and hence rigid\\nsharing plans that are often sub-optimal under stream fluctuations. In this\\nwork, we propose a novel framework Hamlet that is the first to overcome these\\nlimitations. Hamlet introduces two key innovations. First, Hamlet adaptively\\ndecides whether to share or not to share computations depending on the current\\nstream properties at run time to harvest the maximum sharing benefit. Second,\\nHamlet is equipped with a highly efficient shared trend aggregation strategy\\nthat avoids trend construction. Our experimental study on both real and\\nsynthetic data sets demonstrates that Hamlet consistently reduces query latency\\nby up to five orders of magnitude compared to the state-of-the-art approaches.', comment='Technical report for the paper in SIGMOD 2021', journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB', 'cs.PF'], links=[arxiv.Result.Link('http://arxiv.org/abs/2101.00361v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.00361v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2101.01363v1', updated=datetime.datetime(2021, 1, 5, 6, 19, 39, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 1, 5, 6, 19, 39, tzinfo=datetime.timezone.utc), title='Exploring Data and Knowledge combined Anomaly Explanation of Multivariate Industrial Data', authors=[arxiv.Result.Author('Xiaoou Ding'), arxiv.Result.Author('Hongzhi Wang'), arxiv.Result.Author('Chen Wang'), arxiv.Result.Author('Zijue Li'), arxiv.Result.Author('Zheng Liang')], summary='The demand for high-performance anomaly detection techniques of IoT data\\nbecomes urgent, especially in industry field. The anomaly identification and\\nexplanation in time series data is one essential task in IoT data mining. Since\\nthat the existing anomaly detection techniques focus on the identification of\\nanomalies, the explanation of anomalies is not well-solved. We address the\\nanomaly explanation problem for multivariate IoT data and propose a 3-step\\nself-contained method in this paper. We formalize and utilize the domain\\nknowledge in our method, and identify the anomalies by the violation of\\nconstraints. We propose set-cover-based anomaly explanation algorithms to\\ndiscover the anomaly events reflected by violation features, and further\\ndevelop knowledge update algorithms to improve the original knowledge set.\\nExperimental results on real datasets from large-scale IoT systems verify that\\nour method computes high-quality explanation solutions of anomalies. Our work\\nprovides a guide to navigate the explicable anomaly detection in both IoT fault\\ndiagnosis and temporal data cleaning.', comment=None, journal_ref=None, doi=None, primary_category='cs.DB', categories=['cs.DB'], links=[arxiv.Result.Link('http://arxiv.org/abs/2101.01363v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.01363v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2101.01507v1', updated=datetime.datetime(2021, 1, 5, 13, 47, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 1, 5, 13, 47, 45, tzinfo=datetime.timezone.utc), title='A Survey on Advancing the DBMS Query Optimizer: Cardinality Estimation, Cost Model, and Plan Enumeration', authors=[arxiv.Result.Author('Hai Lan'), arxiv.Result.Author('Zhifeng Bao'), arxiv.Result.Author('Yuwei Peng')], summary='Query optimizer is at the heart of the database systems. Cost-based optimizer\\nstudied in this paper is adopted in almost all current database systems. A\\ncost-based optimizer introduces a plan enumeration algorithm to find a\\n(sub)plan, and then uses a cost model to obtain the cost of that plan, and\\nselects the plan with the lowest cost. In the cost model, cardinality, the\\nnumber of tuples through an operator, plays a crucial role. Due to the\\ninaccuracy in cardinality estimation, errors in cost model, and the huge plan\\nspace, the optimizer cannot find the optimal execution plan for a complex query\\nin a reasonable time. In this paper, we first deeply study the causes behind\\nthe limitations above. Next, we review the techniques used to improve the\\nquality of the three key components in the cost-based optimizer, cardinality\\nestimation, cost model, and plan enumeration. We also provide our insights on\\nthe future directions for each of the above aspects.', comment='This paper was accepted by Data Science and Engineering (DSEJ) in\\n  Dec, 2020', journal_ref=None, doi='10.1007/s41019-020-00149-7', primary_category='cs.DB', categories=['cs.DB', 'cs.AI', 'H.2.4'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/s41019-020-00149-7', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2101.01507v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2101.01507v1', title='pdf', rel='related', content_type=None)]),\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the output\n",
    "arxiv_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4975db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list to store the final data\n",
    "final_metadata = []\n",
    "#print(type(paper.authors))\n",
    "# for loop to align the data to specific columns\n",
    "for paper in arxiv_metadata:\n",
    "    final_metadata.append({'title':paper.title,'authors': [author.name for author in paper.authors],'summary':paper.summary,'primary_category':paper.primary_category,'published_date':paper.published})\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8037290f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Corrigendum to \"Counting Database Repairs that Satisfy Conjunctive Queries with Self-Joins\"',\n",
       "  'authors': ['Jef Wijsen'],\n",
       "  'summary': 'The helping Lemma 7 in [Maslowski and Wijsen, ICDT, 2014] is false. The lemma\\nis used in (and only in) the proof of Theorem 3 of that same paper. In this\\ncorrigendum, we provide a new proof for the latter theorem.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 29, 12, 28, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Provenance for Interactive Visualizations',\n",
       "  'authors': ['Fotis Psallidas', 'Eugene Wu'],\n",
       "  'summary': 'We highlight the connections between data provenance and interactive\\nvisualizations. To do so, we first incrementally add interactions to a\\nvisualization and show how these interactions are readily expressible in terms\\nof provenance. We then describe how an interactive visualization system that\\nnatively supports provenance can be easily extended with novel interactions.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 7, 17, 11, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A framework supporting imprecise queries and data',\n",
       "  'authors': ['Giacomo Bergami'],\n",
       "  'summary': 'This technical report provides some lightweight introduction and some generic\\nuse case scenarios motivating the definition of a database supporting\\nuncertainties in both queries and data. This technical report is only providing\\nthe logical framework, which implementation is going to be provided in the\\nfinal paper.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 28, 21, 58, 43, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Open Data Portal Germany (OPAL) Projektergebnisse',\n",
       "  'authors': ['Adrian Wilke', 'Axel-Cyrille Ngonga Ngomo'],\n",
       "  'summary': 'In the Open Data Portal Germany (OPAL) project, a pipeline of the following\\ndata refinement steps has been developed: requirements analysis, data\\nacquisition, analysis, conversion, integration and selection. 800,000 datasets\\nin DCAT format have been produced.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 5, 7, 10, 59, 16, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Transactional Smart Contracts in Blockchain Systems',\n",
       "  'authors': ['Victor Zakhary', 'Divyakant Agrawal', 'Amr El Abbadi'],\n",
       "  'summary': 'This paper presents TXSC, a framework that provides smart contract developers\\nwith transaction primitives. These primitives allow developers to write smart\\ncontracts without the need to reason about the anomalies that can arise due to\\nconcurrent smart contract function executions.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 9, 14, 0, 36, 13, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Big Data Visualization Tools',\n",
       "  'authors': ['Nikos Bikakis'],\n",
       "  'summary': 'Data visualization is the presentation of data in a pictorial or graphical\\nformat, and a data visualization tool is the software that generates this\\npresentation. Data visualization provides users with intuitive means to\\ninteractively explore and analyze data, enabling them to effectively identify\\ninteresting patterns, infer correlations and causalities, and supports\\nsense-making activities.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 25, 10, 16, 48, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Subsumption of Weakly Well-Designed SPARQL Patterns is Undecidable',\n",
       "  'authors': ['Mark Kaminski', 'Egor V. Kostylev'],\n",
       "  'summary': 'Weakly well-designed SPARQL patterns is a recent generalisation of\\nwell-designed patterns, which preserve good computational properties but also\\ncapture almost all patterns that appear in practice. Subsumption is one of\\nstatic analysis problems for SPARQL, along with equivalence and containment. In\\nthis paper we show that subsumption is undecidable for weakly well-designed\\npatterns, which is in stark contrast to well-designed patterns, and to\\nequivalence and containment.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 27, 11, 18, 6, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Column2Vec: Structural Understanding via Distributed Representations of Database Schemas',\n",
       "  'authors': ['Michael J. Mior', 'Alexander G. Ororbia II'],\n",
       "  'summary': 'We present Column2Vec, a distributed representation of database columns based\\non column metadata. Our distributed representation has several applications.\\nUsing known names for groups of columns (i.e., a table name), we train a model\\nto generate an appropriate name for columns in an unnamed table. We demonstrate\\nthe viability of our approach using schema information collected from open\\nsource applications on GitHub.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 20, 17, 7, 11, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Sub-query Fragmentation for Query Analysis and Data Caching in the Distributed Environment',\n",
       "  'authors': ['Santhilata Kuppili Venkata', 'Katarzyna Musial'],\n",
       "  'summary': 'When data stores and users are distributed geographically, it is essential to\\norganize distributed data cache points at ideal locations to minimize data\\ntransfers. To answer this, we are developing an adaptive distributed data\\ncaching framework that can identify suitable data chunks to cache and move\\nacross a network of community cache locations.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 11, 6, 41, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'F-IVM: Learning over Fast-Evolving Relational Data',\n",
       "  'authors': ['Milos Nikolic', 'Haozhe Zhang', 'Ahmet Kara', 'Dan Olteanu'],\n",
       "  'summary': 'F-IVM is a system for real-time analytics such as machine learning\\napplications over training datasets defined by queries over fast-evolving\\nrelational databases. We will demonstrate F-IVM for three such applications:\\nmodel selection, Chow-Liu trees, and ridge linear regression.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 1, 3, 36, 27, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'LMFAO: An Engine for Batches of Group-By Aggregates',\n",
       "  'authors': ['Maximilian Schleich', 'Dan Olteanu'],\n",
       "  'summary': 'LMFAO is an in-memory optimization and execution engine for large batches of\\ngroup-by aggregates over joins. Such database workloads capture the\\ndata-intensive computation of a variety of data science applications.\\n  We demonstrate LMFAO for three popular models: ridge linear regression with\\nbatch gradient descent, decision trees with CART, and clustering with Rk-means.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 19, 20, 15, 32, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'On Declare MAX-SAT and a finite Herbrand Base for data-aware logs',\n",
       "  'authors': ['Giacomo Bergami'],\n",
       "  'summary': 'This technical report provides some lightweight introduction motivating the\\ndefinition of an alignment of log traces against Data-Aware Declare Models\\npotentially containing correlation conditions. This technical report is only\\nproviding the intuition of the logical framework as a feasibility study for a\\nfuture formalization and experiment section.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 14, 22, 26, 22, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Scripting Relational Database Engine Using Transducer',\n",
       "  'authors': ['Feng Tian'],\n",
       "  'summary': 'We allow database user to script a parallel relational database engine with a\\nprocedural language. Procedural language code is executed as a user defined\\nrelational query operator called transducer. Transducer is tightly integrated\\nwith relation engine, including query optimizer, query executor and can be\\nexecuted in parallel like other query operators. With transducer, we can\\nefficiently execute queries that are very difficult to express in SQL. As\\nexample, we show how to run time series and graph queries, etc, within a\\nparallel relational database.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 11, 7, 52, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A collection of database industrial techniques and optimization approaches of database operations',\n",
       "  'authors': ['Jasper Kyle Catapang'],\n",
       "  'summary': 'Databases play an essential role in our society today. Databases are embedded\\nin sectors like corporations, institutions, and government organizations, among\\nothers. These databases are used for our video and audio streaming platforms,\\nsocial gaming, finances, cloud storage, e-commerce, healthcare, economy, etc.\\nIt is therefore imperative that we learn how to properly execute database\\noperations and efficiently implement methodologies so that we may optimize the\\nperformance of databases.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 9, 10, 16, 33, 11, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Frequent Itemset Mining using QUBO',\n",
       "  'authors': ['Jonas Nüßlein'],\n",
       "  'summary': 'In this paper we propose a R-step approximation to solve frequent itemset\\nmining on quantum hardware like quantum annealing or QAOA. The idea is to\\nsearch for the set of items where the minimal 2-item frequency is maximal. This\\ncan be represented as a maximum clique problem.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 16, 14, 9, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Adaptive filter ordering in Spark',\n",
       "  'authors': ['Nikodimos Nikolaidis', 'Anastasios Gounaris'],\n",
       "  'summary': 'This report describes a technical methodology to render the Apache Spark\\nexecution engine adaptive. It presents the engineering solutions, which\\nspecifically target to adaptively reorder predicates in data streams with\\nevolving statistics. The system extension developed is available as an\\nopen-source prototype. Indicative experimental results show its overhead and\\nsensitivity to tuning parameters.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 3, 19, 36, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Answering Summation Queries for Numerical Attributes under Differential Privacy',\n",
       "  'authors': ['Yikai Wu',\n",
       "   'David Pujol',\n",
       "   'Ios Kotsogiannis',\n",
       "   'Ashwin Machanavajjhala'],\n",
       "  'summary': 'In this work we explore the problem of answering a set of sum queries under\\nDifferential Privacy. This is a little understood, non-trivial problem\\nespecially in the case of numerical domains. We show that traditional\\ntechniques from the literature are not always the best choice and a more\\nrigorous approach is necessary to develop low error algorithms.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 27, 15, 22, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'On the Importance of Location Privacy for Users of Location Based Applications',\n",
       "  'authors': ['Sina Shaham',\n",
       "   'Saba Rafieian',\n",
       "   'Ming Ding',\n",
       "   'Mahyar Shirvanimoghaddam',\n",
       "   'Zihuai Lin'],\n",
       "  'summary': 'Do people care about their location privacy while using location-based\\nservice apps? This paper aims to answer this question and several other\\nhypotheses through a survey, and review the privacy preservation techniques.\\nOur results indicate that privacy is indeed an influential factor in the\\nselection of location-based apps by users.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 5, 6, 3, 16, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Manifesto for Improved Foundations of Relational Model',\n",
       "  'authors': ['Witold Litwin'],\n",
       "  'summary': 'Normalized relations extended with inherited attributes can be more faithful\\nto reality and support logical navigation free queries, properties available at\\npresent only through specific views. Adding inherited attributes can be\\nnonetheless always less procedural than to define any such views. Present\\nschemes should even typically suffice for relations with foreign keys.\\nImplementing extended relations on popular DBSs appears also simple. Relational\\nmodel should evolve accordingly, for benefit of likely millions of DBAs,\\nclients, developers.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 16, 21, 28, 44, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Unlocking New York City Crime Insights using Relational Database Embeddings',\n",
       "  'authors': ['Apoorva Nitsure', 'Rajesh Bordawekar', 'Jose Neves'],\n",
       "  'summary': 'This version withdrawn by arXiv administrators because the author did not\\nhave the right to agree to our license at the time of submission.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 19, 17, 46, 34, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Synthetic Dataset Generation with Itemset-Based Generative Models',\n",
       "  'authors': ['Christian Lezcano', 'Marta Arias'],\n",
       "  'summary': 'This paper proposes three different data generators, tailored to\\ntransactional datasets, based on existing itemset-based generative models. All\\nthese generators are intuitive and easy to implement and show satisfactory\\nperformance. The quality of each generator is assessed by means of three\\ndifferent methods that capture how well the original dataset structure is\\npreserved.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 13, 10, 37, 21, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Palette diagram: A Python package for visualization of collective categorical data',\n",
       "  'authors': ['Chihiro Noguchi', 'Tatsuro Kawamoto'],\n",
       "  'summary': 'Categorical data, wherein a numerical quantity is assigned to each category\\n(nominal variable), are ubiquitous in data science. A palette diagram is a\\nvisualization tool for a large number of categorical datasets, each comprising\\nseveral categories.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 3, 10, 50, 5, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'ConQuer-92 -- The revised report on the conceptual query language LISA-D',\n",
       "  'authors': ['H. A. Proper'],\n",
       "  'summary': \"In this report the conceptual query language ConQuer-92 is introduced. This\\nquery language serves as the backbone of InfoAssistant's query facilities.\\nFurthermore, this language can also be used for the specification of derivation\\nrules (e.g. subtype defining rules) and textual constraints in InfoModeler.\\nThis report is solely concerned with a formal definition, and the explanation\\nthereof, of ConQuer-92. The implementation of ConQuer-92 in SQL-92 will be\\ntreated in a separate report.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 5, 21, 14, 9, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Cost models for geo-distributed massively parallel streaming analytics',\n",
       "  'authors': ['Anna-Valentini Michailidou',\n",
       "   'Anastasios Gounaris',\n",
       "   'Konstantinos Tsichlas'],\n",
       "  'summary': 'This report is part of the DataflowOpt project on optimization of modern\\ndataflows and aims to introduce a data quality-aware cost model that covers the\\nfollowing aspects in combination: (1) heterogeneity in compute nodes, (2)\\ngeo-distribution, (3) massive parallelism, (4) complex DAGs and (5) streaming\\napplications. Such a cost model can be then leveraged to devise cost-based\\noptimization solutions that deal with task placement and operator\\nconfiguration.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 5, 26, 12, 18, 32, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Discovering Technology Gaps using the IntSight Knowledge Navigator',\n",
       "  'authors': ['Aurpon Gupta',\n",
       "   'Subhasis Dasgupta',\n",
       "   'Snehasis Sinha',\n",
       "   'Amarnath Gupta'],\n",
       "  'summary': 'Knowledge analysis is an important application of knowledge graphs. In this\\npaper, we present a complex knowledge analysis problem that discovers the gaps\\nin the technology areas of interest to an organization. Our knowledge graph is\\ndeveloped on a heterogeneous data management platform. The analysis combines\\nsemantic search, graph analytics, and polystore query optimization.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 9, 11, 0, 2, 2, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'PREC: semantic translation of property graphs',\n",
       "  'authors': ['Julian Bruyat',\n",
       "   'Pierre-Antoine Champin',\n",
       "   'Lionel Médini',\n",
       "   'Frédérique Laforest'],\n",
       "  'summary': 'Converting property graphs to RDF graphs allows to enhance the\\ninteroperability of knowledge graphs. But existing tools perform the same\\nconversion for every graph, regardless of its content. In this paper, we\\npropose PREC, a user-configured conversion of property graphs to RDF graphs to\\nbetter capture the semantics of the content.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 10, 25, 14, 39, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Demonstration of PI2: Interactive Visualization Interface Generation for SQL Analysis in Notebook',\n",
       "  'authors': ['Jeffrey Tao', 'Yiru Chen', 'Eugene Wu'],\n",
       "  'summary': 'We demonstrate PI2, the first notebook extension that can automatically\\ngenerate interactive visualization interfaces during SQL-based analyses.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 1, 14, 20, 48, 8, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Comparing Flexible Skylines And Top-k Queries: Which Is the Best Alternative?',\n",
       "  'authors': ['Flavio Rizzoglio'],\n",
       "  'summary': 'The question of how to get the best results out of the data we have is an\\neverlasting problem in data science. The two main approaches to tackle the\\nproblem are top-k queries and skyline queries. Since their introduction, a new\\nparadigm called flexible skylines has emerged. The aim of this survey is to\\nprovide a solid comparison between the new and the old approaches,\\nunderstanding and exploring their differences and similarities.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 2, 13, 16, 1, 27, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Nebula Graph: An open source distributed graph database',\n",
       "  'authors': ['Min Wu', 'Xinglu Yi', 'Hui Yu', 'Yu Liu', 'Yujue Wang'],\n",
       "  'summary': 'This paper introduces the recent work of Nebula Graph, an open-source,\\ndistributed, scalable, and native graph database. We present a system design\\ntrade-off and a comprehensive overview of Nebula Graph internals, including\\ngraph data models, partitioning strategies, secondary indexes, optimizer rules,\\nstorage-side transactions, graph query languages, observability, graph\\nprocessing frameworks, and visualization tool-kits. In addition, three sets of\\nlarge-scale graph b',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 6, 15, 3, 38, 1, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Consistent Query Answering for Expressive Constraints under Tuple-Deletion Semantics',\n",
       "  'authors': ['Lorenzo Marconi', 'Riccardo Rosati'],\n",
       "  'summary': 'We study consistent query answering in relational databases. We consider an\\nexpressive class of schema constraints that generalizes both tuple-generating\\ndependencies and equality-generating dependencies. We establish the complexity\\nof consistent query answering and repair checking under tuple-deletion\\nsemantics for different fragments of the above constraint language. In\\nparticular, we identify new subclasses of constraints in which the above\\nproblems are tractable or even first-order rewritable.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 7, 19, 11, 15, 15, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Tree edit distance for hierarchical data compatible with HMIL paradigm',\n",
       "  'authors': ['Břetislav Šopík', 'Tomáš Strenáčik'],\n",
       "  'summary': 'We define edit distance for hierarchically structured data compatible with\\nthe hierarchical multi-instance learning paradigm. Example of such data is\\ndataset represented in JSON format where inner Array objects are interpreted as\\nunordered bags of elements. We prove correct analytical properties of the\\ndefined distance.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 7, 26, 14, 36, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Principles of Query Visualization',\n",
       "  'authors': ['Wolfgang Gatterbauer',\n",
       "   'Cody Dunne',\n",
       "   'H. V. Jagadish',\n",
       "   'Mirek Riedewald'],\n",
       "  'summary': 'Query Visualization (QV) is the problem of transforming a given query into a\\ngraphical representation that helps humans understand its meaning. This task is\\nnotably different from designing a Visual Query Language (VQL) that helps a\\nuser compose a query. This article discusses the principles of relational query\\nvisualization and its potential for simplifying user interactions with\\nrelational data.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 8, 2, 17, 44, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Real and simulated CBM data interacting with an ESCAPE datalake',\n",
       "  'authors': ['E. Clerkin', 'P. -N. Kramp', 'P. -A. Loizeau', 'M. Szuba'],\n",
       "  'summary': 'Integration of the ESCAPE and CBM software environment. The ESCAPE datalake\\nare utilized by the CBM experiment for the storage, distribution and retrieval\\nof real SIS18 and simulated SIS100 particle physics data.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 8, 19, 12, 33, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Re-looking at the View Update Problem',\n",
       "  'authors': ['Terry Brennan'],\n",
       "  'summary': 'Relational databases have always had a means for creating a pseudo-table,\\ncalled a view, defined by a query. Views are like tables in most ways, except\\nthat they are read-only and cannot be updated. The problem of how to update\\nviews has attracted a lot of attention in the 1980s but is unsolved.\\n  The best approach from that time was by Bancilhon and Spyratos. I use one of\\ntheir overlooked theorems and find a number of simple solutions for common\\nrelational operators.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 10, 27, 18, 21, 43, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Imprecise temporal associations and decision support systems',\n",
       "  'authors': ['Giovanni Vincenti'],\n",
       "  'summary': 'The quick and pervasive infiltration of decision support systems, artificial\\nintelligence, and data mining in consumer electronics and everyday life in\\ngeneral has been significant in recent years. Fields such as UX have been\\nfacilitating the integration of such technologies into software and hardware,\\nbut the back-end processing is still based on binary foundations. This article\\ndescribes an approach to mining for imprecise temporal associations among\\nevents in data streams, taking into account the very natural concept of\\napproximation. This type of association analysis is likely to lead to more\\nmeaningful and actionable decision support systems.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 3, 3, 21, 40, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Impacts of Dirty Data: and Experimental Evaluation',\n",
       "  'authors': ['Zhixin Qi', 'Hongzhi Wang', 'Jianzhong Li', 'Hong Gao'],\n",
       "  'summary': 'Data quality issues have attracted widespread attention due to the negative\\nimpacts of dirty data on data mining and machine learning results. The\\nrelationship between data quality and the accuracy of results could be applied\\non the selection of the appropriate algorithm with the consideration of data\\nquality and the determination of the data share to clean. However, rare\\nresearch has focused on exploring such relationship. Motivated by this, this\\npaper conducts an experimental comparison for the effects of missing,\\ninconsistent and conflicting data on classification and clustering algorithms.\\nBased on the experimental findings, we provide guidelines for algorithm\\nselection and data cleaning.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 3, 16, 4, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Semantic Query Language for Temporal Genealogical Trees',\n",
       "  'authors': ['Evgeniy Gryaznov'],\n",
       "  'summary': 'Computers play a crucial role in modern ancestry management, they are used to\\ncollect, store, analyze, sort and display genealogical data. However, current\\napplications do not take into account the kinship structure of a natural\\nlanguage.\\n  In this paper we propose a new domain-specific language KISP which is based\\non a formalization of English kinship system, for accessing and querying\\ntraditional genealogical trees. KISP is a dynamically typed LISP-like\\nprogramming language with a rich set of features, such as kinship term\\nreduction and temporal information expression.\\n  Our solution provides a user with a coherent genealogical framework that\\nallows for a natural navigation over any traditional family tree.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 7, 2, 11, 27, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'QR2: A Third-party Query Reranking Service Over Web Databases',\n",
       "  'authors': ['Yeshwanth D. Gunasekaran',\n",
       "   'Abolfazl Asudeh',\n",
       "   'Sona Hasani',\n",
       "   'Nan Zhang',\n",
       "   'Ali Jaoua',\n",
       "   'Gautam Das'],\n",
       "  'summary': 'The ranked retrieval model has rapidly become the de-facto way for search\\nquery processing in web databases. Despite the extensive efforts on designing\\nbetter ranking mechanisms, in practice, many such databases fail to address the\\ndiverse and sometimes contradicting preferences of users. In this paper, we\\npresent QR2, a third-party service that uses nothing but the public search\\ninterface of a web database and enables the on-the-fly processing of queries\\nwith any user-specified ranking functions, no matter if the ranking function is\\nsupported by the database or not.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 7, 13, 19, 31, 8, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Schema Integration on Massive Data Sources',\n",
       "  'authors': ['Tianbao Lia', 'Hongzhi Wang', 'Jianzhong Li', 'Hong Gao'],\n",
       "  'summary': 'As the fundamental phrase of collecting and analyzing data, data integration\\nis used in many applications, such as data cleaning, bioinformatics and pattern\\nrecognition. In big data era, one of the major problems of data integration is\\nto obtain the global schema of data sources since the global schema could be\\nhardly derived from massive data sources directly. In this paper, we attempt to\\nsolve such schema integration problem. For different scenarios, we develop\\nbatch and incremental schema integration algorithms. We consider the\\nrepresentation difference of attribute names in various data sources and\\npropose ED Join and Semantic Join algorithms to integrate attributes with\\ndifferent representations. Extensive experimental results demonstrate that the\\nproposed algorithms could integrate schemas efficiently and effectively.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 5, 14, 10, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Mining CFD Rules on Big Data',\n",
       "  'authors': ['Hongzhi Wang',\n",
       "   'Mingda Li',\n",
       "   'Jiawei Zhao',\n",
       "   'Jianzhong Li',\n",
       "   'Hong Gao'],\n",
       "  'summary': 'Current conditional functional dependencies (CFDs) discovery algorithms\\nalways need a well-prepared training data set. This makes them difficult to be\\napplied on large datasets which are always in low-quality. To handle the volume\\nissue of big data, we develop the sampling algorithms to obtain a small\\nrepresentative training set. For the low-quality issue of big data, we then\\ndesign the fault-tolerant rule discovery algorithm and the conflict resolution\\nalgorithm. We also propose parameter selection strategy for CFD discovery\\nalgorithm to ensure its effectiveness. Experimental results demonstrate that\\nour method could discover effective CFD rules on billion-tuple data within\\nreasonable time.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 5, 14, 11, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Database Operations in D4M.jl',\n",
       "  'authors': ['Lauren Milechin', 'Vijay Gadepally', 'Jeremy Kepner'],\n",
       "  'summary': 'Each step in the data analytics pipeline is important, including database\\ningest and query. The D4M-Accumulo database connector has allowed analysts to\\nquickly and easily ingest to and query from Apache Accumulo using MATLAB(R)/GNU\\nOctave syntax. D4M.jl, a Julia implementation of D4M, provides much of the\\nfunctionality of the original D4M implementation to the Julia community. In\\nthis work, we extend D4M.jl to include many of the same database capabilities\\nthat the MATLAB(R)/GNU Octave implementation provides. Here we will describe\\nthe D4M.jl database connector, demonstrate how it can be used, and show that it\\nhas comparable or better performance to the original implementation in\\nMATLAB(R)/GNU Octave.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 13, 20, 19, 56, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Automatic Generation of a Hybrid Query Execution Engine',\n",
       "  'authors': ['Aleksei Kashuba', 'Hannes Mühleisen'],\n",
       "  'summary': 'The ever-increasing need for fast data processing demands new methods for\\nefficient query execution. Just-in-time query compilation techniques have been\\ndemonstrated to improve performance in a set of analytical tasks significantly.\\nIn this work, we investigate the possibility of adding this approach to\\nexisting database solutions and the benefits it provides. To that end, we\\ncreate a set of automated tools to create a runtime code generation engine and\\nintegrate such an engine into SQLite which is one of the most popular\\nrelational databases in the world and is used in a large variety of contexts.\\nSpeedups of up to 1.7x were observed in microbenchmarks with queries involving\\na large number of operations.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 16, 12, 42, 29, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The First Order Truth behind Undecidability of Regular Path Queries Determinacy',\n",
       "  'authors': ['Grzegorz Głuch',\n",
       "   'Jerzy Marcinkowski',\n",
       "   'Piotr Ostropolski-Nalewaja'],\n",
       "  'summary': 'In our paper [G{\\\\l}uch, Marcinkowski, Ostropolski-Nalewaja, LICS ACM, 2018]\\nwe have solved an old problem stated in [Calvanese, De Giacomo, Lenzerini,\\nVardi, SPDS ACM, 2000] showing that query determinacy is undecidable for\\nRegular Path Queries. Here a strong generalisation of this result is shown, and\\n-- we think -- a very unexpected one. We prove that no regularity is needed:\\ndeterminacy remains undecidable even for finite unions of conjunctive path\\nqueries.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 22, 15, 12, 34, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Using Graph-Pattern Association Rules On Yago Knowledge Base',\n",
       "  'authors': ['Wahyudi',\n",
       "   'Masayu Leylia Khodra',\n",
       "   'Ary Setijadi Prihatmanto',\n",
       "   'Carmadi Machbub'],\n",
       "  'summary': 'We propose the use of Graph-Pattern Association Rules (GPARs) on the Yago\\nknowledge base. Extending association rules for itemsets, GPARS can help to\\ndiscover regularities between entities in knowledge bases. A rule-generated\\ngraph pattern (RGGP) algorithm was used for extracting rules from the Yago\\nknowledge base and a graph-pattern association rules algorithm for creating\\nassociation rules. Our research resulted in 1114 association rules, where the\\nvalue of standard confidence at 50.18% was better than partial completeness\\nassumption (PCA) confidence at 49.82%. Besides that the computation time for\\nstandard confidence was also better than for PCA confidence',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 9, 30, 6, 42, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Similarity Measure for Weaving Patterns in Textiles',\n",
       "  'authors': ['Sven Helmer', 'Vuong M. Ngo'],\n",
       "  'summary': 'We propose a novel approach for measuring the similarity between weaving\\npatterns that can provide similarity-based search functionality for textile\\narchives. We represent textile structures using hypergraphs and extract\\nmultisets of k-neighborhoods from these graphs. The resulting multisets are\\nthen compared using Jaccard coefficients, Hamming distances, and cosine\\nmeasures. We evaluate the different variants of our similarity measure\\nexperimentally, showing that it can be implemented efficiently and illustrating\\nits quality using it to cluster and query a data set containing more than a\\nthousand textile samples.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 10, 15, 50, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Knowledge Graph Completion to Predict Polypharmacy Side Effects',\n",
       "  'authors': ['Brandon Malone', 'Alberto García-Durán', 'Mathias Niepert'],\n",
       "  'summary': 'The polypharmacy side effect prediction problem considers cases in which two\\ndrugs taken individually do not result in a particular side effect; however,\\nwhen the two drugs are taken in combination, the side effect manifests. In this\\nwork, we demonstrate that multi-relational knowledge graph completion achieves\\nstate-of-the-art results on the polypharmacy side effect prediction problem.\\nEmpirical results show that our approach is particularly effective when the\\nprotein targets of the drugs are well-characterized. In contrast to prior work,\\nour approach provides more interpretable predictions and hypotheses for wet lab\\nvalidation.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 22, 12, 59, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Declarative Data Analytics: a Survey',\n",
       "  'authors': ['Nantia Makrynioti', 'Vasilis Vassalos'],\n",
       "  'summary': 'The area of declarative data analytics explores the application of the\\ndeclarative paradigm on data science and machine learning. It proposes\\ndeclarative languages for expressing data analysis tasks and develops systems\\nwhich optimize programs written in those languages. The execution engine can be\\neither centralized or distributed, as the declarative paradigm advocates\\nindependence from particular physical implementations. The survey explores a\\nwide range of declarative data analysis frameworks by examining both the\\nprogramming model and the optimization techniques used, in order to provide\\nconclusions on the current state of the art in the area and identify open\\nchallenges.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 4, 16, 52, 40, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Finding the Transitive Closure of Functional Dependencies using Strategic Port Graph Rewriting',\n",
       "  'authors': ['János Varga'],\n",
       "  'summary': 'We present a new approach to the logical design of relational databases,\\nbased on strategic port graph rewriting. We show how to model relational\\nschemata as attributed port graphs and provide port graph rewriting rules to\\nperform computations on functional dependencies. Using these rules we present a\\nstrategic graph program to find the transitive closure of a set of functional\\ndependencies. This program is sound, complete and terminating, assuming that\\nthere are no cyclical dependencies in the schema.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 6, 3, 23, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'WarpFlow: Exploring Petabytes of Space-Time Data',\n",
       "  'authors': ['Catalin Popescu',\n",
       "   'Deepak Merugu',\n",
       "   'Giao Nguyen',\n",
       "   'Shiva Shivakumar'],\n",
       "  'summary': 'WarpFlow is a fast, interactive data querying and processing system with a\\nfocus on petabyte-scale spatiotemporal datasets and Tesseract queries. With the\\nrapid growth in smartphones and mobile navigation services, we now have an\\nopportunity to radically improve urban mobility and reduce friction in how\\npeople and packages move globally every minute-mile, with data. WarpFlow speeds\\nup three key metrics for data engineers working on such datasets --\\ntime-to-first-result, time-to-full-scale-result, and time-to-trained-model for\\nmachine learning.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 9, 0, 23, 56, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Scaling Big Data Platform for Big Data Pipeline',\n",
       "  'authors': ['Rebecca Wild', 'Matthew Hubbell', 'Jeremy Kepner'],\n",
       "  'summary': 'Monitoring and Managing High Performance Computing (HPC) systems and\\nenvironments generate an ever growing amount of data. Making sense of this data\\nand generating a platform where the data can be visualized for system\\nadministrators and management to proactively identify system failures or\\nunderstand the state of the system requires the platform to be as efficient and\\nscalable as the underlying database tools used to store and analyze the data.\\nIn this paper we will show how we leverage Accumulo, d4m, and Unity to generate\\na 3D visualization platform to monitor and manage the Lincoln Laboratory\\nSupercomputer systems and how we have had to retool our approach to scale with\\nour systems.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 11, 15, 46, 54, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Snapshot Semantics for Temporal Multiset Relations (Extended Version)',\n",
       "  'authors': ['Anton Dignös',\n",
       "   'Boris Glavic',\n",
       "   'Xing Niu',\n",
       "   'Michael Böhlen',\n",
       "   'Johann Gamper'],\n",
       "  'summary': 'Snapshot semantics is widely used for evaluating queries over temporal data:\\ntemporal relations are seen as sequences of snapshot relations, and queries are\\nevaluated at each snapshot. In this work, we demonstrate that current\\napproaches for snapshot semantics over interval-timestamped multiset relations\\nare subject to two bugs regarding snapshot aggregation and bag difference. We\\nintroduce a novel temporal data model based on K-relations that overcomes these\\nbugs and prove it to correctly encode snapshot semantics. Furthermore, we\\npresent an efficient implementation of our model as a database middleware and\\ndemonstrate experimentally that our approach is competitive with native\\nimplementations and significantly outperforms such implementations on queries\\nthat involve aggregation.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 13, 14, 54, 40, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Compiling PL/SQL Away',\n",
       "  'authors': ['Christian Duta', 'Denis Hirn', 'Torsten Grust'],\n",
       "  'summary': '\"PL/SQL functions are slow,\" is common developer wisdom that derives from the\\ntension between set-oriented SQL evaluation and statement-by-statement PL/SQL\\ninterpretation. We pursue the radical approach of compiling PL/SQL away,\\nturning interpreted functions into regular subqueries that can then be\\nefficiently evaluated together with their embracing SQL query, avoiding any\\nPL/SQL to SQL context switches. Input PL/SQL functions may exhibit arbitrary\\ncontrol flow. Iteration, in particular, is compiled into SQL-level recursion.\\nRDBMSs across the board reward this compilation effort with significant run\\ntime savings that render established developer lore questionable.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 9, 7, 15, 42, 22, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Simple-ML: Towards a Framework for Semantic Data Analytics Workflows',\n",
       "  'authors': ['Simon Gottschalk',\n",
       "   'Nicolas Tempelmeier',\n",
       "   'Günter Kniesel',\n",
       "   'Vasileios Iosifidis',\n",
       "   'Besnik Fetahu',\n",
       "   'Elena Demidova'],\n",
       "  'summary': 'In this paper we present the Simple-ML framework that we develop to support\\nefficient configuration, robustness and reusability of data analytics workflows\\nthrough the adoption of semantic technologies. We present semantic data models\\nthat lay the foundation for the framework development and discuss the data\\nanalytics workflows based on these models. Furthermore, we present an example\\ninstantiation of the Simple-ML data models for a real-world use case in the\\nmobility domain.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 9, 12, 16, 23, 56, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Efficient Data Perturbation for Privacy Preserving and Accurate Data Stream Mining',\n",
       "  'authors': ['M. A. P. Chamikara',\n",
       "   'P. Bertok',\n",
       "   'D. Liu',\n",
       "   'S. Camtepe',\n",
       "   'I. Khalil'],\n",
       "  'summary': 'The widespread use of the Internet of Things (IoT) has raised many concerns,\\nincluding the protection of private information. Existing privacy preservation\\nmethods cannot provide a good balance between data utility and privacy, and\\nalso have problems with efficiency and scalability. This paper proposes an\\nefficient data stream perturbation method (named as $P^2RoCAl$). $P^2RoCAl$\\noffers better data utility than similar methods: classification accuracies of\\n$P^2RoCAl$ perturbed data streams are very close to those of the original data\\nstreams. $P^2RoCAl$ also provides higher resilience against data reconstruction\\nattacks.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 6, 15, 23, 51, 52, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Parallelization of XPath Queries using Modern XQuery Processors',\n",
       "  'authors': ['Shigeyuki Sato', 'Wei Hao', 'Kiminori Matsuzaki'],\n",
       "  'summary': 'A practical and promising approach to parallelizing XPath queries was\\nproposed by Bordawekar et al. in 2009, which enables parallelization on top of\\nexisting XML database engines. Although they experimentally demonstrated the\\nspeedup by their approach, their practice has already been out of date because\\nthe software environment has largely changed with the capability of XQuery\\nprocessing. In this work, we implement their approach in two ways on top of a\\nstate-of-the-art XML database engine and experimentally demonstrate that our\\nimplementations can bring significant speedup on a commodity server.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 6, 20, 13, 45, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A General Framework for Anytime Approximation in Probabilistic Databases',\n",
       "  'authors': ['Maarten Van den Heuvel',\n",
       "   'Floris Geerts',\n",
       "   'Wolfgang Gatterbauer',\n",
       "   'Martin Theobald'],\n",
       "  'summary': \"Anytime approximation algorithms that compute the probabilities of queries\\nover probabilistic databases can be of great use to statistical learning tasks.\\nThose approaches have been based so far on either (i) sampling or (ii)\\nbranch-and-bound with model-based bounds. We present here a more general\\nbranch-and-bound framework that extends the possible bounds by using\\n'dissociation', which yields tighter bounds.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 6, 26, 15, 55, 37, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'On the expressive power of linear algebra on graphs',\n",
       "  'authors': ['Floris Geerts'],\n",
       "  'summary': 'Most graph query languages are rooted in logic. By contrast, in this paper we\\nconsider graph query languages rooted in linear algebra. More specifically, we\\nconsider MATLANG, a matrix query language recently introduced, in which some\\nbasic linear algebra functionality is supported. We investigate the problem of\\ncharacterising equivalence of graphs, represented by their adjacency matrices,\\nfor various fragments of MATLANG. A complete picture is painted of the impact\\nof the linear algebra operations in MATLANG on their ability to distinguish\\ngraphs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 12, 11, 13, 13, 38, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Learning Semantic Annotations for Tabular Data',\n",
       "  'authors': ['Jiaoyan Chen',\n",
       "   'Ernesto Jimenez-Ruiz',\n",
       "   'Ian Horrocks',\n",
       "   'Charles Sutton'],\n",
       "  'summary': \"The usefulness of tabular data such as web tables critically depends on\\nunderstanding their semantics. This study focuses on column type prediction for\\ntables without any meta data. Unlike traditional lexical matching-based\\nmethods, we propose a deep prediction model that can fully exploit a table's\\ncontextual semantics, including table locality features learned by a Hybrid\\nNeural Network (HNN), and inter-column semantics features learned by a\\nknowledge base (KB) lookup and query answering algorithm.It exhibits good\\nperformance not only on individual table sets, but also when transferring from\\none table set to another.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 30, 20, 10, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Geo-L: Linking Geospatial Data Made Easy',\n",
       "  'authors': ['Christian Zinke-Wehlmann', 'Amit Kirschenbaum'],\n",
       "  'summary': 'Geospatial Linked Data is an emerging domain with growing interest in\\nresearch and industry. There is an increasing number of publicly available\\ngeospatial Linked Data resources and they need to be interlinked and easily\\nintegrated with private and industrial Linked Data on the Web. The present\\npaper introduces Geo-L, a system for discovery of RDF spatial links based on\\ntopological relations. Experiments show that the proposed system improves\\nstate-of-the-art spatial linking processes in terms of mapping-time and\\n-accuracy, as well as concerning resources retrieval efficiency and robustness.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 12, 20, 10, 29, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'DeepSPACE: Approximate Geospatial Query Processing with Deep Learning',\n",
       "  'authors': ['Dimitri Vorona',\n",
       "   'Andreas Kipf',\n",
       "   'Thomas Neumann',\n",
       "   'Alfons Kemper'],\n",
       "  'summary': 'The amount of the available geospatial data grows at an ever faster pace.\\nThis leads to the constantly increasing demand for processing power and storage\\nin order to provide data analysis in a timely manner. At the same time, a lot\\nof geospatial processing is visual and exploratory in nature, thus having\\nbounded precision requirements. We present DeepSPACE, a deep learning-based\\napproximate geospatial query processing engine which combines modest hardware\\nrequirements with the ability to answer flexible aggregation queries while\\nkeeping the required state to a few hundred KiBs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 14, 9, 16, 16, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Extracting Basic Graph Patterns from Triple Pattern Fragment Logs',\n",
       "  'authors': ['Nassopoulos Georges',\n",
       "   'Serrano-Alvarado Patricia',\n",
       "   'Molli Pascal',\n",
       "   'Desmontils Emmanuel'],\n",
       "  'summary': 'The Triple Pattern Fragment (TPF) approach is de-facto a new way to publish\\nLinked Data at low cost and with high server availability. However, data\\nproviders hosting TPF servers are not able to analyze the SPARQL queries they\\nexecute because they only receive and evaluate queries with one triple pattern.\\nIn this paper, we propose LIFT: an algorithm to extract Basic Graph Patterns\\n(BGPs) of executed queries from TPF server logs. Experiments show that LIFT\\nextracts BGPs with good precision and good recall generating limited noise.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 20, 12, 22, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards a Theory of Data-Diff: Optimal Synthesis of Succinct Data Modification Scripts',\n",
       "  'authors': ['Tana Wattanawaroon', 'Stephen Macke', 'Aditya Parameswaran'],\n",
       "  'summary': 'This paper addresses the Data-Diff problem: given a dataset and a subsequent\\nversion of the dataset, find the shortest sequence of operations that\\ntransforms the dataset to the subsequent version, under a restricted family of\\noperations. We consider operations similar to SQL UPDATE, each with a condition\\n(WHERE) that matches a subset of tuples and a modifier (SET) that makes changes\\nto those matched tuples. We characterize the problem based on different\\nconstraints on the attributes and the allowed conditions and modifiers,\\nproviding complexity classification and algorithms in each case.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 19, 0, 2, 34, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The Historic Development of the Zooarchaeological Database OssoBook and the xBook Framework for Scientific Databases',\n",
       "  'authors': ['Daniel Kaltenthaler', 'Johannes-Y. Lohrer'],\n",
       "  'summary': 'In this technical report, we describe the historic development of the\\nzooarchaeological database OssoBook and the resulting framework xBook, a\\ngeneric infrastructure for distributed, relational data management that is\\nmainly designed for the needs of scientific data. We describe the concepts of\\nthe architecture and its most important features. We especially point out the\\nServer-Client architecture, the synchronization process, the Launcher\\napplication, and the structure and features of the application.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 24, 16, 7, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Can One Escape Red Chains? Regular Path Queries Determinacy is Undecidable',\n",
       "  'authors': ['Grzegorz Głuch',\n",
       "   'Jerzy Marcinkowski',\n",
       "   'Piotr Ostropolski-Nalewaja'],\n",
       "  'summary': 'For a given set of queries (which are expressions in some query language)\\n$\\\\mathcal{Q}=\\\\{Q_1$, $Q_2, \\\\ldots Q_k\\\\}$ and for another query $Q_0$ we say\\nthat $\\\\mathcal{Q}$ determines $Q_0$ if -- informally speaking -- for every\\ndatabase $\\\\mathbb D$, the information contained in the views\\n$\\\\mathcal{Q}({\\\\mathbb D})$ is sufficient to compute $Q_0({\\\\mathbb D})$. Query\\nDeterminacy Problem is the problem of deciding, for given $\\\\mathcal{Q}$ and\\n$Q_0$, whether $\\\\mathcal{Q}$ determines $Q_0$. Many versions of this problem,\\nfor different query languages, were studied in database theory. In this paper\\nwe solve a problem stated in [CGLV02] and show that Query Determinacy Problem\\nis undecidable for the Regular Path Queries -- the paradigmatic query language\\nof graph databases.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 2, 5, 18, 33, 16, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Database Aggregation',\n",
       "  'authors': ['Francesco Belardinelli', 'Umberto Grandi'],\n",
       "  'summary': 'Knowledge can be represented compactly in a multitude ways, from a set of\\npropositional formulas, to a Kripke model, to a database. In this paper we\\nstudy the aggregation of information coming from multiple sources, each source\\nsubmitting a database modelled as a first-order relational structure. In the\\npresence of an integrity constraint, we identify classes of aggregators that\\nrespect it in the aggregated database, provided all individual databases\\nsatisfy it. We also characterise languages for first-order queries on which the\\nanswer to queries on the aggregated database coincides with the aggregation of\\nthe answers to the query obtained on each individual database. This\\ncontribution is meant to be a first step on the application of techniques from\\nrational choice theory to knowledge representation in databases.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 2, 23, 15, 15, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Formal Semantics of the Language Cypher',\n",
       "  'authors': ['Nadime Francis',\n",
       "   'Alastair Green',\n",
       "   'Paolo Guagliardo',\n",
       "   'Leonid Libkin',\n",
       "   'Tobias Lindaaker',\n",
       "   'Victor Marsault',\n",
       "   'Stefan Plantikow',\n",
       "   'Mats Rydberg',\n",
       "   'Martin Schuster',\n",
       "   'Petra Selmer',\n",
       "   'Andrés Taylor'],\n",
       "  'summary': 'Cypher is a query language for property graphs. It was originally designed\\nand implemented as part of the Neo4j graph database, and it is currently used\\nin a growing number of commercial systems, industrial applications and research\\nprojects. In this work, we provide denotational semantics of the core fragment\\nof the read-only part of Cypher, which features in particular pattern matching,\\nfiltering, and most relational operations on tables.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 2, 27, 16, 1, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data Management in Time-Domain Astronomy: Requirements and Challenges',\n",
       "  'authors': ['Chen Yang',\n",
       "   'Xiaofeng Meng',\n",
       "   'Zhihui Du',\n",
       "   'Zhiqiang Duan',\n",
       "   'Yongjie Du'],\n",
       "  'summary': 'In time-domain astronomy, we need to use the relational database to manage\\nstar catalog data. With the development of sky survey technology, the size of\\nstar catalog data is larger, and the speed of data generation is faster. So, in\\nthis paper, we make a systematic and comprehensive introduction to process the\\ndata in time-domain astronomy, and valuable research questions are detailed.\\nThen, we list candidate systems usually used in astronomy and point out the\\nadvantages and disadvantages of these systems. In addition, we present the key\\ntechniques needed to deal with astronomical data. Finally, we summarize the\\nchallenges faced by the design of our database prototype.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 11, 27, 7, 54, 43, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Improving the data quality in the research information systems',\n",
       "  'authors': ['Otmane Azeroual', 'Mohammad Abuosba'],\n",
       "  'summary': 'In order to introduce an integrated research information system, this will\\nprovide scientific institutions with the necessary information on research\\nactivities and research results in assured quality. Since data collection,\\nduplication, missing values, incorrect formatting, inconsistencies, etc. can\\narise in the collection of research data in different research information\\nsystems, which can have a wide range of negative effects on data quality, the\\nsubject of data quality should be treated with better results. This paper\\nexamines the data quality problems in research information systems and presents\\nthe new techniques that enable organizations to improve their quality of\\nresearch information.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 18, 13, 34, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'TigerGraph: A Native MPP Graph Database',\n",
       "  'authors': ['Alin Deutsch', 'Yu Xu', 'Mingxi Wu', 'Victor Lee'],\n",
       "  'summary': \"We present TigerGraph, a graph database system built from the ground up to\\nsupport massively parallel computation of queries and analytics.\\n  TigerGraph's high-level query language, GSQL, is designed for compatibility\\nwith SQL, while simultaneously allowing NoSQL programmers to continue thinking\\nin Bulk-Synchronous Processing (BSP) terms and reap the benefits of high-level\\nspecification.\\n  GSQL is sufficiently high-level to allow declarative SQL-style programming,\\nyet sufficiently expressive to concisely specify the sophisticated iterative\\nalgorithms required by modern graph analytics and traditionally coded in\\ngeneral-purpose programming languages like C++ and Java.\\n  We report very strong scale-up and scale-out performance over a benchmark we\\npublished on GitHub for full reproducibility.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 24, 6, 34, 15, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'SAVIME: A Multidimensional System for the Analysis and Visualization of Simulation Data',\n",
       "  'authors': ['Hermano Lustosa', 'Fabio Porto'],\n",
       "  'summary': 'Scientific applications produce a huge amount of data, which imposes serious\\nmanagement and analysis challenges. In particular, limitations in current\\ndatabase management systems prevent their adoption in simulation applications,\\nin which in-situ analysis libraries, in-transit I/O interfaces and scientific\\nformat files are preferred over DBMSs. In order to make simulation applications\\nbenefit from DBMS support, the author proposes the development of a system\\ncalled SAVIME in the context of his PhD thesis. SAVIME is an array database\\nsystem designed to manage numerical simulation data. In this document, the\\nauthor presents all work conducted so far and the current state of development.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 7, 14, 46, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Distributed Dependency Discovery',\n",
       "  'authors': ['Hemant Saxena', 'Lukasz Golab', 'Ihab F. Ilyas'],\n",
       "  'summary': 'We analyze the problem of discovering dependencies from distributed big data.\\nExisting (non-distributed) algorithms focus on minimizing computation by\\npruning the search space of possible dependencies. However, distributed\\nalgorithms must also optimize communication costs, especially in shared-nothing\\nsettings, leading to a more complex optimization space. To understand this\\nspace, we introduce six primitives shared by existing dependency discovery\\nalgorithms, corresponding to data processing steps separated by communication\\nbarriers. Through case studies, we show how the primitives allow us to analyze\\nthe design space and develop communication-optimized implementations. Finally,\\nwe support our analysis with an experimental evaluation on real datasets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 12, 21, 29, 8, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Categorical Data Integration for Computational Science',\n",
       "  'authors': ['Kristopher Brown', 'David I. Spivak', 'Ryan Wisnesky'],\n",
       "  'summary': 'Categorical Query Language is an open-source query and data integration\\nscripting language that can be applied to common challenges in the field of\\ncomputational science. We discuss how the structure-preserving nature of CQL\\ndata migrations protect those who publicly share data from the\\nmisinterpretation of their data. Likewise, this feature of CQL migrations\\nallows those who draw from public data sources to be sure only data which meets\\ntheir specification will actually be transferred. We argue some open problems\\nin the field of data sharing in computational science are addressable by\\nworking within this paradigm of functorial data migration. We demonstrate these\\ntools by integrating data from the Open Quantum Materials Database with some\\nalternative materials databases.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 25, 20, 8, 22, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Analysis of Co-Occurrence Patterns in Data through Modular and Clan Decompositions of Gaifman Graphs',\n",
       "  'authors': ['Marie Ely Piceno', 'José Luis Balcázar'],\n",
       "  'summary': 'We argue that the existing knowledge about modular decomposition of graphs\\nand clan decomposition of 2-structures can be put to use advantageously in a\\ncontext of data analysis. We show how to obtain visual descriptions of\\nco-occurrence patterns by employing these decompositions on possibly\\ngeneralized Gaifman graphs associated to datasets. We provide both theoretical\\nadvances that connect the proposed process to other data mining aspects\\n(namely, closed set mining), as well as implemented algorithmics leading to an\\nopen-source tool that demonstrates our approach.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 11, 12, 57, 17, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Queue-oriented Transaction Processing Paradigm',\n",
       "  'authors': ['Thamir M. Qadah'],\n",
       "  'summary': 'Transaction processing has been an active area of research for several\\ndecades. A fundamental characteristic of classical transaction processing\\nprotocols is non-determinism, which causes them to suffer from performance\\nissues on modern computing environments such as main-memory databases using\\nmany-core, and multi-socket CPUs and distributed environments. Recent proposals\\nof deterministic transaction processing techniques have shown great potential\\nin addressing these performance issues. In this position paper, I argue for a\\nqueue-oriented transaction processing paradigm that leads to better design and\\nimplementation of deterministic transaction processing protocols. I support my\\napproach with extensive experimental evaluations and demonstrate significant\\nperformance gains.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 23, 4, 53, 16, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Hardware-Conscious Stream Processing: A Survey',\n",
       "  'authors': ['Shuhao Zhang',\n",
       "   'Feng Zhang',\n",
       "   'Yingjun Wu',\n",
       "   'Bingsheng He',\n",
       "   'Paul Johns'],\n",
       "  'summary': 'Data stream processing systems (DSPSs) enable users to express and run stream\\napplications to continuously process data streams. To achieve real-time data\\nanalytics, recent researches keep focusing on optimizing the system latency and\\nthroughput. Witnessing the recent great achievements in the computer\\narchitecture community, researchers and practitioners have investigated the\\npotential of adoption hardware-conscious stream processing by better utilizing\\nmodern hardware capacity in DSPSs. In this paper, we conduct a systematic\\nsurvey of recent work in the field, particularly along with the following three\\ndirections: 1) computation optimization, 2) stream I/O optimization, and 3)\\nquery deployment. Finally, we advise on potential future research directions.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 16, 6, 22, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Leveraging Neighborhood Summaries for Efficient RDF Queries on RDBMS',\n",
       "  'authors': ['Lei Gai'],\n",
       "  'summary': 'Using structural informations to summarize graph-structured RDF data is\\nhelpful in tackling query performance issues. However, leveraging structural\\nindexes needs to revise or even redesign the internal of RDF systems. Given an\\nRDF dataset that have already been bulk loaded into a relational RDF system, we\\naim at improving the query performance on such systems. We do so by summarizing\\nneighborhood structures and encoding them into triples which can be managed\\nalong side the exist instance data. At query time, we optimally select the\\neffective structural patterns, and adding these patterns to the existing\\nqueries to gain an improved query performance. Empirical evaluations shown the\\neffectiveness of our method.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 23, 1, 27, 41, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Proceedings of Symposium on Data Mining Applications 2014',\n",
       "  'authors': ['Basit Qureshi', 'Yasir Javed'],\n",
       "  'summary': 'The Symposium on Data Mining and Applications (SDMA 2014) is aimed to gather\\nresearchers and application developers from a wide range of data mining related\\nareas such as statistics, computational intelligence, pattern recognition,\\ndatabases, Big Data Mining and visualization. SDMA is organized by MEGDAM to\\nadvance the state of the art in data mining research field and its various real\\nworld applications. The symposium will provide opportunities for technical\\ncollaboration among data mining and machine learning researchers around the\\nSaudi Arabia, GCC countries and Middle-East region. Acceptance will be based\\nprimarily on originality, significance and quality of contribution.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 29, 7, 30, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Theoretical Model and Practical Considerations for Data Lineage Reconstruction',\n",
       "  'authors': ['Egor Pushkin'],\n",
       "  'summary': \"We live in a world driven by data. The amount of it outgrows anyone's ability\\nto oversee it or even observe its scope. Along with all the advances in the\\nspace of data management, there is still a significant lack of formalism and\\nstandardization around defining data ecosystems and processes occurring within\\nthose. In order to address the issue we propose a notation for data flow\\nmodeling and evaluate some of the most common applications of it based on\\nreal-world use cases. To facilitate future work, we provide detailed reference\\nof the data model we defined and consider potential programming paradigms.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 30, 7, 45, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Dynamic Interleaving of Content and Structure for Robust Indexing of Semi-Structured Hierarchical Data (Extended Version)',\n",
       "  'authors': ['Kevin Wellenzohn', 'Michael H. Böhlen', 'Sven Helmer'],\n",
       "  'summary': 'We propose a robust index for semi-structured hierarchical data that supports\\ncontent-and-structure (CAS) queries specified by path and value predicates. At\\nthe heart of our approach is a novel dynamic interleaving scheme that merges\\nthe path and value dimensions of composite keys in a balanced way. We store\\nthese keys in our trie-based Robust Content-And-Structure index, which\\nefficiently supports a wide range of CAS queries, including queries with\\nwildcards and descendant axes. Additionally, we show important properties of\\nour scheme, such as robustness against varying selectivities, and demonstrate\\nimprovements of up to two orders of magnitude over existing approaches in our\\nexperimental evaluation.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 9, 9, 21, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': \"Needles in the 'Sheet'stack: Augmented Analytics to get Insights from Spreadsheets\",\n",
       "  'authors': ['Medha Atre',\n",
       "   'Anand Deshpande',\n",
       "   'Reshma Godse',\n",
       "   'Pooja Deokar',\n",
       "   'Sandip Moharir',\n",
       "   'Dhruva Ray',\n",
       "   'Akshay Chitlangia',\n",
       "   'Trupti Phadnis',\n",
       "   'Yugansh Goyal'],\n",
       "  'summary': 'Business intelligence (BI) tools for database analytics have come a long way\\nand nowadays also provide ready insights or visual query explorations, e.g.\\nQuickInsights by Microsoft Power BI, SpotIQ by ThoughtSpot, Zenvisage, etc. In\\nthis demo, we focus on providing insights by examining periodic spreadsheets of\\ndifferent reports (aka views), without prior knowledge of the schema of the\\ndatabase or reports, or data information. Such a solution is targeted at users\\nwithout the familiarity with the database schema or resources to conduct\\nanalytics in the contemporary way.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 15, 8, 54, 22, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Index Selection for NoSQL Database with Deep Reinforcement Learning',\n",
       "  'authors': ['Shun Yao', 'Hongzhi Wang', 'Yu Yan'],\n",
       "  'summary': 'We propose a new approach of NoSQL database index selection. For different\\nworkloads, we select different indexes and their different parameters to\\noptimize the database performance. The approach builds a deep reinforcement\\nlearning model to select an optimal index for a given fixed workload and adapts\\nto a changing workload. Experimental results show that, Deep Reinforcement\\nLearning Index Selection Approach (DRLISA) has improved performance to varying\\ndegrees according to traditional single index structures.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 16, 0, 40, 50, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Curating Covid-19 data in Links',\n",
       "  'authors': ['Vashti Galpin', 'James Cheney'],\n",
       "  'summary': 'Curated scientific databases play an important role in the scientific\\nendeavour and support is needed for the significant effort that goes into their\\ncreation and maintenance. This demonstration and case study illustrate how\\ncuration support has been developed in the Links cross-tier programming\\nlanguage, a functional, strongly typed language with language-integrated query\\nand support for temporal databases. The chosen case study uses weekly released\\nCovid-19 fatality figures from the Scottish government which exhibit updates to\\npreviously released data. This data allows the capture and query of update\\nprovenance in our prototype. This demonstration will highlight the potential\\nfor language-integrated support for curation to simplify and streamline\\nprototyping of web-applications in support of scientific databases',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 1, 11, 52, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Survey on Mining and Analysis of Uncertain Graphs',\n",
       "  'authors': ['Suman Banerjee'],\n",
       "  'summary': '\\\\emph{Uncertain Graph} (also known as \\\\emph{Probabilistic Graph}) is a\\ngeneric model to represent many real\\\\mbox{-}world networks from social to\\nbiological. In recent times analysis and mining of uncertain graphs have drawn\\nsignificant attention from the researchers of the data management community.\\nSeveral noble problems have been introduced and efficient methodologies have\\nbeen developed to solve those problems. Hence, there is a need to summarize the\\nexisting results on this topic in a self\\\\mbox{-}organized way. In this paper,\\nwe present a comprehensive survey on uncertain graph mining focusing on mainly\\nthree aspects: (i) different problems studied, (ii) computational challenges\\nfor solving those problems, and (iii) proposed methodologies. Finally, we list\\nout important future research directions.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 15, 2, 6, 34, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Querying in the Age of Graph Databases and Knowledge Graphs',\n",
       "  'authors': ['Marcelo Arenas', 'Claudio Gutierrez', 'Juan F. Sequeda'],\n",
       "  'summary': 'Graphs have become the best way we know of representing knowledge. The\\ncomputing community has investigated and developed the support for managing\\ngraphs by means of digital technology. Graph databases and knowledge graphs\\nsurface as the most successful solutions to this program. The goal of this\\ndocument is to provide a conceptual map of the data management tasks underlying\\nthese developments, paying particular attention to data models and query\\nlanguages for graphs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 22, 0, 17, 6, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Is 2NF a Stable Normal Form?',\n",
       "  'authors': ['Amir Sapir', 'Ariel Sapir'],\n",
       "  'summary': 'Traditionally, it was accepted that a relational database can be normalized\\nstep-by-step, from a set of un-normalized tables to tables in $1NF$, then to\\n$2NF$, then to $3NF$, then (possibly) to $BCNF$. The rule applied to a table in\\n$1NF$ in order to transform it to a set of tables in $2NF$ seems to be too\\nstraightforward to pose any difficulty.\\n  However, we show that, depending on the set of functional dependencies, it is\\nimpossible to reach $2NF$ and stop there; one must, in these cases, perform the\\nnormalization from $1NF$ to $3NF$ as an indecomposable move. The minimal setup\\nto exhibit the phenomena requires a single composite key, and two partially\\noverlapping chains of transitive dependencies.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 29, 18, 14, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Database as a Service - Current Issues and Its Future',\n",
       "  'authors': ['Xi Zheng'],\n",
       "  'summary': 'With the prevalence of applications in cloud, Database as a Service (DBaaS)\\nbecomes a promising method to provide cloud applications with reliable and\\nflexible data storage services. It provides a number of interesting features to\\ncloud developers, however, it suffers a few drawbacks: long learning curve and\\ndevelopment cycle, lacking of in-depth support for NoSQL, lacking of flexible\\nconfiguration for security and privacy, and high cost models. In this paper, we\\ninvestigate these issues among current DBaaS providers and propose a novel\\nTrinity Model that can significantly reduce the learning curves, improve the\\nsecurity and privacy, and accelerate database design and development. We\\nfurther elaborate our ongoing and future work on developing large real-world\\nSaaS projects using this new DBaaS model.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 4, 2, 12, 8, 1, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Accelerating Human-in-the-loop Machine Learning: Challenges and Opportunities',\n",
       "  'authors': ['Doris Xin',\n",
       "   'Litian Ma',\n",
       "   'Jialin Liu',\n",
       "   'Stephen Macke',\n",
       "   'Shuchen Song',\n",
       "   'Aditya Parameswaran'],\n",
       "  'summary': 'Development of machine learning (ML) workflows is a tedious process of\\niterative experimentation: developers repeatedly make changes to workflows\\nuntil the desired accuracy is attained. We describe our vision for a\\n\"human-in-the-loop\" ML system that accelerates this process: by intelligently\\ntracking changes and intermediate results over time, such a system can enable\\nrapid iteration, quick responsive feedback, introspection and debugging, and\\nbackground execution and automation. We finally describe Helix, our preliminary\\nattempt at such a system that has already led to speedups of up to 10x on\\ntypical iterative workflows against competing systems.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 4, 16, 18, 54, 11, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Measuring and Computing Database Inconsistency via Repairs',\n",
       "  'authors': ['Leopoldo Bertossi'],\n",
       "  'summary': 'We propose a generic numerical measure of inconsistency of a database with\\nrespect to a set of integrity constraints. It is based on an abstract repair\\nsemantics. A particular inconsistency measure associated to cardinality-repairs\\nis investigated; and we show that it can be computed via answer-set programs.\\n  Keywords: Integrity constraints in databases, inconsistent databases,\\ndatabase repairs, inconsistency measure.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 4, 24, 4, 4, 27, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'BUDAMAF: Data Management in Cloud Federations',\n",
       "  'authors': ['Evangelos Psomakelis',\n",
       "   'Konstantinos Tserpes',\n",
       "   'Dimosthenis Anagnostopoulos',\n",
       "   'Theodora Varvarigou'],\n",
       "  'summary': 'Data management has always been a multi-domain problem even in the simplest\\ncases. It involves, quality of service, security, resource management, cost\\nmanagement, incident identification, disaster avoidance and/or recovery, as\\nwell as many other concerns. In our case, this situation gets ever more\\ncomplicated because of the divergent nature of a cloud federation like BASMATI.\\nIn this federation, the BASMATI Unified Data Management Framework (BUDaMaF),\\ntries to create an automated uniform way of managing all the data transactions,\\nas well as the data stores themselves, in a polyglot multi-cloud, consisting of\\na plethora of different machines and data store systems.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 2, 8, 53, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Decomposition of quantitative Gaifman graphs as a data analysis tool',\n",
       "  'authors': ['José Luis Balcázar',\n",
       "   'Marie Ely Piceno',\n",
       "   'Laura Rodríguez-Navas'],\n",
       "  'summary': 'We argue the usefulness of Gaifman graphs of first-order relational\\nstructures as an exploratory data analysis tool. We illustrate our approach\\nwith cases where the modular decompositions of these graphs reveal interesting\\nfacts about the data. Then, we introduce generalized notions of Gaifman graphs,\\nenhanced with quantitative information, to which we can apply more general,\\nexisting decomposition notions via 2-structures; thus enlarging the analytical\\ncapabilities of the scheme. The very essence of Gaifman graphs makes this\\napproach immediately appropriate for the multirelational data framework.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 14, 15, 37, 2, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A New Finitely Controllable Class of Tuple Generating Dependencies: The Triangularly-Guarded Class',\n",
       "  'authors': ['Vernon Asuncion', 'Yan Zhang'],\n",
       "  'summary': 'In this paper we introduce a new class of tuple-generating dependencies\\n(TGDs) called triangularly-guarded (TG) TGDs. We show that conjunctive query\\nanswering under this new class of TGDs is decidable since this new class of\\nTGDs also satisfies the finite controllability (FC) property. We further show\\nthat this new class strictly contains some other decidable classes such as\\nweak-acyclic, guarded, sticky and shy. In this sense, the class TG provides a\\nunified representation of all these aforementioned classes of TGDs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 21, 23, 29, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Learned Cardinalities: Estimating Correlated Joins with Deep Learning',\n",
       "  'authors': ['Andreas Kipf',\n",
       "   'Thomas Kipf',\n",
       "   'Bernhard Radke',\n",
       "   'Viktor Leis',\n",
       "   'Peter Boncz',\n",
       "   'Alfons Kemper'],\n",
       "  'summary': 'We describe a new deep learning approach to cardinality estimation. MSCN is a\\nmulti-set convolutional network, tailored to representing relational query\\nplans, that employs set semantics to capture query features and true\\ncardinalities. MSCN builds on sampling-based estimation, addressing its\\nweaknesses when no sampled tuples qualify a predicate, and in capturing\\njoin-crossing correlations. Our evaluation of MSCN using a real-world dataset\\nshows that deep learning significantly enhances the quality of cardinality\\nestimation, which is the core problem in query optimization.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 9, 3, 18, 5, 12, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The Skiplist-Based LSM Tree',\n",
       "  'authors': ['Aron Szanto'],\n",
       "  'summary': 'Log-Structured Merge (LSM) Trees provide a tiered data storage and retrieval\\nparadigm that is attractive for write-optimized data systems. Maintaining an\\nefficient buffer in memory and deferring updates past their initial write-time,\\nthe structure provides quick operations over hot data. Because each layer of\\nthe structure is logically separate from the others, the structure is also\\nconducive to opportunistic and granular optimization. In this paper, we\\nintroduce the Skiplist-Based LSM Tree (sLSM), a novel system in which the\\nmemory buffer of the LSM is composed of a sequence of skiplists. We develop\\ntheoretical and experimental results that demonstrate that the breadth of\\ntuning parameters inherent to the sLSM allows it broad flexibility for\\nexcellent performance across a wide variety of workloads.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 9, 10, 12, 11, 29, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Integration of Relational and Graph Databases Functionally',\n",
       "  'authors': ['Jaroslav Pokorny'],\n",
       "  'summary': 'A significant category of NoSQL approaches is known as graph da-tabases. They\\nare usually represented by one property graph. We introduce a functional\\napproach to modelling relations and property graphs. Single-valued and\\nmultivalued functions will be sufficient in this case. Then, a typed\\n{\\\\lambda}-calculus, i.e., the language of lambda terms, will be used as a data\\nmanipulation lan-guage. Some integration options at the query language level\\nare discussed.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 9, 11, 12, 39, 17, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Repair-Based Degrees of Database Inconsistency: Computation and Complexity',\n",
       "  'authors': ['Leopoldo Bertossi'],\n",
       "  'summary': 'We propose a generic numerical measure of the inconsistency of a database\\nwith respect to a set of integrity constraints. It is based on an abstract\\nrepair semantics. In particular, an inconsistency measure associated to\\ncardinality-repairs is investigated in detail. More specifically, it is shown\\nthat it can be computed via answer-set programs, but sometimes its computation\\ncan be intractable in data complexity. However, polynomial-time deterministic\\nand randomized approximations are exhibited. The behavior of this measure under\\nsmall updates is analyzed, obtaining fixed-parameter tractability results.\\nFurthermore, alternative inconsistency measures are proposed and discussed.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 9, 27, 0, 57, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Answering Analytical Queries on Text Data with Temporal Term Histograms',\n",
       "  'authors': ['Kai Lin', 'Subhasis Dasgupta', 'Amarnath Gupta'],\n",
       "  'summary': \"Temporal text, i.e., time-stamped text data are found abundantly in a variety\\nof data sources like newspapers, blogs and social media posts. While today's\\ndata management systems provide facilities for searching full-text data, they\\ndo not provide any simple primitives for performing analytical operations with\\ntext. This paper proposes the temporal term histograms (TTH) as an intermediate\\nprimitive that can be used for analytical tasks. We propose an algebra, with\\noperators and equivalence rules for TTH and present a reference implementation\\non a relational database system.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 9, 28, 4, 47, 54, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Query the model: precomputations for efficient inference with Bayesian Networks',\n",
       "  'authors': ['Cigdem Aslay',\n",
       "   'Martino Ciaperoni',\n",
       "   'Aristides Gionis',\n",
       "   'Michael Mathioudakis'],\n",
       "  'summary': 'Variable Elimination is a fundamental algorithm for probabilistic inference\\nover Bayesian networks. In this paper, we propose a novel materialization\\nmethod for Variable Elimination, which can lead to significant efficiency gains\\nwhen answering inference queries. We evaluate our technique using real-world\\nBayesian networks. Our results show that a modest amount of materialization can\\nlead to significant improvements in the running time of queries. Furthermore,\\nin comparison with junction tree methods that also rely on materialization, our\\napproach achieves comparable efficiency during inference using significantly\\nlighter materialization.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 29, 20, 17, 48, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Persistent Memory I/O Primitives',\n",
       "  'authors': ['Alexander van Renen',\n",
       "   'Lukas Vogel',\n",
       "   'Viktor Leis',\n",
       "   'Thomas Neumann',\n",
       "   'Alfons Kemper'],\n",
       "  'summary': \"I/O latency and throughput is one of the major performance bottlenecks for\\ndisk-based database systems. Upcoming persistent memory (PMem) technologies,\\nlike Intel's Optane DC Persistent Memory Modules, promise to bridge the gap\\nbetween NAND-based flash (SSD) and DRAM, and thus eliminate the I/O bottleneck.\\nIn this paper, we provide one of the first performance evaluations of PMem in\\nterms of bandwidth and latency. Based on the results, we develop guidelines for\\nefficient PMem usage and two essential I/O primitives tuned for PMem: log\\nwriting and block flushing.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 2, 18, 39, 16, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Inconsistency Measures for Relational Databases',\n",
       "  'authors': ['Francesco Parisi', 'John Grant'],\n",
       "  'summary': 'In this paper, building on work done on measuring inconsistency in knowledge\\nbases, we introduce inconsistency measures for databases. In particular,\\nfocusing on databases with denial constraints, we first consider the natural\\napproach of virtually transforming a database into a propositional knowledge\\nbase and then applying well-known measures. However, using this method, tuples\\nand constraints are equally considered in charge of inconsistencies. Then, we\\nintroduce a version of inconsistency measures blaming database tuples only,\\ni.e., treating integrity constraints as irrefutable statements.\\n  We analyze the compliance of database inconsistency measures with standard\\nrationality postulates and find interesting relationships between measures.\\nFinally, we investigate the complexity of the inconsistency measurement problem\\nas well as of the problems of deciding whether the inconsistency is lower than,\\ngreater than, or equal to a given threshold.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 6, 9, 48, 57, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'BriskStream: Scaling Data Stream Processing on Shared-Memory Multicore Architectures',\n",
       "  'authors': ['Shuhao Zhang', 'Jiong He', 'Amelie Chi Zhou', 'Bingsheng He'],\n",
       "  'summary': \"We introduce BriskStream, an in-memory data stream processing system (DSPSs)\\nspecifically designed for modern shared-memory multicore architectures.\\nBriskStream's key contribution is an execution plan optimization paradigm,\\nnamely RLAS, which takes relative-location (i.e., NUMA distance) of each pair\\nof producer-consumer operators into consideration. We propose a branch and\\nbound based approach with three heuristics to resolve the resulting nontrivial\\noptimization problem. The experimental evaluations demonstrate that BriskStream\\nyields much higher throughput and better scalability than existing DSPSs on\\nmulti-core architectures when processing different types of workloads.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 7, 8, 22, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'On matrices and $K$-relations',\n",
       "  'authors': ['Robert Brijder', 'Marc Gyssens', 'Jan Van den Bussche'],\n",
       "  'summary': 'We show that the matrix query language $\\\\mathsf{MATLANG}$ corresponds to a\\nnatural fragment of the positive relational algebra on $K$-relations. The\\nfragment is defined by introducing a composition operator and restricting\\n$K$-relation arities to two. We then proceed to show that $\\\\mathsf{MATLANG}$\\ncan express all matrix queries expressible in the positive relational algebra\\non $K$-relations, when intermediate arities are restricted to three. Thus we\\noffer an analogue, in a model with numerical data, to the situation in\\nclassical logic, where the algebra of binary relations is equivalent to\\nfirst-order logic with three variables.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 8, 10, 26, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Estimating Cardinalities with Deep Sketches',\n",
       "  'authors': ['Andreas Kipf',\n",
       "   'Dimitri Vorona',\n",
       "   'Jonas Müller',\n",
       "   'Thomas Kipf',\n",
       "   'Bernhard Radke',\n",
       "   'Viktor Leis',\n",
       "   'Peter Boncz',\n",
       "   'Thomas Neumann',\n",
       "   'Alfons Kemper'],\n",
       "  'summary': 'We introduce Deep Sketches, which are compact models of databases that allow\\nus to estimate the result sizes of SQL queries. Deep Sketches are powered by a\\nnew deep learning approach to cardinality estimation that can capture\\ncorrelations between columns, even across tables. Our demonstration allows\\nusers to define such sketches on the TPC-H and IMDb datasets, monitor the\\ntraining process, and run ad-hoc queries against trained sketches. We also\\nestimate query cardinalities with HyPer and PostgreSQL to visualize the gains\\nover traditional cardinality estimators.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 17, 12, 29, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'tsmp: An R Package for Time Series with Matrix Profile',\n",
       "  'authors': ['Francisco Bischoff', 'Pedro Pereira Rodrigues'],\n",
       "  'summary': 'This article describes tsmp, an R package that implements the matrix profile\\nconcept for time series. The tsmp package is a toolkit that allows all-pairs\\nsimilarity joins, motif, discords and chains discovery, semantic segmentation,\\netc. Here we describe how the tsmp package may be used by showing some of the\\nuse-cases from the original articles and evaluate the algorithm speed in the R\\nenvironment. This package can be downloaded at\\nhttps://CRAN.R-project.org/package=tsmp.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 18, 5, 27, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Learning Restricted Regular Expressions with Interleaving',\n",
       "  'authors': ['Chunmei Dong', 'Yeting Li', 'Haiming Chen'],\n",
       "  'summary': 'The advantages for the presence of an XML schema for XML documents are\\nnumerous. However, many XML documents in practice are not accompanied by a\\nschema or by a valid schema. Relax NG is a popular and powerful schema\\nlanguage, which supports the unconstrained interleaving operator. Focusing on\\nthe inference of Relax NG, we propose a new subclass of regular expressions\\nwith interleaving and design a polynomial inference algorithm. Then we\\nconducted a series of experiments based on large-scale real data and on three\\nXML data corpora, and experimental results show that our subclass has a better\\npracticality than previous ones, and the regular expressions inferred by our\\nalgorithm are more precise.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 30, 11, 29, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'On the Impact of Memory Allocation on High-Performance Query Processing',\n",
       "  'authors': ['Dominik Durner', 'Viktor Leis', 'Thomas Neumann'],\n",
       "  'summary': 'Somewhat surprisingly, the behavior of analytical query engines is crucially\\naffected by the dynamic memory allocator used. Memory allocators highly\\ninfluence performance, scalability, memory efficiency and memory fairness to\\nother processes. In this work, we provide the first comprehensive experimental\\nanalysis on the impact of memory allocation for high-performance query engines.\\nWe test five state-of-the-art dynamic memory allocators and discuss their\\nstrengths and weaknesses within our DBMS. The right allocator can increase the\\nperformance of TPC-DS (SF 100) by 2.7x on a 4-socket Intel Xeon server.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 3, 12, 10, 2, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'RedisGraph GraphBLAS Enabled Graph Database',\n",
       "  'authors': ['Pieter Cailliau',\n",
       "   'Tim Davis',\n",
       "   'Vijay Gadepally',\n",
       "   'Jeremy Kepner',\n",
       "   'Roi Lipman',\n",
       "   'Jeffrey Lovitz',\n",
       "   'Keren Ouaknine'],\n",
       "  'summary': 'RedisGraph is a Redis module developed by Redis Labs to add graph database\\nfunctionality to the Redis database. RedisGraph represents connected data as\\nadjacency matrices. By representing the data as sparse matrices and employing\\nthe power of GraphBLAS (a highly optimized library for sparse matrix\\noperations), RedisGraph delivers a fast and efficient way to store, manage and\\nprocess graphs. Initial benchmarks indicate that RedisGraph is significantly\\nfaster than comparable graph databases.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 1, 23, 39, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Big Data Model \"Entity and Features\"',\n",
       "  'authors': ['Nataliya Shakhovska', 'Uyrii Bolubash', 'Oleh Veres'],\n",
       "  'summary': 'The article deals with the problem which led to Big Data. Big Data\\ninformation technology is the set of methods and means of processing different\\ntypes of structured and unstructured dynamic large amounts of data for their\\nanalysis and use of decision support. Features of NoSQL databases and\\ncategories are described. The developed Big Data Model \"Entity and Features\"\\nallows determining the distance between the sources of data on the availability\\nof information about a particular entity. The information structure of Big Data\\nhas been devised. It became a basis for further research and for concentrating\\non a problem of development of diverse data without their preliminary\\nintegration.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 3, 16, 13, 52, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Moving Processing to Data: On the Influence of Processing in Memory on Data Management',\n",
       "  'authors': ['Tobias Vincon', 'Andreas Koch', 'Ilia Petrov'],\n",
       "  'summary': 'Near-Data Processing refers to an architectural hardware and software\\nparadigm, based on the co-location of storage and compute units. Ideally, it\\nwill allow to execute application-defined data- or compute-intensive operations\\nin-situ, i.e. within (or close to) the physical data storage. Thus, Near-Data\\nProcessing seeks to minimize expensive data movement, improving performance,\\nscalability, and resource-efficiency. Processing-in-Memory is a sub-class of\\nNear-Data processing that targets data processing directly within memory (DRAM)\\nchips. The effective use of Near-Data Processing mandates new architectures,\\nalgorithms, interfaces, and development toolchains.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 12, 18, 27, 44, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'DataPop: Knowledge Base Population using Distributed Voice Enabled Devices',\n",
       "  'authors': ['Elena Montes',\n",
       "   'Monique Shotande',\n",
       "   'Daniel Helm',\n",
       "   'Christan Grant'],\n",
       "  'summary': 'Data scientists are constantly creating methods to efficiently and accurately\\npopulate big data sets for use in large-scale applications. Many recent efforts\\nutilize crowd-sourcing and textual interfaces. In this paper, we propose a new\\nmethod of curating data; namely, creating a multi-device Amazon Alexa Skill in\\nthe form of a research trivia game. Users experience a synchronized gaming\\nexperience with other Amazon Echo users, competing against one another while\\nfilling in gaps of a connected knowledge base. This allows for full\\nexploitation of the speed improvement offered by voice interface technology in\\na game-based format.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 29, 4, 45, 37, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Detecting coherent explorations in SQL workloads',\n",
       "  'authors': ['Veronika Peralta',\n",
       "   'Patrick Marcel',\n",
       "   'Willeme Verdeaux',\n",
       "   'Aboubakar Sidikhy Diakhaby'],\n",
       "  'summary': 'This paper presents a proposal aiming at better understanding a workload of\\nSQL queries and detecting coherent explorations hidden within the workload. In\\nparticular, our work investigates SQLShare [11], a database-as-a-service\\nplatform targeting scientists and data scientists with minimal database\\nexperience, whose workload was made available to the research community.\\nAccording to the authors of [11], this workload is the only one containing\\nprimarily ad-hoc hand-written queries over user-uploaded datasets. We analyzed\\nthis workload by extracting features that characterize SQL queries and we show\\nhow to use these features to separate sequences of SQL queries into meaningful\\nexplorations. We ran several tests over various query workloads to validate\\nempirically our approach.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 12, 8, 35, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Association rule mining and itemset-correlation based variants',\n",
       "  'authors': ['Niels Mündler'],\n",
       "  'summary': 'Association rules express implication formed relations among attributes in\\ndatabases of itemsets. The apriori algorithm is presented, the basis for most\\nassociation rule mining algorithms. It works by pruning away rules that need\\nnot be evaluated based on the user specified minimum support confidence.\\nAdditionally, variations of the algorithm are presented that enable it to\\nhandle quantitative attributes and to extract rules about generalizations of\\nitems, but preserve the downward closure property that enables pruning.\\nIntertransformation of the extensions is proposed for special cases.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 22, 19, 11, 49, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data Management for Causal Algorithmic Fairness',\n",
       "  'authors': ['Babak Salimi', 'Bill Howe', 'Dan Suciu'],\n",
       "  'summary': 'Fairness is increasingly recognized as a critical component of machine\\nlearning systems. However, it is the underlying data on which these systems are\\ntrained that often reflects discrimination, suggesting a data management\\nproblem. In this paper, we first make a distinction between associational and\\ncausal definitions of fairness in the literature and argue that the concept of\\nfairness requires causal reasoning. We then review existing works and identify\\nfuture opportunities for applying data management techniques to causal\\nalgorithmic fairness.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 20, 17, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Schema Matching using Machine Learning',\n",
       "  'authors': ['Tanvi Sahay', 'Ankita Mehta', 'Shruti Jadon'],\n",
       "  'summary': 'Schema Matching is a method of finding attributes that are either similar to\\neach other linguistically or represent the same information. In this project,\\nwe take a hybrid approach at solving this problem by making use of both the\\nprovided data and the schema name to perform one to one schema matching and\\nintroduce the creation of a global dictionary to achieve one to many schema\\nmatching. We experiment with two methods of one to one matching and compare\\nboth based on their F-scores, precision, and recall. We also compare our method\\nwith the ones previously suggested and highlight differences between them.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 24, 2, 40, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Multi-version Indexing in Flash-based Key-Value Stores',\n",
       "  'authors': ['Pulkit A. Misra',\n",
       "   'Jeffrey S. Chase',\n",
       "   'Johannes Gehrke',\n",
       "   'Alvin R. Lebeck'],\n",
       "  'summary': 'Maintaining multiple versions of data is popular in key-value stores since it\\nincreases concurrency and improves performance. However, designing a\\nmulti-version key-value store entails several challenges, such as additional\\ncapacity for storing extra versions and an indexing mechanism for mapping\\nversions of a key to their values. We present SkimpyFTL, a FTL-integrated\\nmulti-version key-value store that exploits the remap-on-write property of\\nflash-based SSDs for multi-versioning and provides a tradeoff between memory\\ncapacity and lookup latency for indexing.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 2, 5, 5, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Approximate Summaries for Why and Why-not Provenance (Extended Version)',\n",
       "  'authors': ['Seokki Lee', 'Bertram Ludaescher', 'Boris Glavic'],\n",
       "  'summary': 'Why and why-not provenance have been studied extensively in recent years.\\nHowever, why-not provenance, and to a lesser degree why provenance, can be very\\nlarge resulting in severe scalability and usability challenges. In this paper,\\nwe introduce a novel approximate summarization technique for provenance which\\novercomes these challenges. Our approach uses patterns to encode (why-not)\\nprovenance concisely. We develop techniques for efficiently computing\\nprovenance summaries balancing informativeness, conciseness, and completeness.\\nTo achieve scalability, we integrate sampling techniques into provenance\\ncapture and summarization. Our approach is the first to scale to large datasets\\nand to generate comprehensive and meaningful summaries.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 31, 22, 47, 43, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Explainable Queries over Event Logs',\n",
       "  'authors': ['Sylvain Hallé'],\n",
       "  'summary': 'Added value can be extracted from event logs generated by business processes\\nin various ways. However, although complex computations can be performed over\\nevent logs, the result of such computations is often difficult to explain; in\\nparticular, it is hard to determine what parts of an input log actually matters\\nin the production of that result. This paper describes how an existing log\\nprocessing library, called BeepBeep, can be extended in order to provide a form\\nof provenance: individual output events produced by a query can be precisely\\ntraced back to the data elements of the log that contribute to (i.e. \"explain\")\\nthe result.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 13, 16, 6, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Benchmarking Knowledge Graphs on the Web',\n",
       "  'authors': ['Michael Röder',\n",
       "   'Mohamed Ahmed Sherif',\n",
       "   'Muhammad Saleem',\n",
       "   'Felix Conrads',\n",
       "   'Axel-Cyrille Ngonga Ngomo'],\n",
       "  'summary': 'The growing interest in making use of Knowledge Graphs for developing\\nexplainable artificial intelligence, there is an increasing need for a\\ncomparable and repeatable comparison of the performance of Knowledge\\nGraph-based systems. History in computer science has shown that a main driver\\nto scientific advances, and in fact a core element of the scientific method as\\na whole, is the provision of benchmarks to make progress measurable. This paper\\ngives an overview of benchmarks used to evaluate systems that process Knowledge\\nGraphs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 14, 14, 2, 29, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The Knowledge Graph Track at OAEI -- Gold Standards, Baselines, and the Golden Hammer Bias',\n",
       "  'authors': ['Sven Hertling', 'Heiko Paulheim'],\n",
       "  'summary': 'The Ontology Alignment Evaluation Initiative (OAEI) is an annual evaluation\\nof ontology matching tools. In 2018, we have started the Knowledge Graph track,\\nwhose goal is to evaluate the simultaneous matching of entities and schemas of\\nlarge-scale knowledge graphs. In this paper, we discuss the design of the track\\nand two different strategies of gold standard creation. We analyze results and\\nexperiences obtained in first editions of the track, and, by revealing a hidden\\ntask, we show that all tools submitted to the track (and probably also to other\\ntracks) suffer from a bias which we name the golden hammer bias.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 24, 14, 35, 2, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Distributed Cross-Blockchain Transactions',\n",
       "  'authors': ['Dongfang Zhao', 'Tonglin Li'],\n",
       "  'summary': 'The interoperability across multiple or many blockchains would play a\\ncritical role in the forthcoming blockchain-based data management paradigm. In\\nparticular, how to ensure the ACID properties of those transactions across an\\narbitrary number of blockchains remains an open problem in both academic and\\nindustry: Existing solutions either work for only two blockchains or requires a\\ncentralized component, neither of which would meet the scalability requirement\\nin practice. This short paper shares our vision and some early results toward\\nscalable cross-blockchain transactions. Specifically, we design two distributed\\ncommit protocols and, both analytically and experimentally, demonstrate their\\neffectiveness.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 26, 19, 57, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An Empirical Study on the Design and Evolution of NoSQL Database Schemas',\n",
       "  'authors': ['Stefanie Scherzinger', 'Sebastian Sidortschuck'],\n",
       "  'summary': 'We study how software engineers design and evolve their domain model when\\nbuilding applications against NoSQL data stores. Specifically, we target Java\\nprojects that use object-NoSQL mappers to interface with schema-free NoSQL data\\nstores. Given the source code of ten real-world database applications, we\\nextract the implicit NoSQL database schema. We capture the sizes of the\\nschemas, and investigate whether the schema is denormalized, as is recommended\\npractice in data modeling for NoSQL data stores. Further, we analyze the entire\\nproject history, and with it, the evolution history of the NoSQL database\\nschema. In doing so, we conduct the so far largest empirical study on NoSQL\\nschema design and evolution.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 28, 20, 34, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Distribution Constraints: The Chase for Distributed Data',\n",
       "  'authors': ['Gaetano Geck', 'Frank Neven', 'Thomas Schwentick'],\n",
       "  'summary': 'This paper introduces a declarative framework to specify and reason about\\ndistributions of data over computing nodes in a distributed setting. More\\nspecifically, it proposes distribution constraints which are tuple and equality\\ngenerating dependencies (tgds and egds) extended with node variables ranging\\nover computing nodes. In particular, they can express co-partitioning\\nconstraints and constraints about range-based data distributions by using\\ncomparison atoms. The main technical contribution is the study of the\\nimplication problem of distribution constraints. While implication is\\nundecidable in general, relevant fragments of so-called data-full constraints\\nare exhibited for which the corresponding implication problems are complete for\\nEXPTIME, PSPACE and NP. These results yield bounds on deciding\\nparallel-correctness for conjunctive queries in the presence of distribution\\nconstraints.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 2, 15, 25, 15, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data Migration using Datalog Program Synthesis',\n",
       "  'authors': ['Yuepeng Wang',\n",
       "   'Rushi Shah',\n",
       "   'Abby Criswell',\n",
       "   'Rong Pan',\n",
       "   'Isil Dillig'],\n",
       "  'summary': 'This paper presents a new technique for migrating data between different\\nschemas. Our method expresses the schema mapping as a Datalog program and\\nautomatically synthesizes a Datalog program from simple input-output examples\\nto perform data migration. This approach can transform data between different\\ntypes of schemas (e.g., relational-to-graph, document-to-relational) and\\nperforms synthesis efficiently by leveraging the semantics of Datalog. We\\nimplement the proposed technique as a tool called Dynamite and show its\\neffectiveness by evaluating Dynamite on 28 realistic data migration scenarios.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 3, 4, 48, 40, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'LAQP: Learning-based Approximate Query Processing',\n",
       "  'authors': ['Meifan Zhang', 'Hongzhi Wang'],\n",
       "  'summary': 'Querying on big data is a challenging task due to the rapid growth of data\\namount. Approximate query processing (AQP) is a way to meet the requirement of\\nfast response. In this paper, we propose a learning-based AQP method called the\\nLAQP. The LAQP builds an error model learned from the historical queries to\\npredict the sampling-based estimation error of each new query. It makes a\\ncombination of the sampling-based AQP, the pre-computed aggregations and the\\nlearned error model to provide high-accurate query estimations with a small\\noff-line sample. The experimental results indicate that our LAQP outperforms\\nthe sampling-based AQP, the pre-aggregation-based AQP and the most recent\\nlearning-based AQP method.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 5, 6, 8, 25, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data Management for Context-Aware Computing',\n",
       "  'authors': ['Wenwei Xue', 'Hungkeng Pung', 'Wenlong Ng', 'Tao Gu'],\n",
       "  'summary': 'We envisage future context-aware applications will dynamically adapt their\\nbehaviors to various context data from sources in wide-area networks, such as\\nthe Internet. Facing the changing context and the sheer number of context\\nsources, a data management system that supports effective source organization\\nand efficient data lookup becomes crucial to the easy development of\\ncontext-aware applications. In this paper, we propose the design of a new\\ncontext data management system that is equipped with query processing\\ncapabilities. We encapsulate the context sources into physical spaces belonging\\nto different context spaces and organize them as peers in semantic overlay\\nnetworks. Initial evaluation results of an experimental system prototype\\ndemonstrate the effectiveness of our design',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 7, 3, 34, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Multi-dimensional Skyline Query to Find Best Shopping Mall for Customers',\n",
       "  'authors': ['Md Amiruzzaman', 'Suphanut Jamonnak'],\n",
       "  'summary': 'This paper presents a new application for multi-dimensional Skyline query.\\nThe idea presented in this paper can be used to find best shopping malls based\\non users requirements. A web-based application was used to simulate the problem\\nand proposed solution. Also, a mathematical definition was developed to define\\nthe problem and show how multi-dimensional Skyline query can be used to solve\\ncomplex problems, such as, finding shopping malls using multiple different\\ncriteria. The idea of this paper can be used in other fields, where different\\ncriteria should be considered.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 17, 2, 50, 13, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Implementing Suffix Array Algorithm Using Apache Big Table Data Implementation',\n",
       "  'authors': ['Piero Giacomelli'],\n",
       "  'summary': 'In this paper we will describe a new approach on the well-known suffix-array\\nalgorithm using Big Table Data Technology. We will demonstrate how it is\\npossible to refactor a well-known algorithm coupled by taking advantage of an\\nhigh-performance distributed datastore, to illustrate the advantages of using\\ndatastore cloud related technology for storing large text sequences and\\nretrieving them. A case study using DNA strings, considered one of the most\\ndifficult pattern matching problem, will be described and evaluated to\\ndemonstrate the potentiality of this implementation. Further discussion on\\nperformances and other big data related issues will be described as well as new\\npossible lines of research in big data technology for precise medical\\napplications.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 24, 21, 38, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'On the Efficient Design of LSM Stores',\n",
       "  'authors': ['Martin Weise'],\n",
       "  'summary': 'In the last decade, key-value data storage systems have gained significantly\\nmore interest from academia and industry. These systems face numerous\\nchallenges concerning storage space- and read optimization. There exists a\\nlarge potential for improving current solutions by introducing new management\\ntechniques and algorithms.\\n  In this paper we give an overview of the basic concept of key-value data\\nstorage systems and provide an explanation for bottlenecks. Further we\\nintroduce two new memory management algorithms and a improved index structure.\\nFinally, these solutions are compared to each other and discussed.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 4, 1, 47, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'GGDs: Graph Generating Dependencies',\n",
       "  'authors': ['Larissa C. Shimomura', 'George Fletcher', 'Nikolay Yakovets'],\n",
       "  'summary': 'We propose Graph Generating Dependencies (GGDs), a new class of dependencies\\nfor property graphs. Extending the expressivity of state of the art constraint\\nlanguages, GGDs can express both tuple- and equality-generating dependencies on\\nproperty graphs, both of which find broad application in graph data management.\\nWe provide the formal definition of GGDs, analyze the validation problem for\\nGGDs, and demonstrate the practical utility of GGDs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 21, 19, 20, 2, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An Algebraic Approach for High-level Text Analytics',\n",
       "  'authors': ['Xiuwen Zheng', 'Amarnath Gupta'],\n",
       "  'summary': 'Text analytical tasks like word embedding, phrase mining, and topic modeling,\\nare placing increasing demands as well as challenges to existing database\\nmanagement systems.\\n  In this paper, we provide a novel algebraic approach based on associative\\narrays. Our data model and algebra can bring together relational operators and\\ntext operators, which enables interesting optimization opportunities for hybrid\\ndata sources that have both relational and textual data. We demonstrate its\\nexpressive power in text analytics using several real-world tasks.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 3, 5, 41, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Knowledge Graph Validation',\n",
       "  'authors': ['Elwin Huaman', 'Elias Kärle', 'Dieter Fensel'],\n",
       "  'summary': 'Knowledge graphs (KGs) have shown to be an important asset of large companies\\nlike Google and Microsoft. KGs play an important role in providing structured\\nand semantically rich information, making them available to people and\\nmachines, and supplying accurate, correct and reliable knowledge. To do so a\\ncritical task is knowledge validation, which measures whether statements from\\nKGs are semantically correct and correspond to the so-called \"real\" world. In\\nthis paper, we provide an overview and review of the state-of-the-art\\napproaches, methods and tools on knowledge validation for KGs, as well as an\\nevaluation of them. As a result, we demonstrate a lack of reproducibility of\\ntools results, give insights, and state our future research direction.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 4, 11, 9, 11, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Improving Reverse k Nearest Neighbors Queries',\n",
       "  'authors': ['Lixin Ye'],\n",
       "  'summary': 'The reverse $k$ nearest neighbor query finds all points that have the query\\npoint as one of their $k$ nearest neighbors, where the $k$NN query finds the\\n$k$ closest points to its query point. Based on conics, we propose an efficent\\nR$k$NN verification method. By using the proposed verification method, we\\nimplement an efficient R$k$NN algorithm on VoR-tree, which has a computational\\ncomplexity of $O(k^{1.5}\\\\cdot log\\\\,k)$. The comparative experiments are\\nconducted between our algorithm and other two state-of-the-art R$k$NN\\nalgorithms. The experimental results indicate that the efficiency of our\\nalgorithm is significantly higher than its competitors.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 18, 6, 48, 45, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'CICLAD: A Fast and Memory-efficient Closed Itemset Miner for Streams',\n",
       "  'authors': ['Tomas Martin', 'Guy Francoeur', 'Petko Valtchev'],\n",
       "  'summary': \"Mining association rules from data streams is a challenging task due to the\\n(typically) limited resources available vs. the large size of the result.\\nFrequent closed itemsets (FCI) enable an efficient first step, yet current FCI\\nstream miners are not optimal on resource consumption, e.g. they store a large\\nnumber of extra itemsets at an additional cost. In a search for a better\\nstorage-efficiency trade-off, we designed Ciclad,an intersection-based\\nsliding-window FCI miner. Leveraging in-depth insights into FCI evolution, it\\ncombines minimal storage with quick access. Experimental results indicate\\nCiclad's memory imprint is much lower and its performances globally better than\\ncompetitor methods.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 3, 21, 50, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Tractability Beyond $β$-Acyclicity for Conjunctive Queries with Negation',\n",
       "  'authors': ['Matthias Lanzinger'],\n",
       "  'summary': 'Numerous fundamental database and reasoning problems are known to be NP-hard\\nin general but tractable on instances where the underlying hypergraph structure\\nis $\\\\beta$-acyclic. Despite the importance of many of these problems, there has\\nbeen little success in generalizing these results beyond acyclicity.\\n  In this paper, we take on this challenge and propose nest-set width, a novel\\ngeneralization of hypergraph $\\\\beta$-acyclicity. We demonstrate that nest-set\\nwidth has desirable properties and algorithmic significance. In particular,\\nevaluation of boolean conjunctive queries with negation is tractable for\\nclasses with bounded nest-set width. Furthermore, propositional satisfiability\\nis fixed-parameter tractable when parameterized by nest-set width.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 17, 10, 10, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Score-Based Explanations in Data Management and Machine Learning',\n",
       "  'authors': ['Leopoldo Bertossi'],\n",
       "  'summary': 'We describe some approaches to explanations for observed outcomes in data\\nmanagement and machine learning. They are based on the assignment of numerical\\nscores to predefined and potentially relevant inputs. More specifically, we\\nconsider explanations for query answers in databases, and for results from\\nclassification models. The described approaches are mostly of a causal and\\ncounterfactual nature. We argue for the need to bring domain and semantic\\nknowledge into score computations; and suggest some ways to do this.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 24, 23, 13, 27, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'GeoSPARQL+: Syntax, Semantics and System for Integrated Querying of Graph, Raster and Vector Data -- Technical Report',\n",
       "  'authors': ['Timo Homburg', 'Steffen Staab', 'Daniel Janke'],\n",
       "  'summary': 'We introduce an approach to semantically represent and query raster data in a\\nSemantic Web graph. We extend the GeoSPARQL vocabulary and query language to\\nsupport raster data as a new type of geospatial data. We define new filter\\nfunctions and illustrate our approach using several use cases on real-world\\ndata sets. Finally, we describe a prototypical implementation and validate the\\nfeasibility of our approach.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 10, 17, 53, 19, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Simple and Efficient Framework for Identifying Relation-gaps in Ontologies',\n",
       "  'authors': ['Subhashree S', 'P Sreenivasa Kumar'],\n",
       "  'summary': 'Though many ontologies have huge number of classes, one cannot find a good\\nnumber of object properties connecting the classes in most of the cases. Adding\\nobject properties makes an ontology richer and more applicable for tasks such\\nas Question Answering. In this context, the question of which two classes\\nshould be considered for discovering object properties becomes very important.\\nWe address the above question in this paper. We propose a simple machine\\nlearning framework which exhibits low time complexity and yet gives promising\\nresults with respect to both precision as well as number of class-pairs\\nretrieved.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 12, 14, 6, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data mining and time series segmentation via extrema: preliminary investigations',\n",
       "  'authors': ['Michel Fliess', 'Cédric Join'],\n",
       "  'summary': 'Time series segmentation is one of the many data mining tools. This paper, in\\nFrench, takes local extrema as perceptually interesting points (PIPs). The\\nblurring of those PIPs by the quick fluctuations around any time series is\\ntreated via an additive decomposition theorem, due to Cartier and Perrin, and\\nalgebraic estimation techniques, which are already useful in automatic control\\nand signal processing. Our approach is validated by several computer\\nillustrations. They underline the importance of the choice of a threshold for\\nthe extrema detection.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 2, 16, 24, 1, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Compressed Key Sort and Fast Index Reconstruction',\n",
       "  'authors': ['Yongsik Kwon',\n",
       "   'Cheol Ryu',\n",
       "   'Sang Kyun Cha',\n",
       "   'Arthur H. Lee',\n",
       "   'Kunsoo Park',\n",
       "   'Bongki Moon'],\n",
       "  'summary': 'In this paper we propose an index key compression scheme based on the notion\\nof distinction bits by proving that the distinction bits of index keys are\\nsufficient information to determine the sorted order of the index keys\\ncorrectly. While the actual compression ratio may vary depending on the\\ncharacteristics of datasets (an average of 2.76 to one compression ratio was\\nobserved in our experiments), the index key compression scheme leads to\\nsignificant performance improvements during the reconstruction of large-scale\\nindexes. Our index key compression can be effectively used in database\\nreplication and index recovery of modern main-memory database systems.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 24, 8, 21, 10, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Systematic Method for On-The-Fly Denormalization of Relational Databases',\n",
       "  'authors': ['Sareen Shah'],\n",
       "  'summary': 'Normalized relational databases are a common method for storing data, but\\npulling out usable denormalized data for consumption generally requires either\\ndirect access to the source data or creation of an appropriate view or table by\\na database administrator. End-users are thus limited in their ability to\\nexplore and use data that is stored in this manner. Presented here is a method\\nfor performing automated denormalization of relational databases at run-time,\\nwithout requiring access to source data or ongoing intervention by a database\\nadministrator. Furthermore, this method does not require a restructure of the\\ndatabase itself and so it can be flexibly applied as a layer on top of already\\nexisting databases.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 30, 17, 51, 37, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Validating UTF-8 In Less Than One Instruction Per Byte',\n",
       "  'authors': ['John Keiser', 'Daniel Lemire'],\n",
       "  'summary': 'The majority of text is stored in UTF-8, which must be validated on\\ningestion. We present the lookup algorithm, which outperforms UTF-8 validation\\nroutines used in many libraries and languages by more than 10 times using\\ncommonly available SIMD instructions. To ensure reproducibility, our work is\\nfreely available as open source software.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 6, 23, 40, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'MithraDetective: A System for Cherry-picked Trendlines Detection',\n",
       "  'authors': ['Yoko Nagafuchi',\n",
       "   'Yin Lin',\n",
       "   'Kaushal Mamgain',\n",
       "   'Abolfazl Asudeh',\n",
       "   'H. V. Jagadish',\n",
       "   'You',\n",
       "   'Wu',\n",
       "   'Cong Yu'],\n",
       "  'summary': \"Given a data set, misleading conclusions can be drawn from it by\\ncherry-picking selected samples. One important class of conclusions is a trend\\nderived from a data set of values over time. Our goal is to evaluate whether\\nthe 'trends' described by the extracted samples are representative of the true\\nsituation represented in the data. We demonstrate MithraDetective, a system to\\ncompute a support score to indicate how cherry-picked a statement is; that is,\\nwhether the reported trend is well-supported by the data. The system can also\\nbe used to discover more supported alternatives. MithraDetective provides an\\ninteractive visual interface for both tasks.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 17, 15, 7, 45, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An analysis of the SIGMOD 2014 Programming Contest: Complex queries on the LDBC social network graph',\n",
       "  'authors': ['Márton Elekes', 'János Benjamin Antal', 'Gábor Szárnyas'],\n",
       "  'summary': 'This report contains an analysis of the queries defined in the SIGMOD 2014\\nProgramming Contest. We first describe the data set, then present the queries,\\nproviding graphical illustrations for them and pointing out their caveats. Our\\nintention is to document our lessons learnt and simplify the work of those who\\nwill attempt to create a solution to this contest. We also demonstrate the\\ninfluence of this contest by listing followup works which used these queries as\\ninspiration to design better algorithms or to define interesting graph queries.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 23, 9, 4, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Answer Graph: Factorization Matters in Large Graphs',\n",
       "  'authors': ['Zahid Abul-Basher',\n",
       "   'Nikolay Yakovets',\n",
       "   'Parke Godfrey',\n",
       "   'Stanley Clark',\n",
       "   'Mark Chignell'],\n",
       "  'summary': \"Our answer-graph method to evaluate SPARQL conjunctive queries (CQs) finds a\\nfactorized answer set first, an answer graph, and then finds the embedding\\ntuples from this. This approach can reduce greatly the cost to evaluate CQs.\\nThis affords a second advantage: we can construct a cost-based planner. We\\npresent the answer-graph approach, and overview our prototype system,\\nWireframe. We then offer proof of concept via a micro-benchmark over the YAGO2s\\ndataset with two prevalent shapes of queries, snowflake and diamond. We compare\\nWireframe's performance over these against PostgreSQL, Virtuoso, MonetDB, and\\nNeo4J to illustrate the performance advantages of our answer-graph approach.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 10, 0, 10, 43, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'First-Order Rewritability of Frontier-Guarded Ontology-Mediated Queries',\n",
       "  'authors': ['Pablo Barcelo',\n",
       "   'Gerald Berger',\n",
       "   'Carsten Lutz',\n",
       "   'Andreas Pieris'],\n",
       "  'summary': 'We focus on ontology-mediated queries (OMQs) based on (frontier-)guarded\\nexistential rules and (unions of) conjunctive queries, and we investigate the\\nproblem of FO-rewritability, i.e., whether an OMQ can be rewritten as a\\nfirst-order query. We adopt two different approaches. The first approach\\nemploys standard two-way alternating parity tree automata. Although it does not\\nlead to a tight complexity bound, it provides a transparent solution based on\\nwidely known tools. The second approach relies on a sophisticated automata\\nmodel, known as cost automata. This allows us to show that our problem is\\n2ExpTime-complete. In both approaches, we provide semantic characterizations of\\nFO-rewritability that are of independent interest.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 18, 14, 31, 17, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data Lakes for Digital Humanities',\n",
       "  'authors': ['Jérôme Darmont',\n",
       "   'Cécile Favre',\n",
       "   'Sabine Loudcher',\n",
       "   'Camille Noûs'],\n",
       "  'summary': 'Traditional data in Digital Humanities projects bear various formats\\n(structured, semi-structured, textual) and need substantial transformations\\n(encoding and tagging, stemming, lemmatization, etc.) to be managed and\\nanalyzed. To fully master this process, we propose the use of data lakes as a\\nsolution to data siloing and big data variety problems. We describe data lake\\nprojects we currently run in close collaboration with researchers in humanities\\nand social sciences and discuss the lessons learned running these projects.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 4, 8, 18, 48, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Computational Complexity of Three Central Problems in Itemset Mining',\n",
       "  'authors': ['Christian Bessiere', 'Mohamed-Bachir Belaid', 'Nadjib Lazaar'],\n",
       "  'summary': 'Itemset mining is one of the most studied tasks in knowledge discovery. In\\nthis paper we analyze the computational complexity of three central itemset\\nmining problems. We prove that mining confident rules with a given item in the\\nhead is NP-hard. We prove that mining high utility itemsets is NP-hard. We\\nfinally prove that mining maximal or closed itemsets is coNP-hard as soon as\\nthe users can specify constraints on the kind of itemsets they are interested\\nin.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 4, 14, 26, 21, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Knowledge Graph Management on the Edge',\n",
       "  'authors': ['Weiqin Xu', 'Olivier Curé', 'Philippe Calvez'],\n",
       "  'summary': 'Edge computing emerges as an innovative platform for services requiring low\\nlatency decision making. Its success partly depends on the existence of\\nefficient data management systems. We consider that knowledge graph management\\nsystems have a key role to play in this context due to their data integration\\nand reasoning features. In this paper, we present SuccinctEdge, a compact,\\ndecompression-free, self-index, in-memory RDF store that can answer SPARQL\\nqueries, including those requiring reasoning services associated to some\\nontology. We provide details on its design and implementation before\\ndemonstrating its efficiency on real-world and synthetic datasets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 13, 17, 25, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Event Data Quality: A Survey',\n",
       "  'authors': ['Ruihong Huang', 'Jianmin Wang'],\n",
       "  'summary': 'Event data are prevalent in diverse domains such as financial trading,\\nbusiness workflows and industrial IoT nowadays. An event is often characterized\\nby several attributes denoting the meaning associated with the corresponding\\noccurrence time/duration. From traditional operational systems in enterprises\\nto online systems for Web services, event data is generated from physical world\\nuninterruptedly. However, due to the variety and veracity features of Big data,\\nevent data generated from heterogeneous and dirty sources could have very\\ndifferent event representations and data quality issues. In this work, we\\nsummarize several typical works on studying data quality issues of event data,\\nincluding: (1) event matching, (2) event error detection, (3) event data\\nrepair, and (4) approximate pattern matching.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 14, 7, 49, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Journey to the Frontiers of Query Rewritability',\n",
       "  'authors': ['Piotr Ostropolski-Nalewaja',\n",
       "   'Jerzy Marcinkowski',\n",
       "   'David Carral',\n",
       "   'Sebastian Rudolph'],\n",
       "  'summary': 'This paper is about (first order) query rewritability in the context of\\ntheory-mediated query answering. The starting point of our journey is the\\nFUS/FES conjecture, saying that if a theory is core-terminating (FES) and\\nadmits query rewriting (BDD, FUS) then it is uniformly bounded. We show that\\nthis conjecture is true for a wide class of \"local\" BDD theories. Then we ask\\nhow non-local can a BDD theory actually be and we discover phenomena which we\\nthink are quite counter-intuitive.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 21, 11, 54, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Toward Compact Data from Big Data',\n",
       "  'authors': ['Song-Kyoo', 'Kim'],\n",
       "  'summary': 'Bigdata is a dataset of which size is beyond the ability of handling a\\nvaluable raw material that can be refined and distilled into valuable specific\\ninsights. Compact data is a method that optimizes the big dataset that gives\\nbest assets without handling complex bigdata. The compact dataset contains the\\nmaximum knowledge patterns at fine grained level for effective and personalized\\nutilization of bigdata systems without bigdata. The compact data method is a\\ntailor-made design which depends on problem situations. Various compact data\\ntechniques have been demonstrated into various data-driven research area in the\\npaper.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 26, 4, 45, 40, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Visualization Techniques with Data Cubes: Utilizing Concurrency for Complex Data',\n",
       "  'authors': ['Daniel Szelogowski'],\n",
       "  'summary': 'With web and mobile platforms becoming more prominent devices utilized in\\ndata analysis, there are currently few systems which are not without flaw. In\\norder to increase the performance of these systems and decrease errors of data\\noversimplification, we seek to understand how other programming languages can\\nbe used across these platforms which provide data and type safety, as well as\\nutilizing concurrency to perform complex data manipulation tasks.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 1, 1, 5, 41, 44, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Optimizing Data Cube Visualization for Web Applications: Performance and User-Friendly Data Aggregation',\n",
       "  'authors': ['Daniel Szelogowski'],\n",
       "  'summary': 'Current open source applications which allow for cross-platform data\\nvisualization of OLAP cubes feature issues of high overhead and inconsistency\\ndue to data oversimplification. To improve upon this issue, there is a need to\\ncut down the number of pipelines that the data must travel between for these\\naggregation operations and create a single, unified application which performs\\nefficiently without sacrificing data, and allows for ease of usability and\\nextension.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 1, 1, 5, 42, 38, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Efficient Discovery of Approximate Order Dependencies',\n",
       "  'authors': ['Reza Karegar',\n",
       "   'Parke Godfrey',\n",
       "   'Lukasz Golab',\n",
       "   'Mehdi Kargar',\n",
       "   'Divesh Srivastava',\n",
       "   'Jaroslaw Szlichta'],\n",
       "  'summary': 'Order dependencies (ODs) capture relationships between ordered domains of\\nattributes. Approximate ODs (AODs) capture such relationships even when there\\nexist exceptions in the data. During automated discovery of ODs, validation is\\nthe process of verifying whether an OD holds. We present an algorithm for\\nvalidating approximate ODs with significantly improved runtime performance over\\nexisting methods for AODs, and prove that it is correct and has optimal\\nruntime. By replacing the validation step in a leading algorithm for\\napproximate OD discovery with ours, we achieve orders-of-magnitude improvements\\nin performance.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 1, 6, 18, 22, 52, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'AMALGAM: A Matching Approach to fairfy tabuLar data with knowledGe grAph Model',\n",
       "  'authors': ['Rabia Azzi', 'Gayo Diallo'],\n",
       "  'summary': 'In this paper we present AMALGAM, a matching approach to fairify tabular data\\nwith the use of a knowledge graph. The ultimate goal is to provide fast and\\nefficient approach to annotate tabular data with entities from a background\\nknowledge. The approach combines lookup and filtering services combined with\\ntext pre-processing techniques. Experiments conducted in the context of the\\n2020 Semantic Web Challenge on Tabular Data to Knowledge Graph Matching with\\nboth Column Type Annotation and Cell Type Annotation tasks showed promising\\nresults.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 1, 17, 10, 17, 6, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A GeoSPARQL Compliance Benchmark',\n",
       "  'authors': ['Milos Jovanovik', 'Timo Homburg', 'Mirko Spasić'],\n",
       "  'summary': 'We propose a series of tests that check for the compliance of RDF\\ntriplestores with the GeoSPARQL standard. The purpose of the benchmark is to\\ntest how many of the requirements outlined in the standard a tested system\\nsupports and to push triplestores forward in achieving a full GeoSPARQL\\ncompliance. This topic is of concern because the support of GeoSPARQL varies\\ngreatly between different triplestore implementations, and such support is of\\ngreat importance for the domain of geospatial RDF data. Additionally, we\\npresent a comprehensive comparison of triplestores, providing an insight into\\ntheir current GeoSPARQL support.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 2, 11, 17, 28, 52, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Querying collections of tree-structured records in the presence of within-record referential constraints',\n",
       "  'authors': ['Foto N. Afrati', 'Matthew Damigos'],\n",
       "  'summary': 'In this paper, we consider a tree-structured data model used in many\\ncommercial databases like Dremel, F1, JSON stores. We define identity and\\nreferential constraints within each tree-structured record. The query language\\nis a variant of SQL and flattening is used as an evaluation mechanism. We\\ninvestigate querying in the presence of these constraints, and point out the\\nchallenges that arise from taking them into account during query evaluation.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 2, 12, 15, 0, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data provenance, curation and quality in metrology',\n",
       "  'authors': ['James Cheney',\n",
       "   'Adriane Chapman',\n",
       "   'Joy Davidson',\n",
       "   'Alistair Forbes'],\n",
       "  'summary': 'Data metrology -- the assessment of the quality of data -- particularly in\\nscientific and industrial settings, has emerged as an important requirement for\\nthe UK National Physical Laboratory (NPL) and other national metrology\\ninstitutes. Data provenance and data curation are key components for emerging\\nunderstanding of data metrology. However, to date provenance research has had\\nlimited visibility to or uptake in metrology. In this work, we summarize a\\nscoping study carried out with NPL staff and industrial participants to\\nunderstand their current and future needs for provenance, curation and data\\nquality. We then survey provenance technology and standards that are relevant\\nto metrology. We analyse the gaps between requirements and the current state of\\nthe art.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 2, 16, 15, 44, 27, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Survey on Locality Sensitive Hashing Algorithms and their Applications',\n",
       "  'authors': ['Omid Jafari',\n",
       "   'Preeti Maurya',\n",
       "   'Parth Nagarkar',\n",
       "   'Khandker Mushfiqul Islam',\n",
       "   'Chidambaram Crushev'],\n",
       "  'summary': 'Finding nearest neighbors in high-dimensional spaces is a fundamental\\noperation in many diverse application domains. Locality Sensitive Hashing (LSH)\\nis one of the most popular techniques for finding approximate nearest neighbor\\nsearches in high-dimensional spaces. The main benefits of LSH are its\\nsub-linear query performance and theoretical guarantees on the query accuracy.\\nIn this survey paper, we provide a review of state-of-the-art LSH and\\nDistributed LSH techniques. Most importantly, unlike any other prior survey, we\\npresent how Locality Sensitive Hashing is utilized in different application\\ndomains.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 2, 17, 18, 56, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Fast Distributed Complex Join Processing',\n",
       "  'authors': ['Hao Zhang', 'Miao Qiao', 'Jeffrey Xu Yu', 'Hong Cheng'],\n",
       "  'summary': 'In this work, we study the problem of co-optimize communication,\\npre-computing, and computation cost in one-round multi-way join evaluation. We\\npropose a multi-way join approach ADJ (Adaptive Distributed Join) for complex\\njoin which finds one optimal query plan to process by exploring cost-effective\\npartial results in terms of the trade-off between pre-computing, communication,\\nand computation.We analyze the input relations for a given join query and find\\none optimal over a set of query plans in some specific form, with high-quality\\ncost estimation by sampling. Our extensive experiments confirm that ADJ\\noutperforms the existing multi-way join methods by up to orders of magnitude.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 2, 26, 9, 41, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Enhancing the Interactivity of Dataframe Queries by Leveraging Think Time',\n",
       "  'authors': ['Doris Xin',\n",
       "   'Devin Petersohn',\n",
       "   'Dixin Tang',\n",
       "   'Yifan Wu',\n",
       "   'Joseph E. Gonzalez',\n",
       "   'Joseph M. Hellerstein',\n",
       "   'Anthony D. Joseph',\n",
       "   'Aditya G. Parameswaran'],\n",
       "  'summary': 'We propose opportunistic evaluation, a framework for accelerating\\ninteractions with dataframes. Interactive latency is critical for iterative,\\nhuman-in-the-loop dataframe workloads for supporting exploratory data analysis.\\nOpportunistic evaluation significantly reduces interactive latency by 1)\\nprioritizing computation directly relevant to the interactions and 2)\\nleveraging think time for asynchronous background computation for non-critical\\noperators that might be relevant to future interactions. We show, through\\nempirical analysis, that current user behavior presents ample opportunities for\\noptimization, and the solutions we propose effectively harness such\\nopportunities.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 3, 3, 2, 56, 46, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Consistent Answers of Aggregation Queries using SAT Solvers',\n",
       "  'authors': ['Akhil A. Dixit', 'Phokion G. Kolaitis'],\n",
       "  'summary': 'The framework of database repairs and consistent answers to queries is a\\nprincipled approach to managing inconsistent databases. We describe the first\\nsystem able to compute the consistent answers of general aggregation queries\\nwith the COUNT(A), COUNT(*), SUM(A), MIN(A), and MAX(A) operators, and with or\\nwithout grouping constructs. Our system uses reductions to optimization\\nversions of Boolean satisfiability (SAT) and then leverages powerful SAT\\nsolvers. We carry out an extensive set of experiments on both synthetic and\\nreal-world data that demonstrate the usefulness and scalability of this\\napproach.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 3, 4, 20, 35, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Putting Data Science Pipelines on the Edge',\n",
       "  'authors': ['Ali Akoglu', 'Genoveva Vargas-Solar'],\n",
       "  'summary': 'This paper proposes a composable \"Just in Time Architecture\" for Data Science\\n(DS) Pipelines named JITA-4DS and associated resource management techniques for\\nconfiguring disaggregated data centers (DCs). DCs under our approach are\\ncomposable based on vertical integration of the application,\\nmiddleware/operating system, and hardware layers customized dynamically to meet\\napplication Service Level Objectives (SLO - application-aware management).\\nThereby, pipelines utilize a set of flexible building blocks that can be\\ndynamically and automatically assembled and re-assembled to meet the dynamic\\nchanges in the workload\\'s SLOs. To assess disaggregated DC\\'s, we study how to\\nmodel and validate their performance in large-scale settings.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 3, 14, 17, 21, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Some Results of Experimental Check of The Model of the Object Innovativeness Quantitative Evaluation',\n",
       "  'authors': ['V. K. Ivanov'],\n",
       "  'summary': 'The paper presents the results of the experiments that were conducted to\\nconfirm the main ideas of the proposed approach to determining the objects\\ninnovativeness. This approach assumed that the product life cycle of whose\\ndescriptions are placed in different data warehouses is adequate. The proposed\\nformal model allows us to calculate the quantitative value of the additive\\nevaluation criterion of objects innovativeness. The obtained experimental data\\nmake it possible to evaluate the adopted approach correctness.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 3, 27, 8, 26, 45, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Linked Data Application Framework to Enable Rapid Prototyping',\n",
       "  'authors': ['Markus Schröder', 'Christian Jilek', 'Andreas Dengel'],\n",
       "  'summary': 'Application developers, in our experience, tend to hesitate when dealing with\\nlinked data technologies. To reduce their initial hurdle and enable rapid\\nprototyping, we propose in this paper a framework for building linked data\\napplications. Our approach especially considers the participation of web\\ndevelopers and non-technical users without much prior knowledge about linked\\ndata concepts. Web developers are supported with bidirectional RDF to JSON\\nconversions and suitable CRUD endpoints. Non-technical users can browse\\nwebsites generated from JSON data by means of a template language. A\\nprototypical open source implementation demonstrates its capabilities.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 4, 28, 7, 31, 20, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards a Model for LSH',\n",
       "  'authors': ['Li Wang'],\n",
       "  'summary': 'As data volumes continue to grow, clustering and outlier detection algorithms\\nare becoming increasingly time-consuming. Classical index structures for\\nneighbor search are no longer sustainable due to the \"curse of dimensionality\".\\nInstead, approximated index structures offer a good opportunity to\\nsignificantly accelerate the neighbor search for clustering and outlier\\ndetection and to have the lowest possible error rate in the results of the\\nalgorithms. Locality-sensitive hashing is one of those. We indicate directions\\nto model the properties of LSH.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 5, 11, 15, 39, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Nearly Instance-optimal Differentially Private Mechanism for Conjunctive Queries',\n",
       "  'authors': ['Wei Dong', 'Ke Yi'],\n",
       "  'summary': 'Releasing the result size of conjunctive queries and graph pattern queries\\nunder differential privacy (DP) has received considerable attention in the\\nliterature, but existing solutions do not offer any optimality guarantees. We\\nprovide the first DP mechanism for this problem with a fairly strong notion of\\noptimality, which can be considered as a natural relaxation of\\ninstance-optimality to a constant.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 5, 12, 5, 47, 7, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Essence of Factual Knowledge',\n",
       "  'authors': ['Ruoyu Wang',\n",
       "   'Daniel Sun',\n",
       "   'Guoqiang Li',\n",
       "   'Raymond Wong',\n",
       "   'Shiping Chen'],\n",
       "  'summary': 'Knowledge bases are collections of domain-specific and commonsense facts.\\nRecently, the sizes of KBs are rocketing due to automatic extraction for\\nknowledge and facts. For example, the number of facts in WikiData is up to 974\\nmillion! According to our observation, current KBs, especially domain KBs, show\\nstrong relevance in relations according to some topics. These patterns can be\\nused to conclude and infer for part of facts in the KBs. Therefore, the\\noriginal KBs can be minimzed by extracting patterns and essential facts.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 7, 1, 20, 9, 56, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data Lake Ingestion Management',\n",
       "  'authors': ['Yan Zhao', 'Imen Megdiche', 'Franck Ravat'],\n",
       "  'summary': 'Data Lake (DL) is a Big Data analysis solution which ingests raw data in\\ntheir native format and allows users to process these data upon usage. Data\\ningestion is not a simple copy and paste of data, it is a complicated and\\nimportant phase to ensure that ingested data are findable, accessible,\\ninteroperable and reusable at all times. Our solution is threefold. Firstly, we\\npropose a metadata model that includes information about external data sources,\\ndata ingestion processes, ingested data, dataset veracity and dataset security.\\nSecondly, we present the algorithms that ensure the ingestion phase (data\\nstorage and metadata instanciation). Thirdly, we introduce a developed metadata\\nmanagement system whereby users can easily consult different elements stored in\\nDL.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 7, 5, 14, 3, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Probabilistic Trace Alignment',\n",
       "  'authors': ['Giacomo Bergami',\n",
       "   'Fabrizio Maria Maggi',\n",
       "   'Marco Montali',\n",
       "   'Rafael Peñaloza'],\n",
       "  'summary': 'Alignments provide sophisticated diagnostics that pinpoint deviations in a\\ntrace with respect to a process model and their severity. However, approaches\\nbased on trace alignments use crisp process models as reference and recent\\nprobabilistic conformance checking approaches check the degree of conformance\\nof an event log with respect to a stochastic process model instead of finding\\ntrace alignments. In this paper, for the first time, we provide a conformance\\nchecking approach based on trace alignments using stochastic Workflow nets.\\nConceptually, this requires to handle the two possibly contrasting forces of\\nthe cost of the alignment on the one hand and the likelihood of the model trace\\nwith respect to which the alignment is computed on the other.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 7, 8, 17, 42, 57, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'goldMEDAL : une nouvelle contribution {à} la mod{é}lisation g{é}n{é}rique des m{é}tadonn{é}es des lacs de donn{é}es',\n",
       "  'authors': ['Etienne Scholly',\n",
       "   'Pegdwendé Sawadogo',\n",
       "   'Pengfei Liu',\n",
       "   'Javier Espinosa-Oviedo',\n",
       "   'Cécile Favre',\n",
       "   'Sabine Loudcher',\n",
       "   'Jérôme Darmont',\n",
       "   'Camille Noûs'],\n",
       "  'summary': 'We summarize here a paper published in 2021 in the DOLAP international\\nworkshop DOLAP associated with the EDBT and ICDT conferences. We propose\\ngoldMEDAL, a generic metadata model for data lakes based on four concepts and a\\nthree-level modeling: conceptual, logical and physical.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 7, 5, 7, 56, 27, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Spatial Data Generators',\n",
       "  'authors': ['Tin Vu', 'Sara Migliorini', 'Ahmed Eldawy', 'Alberto Belussi'],\n",
       "  'summary': 'This gem describes a standard method for generating synthetic spatial data\\nthat can be used in benchmarking and scalability tests. The goal is to improve\\nthe reproducibility and increase the trust in experiments on synthetic data by\\nusing standard widely acceptable dataset distributions. In addition, this\\narticle describes how to assign a unique identifier to each synthetic dataset\\nthat can be shared in papers for reproducibility of results. Finally, this gem\\nprovides a supplementary material that gives a reference implementation for all\\nthe provided distributions.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 7, 17, 18, 8, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'MARC: Mining Association Rules from datasets by using Clustering models',\n",
       "  'authors': ['Shadi Al Shehabi', 'Abdullatif Baba'],\n",
       "  'summary': 'Association rules are useful to discover relationships, which are mostly\\nhidden, between the different items in large datasets. Symbolic models are the\\nprincipal tools to extract association rules. This basic technique is\\ntime-consuming, and it generates a big number of associated rules. To overcome\\nthis drawback, we suggest a new method, called MARC, to extract the more\\nimportant association rules of two important levels: Type I, and Type II. This\\napproach relies on a multi-topographic unsupervised neural network model as\\nwell as clustering quality measures that evaluate the success of a given\\nnumerical classification model to behave as a natural symbolic model.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 7, 14, 6, 28, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Blockchain Transaction Processing',\n",
       "  'authors': ['Suyash Gupta', 'Mohammad Sadoghi'],\n",
       "  'summary': 'A blockchain is an append-only linked-list of blocks, which is maintained at\\neach participating node. Each block records a set of transactions and their\\nassociated metadata. Blockchain transactions act on the identical ledger data\\nstored at each node. Blockchain was first perceived by Satoshi Nakamoto as a\\npeer-to-peer digital-commodity (also known as crypto-currency) exchange system.\\nBlockchains received traction due to their inherent property of\\nimmutability-once a block is accepted, it cannot be reverted.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 7, 24, 12, 20, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Relational Boosted Regression Trees',\n",
       "  'authors': ['Sonia Cromp', 'Alireza Samadian', 'Kirk Pruhs'],\n",
       "  'summary': 'Many tasks use data housed in relational databases to train boosted\\nregression tree models. In this paper, we give a relational adaptation of the\\ngreedy algorithm for training boosted regression trees. For the subproblem of\\ncalculating the sum of squared residuals of the dataset, which dominates the\\nruntime of the boosting algorithm, we provide a $(1 + \\\\epsilon)$-approximation\\nusing the tensor sketch technique. Employing this approximation within the\\nrelational boosted regression trees algorithm leads to learning similar model\\nparameters, but with asymptotically better runtime.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 7, 25, 20, 29, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Machine Learning over Static and Dynamic Relational Data',\n",
       "  'authors': ['Ahmet Kara', 'Milos Nikolic', 'Dan Olteanu', 'Haozhe Zhang'],\n",
       "  'summary': 'This tutorial overviews principles behind recent works on training and\\nmaintaining machine learning models over relational data, with an emphasis on\\nthe exploitation of the relational data structure to improve the runtime\\nperformance of the learning task.\\n  The tutorial has the following parts:\\n  1) Database research for data science\\n  2) Three main ideas to achieve performance improvements\\n  2.1) Turn the ML problem into a DB problem\\n  2.2) Exploit structure of the data and problem\\n  2.3) Exploit engineering tools of a DB researcher\\n  3) Avenues for future research',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 7, 29, 12, 0, 11, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Interactive Region-of-Interest Discovery using Exploratory Feedback',\n",
       "  'authors': ['Behrooz Omidvar-Tehrani'],\n",
       "  'summary': \"In this paper, we propose a geospatial data management framework called\\nIRIDEF which captures and analyzes user's exploratory feedback for an enriched\\nguidance mechanism in the context of interactive analysis. We discuss that\\nexploratory feedback can be a proxy for decision-making feedback when the\\nlatter is scarce or unavailable. IRIDEF identifies regions of interest (ROIs)\\nvia exploratory feedback and highlights a few interesting and out-of-sight POIs\\nin each ROI. These highlights enable the user to shape up his/her future\\ninteractions with the system. We detail the components of our proposed\\nframework in the form of a data analysis pipeline and present the aspects of\\nefficiency and effectiveness for each component. We also discuss evaluation\\nplans and future directions for IRIDEF.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 7, 29, 18, 42, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Crowdsourced Databases and Sui Generis Rights',\n",
       "  'authors': ['Gonçalo Simões de Almeida', 'Gonçalo Faria Abreu'],\n",
       "  'summary': 'In this study we propose a new concept of databases (crowdsourced databases),\\nadding a new conceptual approach to the debate on legal protection of databases\\nin Europe. We also summarise the current legal framework and current indexing\\nand web scraping practices - it would not be prudent to suggest a new theory\\nwithout contextualising it in the legal and practical context in which it is\\ndeveloped.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 8, 10, 14, 44, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Version Reconciliation for Collaborative Databases',\n",
       "  'authors': ['Nalin Ranjan',\n",
       "   'Zechao Shang',\n",
       "   'Aaron J. Elmore',\n",
       "   'Sanjay Krishnan'],\n",
       "  'summary': 'We propose MindPalace, a prototype of a versioned database for efficient\\ncollaborative data management. MindPalace supports offline collaboration, where\\nusers work independently without real-time correspondence. The core of\\nMindPalace is a critical step of offline collaboration: reconciling divergent\\nbranches made by simultaneous data manipulation. We formalize the concept of\\nauto-mergeability, a condition under which branches may be reconciled without\\nhuman intervention, and propose an efficient framework for determining whether\\ntwo branches are auto-mergeable and identifying particular records for manual\\nreconciliation.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 10, 5, 1, 38, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Reconsidering Optimistic Algorithms for Relational DBMS',\n",
       "  'authors': ['Malcolm Crowe', 'Fritz Laux'],\n",
       "  'summary': 'At DBKDA 2019, we demonstrated that StrongDBMS with simple but rigorous\\noptimistic algorithms, provides better performance in situations of high\\nconcurrency than major commercial database management systems (DBMS). The\\ndemonstration was convincing but the reasons for its success were not fully\\nanalysed. There is a brief account of the results below. In this short\\ncontribution, we wish to discuss the reasons for the results. The analysis\\nleads to a strong criticism of all DBMS algorithms based on locking, and based\\non these results, it is not fanciful to suggest that it is time to re-engineer\\nexisting DBMS.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 10, 6, 19, 21, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Time- and Space-Efficient Regular Path Queries on Graphs',\n",
       "  'authors': ['Diego Arroyuelo',\n",
       "   'Aidan Hogan',\n",
       "   'Gonzalo Navarro',\n",
       "   'Javiel Rojas-Ledesma'],\n",
       "  'summary': 'We introduce a time- and space-efficient technique to solve regularpath\\nqueries over labeled graphs. We combine a bit-parallel simula-tion of the\\nGlushkov automaton of the regular expression with thering index introduced by\\nArroyuelo et al., exploiting its wavelettree representation of the triples in\\norder to efficiently reach thestates of the product graph that are relevant for\\nthe query. Ourquery algorithm is able to simultaneously process several\\nautoma-ton states, as well as several graph nodes/labels. Our\\nexperimentalresults show that our representation uses 3-5 times less space\\nthanthe alternatives in the literature, while generally outperformingthem in\\nquery times (1.67 times faster than the next best).',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 11, 8, 15, 12, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Distributed Evaluation of Graph Queries using Recursive Relational Algebra',\n",
       "  'authors': ['Sarah Chlyah', 'Pierre Genevès', 'Nabil Layaïda'],\n",
       "  'summary': 'We present a system called Dist-$\\\\mu$-RA for the distributed evaluation of\\nrecursive graph queries. Dist-$\\\\mu$-RA builds on the recursive relational\\nalgebra and extends it with evaluation plans suited for the distributed\\nsetting. The goal is to offer expressivity for high-level queries while\\nproviding efficiency at scale and reducing communication costs. Experimental\\nresults on both real and synthetic graphs show the effectiveness of the\\nproposed approach compared to existing systems.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 11, 24, 13, 31, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'SchemaDB: Structures in Relational Datasets',\n",
       "  'authors': ['Cody James Christopher', 'Kristen Moore', 'David Liebowitz'],\n",
       "  'summary': 'In this paper we introduce the SchemaDB data-set; a collection of relational\\ndatabase schemata in both sql and graph formats. Databases are not commonly\\nshared publicly for reasons of privacy and security, so schemata are not\\navailable for study. Consequently, an understanding of database structures in\\nthe wild is lacking, and most examples found publicly belong to common\\ndevelopment frameworks or are derived from textbooks or engine benchmark\\ndesigns. SchemaDB contains 2,500 samples of relational schemata found in public\\nrepositories which we have standardised to MySQL syntax. We provide our\\ngathering and transformation methodology, summary statistics, and structural\\nanalysis, and discuss potential downstream research tasks in several domains.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 11, 24, 23, 6, 2, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Operation-based Collaborative Data Sharing for Distributed Systems',\n",
       "  'authors': ['Masato Takeichi'],\n",
       "  'summary': 'Collaborative Data Sharing raises a fundamental issue in distributed systems.\\nSeveral strategies have been proposed for making shared data consistent between\\npeers in such a way that the shared part of their local data become equal. Most\\nof the proposals rely on state-based semantics. But this suffers from a lack of\\ndescriptiveness in conflict-free features of synchronization required for\\nflexible network connections. Recent applications tend to use non-permanent\\nconnection with mobile devices or allow temporary breakaways from the system,\\nfor example. To settle ourselves in conflict-free data sharing, we propose a\\nnovel scheme \"Operation-based Collaborative Data Sharing\" that enables\\nconflict-free strategies for synchronization based on operational semantics.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 12, 1, 5, 47, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Practical Dynamic Programming Approach to Datalog Provenance Computation',\n",
       "  'authors': ['Yann Ramusat', 'Silviu Maniu', 'Pierre Senellart'],\n",
       "  'summary': \"We establish a translation between a formalism for dynamic programming over\\nhypergraphs and the computation of semiring-based provenance for Datalog\\nprograms. The benefit of this translation is a new method for computing\\nprovenance for a specific class of semirings. Theoretical and practical\\noptimizations lead to an efficient implementation using \\\\textsc{Souffl\\\\'e}, a\\nstate-of-the-art Datalog interpreter. Experimental results on real-world data\\nsuggest this approach to be efficient in practical contexts, even competing\\nwith our previous dedicated solutions for computing provenance in annotated\\ngraph databases. The cost overhead compared to plain Datalog evaluation is\\nfairly moderate in many cases of interest.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 12, 2, 11, 12, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Designing a Visual Tool for Property Graph Schema Extraction and Refinement: An Expert Study',\n",
       "  'authors': ['Nimo Beeren'],\n",
       "  'summary': 'The design space of visual tools that aim to help people create schemas for\\nproperty graphs is explored. Interviews are conducted with experts in the\\ndomain of property graphs and data management in general. Through this\\ncollaboration, we determine how a schema extraction tool can provide value.\\nThese insights are used to establish design requirements and design a UI\\nprototype, which are then relayed back to the experts. Positive reactions were\\nreceived, which encourage future work in the property graph schema space.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 1, 10, 20, 55, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Parallel Acyclic Joins with Canonical Edge Covers',\n",
       "  'authors': ['Yufei Tao'],\n",
       "  'summary': 'In PODS\\'21, Hu presented an algorithm in the massively parallel computation\\n(MPC) model that processes any acyclic join with an asymptotically optimal\\nload. In this paper, we present an alternative analysis of her algorithm. The\\nnovelty of our analysis is in the revelation of a new mathematical structure --\\nwhich we name \"canonical edge cover\" -- for acyclic hypergraphs. We prove\\nnon-trivial properties for canonical edge covers that offer us a\\ngraph-theoretic perspective about why Hu\\'s algorithm works.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 1, 11, 8, 25, 6, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Finding Your Way Through the Jungle of Big Data Architectures',\n",
       "  'authors': ['Torsten Priebe', 'Sebastian Neumaier', 'Stefan Markus'],\n",
       "  'summary': 'This paper presents a systematic review of common analytical data\\narchitectures based on DAMA-DMBOK and ArchiMate. The paper is work in progress\\nand provides a first view on Gartner\\'s Logical Data Warehouse paradigm, Data\\nFabric and Dehghani\\'s Data Mesh proposal as well as their interdependencies. It\\nfurthermore sketches the way forward how this work can be extended by covering\\nmore architecture paradigms (incl. classic Data Warehouse, Data Vault, Data\\nLake, Lambda and Kappa architectures) and introducing a template with among\\nothers \"context\", \"problem\" and \"solution\" descriptions, leading ultimately to\\na pattern system providing guidance for choosing the right architecture\\nparadigm for the right situation.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 1, 11, 23, 0, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An outline of multi objective optimization in databases with focus on flexible skyline queries',\n",
       "  'authors': ['Matteo Savino'],\n",
       "  'summary': 'The problem of optimizing across different, conceivably conflicting, criteria\\nis called multi-objective optimization and it is widely spread across many\\nfields. This is a recurring problem in database queries when there is the need\\nof obtaining the best objects from a very large data set. In this article, I\\nincluded a complete review of the main approaches typically used to achieve\\nmulti-criteria optimization. Starting from ranking queries and skylines and\\nthen proceeding to more advanced methods, this paper aims to define a clear\\noutline of multi-objective optimization in databases. In particular, the\\nflexible skyline paradigm is considered and thoroughly discussed as it\\novercomes many of the critical issues that arise with other methods.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 1, 13, 11, 41, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Flexible Skyline: one query to rule them all',\n",
       "  'authors': ['Giacomo Vinati'],\n",
       "  'summary': 'The most common archetypes to identify relevant information in large datasets\\nand find the bestoptions according to some preferences or user criteria, are\\nthe top-k queries (ranking method based ona score function defined over the\\nrecords attributes) and skyline queries (based on Pareto dominance oftuples).\\nDespite their large diffusion, both approaches have their pros and cons. In\\nthis survey paper, a comparison is made between these methods and the Flexible\\nSkylines, which is a framework that combines the ranking and skyline approaches\\nusing the novel concept ofF-dominanceto a set of monotone scoring function F.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 1, 11, 13, 15, 8, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Comparison of 6 different approaches to outclass Top-k queries and Skyline queries',\n",
       "  'authors': ['Martino Manzolini'],\n",
       "  'summary': 'Topk queries and skyline queries have well explored limitations which recent\\nresearch have tried to complete through new techniques. In this survey, after\\nresuming such limitations, we consider Restricted Skyline Queries, ORD and ORU\\napproach, Krepresentative minimization queries, Skyline ordering queries, UTK\\nqueries approach and Skyrank that aim to overcome them. After introducing and\\ncomparing their main concepts, pros and cons, we briefly report the algorithms\\nand confront some of the experimental data collected from the bibliography. To\\nconclude the paper, we summarize the results presented with a short guide on\\nhow to select the best approach according to specific needs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 1, 22, 10, 50, 17, tzinfo=datetime.timezone.utc)},\n",
       " {'title': \"Poisson's CDF applied to Flexible Skylines\",\n",
       "  'authors': ['Jaime Pons Garrido'],\n",
       "  'summary': \"The evolution of skyline and ranking queries has created new archetypes like\\nflexible skylines, which have proven to be an efficient method to select\\nrelevant data from large datasets using multi objective optimization. This\\npaper aims to study the possible applications of Poisson distribution mass\\nfunction as a monotonic scoring function in flexible skyline processes,\\nespecially those featuring schemas whose attributes can be translated to\\nconstant mean rates. Moreover, a method to express users's requirement by means\\nof the F-dominant set of tuples will be proposed using parametrical variations\\nin F[1], simultaneously, algorithm construction and potential applications will\\nbe studied.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 1, 25, 10, 18, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A survey on flexible/restricted skyline and their applicability',\n",
       "  'authors': ['Davide Canali'],\n",
       "  'summary': \"Skyline and Top-k are two of the most important methods to extract\\ninformation from datasets, but both come with their drawbacks, that's why\\nlately some new technics that try to mix the features of the two have been\\nstudied. In this survey three new operators are analysed, F-Skyline, ORU/ORD,\\nand ${\\\\epsilon}$-Skyline. After giving the main ideas behind those and their\\nproperties, they are compered on 3 fundamental features such as\\npersonalization, cardinality control, and generalization to guide the user to\\nchoose the best one for any task.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 1, 28, 11, 6, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'QueryER: A Framework for Fast Analysis-Aware Deduplication over Dirty Data',\n",
       "  'authors': ['Giorgos Alexiou',\n",
       "   'George Papastefanatos',\n",
       "   'Vassilis Stamatopoulos',\n",
       "   'Georgia Koutrika',\n",
       "   'Nectarios Koziris'],\n",
       "  'summary': 'In this work, we explore the problem of correctly and efficiently answering\\ncomplex SPJ queries issued directly on top of dirty data. We introduce QueryER,\\na framework that seamlessly integrates Entity Resolution into Query Processing.\\nQueryER executes analysis-aware deduplication by weaving ER operators into the\\nquery plan. The experimental evaluation of our approach exhibits that it adapts\\nto the workload and scales on both real and synthetic datasets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 2, 3, 12, 12, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Multi-Objective Optimization, different approach to query a database',\n",
       "  'authors': ['Matteo Cordioli'],\n",
       "  'summary': \"The datasets available nowadays are very rich and complex, but how do we\\nreach the information we are looking for? In this survey, two different\\napproaches to query a dataset are analyzed and algorithms for each type are\\nexplained. Specifically, the TA and NRA have been analyzed for the Top-K query\\nand the Basic Block Nested Loops has been examined for the skyline query.\\nMoreover, it's explained the core idea behind the Prioritized and Flexible\\nskyline. In the end, the pros and cons of each type of analyzed query have been\\nevaluated based on different criteria.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 2, 3, 12, 22, 19, tzinfo=datetime.timezone.utc)},\n",
       " {'title': \"A bird's eye view on Multi-Objective Optimization techniques in Relational Databases\",\n",
       "  'authors': ['Giuseppe Tortorelli'],\n",
       "  'summary': 'Multi-objective optimization is the problem of optimizing simultaneously\\nmultiple objective functions and several techniques exist to deal with this\\nproblem. This paper aims to present the main methods that can be used to solve\\nthis issue in the context of relational databases. In particular, this work\\nexamines Top-k query to get the k best result from a dataset and Skyline query\\nthat provides a more general overview of the best results. We also discuss\\nFlexible-skyline, a new method designed to improve upon the previous\\ntechniques, mitigating their shortcomings. For each method, we describe the\\nmain characteristics and present an overview of the algorithms implementing\\nsuch thecniques, while comparing advantages and disadvantages.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 2, 5, 19, 40, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Conservative Extensions for Existential Rules',\n",
       "  'authors': ['Jean Christoph Jung', 'Carsten Lutz', 'Jerzy Marcinkowski'],\n",
       "  'summary': 'We study the problem to decide, given sets T1,T2 of tuple-generating\\ndependencies (TGDs), also called existential rules, whether T2 is a\\nconservative extension of T1. We consider two natural notions of conservative\\nextension, one pertaining to answers to conjunctive queries over databases and\\none to homomorphisms between chased databases. Our main results are that these\\nproblems are undecidable for linear TGDs, undecidable for guarded TGDs even\\nwhen T1 is empty, and decidable for frontier-one TGDs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 2, 11, 15, 21, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Giving the Right Answer: a Brief Overview on How to Extend Ranking and Skyline Queries',\n",
       "  'authors': ['Sergio Cuzzucoli'],\n",
       "  'summary': 'To retrieve the best results in a database we use Top-K queries and Skyline\\nqueries but some problems arise. The formers rely too much on user preferences,\\nwhich are difficult to quantify and may skew the fetching of the data, while\\nthe latters tend to output too much data. In this paper, we explore three\\ndifferent branches of research that seek to overcome such limitations:\\nFlexible/Restricted Skylines, Skyline Ordering/Ranking, and Regret\\nMinimization. We analyze how they work and we make comparisons among them to\\nguide the reader to choose the approach that best fits their use cases.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 2, 13, 22, 36, 16, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A survey on making skylines more flexible',\n",
       "  'authors': ['Cem Cebeci'],\n",
       "  'summary': 'Top-$k$ queries and skylines are the two most common approaches to finding\\nthe most interesting entries in a homogeneous multi-dimensional dataset.\\nHowever, both of these strategies have some shortcomings. Top-$k$ queries are\\nvery challenging to specify precisely and skylines are not customizable to\\nspecific scenarios, on top of having unpredictable output cardinalities. We\\ndescribe some alternative methods aimed at addressing the shortcomings of\\ntop-$k$ queries and skylines and compare all approaches to illustrate which of\\nthe desired properties each of them possesses.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 2, 19, 8, 52, 11, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Flexible Skylines: Customizing Skyline Queries Catching Desired Preferences',\n",
       "  'authors': ['Giuseppe Montanaro'],\n",
       "  'summary': \"The techniques most extensively used to retrieve interesting data from\\ndata-sets are the Skyline and the Top-k queries. Sadly, they are not enough for\\nfacing modern problems, so the needing of something more usable and reliable\\nhas come. In this survey we are going to explore Flexible Skylines which are\\nproposed to overcame the old fashion techniques' problems by extending the\\nconcept of dominance. After, we are going to compare this approach with the old\\nand new ones evaluating pros and cons. Finally, we will see some interesting\\napplications.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 2, 21, 19, 26, 52, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Comparing the latest ranking techniques: pros and cons of flexible skylines, regret minimization and skyline ranking queries',\n",
       "  'authors': ['Davide Foini'],\n",
       "  'summary': 'Long-established ranking approaches, such as top-k and skyline queries, have\\nbeen thoroughly discussed and their drawbacks are well acknowledged. New\\ntechniques have been developed in recent years that try to combine traditional\\nones to overcome their limitations. In this paper we focus our attention on\\nsome of them: flexible skylines, regret minimization and skyline ranking\\nqueries, because, while these new methods are promising and have shown\\ninteresting results, a comparison between them is still not available. After a\\nshort introduction of each approach, we discuss analogies and differences\\nbetween them with the advantages and disadvantages of every technique debated.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 2, 22, 10, 15, 29, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Getting the best from skylines and top-k queries',\n",
       "  'authors': ['Marco Costanzo'],\n",
       "  'summary': 'Top-k and skylines are two important techniques that can be used to extract\\nthe best objects from a set. Both the approaches have well-known pros and cons:\\na quite big limitation of skyline queries is the impossibility to control the\\ncardinality of the output and the difficulty in specifying a trade-off among\\nattributes, whereas the ranking queries allow so. On the other hand, the usage\\nof ranking implies that ranking functions need to be specified by users and\\nrenouncing the simplicity of skylines. Flexible/ restricted skylines present a\\nnew approach to tackle this problem, combining the best characteristics of both\\ntechniques making use of a new flexible relation of dominance.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 2, 25, 11, 10, 50, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Counting stars: a survey on flexible Skyline Query approaches',\n",
       "  'authors': ['Alessandro Del Giudice'],\n",
       "  'summary': 'Nowadays, as the quantity of data to process began to rise, so did the need\\nfor a method to discern what pieces of information could be useful for the\\nuser; in response, researchers focused their efforts on improving the already\\nexisting ranking methods or creating new ones starting from them. This survey\\nwill be presented a small list of some of the most known and/or most recent\\nsolutions proposed, with some possible applications for them, concerning a\\nstate of the art restricted to around the last ten years, comparing their\\nperformance with the traditional one top-k and skyline queries.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 3, 1, 10, 12, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Weighing the techniques for data optimization in a database',\n",
       "  'authors': ['Anagha Radhakrishnan'],\n",
       "  'summary': 'A set of preferred records can be obtained from a large database in a\\nmulti-criteria setting using various computational methods which either depend\\non the concept of dominance or on the concept of utility or scoring function\\nbased on the attributes of the database record. A skyline approach relies on\\nthe dominance relationship between different data points to discover\\ninteresting data from a huge database. On the other hand, ranking queries make\\nuse of specific scoring functions to rank tuples in a database. An experimental\\nevaluation of datasets can provides us with information on the effectiveness of\\neach of these methods.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 3, 17, 10, 56, 30, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A flexible solution to embrace Ranking and Skyline queries approaches',\n",
       "  'authors': ['Simone Censuales'],\n",
       "  'summary': 'The multi-objective optimization problem has always been the main objective\\nof the principal traditional approaches, such as Ranking queries and Skyline\\nqueries. The conventional idea was to either use one or the other, trying to\\nexploit both ranking queries advantages when it comes to taking into account\\nuser preferences, and skyline queries points of strength when the main\\nobjective was to obtain interesting results from a dataset in a simple, yet\\neffective fashion, both of them showing limitations when entering specific\\nfields of interest.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 3, 17, 11, 53, 21, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Skyline and ranking query odyssey: a journey from skyline and ranking queries up to f-skyline queries',\n",
       "  'authors': ['Giuseppe Sorrentino'],\n",
       "  'summary': 'Skyline and ranking queries are two of the most used tools to manage large\\ndata sets. The former is based on non-dominance, while the latter on a scoring\\nfunction. Despite their effectiveness, they have some drawbacks like the result\\nsize or the need for a utility function that must be taken into account. To do\\nthis, in the last years, new kinds of queries, called flexible skyline queries,\\nhave been developed. In the present article, a description of skyline and\\nranking queries, f-skyline queries and a comparison among them are provided to\\nhighlight the improvements achieved and how some limitations have been\\novercome.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 4, 10, 8, 7, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'PM4Py-GPU: a High-Performance General-Purpose Library for Process Mining',\n",
       "  'authors': ['Alessandro Berti',\n",
       "   'Minh Phan Nghia',\n",
       "   'Wil M. P. van der Aalst'],\n",
       "  'summary': 'Open-source process mining provides many algorithms for the analysis of event\\ndata which could be used to analyze mainstream processes (e.g., O2C, P2P, CRM).\\nHowever, compared to commercial tools, they lack the performance and struggle\\nto analyze large amounts of data. This paper presents PM4Py-GPU, a Python\\nprocess mining library based on the NVIDIA RAPIDS framework. Thanks to the\\ndataframe columnar storage and the high level of parallelism, a significant\\nspeed-up is achieved on classic process mining computations and processing\\nactivities.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 4, 11, 6, 53, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Understanding the compromise between skyline and ranking queries',\n",
       "  'authors': ['Marco Tonnarelli'],\n",
       "  'summary': 'Skyline and Ranking queries have gained great popularity in the recent years.\\nThese two techniques are crucial for multi-criteria decision support\\napplications, which are now more popular than ever before. Skyline and Ranking\\nqueries are, however, affected by well-known limitations. In the past recent\\nyears, the database community provided numerous studies in this field with the\\naim to overcome the weaknesses of these two approaches. This survey introduces\\nthe reader to Skyline and Ranking queries, explaining the concepts on which\\nthey are based, with the intent to present the compromise between the two\\ntechniques: flexible skylines.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 4, 12, 20, 42, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex',\n",
       "  'authors': ['Immanuel Trummer'],\n",
       "  'summary': \"CodexDB is an SQL processing engine whose internals can be customized via\\nnatural language instructions. CodexDB is based on OpenAI's GPT-3 Codex model\\nwhich translates text into code. It is a framework on top of GPT-3 Codex that\\ndecomposes complex SQL queries into a series of simple processing steps,\\ndescribed in natural language. Processing steps are enriched with user-provided\\ninstructions and descriptions of database properties. Codex translates the\\nresulting text into query processing code. An early prototype of CodexDB is\\nable to generate correct code for a majority of queries of the WikiSQL\\nbenchmark and can be customized in various ways.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 4, 19, 15, 19, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Use of Context in Data Quality Management: a Systematic Literature Review',\n",
       "  'authors': ['Flavia Serra',\n",
       "   'Veronika Peralta',\n",
       "   'Adriana Marotta',\n",
       "   'Patrick Marcel'],\n",
       "  'summary': 'The importance of context in data quality (DQ) was shown many years ago and\\nnowadays is widely accepted. Early approaches and surveys defined DQ as\\n\\\\textit{fitness for use} and showed the influence of context on DQ. This paper\\npresents a Systematic Literature Review (SLR) for investigating how context is\\ntaken into account in recent proposals for DQ management. We specifically\\npresent the planning and execution of the SLR, the analysis criteria and our\\nresults reflecting the relationship between context and DQ in the state of the\\nart and, particularly, how that context is defined and used for DQ management.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 4, 22, 11, 54, 38, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Evaluating regular path queries under the all-shortest paths semantics',\n",
       "  'authors': ['Domagoj Vrgoč'],\n",
       "  'summary': 'The purpose of this report is to explain how the textbook breadth-first\\nsearch algorithm (BFS) can be modified in order to also create a compact\\nrepresentation of all shortest paths connecting a single source node to all the\\nnodes reachable from it. From this representation, all these paths can also be\\nefficiently enumerated. We then apply this algorithm to solve a similar problem\\nin edge labelled graphs, where paths also have an additional restriction that\\ntheir edge labels form a word belonging to a regular language. Namely, we solve\\nthe problem of evaluating regular path queries (RPQs) under the all-shortest\\npaths semantics.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 4, 23, 20, 5, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Flexible skyline: overview and applicability',\n",
       "  'authors': ['Carlo Bellacoscia'],\n",
       "  'summary': \"Ranking (or top-k) and skyline queries are the most popular approaches used\\nto extract interesting data from large datasets. The first one is based on a\\nscoring function to evaluate and rank tuples. Its computation is fast, but it\\nis sensitive to the choice of the evaluating function. Skyline queries are\\nbased on the idea of dominance and the result is the set of all non-dominated\\ntuples. This is a very interesting approach, but it can't allow to control the\\ncardinality of the output. Recent researches discovered more techniques to\\ncompensate for these drawbacks. In particular, this paper will focus on the\\nflexible skyline approach.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 4, 30, 14, 55, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Filtering and Sampling Object-Centric Event Logs',\n",
       "  'authors': ['Alessandro Berti'],\n",
       "  'summary': 'The scalability of process mining techniques is one of the main challenges to\\ntackling the massive amount of event data produced every day in enterprise\\ninformation systems. To this purpose, filtering and sampling techniques are\\nproposed to keep a subset of the behavior of the original log and make the\\napplication of process mining techniques feasible. While techniques for\\nfiltering/sampling traditional event logs have been already proposed,\\nfiltering/sampling object-centric event logs is more challenging as the number\\nof factors (events, objects, object types) to consider is significantly higher.\\nThis paper provides some techniques to filter/sample object-centric event logs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 5, 3, 11, 33, 50, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Ontology-Mediated Querying on Databases of Bounded Cliquewidth',\n",
       "  'authors': ['Carsten Lutz', 'Leif Sabellek', 'Lukas Schulze'],\n",
       "  'summary': 'We study the evaluation of ontology-mediated queries (OMQs) on databases of\\nbounded cliquewidth from the viewpoint of parameterized complexity theory. As\\nthe ontology language, we consider the description logics $\\\\mathcal{ALC}$ and\\n$\\\\mathcal{ALCI}$ as well as the guarded two-variable fragment GF$_2$ of\\nfirst-order logic. Queries are atomic queries (AQs), conjunctive queries (CQs),\\nand unions of CQs. All studied OMQ problems are fixed-parameter linear (FPL)\\nwhen the parameter is the size of the OMQ plus the cliquewidth. Our main\\ncontribution is a detailed analysis of the dependence of the running time on\\nthe parameter, exhibiting several interesting effects.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 5, 4, 17, 13, 8, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Discovery of Keys for Graphs [Extended Version]',\n",
       "  'authors': ['Morteza Alipourlangouri', 'Fei Chiang'],\n",
       "  'summary': 'Keys for graphs uses the topology and value constraints needed to uniquely\\nidentify entities in a graph database. They have been studied to support object\\nidentification, knowledge fusion, data deduplication, and social network\\nreconciliation. In this paper, we present our algorithm to mine keys over\\ngraphs. Our algorithm discovers keys in a graph via frequent subgraph\\nexpansion. We present two properties that define a meaningful key, including\\nminimality and support. Lastly, using real-world graphs, we experimentally\\nverify the efficiency of our algorithm on real world graphs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 5, 31, 5, 40, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'P4DB -- The Case for In-Network OLTP (Extended Technical Report)',\n",
       "  'authors': ['Matthias Jasny',\n",
       "   'Lasse Thostrup',\n",
       "   'Tobias Ziegler',\n",
       "   'Carsten Binnig'],\n",
       "  'summary': 'In this paper we present a new approach for distributed DBMSs called P4DB,\\nthat uses a programmable switch to accelerate OLTP workloads. The main idea of\\nP4DB is that it implements a transaction processing engine on top of a\\nP4-programmable switch. The switch can thus act as an accelerator in the\\nnetwork, especially when it is used to store and process hot (contended) tuples\\non the switch. In our experiments, we show that P4DB hence provides significant\\nbenefits compared to traditional DBMS architectures and can achieve a speedup\\nof up to 8x.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 6, 1, 16, 48, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'ChaTEAU: A Universal Toolkit for Applying the Chase',\n",
       "  'authors': ['Tanja Auge',\n",
       "   'Nic Scharlau',\n",
       "   'Andreas Görres',\n",
       "   'Jakob Zimmer',\n",
       "   'Andreas Heuer'],\n",
       "  'summary': 'What do applications like semantic optimization, data exchange and\\nintegration, answering queries under dependencies, query reformulation with\\nconstraints, and data cleaning have in common? All these applications can be\\nprocessed by the Chase, a family of algorithms for reasoning with constraints.\\nWhile the theory of the Chase is well understood, existing implementations are\\nconfined to specific use cases and application scenarios, making it difficult\\nto reuse them in other settings. ChaTEAU overcomes this limitation: It takes\\nthe logical core of the Chase, generalizes it, and provides a software library\\nfor different Chase applications in a single toolkit.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 6, 3, 15, 38, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Comparing modern techniques for querying data starting from top-k and skyline queries',\n",
       "  'authors': ['Fabio Patella'],\n",
       "  'summary': 'To make intelligent decisions over complex data by discovering a set of\\ninteresting options is something that has become very important for users of\\nmodern applications. Consequently, researchers are studying new techniques to\\novercome limitations of traditional ways of querying data from databases as\\ntop-k queries and skyline queries. Over the past few years new methods have\\nbeen developed as Flexible Skylines, Regret Minimization and Skyline\\nordering/ranking. The aim of this survey is to describe these techniques and\\nsome their possible variants comparing them and explaining how they improve\\ntraditional methods.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 6, 6, 16, 46, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Spatiotemporal Data Mining: A Survey',\n",
       "  'authors': ['Arun Sharma', 'Zhe Jiang', 'Shashi Shekhar'],\n",
       "  'summary': 'Spatiotemporal data mining aims to discover interesting, useful but\\nnon-trivial patterns in big spatial and spatiotemporal data. They are used in\\nvarious application domains such as public safety, ecology, epidemiology, earth\\nscience, etc. This problem is challenging because of the high societal cost of\\nspurious patterns and exorbitant computational cost. Recent surveys of\\nspatiotemporal data mining need update due to rapid growth. In addition, they\\ndid not adequately survey parallel techniques for spatiotemporal data mining.\\nThis paper provides a more up-to-date survey of spatiotemporal data mining\\nmethods. Furthermore, it has a detailed survey of parallel formulations of\\nspatiotemporal data mining.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 6, 26, 0, 8, 6, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Performance Analysis: Discovering Semi-Markov Models From Event Logs',\n",
       "  'authors': ['Anna Kalenkova', 'Lewis Mitchell', 'Matthew Roughan'],\n",
       "  'summary': 'Process mining methods and tools are largely used in industry to monitor and\\nimprove operational processes. This paper presents a new technique to analyze\\nperformance characteristics of processes using event data. Based on event\\nsequences and their timestamps, semi-Markov models are discovered. The\\ndiscovered models are further used for performance what-if analysis of the\\nprocesses. The paper studies a trade-off between the order of models discovered\\nand accuracy of representing performance information. The proposed discovery\\nand analysis techniques are implemented and tested on real-world event data.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 6, 29, 6, 4, 19, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards Specificationless Monitoring of Provenance-Emitting Systems',\n",
       "  'authors': ['Martin Stoffers', 'Alexander Weinert'],\n",
       "  'summary': 'Monitoring often requires insight into the monitored system as well as\\nconcrete specifications of expected behavior. More and more systems, however,\\nprovide information about their inner procedures by emitting provenance\\ninformation in a W3C-standardized graph format.\\n  In this work, we present an approach to monitor such provenance data for\\nanomalous behavior by performing spectral graph analysis on slices of the\\nconstructed provenance graph and by comparing the characteristics of each slice\\nwith those of a sliding window over recently seen slices. We argue that this\\napproach not only simplifies the monitoring of heterogeneous distributed\\nsystems, but also enables applying a host of well-studied techniques to monitor\\nsuch systems.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 7, 21, 5, 35, 2, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'WShEx: A language to describe and validate Wikibase entities',\n",
       "  'authors': ['Jose Emilio Labra Gayo'],\n",
       "  'summary': 'Wikidata is one of the most successful Semantic Web projects. Its underlying\\nWikibase data model departs from RDF with the inclusion of several features\\nlike qualifiers and references, built-in datatypes, etc. Those features are\\nserialized to RDF for content negotiation, RDF dumps and in the SPARQL\\nendpoint. Wikidata adopted the entity schemas namespace using the ShEx language\\nto describe and validate the RDF serialization of Wikidata entities. In this\\npaper we propose WShEx, a language inspired by ShEx that directly supports the\\nWikibase data model and can be used to describe and validate Wikibase entities.\\nThe paper presents a the abstract syntax and semantic of the WShEx language.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 8, 4, 14, 51, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Automatically Finding Optimal Index Structure',\n",
       "  'authors': ['Supawit Chockchowwat', 'Wenjie Liu', 'Yongjoo Park'],\n",
       "  'summary': 'Existing learned indexes (e.g., RMI, ALEX, PGM) optimize the internal\\nregressor of each node, not the overall structure such as index height, the\\nsize of each layer, etc. In this paper, we share our recent findings that we\\ncan achieve significantly faster lookup speed by optimizing the structure as\\nwell as internal regressors. Specifically, our approach (called AirIndex)\\nexpresses the end-to-end lookup time as a novel objective function, and\\nsearches for optimal design decisions using a purpose-built optimizer. In our\\nexperiments with state-of-the-art methods, AirIndex achieves 3.3x-7.7x faster\\nlookup for the data stored on local SSD, and 1.4x-3.0x faster lookup for the\\ndata on Azure Cloud Storage.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 8, 7, 21, 25, 7, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Comparing graph data science libraries for querying and analysing datasets: towards data science queries on graphs',\n",
       "  'authors': ['Genoveva Vargas-Solar',\n",
       "   'Pierre Marrec',\n",
       "   'Mirian Halfeld Ferrari Alves'],\n",
       "  'summary': 'This paper presents an experimental study to compare analysis tools with\\nmanagement systems for querying and analysing graphs. Our experiment compares\\nclassic graph navigational operations queries where analytics tools and\\nmanagement systems adopt different execution strategies. Then, our experiment\\naddresses data science pipelines with clustering and prediction models applied\\nto graphs. In this kind of experiment, we underline the interest in combining\\nboth approaches and the interest of relying on a parallel execution platform\\nfor executing queries.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 8, 20, 12, 34, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Graph analytics workflows enactment on just in time data centres, Position Paper',\n",
       "  'authors': ['Ali Akoglu',\n",
       "   'José-Luis Zechinelli-Martini',\n",
       "   'Hamamache Kheddouci',\n",
       "   'Genoveva Vargas-Solar'],\n",
       "  'summary': 'This paper discusses our vision of multirole-capable decision-making systems\\nacross a broad range of Data Science (DS) workflows working on graphs through\\ndisaggregated data centres. Our vision is that an alternative is possible to\\nwork on a disaggregated solution for the provision of computational services\\nunder the notion of a disaggregated data centre. We define this alternative as\\na virtual entity that dynamically provides resources crosscutting the layers of\\nedge, fog and data centre according to the workloads submitted by the workflows\\nand their Service Level Objectives.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 8, 20, 12, 40, 25, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'NLDS-QL: From natural language data science questions to queries on graphs: analysing patients conditions & treatments',\n",
       "  'authors': ['Genoveva Vargas-Solar',\n",
       "   'Karim Dao',\n",
       "   'Mirian Halfeld Ferrari Alves'],\n",
       "  'summary': 'This paper introduces NLDS-QL, a translator of data science questions\\nexpressed in natural language (NL) into data science queries on graph\\ndatabases. Our translator is based on a simplified NL described by a grammar\\nthat specifies sentences combining keywords to refer to operations on graphs\\nwith the vocabulary of the graph schema. The demonstration proposed in this\\npaper shows NLDS-QL in action within a scenario to explore and analyse a graph\\nbase on patient diagnoses generated with the open-source Synthea.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 8, 22, 15, 53, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Compressing integer lists with Contextual Arithmetic Trits',\n",
       "  'authors': ['Yann Barsamian', 'André Chailloux'],\n",
       "  'summary': 'Inverted indexes allow to query large databases without needing to search in\\nthe database at each query. An important line of research is to construct the\\nmost efficient inverted indexes, both in terms of compression ratio and time\\nefficiency. In this article, we show how to use trit encoding, combined with\\ncontextual methods for computing inverted indexes. We perform an extensive\\nstudy of different variants of these methods and show that our method\\nconsistently outperforms the Binary Interpolative Method -- which is one of the\\ngolden standards in this topic -- with respect to compression size. We apply\\nour methods to a variety of datasets and make available the source code that\\nproduced the results, together with all our datasets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 9, 5, 18, 24, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Designing PIDs for Reproducible Science Using Time-Series Data',\n",
       "  'authors': ['Wen Ting Maria Tu', 'Stephen Makonin'],\n",
       "  'summary': 'As part of the investigation done by the IEEE Standards Association P2957\\nWorking Group, called Big Data Governance and Metadata Management, the use of\\npersistent identifiers (PIDs) is looked at for tackling the problem of\\nreproducible research and science. This short paper proposes a preliminary\\nmethod using PIDs to reproduce research results using time-series data.\\nFurthermore, we feel it is possible to use the methodology and design for other\\ntypes of datasets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 9, 21, 16, 15, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Dimensional Data KNN-Based Imputation',\n",
       "  'authors': ['Yuzhao Yang',\n",
       "   'Jérôme Darmont',\n",
       "   'Franck Ravat',\n",
       "   'Olivier Teste'],\n",
       "  'summary': 'Data Warehouses (DWs) are core components of Business Intelligence (BI).\\nMissing data in DWs have a great impact on data analyses. Therefore, missing\\ndata need to be completed. Unlike other existing data imputation methods mainly\\nadapted for facts, we propose a new imputation method for dimensions. This\\nmethod contains two steps: 1) a hierarchical imputation and 2) a k-nearest\\nneighbors (KNN) based imputation. Our solution has the advantage of taking into\\naccount the DW structure and dependency constraints. Experimental assessments\\nvalidate our method in terms of effectiveness and efficiency.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 10, 5, 13, 17, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Path association rule mining',\n",
       "  'authors': ['Yuya Sasaki'],\n",
       "  'summary': 'Graph association rule mining is a data mining technique used for discovering\\nregularities in graph data. In this study, we propose a novel concept, {\\\\it\\npath association rule mining}, to discover the correlations of path patterns\\nthat frequently appear in a given graph. Reachability path patterns (i.e.,\\nexistence of paths from a vertex to another vertex) are applied in our concept\\nto discover diverse regularities. We show that the problem is NP-hard, and we\\ndevelop an efficient algorithm in which the anti-monotonic property is used on\\npath patterns. Subsequently, we develop approximation and parallelization\\ntechniques to efficiently and scalably discover rules. We use real-life graphs\\nto experimentally verify the effective',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 10, 24, 11, 42, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'EEMARQ: Efficient Lock-Free Range Queries with Memory Reclamation',\n",
       "  'authors': ['Gali Sheffi', 'Pedro Ramalhete', 'Erez Petrank'],\n",
       "  'summary': 'Multi-Version Concurrency Control (MVCC) is a common mechanism for achieving\\nlinearizable range queries in database systems and concurrent data-structures.\\nThe core idea is to keep previous versions of nodes to serve range queries,\\nwhile still providing atomic reads and updates. Existing concurrent\\ndata-structure implementations, that support linearizable range queries, are\\neither slow, use locks, or rely on blocking reclamation schemes. We present\\nEEMARQ, the first scheme that uses MVCC with lock-free memory reclamation to\\nobtain a fully lock-free data-structure supporting linearizable inserts,\\ndeletes, contains, and range queries. Evaluation shows that EEMARQ outperforms\\nexisting solutions across most workloads, with lower space overhead and while\\nproviding full lock freedom.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 10, 31, 6, 23, 5, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Wikidata-lite for Knowledge Extraction and Exploration',\n",
       "  'authors': ['Phuc Nguyen', 'Hideaki Takeda'],\n",
       "  'summary': 'Wikidata is the largest collaborative general knowledge graph supported by a\\nworldwide community. It includes many helpful topics for knowledge exploration\\nand data science applications. However, due to the enormous size of Wikidata,\\nit is challenging to retrieve a large amount of data with millions of results,\\nmake complex queries requiring large aggregation operations, or access too many\\nstatement references. This paper introduces our preliminary works on\\nWikidata-lite, a toolkit to build a database offline for knowledge extraction\\nand exploration, e.g., retrieving item information, statements, provenances, or\\nsearching entities by their keywords and attributes. Wikidata-lite has high\\nperformance and memory efficiency, much faster than the official Wikidata\\nSPARQL endpoint for big queries. The Wikidata-lite repository is available at\\nhttps://github.com/phucty/wikidb.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 11, 10, 8, 46, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Converting OpenStreetMap Data to Road Networks for Downstream Applications',\n",
       "  'authors': ['Md Kaisar Ahmed'],\n",
       "  'summary': 'We study how to convert OpenStreetMap data to road networks for downstream\\napplications. OpenStreetMap data has different formats. Extensible Markup\\nLanguage (XML) is one of them. OSM data consist of nodes, ways, and relations.\\nWe process OSM XML data to extract the information of nodes and ways to obtain\\nthe map of streets of the Memphis area. We can use this map for different\\ndownstream applications.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 11, 22, 17, 46, 56, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The World of Graph Databases from An Industry Perspective',\n",
       "  'authors': ['Yuanyuan Tian'],\n",
       "  'summary': 'Rapidly growing social networks and other graph data have created a high\\ndemand for graph technologies in the market. A plethora of graph databases,\\nsystems, and solutions have emerged, as a result. On the other hand, graph has\\nlong been a well studied area in the database research community. Despite the\\nnumerous surveys on various graph research topics, there is a lack of survey on\\ngraph technologies from an industry perspective. The purpose of this paper is\\nto provide the research community with an industrial perspective on the graph\\ndatabase landscape, so that graph researcher can better understand the industry\\ntrend and the challenges that the industry is facing, and work on solutions to\\nhelp address these problems.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 11, 23, 17, 47, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Cube Interestingness: Novelty, Relevance, Peculiarity and Surprise',\n",
       "  'authors': ['Dimos Gkitsakis',\n",
       "   'Spyridon Kaloudis',\n",
       "   'Eirini Mouselli',\n",
       "   'Veronika Peralta',\n",
       "   'Patrick Marcel',\n",
       "   'Panos Vassiliadis'],\n",
       "  'summary': 'In this paper, we discuss methods to assess the interestingness of a query in\\nan environment of data cubes. We assume a hierarchical multidimensional\\ndatabase, storing data cubes and level hierarchies. We provide a systematic\\ntaxonomy of the dimensions of interestingness, and specifically, relevance,\\nsurprise, novelty, and peculiarity. We propose specific measures and algorithms\\nfor assessing the different dimensions of cube query interestingness in a\\nquantitative fashion.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 12, 6, 19, 47, 10, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A new PCA-based utility measure for synthetic data evaluation',\n",
       "  'authors': ['F. K. Dankar', 'M. K. Ibrahim'],\n",
       "  'summary': 'Data synthesis is a privacy enhancing technology aiming to produce realistic\\nand timely data when real data is hard to obtain. Utility of synthetic data\\ngenerators (SDGs) has been investigated through different utility metrics.\\nThese metrics have been found to generate conflicting conclusions making direct\\ncomparison of SDGs surprisingly difficult. Moreover, prior research found no\\ncorrelation between popular metrics, concluding they tackle different\\nutility-dimensions. This paper aggregates four popular utility metrics\\n(representing different utility dimensions) into one using\\nprincipal-component-analysis and checks whether the new measure can generate\\nsynthetic data that perform well in real-life. The new measure is used to\\ncompare four well-recognized SDGs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 11, 26, 20, 24, 15, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Privacy-Preserving Record Linkage',\n",
       "  'authors': ['Dinusha Vatsalan',\n",
       "   'Dimitrios Karapiperis',\n",
       "   'Vassilios S. Verykios'],\n",
       "  'summary': 'Given several databases containing person-specific data held by different\\norganizations, Privacy-Preserving Record Linkage (PPRL) aims to identify and\\nlink records that correspond to the same entity/individual across different\\ndatabases based on the matching of personal identifying attributes, such as\\nname and address, without revealing the actual values in these attributes due\\nto privacy concerns. This reference work entry defines the PPRL problem,\\nreviews the literature and key findings, and discusses applications and\\nresearch challenges.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 12, 12, 3, 38, 34, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Introducing Hermes: Executing Clinical Quality Language (CQL) at over 66 Million Resources per Second (inexpensively)',\n",
       "  'authors': ['Angelo Kastroulis', 'Paolo Bonfini', 'Anastasios Litsas'],\n",
       "  'summary': \"Clinical Quality Language (CQL) has emerged as a standard for rule\\nrepresentation in Clinical Decision Support (CDS) and Electronic Clinical\\nQuality Measurement (eCQM) in healthcare. While open-source reference\\nimplementations and a few commercial engines exist, there is still a market\\nneed for high-performance engines that can execute CQL queries on the scales of\\nmillions of patients. We introduce the \\\\Hermes{} engine as the world's fastest\\ncommercial CQL execution engine.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 11, 3, 3, 32, 22, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Very Large Language Model as a Unified Methodology of Text Mining',\n",
       "  'authors': ['Meng Jiang'],\n",
       "  'summary': 'Text data mining is the process of deriving essential information from\\nlanguage text. Typical text mining tasks include text categorization, text\\nclustering, topic modeling, information extraction, and text summarization.\\nVarious data sets are collected and various algorithms are designed for the\\ndifferent types of tasks. In this paper, I present a blue sky idea that very\\nlarge language model (VLLM) will become an effective unified methodology of\\ntext mining. I discuss at least three advantages of this new methodology\\nagainst conventional methods. Finally I discuss the challenges in the design\\nand development of VLLM techniques for text mining.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2022, 12, 19, 6, 52, 13, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Deep Reinforcement Learning for Join Order Enumeration',\n",
       "  'authors': ['Ryan Marcus', 'Olga Papaemmanouil'],\n",
       "  'summary': 'Join order selection plays a significant role in query performance. However,\\nmodern query optimizers typically employ static join enumeration algorithms\\nthat do not receive any feedback about the quality of the resulting plan.\\nHence, optimizers often repeatedly choose the same bad plan, as they do not\\nhave a mechanism for \"learning from their mistakes\". In this paper, we argue\\nthat existing deep reinforcement learning techniques can be applied to address\\nthis challenge. These techniques, powered by artificial neural networks, can\\nautomatically improve decision making by incorporating feedback from their\\nsuccesses and failures. Towards this goal, we present ReJOIN, a\\nproof-of-concept join enumerator, and present preliminary results indicating\\nthat ReJOIN can match or outperform the PostgreSQL optimizer in terms of plan\\nquality and join enumeration efficiency.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 2, 28, 20, 0, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data Curation with Deep Learning [Vision]',\n",
       "  'authors': ['Saravanan Thirumuruganathan',\n",
       "   'Nan Tang',\n",
       "   'Mourad Ouzzani',\n",
       "   'AnHai Doan'],\n",
       "  'summary': 'Data curation - the process of discovering, integrating, and cleaning data -\\nis one of the oldest, hardest, yet inevitable data management problems. Despite\\ndecades of efforts from both researchers and practitioners, it is still one of\\nthe most time consuming and least enjoyable work of data scientists. In most\\norganizations, data curation plays an important role so as to fully unlock the\\nvalue of big data. Unfortunately, the current solutions are not keeping up with\\nthe ever-changing data ecosystem, because they often require substantially high\\nhuman cost. Meanwhile, deep learning is making strides in achieving remarkable\\nsuccesses in multiple areas, such as image recognition, natural language\\nprocessing, and speech recognition. In this vision paper, we explore how some\\nof the fundamental innovations in deep learning could be leveraged to improve\\nexisting data curation solutions and to help build new ones. In particular, we\\nprovide a thorough overview of the current deep learning landscape, and\\nidentify interesting research opportunities and dispel common myths. We hope\\nthat the synthesis of these important domains will unleash a series of research\\nactivities that will lead to significantly improved solutions for many data\\ncuration tasks.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 3, 4, 17, 8, 45, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Comparing Downward Fragments of the Relational Calculus with Transitive Closure on Trees',\n",
       "  'authors': ['Jelle Hellings',\n",
       "   'Marc Gyssens',\n",
       "   'Yuqing Wu',\n",
       "   'Dirk Van Gucht',\n",
       "   'Jan Van den Bussche',\n",
       "   'Stijn Vansummeren',\n",
       "   'George H. L. Fletcher'],\n",
       "  'summary': 'Motivated by the continuing interest in the tree data model, we study the\\nexpressive power of downward navigational query languages on trees and chains.\\nBasic navigational queries are built from the identity relation and edge\\nrelations using composition and union. We study the effects on relative\\nexpressiveness when we add transitive closure, projections, coprojections,\\nintersection, and difference; this for boolean queries and path queries on\\nlabeled and unlabeled structures. In all cases, we present the complete Hasse\\ndiagram. In particular, we establish, for each query language fragment that we\\nstudy on trees, whether it is closed under difference and intersection.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 3, 4, 17, 38, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Moment-Based Quantile Sketches for Efficient High Cardinality Aggregation Queries',\n",
       "  'authors': ['Edward Gan',\n",
       "   'Jialin Ding',\n",
       "   'Kai Sheng Tai',\n",
       "   'Vatsal Sharan',\n",
       "   'Peter Bailis'],\n",
       "  'summary': 'Interactive analytics increasingly involves querying for quantiles over\\nsub-populations of high cardinality datasets. Data processing engines such as\\nDruid and Spark use mergeable summaries to estimate quantiles, but summary\\nmerge times can be a bottleneck during aggregation. We show how a compact and\\nefficiently mergeable quantile sketch can support aggregation workloads. This\\ndata structure, which we refer to as the moments sketch, operates with a small\\nmemory footprint (200 bytes) and computationally efficient (50ns) merges by\\ntracking only a set of summary statistics, notably the sample moments. We\\ndemonstrate how we can efficiently and practically estimate quantiles using the\\nmethod of moments and the maximum entropy principle, and show how the use of a\\ncascade further improves query time for threshold predicates. Empirical\\nevaluation on real-world datasets shows that the moments sketch can achieve\\nless than 1 percent error with 15 times less merge overhead than comparable\\nsummaries, improving end query time in the MacroBase engine by up to 7 times\\nand the Druid engine by up to 60 times.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 3, 6, 0, 48, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'TRAJEDI: Trajectory Dissimilarity',\n",
       "  'authors': ['Pedram Gharani', 'Kenrick Fernande', 'Vineet Raghu'],\n",
       "  'summary': 'The vast increase in our ability to obtain and store trajectory data\\nnecessitates trajectory analytics techniques to extract useful information from\\nthis data. Pair-wise distance functions are a foundation building block for\\ncommon operations on trajectory datasets including constrained SELECT queries,\\nk-nearest neighbors, and similarity and diversity algorithms. The accuracy and\\nperformance of these operations depend heavily on the speed and accuracy of the\\nunderlying trajectory distance function, which is in turn affected by\\ntrajectory calibration. Current methods either require calibrated data, or\\nperform calibration of the entire relevant dataset first, which is expensive\\nand time consuming for large datasets. We present TRAJEDI, a calibrationaware\\npair-wise distance calculation scheme that outperforms naive approaches while\\npreserving accuracy. We also provide analyses of parameter tuning to trade-off\\nbetween speed and accuracy. Our scheme is usable with any diversity, similarity\\nor k-nearest neighbor algorithm.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 3, 9, 23, 7, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Bias in OLAP Queries: Detection, Explanation, and Removal',\n",
       "  'authors': ['Babak Salimi', 'Johannes Gehrke', 'Dan Suciu'],\n",
       "  'summary': 'On line analytical processing (OLAP) is an essential element of\\ndecision-support systems. OLAP tools provide insights and understanding needed\\nfor improved decision making. However, the answers to OLAP queries can be\\nbiased and lead to perplexing and incorrect insights. In this paper, we propose\\nHypDB, a system to detect, explain, and to resolve bias in decision-support\\nqueries. We give a simple definition of a \\\\emph{biased query}, which performs a\\nset of independence tests on the data to detect bias. We propose a novel\\ntechnique that gives explanations for bias, thus assisting an analyst in\\nunderstanding what goes on. Additionally, we develop an automated method for\\nrewriting a biased query into an unbiased query, which shows what the analyst\\nintended to examine. In a thorough evaluation on several real datasets we show\\nboth the quality and the performance of our techniques, including the\\ncompletely automatic discovery of the revolutionary insights from a famous 1973\\ndiscrimination case.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 3, 12, 22, 54, 11, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Constant delay algorithms for regular document spanners',\n",
       "  'authors': ['Fernando Florenzano',\n",
       "   'Cristian Riveros',\n",
       "   'Martin Ugarte',\n",
       "   'Stijn Vansummeren',\n",
       "   'Domagoj Vrgoc'],\n",
       "  'summary': \"Regular expressions and automata models with capture variables are core tools\\nin rule-based information extraction. These formalisms, also called regular\\ndocument spanners, use regular languages in order to locate the data that a\\nuser wants to extract from a text document, and then store this data into\\nvariables. Since document spanners can easily generate large outputs, it is\\nimportant to have good evaluation algorithms that can generate the extracted\\ndata in a quick succession, and with relatively little precomputation time.\\nTowards this goal, we present a practical evaluation algorithm that allows\\nconstant delay enumeration of a spanner's output after a precomputation phase\\nthat is linear in the document. While the algorithm assumes that the spanner is\\nspecified in a syntactic variant of variable set automata, we also study how it\\ncan be applied when the spanner is specified by general variable set automata,\\nregex formulas, or spanner algebras. Finally, we study the related problem of\\ncounting the number of outputs of a document spanner, providing a fine grained\\nanalysis of the classes of document spanners that support efficient enumeration\\nof their results.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 3, 14, 13, 44, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Datalog: Bag Semantics via Set Semantics',\n",
       "  'authors': ['Leopoldo Bertossi', 'Georg Gottlob', 'Reinhard Pichler'],\n",
       "  'summary': 'Duplicates in data management are common and problematic. In this work, we\\npresent a translation of Datalog under bag semantics into a well-behaved\\nextension of Datalog, the so-called {\\\\em warded Datalog}$^\\\\pm$, under set\\nsemantics. From a theoretical point of view, this allows us to reason on bag\\nsemantics by making use of the well-established theoretical foundations of set\\nsemantics. From a practical point of view, this allows us to handle the bag\\nsemantics of Datalog by powerful, existing query engines for the required\\nextension of Datalog. This use of Datalog$^\\\\pm$ is extended to give a set\\nsemantics to duplicates in Datalog$^\\\\pm$ itself. We investigate the properties\\nof the resulting Datalog$^\\\\pm$ programs, the problem of deciding\\nmultiplicities, and expressibility of some bag operations. Moreover, the\\nproposed translation has the potential for interesting applications such as to\\nMultiset Relational Algebra and the semantic web query language SPARQL with bag\\nsemantics.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 3, 17, 2, 0, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Guided FP-growth algorithm for multitude-targeted mining of big data',\n",
       "  'authors': ['Lior Shabtay', 'Rami Yaari', 'Itai Dattner'],\n",
       "  'summary': 'In this paper we present the GFP-growth (Guided FP-growth) algorithm, a novel\\nmethod for multitude-targeted mining: finding the count of a given large list\\nof itemsets in large data. The GFP-growth algorithm is designed to focus on the\\nspecific multitude itemsets of interest and optimizes the time and memory\\ncosts. We prove that the GFP-growth algorithm yields the exact frequency-counts\\nfor the required itemsets. We show that for a number of different problems, a\\nsolution can be devised which takes advantage of the efficient implementation\\nof multitude-targeted mining for boosting the performance. In particular, we\\nstudy in detail the problem of generating the minority-class rules from\\nimbalanced data, a scenario that appears in many real-life domains such as\\nmedical applications, failure prediction, network and cyber security, and\\nmaintenance. We develop the Minority-Report Algorithm that uses the GFP-growth\\nfor boosting performance. We prove some theoretical properties of the\\nMinority-Report Algorithm and demonstrate its performance gain using\\nsimulations and real data.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 3, 18, 9, 57, 34, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'On-demand Relational Concept Analysis',\n",
       "  'authors': ['Alexandre Bazin',\n",
       "   'Jessie Carbonnel',\n",
       "   'Marianne Huchard',\n",
       "   'Giacomo Kahn'],\n",
       "  'summary': 'Formal Concept Analysis and its associated conceptual structures have been\\nused to support exploratory search through conceptual navigation. Relational\\nConcept Analysis (RCA) is an extension of Formal Concept Analysis to process\\nrelational datasets. RCA and its multiple interconnected structures represent\\ngood candidates to support exploratory search in relational datasets, as they\\nare enabling navigation within a structure as well as between the connected\\nstructures. However, building the entire structures does not present an\\nefficient solution to explore a small localised area of the dataset, for\\ninstance to retrieve the closest alternatives to a given query. In these cases,\\ngenerating only a concept and its neighbour concepts at each navigation step\\nappears as a less costly alternative. In this paper, we propose an algorithm to\\ncompute a concept and its neighbourhood in extended concept lattices. The\\nconcepts are generated directly from the relational context family, and possess\\nboth formal and relational attributes. The algorithm takes into account two RCA\\nscaling operators. We illustrate it on an example.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 3, 21, 10, 50, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Learning State Representations for Query Optimization with Deep Reinforcement Learning',\n",
       "  'authors': ['Jennifer Ortiz',\n",
       "   'Magdalena Balazinska',\n",
       "   'Johannes Gehrke',\n",
       "   'S. Sathiya Keerthi'],\n",
       "  'summary': 'Deep reinforcement learning is quickly changing the field of artificial\\nintelligence. These models are able to capture a high level understanding of\\ntheir environment, enabling them to learn difficult dynamic tasks in a variety\\nof domains. In the database field, query optimization remains a difficult\\nproblem. Our goal in this work is to explore the capabilities of deep\\nreinforcement learning in the context of query optimization. At each state, we\\nbuild queries incrementally and encode properties of subqueries through a\\nlearned representation. The challenge here lies in the formation of the state\\ntransition function, which defines how the current subquery state combines with\\nthe next query operation (action) to yield the next state. As a first step in\\nthis direction, we focus the state representation problem and the formation of\\nthe state transition function. We describe our approach and show preliminary\\nresults. We further discuss how we can use the state representation to improve\\nquery optimization using reinforcement learning.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 3, 22, 22, 39, 32, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Datasheets for Datasets',\n",
       "  'authors': ['Timnit Gebru',\n",
       "   'Jamie Morgenstern',\n",
       "   'Briana Vecchione',\n",
       "   'Jennifer Wortman Vaughan',\n",
       "   'Hanna Wallach',\n",
       "   'Hal Daumé III',\n",
       "   'Kate Crawford'],\n",
       "  'summary': 'The machine learning community currently has no standardized process for\\ndocumenting datasets, which can lead to severe consequences in high-stakes\\ndomains. To address this gap, we propose datasheets for datasets. In the\\nelectronics industry, every component, no matter how simple or complex, is\\naccompanied with a datasheet that describes its operating characteristics, test\\nresults, recommended uses, and other information. By analogy, we propose that\\nevery dataset be accompanied with a datasheet that documents its motivation,\\ncomposition, collection process, recommended uses, and so on. Datasheets for\\ndatasets will facilitate better communication between dataset creators and\\ndataset consumers, and encourage the machine learning community to prioritize\\ntransparency and accountability.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 3, 23, 23, 22, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'GreyCat: Efficient What-If Analytics for Data in Motion at Scale',\n",
       "  'authors': ['Thomas Hartmann',\n",
       "   'Francois Fouquet',\n",
       "   'Assaad Moawad',\n",
       "   'Romain Rouvoy',\n",
       "   'Yves Le Traon'],\n",
       "  'summary': 'Over the last few years, data analytics shifted from a descriptive era,\\nconfined to the explanation of past events, to the emergence of predictive\\ntechniques. Nonetheless, existing predictive techniques still fail to\\neffectively explore alternative futures, which continuously diverge from\\ncurrent situations when exploring the effects of what-if decisions. Enabling\\nprescriptive analytics therefore calls for the design of scalable systems that\\ncan cope with the complexity and the diversity of underlying data models. In\\nthis article, we address this challenge by combining graphs and time series\\nwithin a scalable storage system that can organize a massive amount of\\nunstructured and continuously changing data into multi-dimensional data models,\\ncalled Many-Worlds Graphs. We demonstrate that our open source implementation,\\nGreyCat, can efficiently fork and update thousands of parallel worlds composed\\nof millions of timestamped nodes, such as what-if exploration.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 3, 23, 7, 48, 15, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Worst-Case Optimal Join Algorithms: Techniques, Results, and Open Problems',\n",
       "  'authors': ['Hung Q. Ngo'],\n",
       "  'summary': 'Worst-case optimal join algorithms are the class of join algorithms whose\\nruntime match the worst-case output size of a given join query. While the first\\nprovably worst-case optimal join algorithm was discovered relatively recently,\\nthe techniques and results surrounding these algorithms grow out of decades of\\nresearch from a wide range of areas, intimately connecting graph theory,\\nalgorithms, information theory, constraint satisfaction, database theory, and\\ngeometric inequalities. These ideas are not just paperware: in addition to\\nacademic project implementations, two variations of such algorithms are the\\nwork-horse join algorithms of commercial database and data analytics engines.\\n  This paper aims to be a brief introduction to the design and analysis of\\nworst-case optimal join algorithms. We discuss the key techniques for proving\\nruntime and output size bounds. We particularly focus on the fascinating\\nconnection between join algorithms and information theoretic inequalities, and\\nthe idea of how one can turn a proof into an algorithm. Finally, we conclude\\nwith a representative list of fundamental open problems in this area.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 3, 27, 7, 13, 49, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Statistical Validity and Consistency of Big Data Analytics: A General Framework',\n",
       "  'authors': ['Bikram Karmakar', 'Indranil Mukhopadhyay'],\n",
       "  'summary': 'Informatics and technological advancements have triggered generation of huge\\nvolume of data with varied complexity in its management and analysis. Big Data\\nanalytics is the practice of revealing hidden aspects of such data and making\\ninferences from it. Although storage, retrieval and management of Big Data seem\\npossible through efficient algorithm and system development, concern about\\nstatistical consistency remains to be addressed in view of its specific\\ncharacteristics. Since Big Data does not conform to standard analytics, we need\\nproper modification of the existing statistical theory and tools. Here we\\npropose, with illustrations, a general statistical framework and an algorithmic\\nprinciple for Big Data analytics that ensure statistical accuracy of the\\nconclusions. The proposed framework has the potential to push forward\\nadvancement of Big Data analytics in the right direction. The\\npartition-repetition approach proposed here is broad enough to encompass all\\npractical data analytic problems.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 3, 29, 2, 15, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Analytics for the Internet of Things: A Survey',\n",
       "  'authors': ['Eugene Siow', 'Thanassis Tiropanis', 'Wendy Hall'],\n",
       "  'summary': 'The Internet of Things (IoT) envisions a world-wide, interconnected network\\nof smart physical entities. These physical entities generate a large amount of\\ndata in operation and as the IoT gains momentum in terms of deployment, the\\ncombined scale of those data seems destined to continue to grow. Increasingly,\\napplications for the IoT involve analytics. Data analytics is the process of\\nderiving knowledge from data, generating value like actionable insights from\\nthem. This article reviews work in the IoT and big data analytics from the\\nperspective of their utility in creating efficient, effective and innovative\\napplications and services for a wide spectrum of domains. We review the broad\\nvision for the IoT as it is shaped in various communities, examine the\\napplication of data analytics across IoT domains, provide a categorisation of\\nanalytic approaches and propose a layered taxonomy from IoT data to analytics.\\nThis taxonomy provides us with insights on the appropriateness of analytical\\ntechniques, which in turn shapes a survey of enabling technology and\\ninfrastructure for IoT analytics. Finally, we look at some tradeoffs for\\nanalytics in the IoT that can shape future research.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 7, 3, 4, 15, 8, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Mining Periodic Patterns with a MDL Criterion',\n",
       "  'authors': ['Esther Galbrun',\n",
       "   'Peggy Cellier',\n",
       "   'Nikolaj Tatti',\n",
       "   'Alexandre Termier',\n",
       "   'Bruno Crémilleux'],\n",
       "  'summary': 'The quantity of event logs available is increasing rapidly, be they produced\\nby industrial processes, computing systems, or life tracking, for instance. It\\nis thus important to design effective ways to uncover the information they\\ncontain. Because event logs often record repetitive phenomena, mining periodic\\npatterns is especially relevant when considering such data. Indeed, capturing\\nsuch regularities is instrumental in providing condensed representations of the\\nevent sequences.\\n  We present an approach for mining periodic patterns from event logs while\\nrelying on a Minimum Description Length (MDL) criterion to evaluate candidate\\npatterns. Our goal is to extract a set of patterns that suitably characterises\\nthe periodic structure present in the data. We evaluate the interest of our\\napproach on several real-world event log datasets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 7, 4, 13, 21, 19, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Modeling Data Lake Metadata with a Data Vault',\n",
       "  'authors': ['Iuri Nogueira', 'Maram Romdhane', 'Jérôme Darmont'],\n",
       "  'summary': 'With the rise of big data, business intelligence had to find solutions for\\nmanaging even greater data volumes and variety than in data warehouses, which\\nproved ill-adapted. Data lakes answer these needs from a storage point of view,\\nbut require managing adequate metadata to guarantee an efficient access to\\ndata. Starting from a multidimensional metadata model designed for an\\nindustrial heritage data lake presenting a lack of schema evolutivity, we\\npropose in this paper to use ensemble modeling, and more precisely a data\\nvault, to address this issue. To illustrate the feasibility of this approach,\\nwe instantiate our metadata conceptual model into relational and\\ndocument-oriented logical and physical models, respectively. We also compare\\nthe physical models in terms of metadata storage and query response time.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 7, 11, 9, 36, 34, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Indexing Execution Patterns in Workflow Provenance Graphs through Generalized Trie Structures',\n",
       "  'authors': ['Esteban García-Cuesta', 'José M. Gómez-Pérez'],\n",
       "  'summary': 'Over the last years, scientific workflows have become mature enough to be\\nused in a production style. However, despite the increasing maturity, there is\\nstill a shortage of tools for searching, adapting, and reusing workflows that\\nhinders a more generalized adoption by the scientific communities. Indeed, due\\nto the limited availability of machine-readable scientific metadata and the\\nheterogeneity of workflow specification formats and representations, new ways\\nto leverage alternative sources of information that complement existing\\napproaches are needed. In this paper we address such limitations by applying\\nstatistically enriched generalized trie structures to exploit workflow\\nexecution provenance information in order to assist the analysis, indexing and\\nsearch of scientific workflows. Our method bridges the gap between the\\ndescription of what a workflow is supposed to do according to its specification\\nand related metadata and what it actually does as recorded in its provenance\\nexecution trace. In doing so, we also prove that the proposed method\\noutperforms SPARQL 1.1 Property Paths for querying provenance graphs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 7, 19, 11, 29, 40, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Cache-based Optimizer for Querying Enhanced Knowledge Bases',\n",
       "  'authors': ['Wei Emma Zhang', 'Quan Z. Sheng', 'Schahram Dustdar'],\n",
       "  'summary': 'With recent emerging technologies such as the Internet of Things (IoT),\\ninformation collection on our physical world and environment can be achieved at\\na much higher granularity and such detailed knowledge will play a critical role\\nin improving the productivity, operational effectiveness, decision making, and\\nin identifying new business models for economic growth. Efficient discovery and\\nquerying such knowledge remains a key challenge due to the limited capability\\nand high latency of connections to the interfaces of knowledge bases, e.g., the\\nSPARQL endpoints. In this article, we present a querying system on SPARQL\\nendpoints for knowledge bases that performs queries faster than the\\nstate-of-the-art systems. Our system features a cache-based optimization scheme\\nto improve querying performance by prefetching and caching the results of\\npredicted potential queries. The evaluations on query sets from SPARQL\\nendpoints of DBpedia and Linked GeoData showcase the effectiveness of our\\napproach.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 7, 23, 7, 47, 32, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The Vadalog System: Datalog-based Reasoning for Knowledge Graphs',\n",
       "  'authors': ['Luigi Bellomarini', 'Georg Gottlob', 'Emanuel Sallinger'],\n",
       "  'summary': \"Over the past years, there has been a resurgence of Datalog-based systems in\\nthe database community as well as in industry. In this context, it has been\\nrecognized that to handle the complex knowl\\\\-edge-based scenarios encountered\\ntoday, such as reasoning over large knowledge graphs, Datalog has to be\\nextended with features such as existential quantification. Yet, Datalog-based\\nreasoning in the presence of existential quantification is in general\\nundecidable. Many efforts have been made to define decidable fragments. Warded\\nDatalog+/- is a very promising one, as it captures PTIME complexity while\\nallowing ontological reasoning. Yet so far, no implementation of Warded\\nDatalog+/- was available. In this paper we present the Vadalog system, a\\nDatalog-based system for performing complex logic reasoning tasks, such as\\nthose required in advanced knowledge graphs. The Vadalog system is Oxford's\\ncontribution to the VADA research programme, a joint effort of the universities\\nof Oxford, Manchester and Edinburgh and around 20 industrial partners. As the\\nmain contribution of this paper, we illustrate the first implementation of\\nWarded Datalog+/-, a high-performance Datalog+/- system utilizing an aggressive\\ntermination control strategy. We also provide a comprehensive experimental\\nevaluation.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 7, 23, 16, 38, 5, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data Science with Vadalog: Bridging Machine Learning and Reasoning',\n",
       "  'authors': ['Luigi Bellomarini',\n",
       "   'Ruslan R. Fayzrakhmanov',\n",
       "   'Georg Gottlob',\n",
       "   'Andrey Kravchenko',\n",
       "   'Eleonora Laurenza',\n",
       "   'Yavor Nenov',\n",
       "   'Stephane Reissfelder',\n",
       "   'Emanuel Sallinger',\n",
       "   'Evgeny Sherkhonov',\n",
       "   'Lianlong Wu'],\n",
       "  'summary': 'Following the recent successful examples of large technology companies, many\\nmodern enterprises seek to build knowledge graphs to provide a unified view of\\ncorporate knowledge and to draw deep insights using machine learning and\\nlogical reasoning. There is currently a perceived disconnect between the\\ntraditional approaches for data science, typically based on machine learning\\nand statistical modelling, and systems for reasoning with domain knowledge. In\\nthis paper we present a state-of-the-art Knowledge Graph Management System,\\nVadalog, which delivers highly expressive and efficient logical reasoning and\\nprovides seamless integration with modern data science toolkits, such as the\\nJupyter platform. We demonstrate how to use Vadalog to perform traditional data\\nwrangling tasks, as well as complex logical and probabilistic reasoning. We\\nargue that this is a significant step forward towards combining machine\\nlearning and reasoning in data science.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 7, 23, 16, 40, 37, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'GPU-based Commonsense Paradigms Reasoning for Real-Time Query Answering and Multimodal Analysis',\n",
       "  'authors': ['Nguyen Ha Tran', 'Erik Cambria'],\n",
       "  'summary': 'We utilize commonsense knowledge bases to address the problem of real- time\\nmultimodal analysis. In particular, we focus on the problem of multimodal\\nsentiment analysis, which consists in the simultaneous analysis of different\\nmodali- ties, e.g., speech and video, for emotion and polarity detection. Our\\napproach takes advantages of the massively parallel processing power of modern\\nGPUs to enhance the performance of feature extraction from different\\nmodalities. In addition, in order to ex- tract important textual features from\\nmultimodal sources we generate domain-specific graphs based on commonsense\\nknowledge and apply GPU-based graph traversal for fast feature detection. Then,\\npowerful ELM classifiers are applied to build the senti- ment analysis model\\nbased on the extracted features. We conduct our experiments on the YouTube\\ndataset and achieve an accuracy of 78% which outperforms all previous systems.\\nIn term of processing speed, our method shows improvements of several orders of\\nmagnitude for feature extraction compared to CPU-based counterparts.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 7, 14, 14, 46, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Compiling Database Application Programs',\n",
       "  'authors': ['Mohammad Dashti',\n",
       "   'Sachin Basil John',\n",
       "   'Thierry Coppey',\n",
       "   'Amir Shaikhha',\n",
       "   'Vojin Jovanovic',\n",
       "   'Christoph Koch'],\n",
       "  'summary': 'There is a trend towards increased specialization of data management software\\nfor performance reasons. In this paper, we study the automatic specialization\\nand optimization of database application programs -- sequences of queries and\\nupdates, augmented with control flow constructs as they appear in database\\nscripts, UDFs, transactional workloads and triggers in languages such as\\nPL/SQL. We show how to build an optimizing compiler for database application\\nprograms using generative programming and state-of-the-art compiler technology.\\n  We evaluate a hand-optimized low-level implementation of TPC-C, and identify\\nthe key optimization techniques that account for its good performance. Our\\ncompiler fully automates these optimizations and, applied to this benchmark,\\noutperforms the manually optimized baseline by a factor of two. By selectively\\ndisabling some of the optimizations in the compiler, we derive a clinical and\\nprecise way of obtaining insight into their individual performance\\ncontributions.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 7, 25, 22, 39, 52, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'NDBench: Benchmarking Microservices at Scale',\n",
       "  'authors': ['Ioannis Papapanagiotou', 'Vinay Chella'],\n",
       "  'summary': 'Software vendors often report performance numbers for the sweet spot or\\nrunning on specialized hardware with specific workload parameters and without\\nrealistic failures. Accurate benchmarks at the persistence layer are crucial,\\nas failures may cause unrecoverable errors such as data loss, inconsistency or\\ncorruption. To accurately evaluate data stores and other microservices at\\nNetflix, we developed Netflix Data Benchmark (NDBench), a Cloud benchmark tool.\\nIt can be deployed in a loosely-coupled fashion with the ability to dynamically\\nchange the benchmark parameters at runtime so we can rapidly iterate on\\ndifferent tests and failure modes. NDBench offers pluggable patterns and loads,\\nsupport for pluggable client APIs, and was designed to run continually. This\\ndesign enabled us to test long-running maintenance jobs that may affect the\\nperformance, test numerous different systems under adverse conditions, and\\nuncover long-term issues like memory leaks or heap pressure.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 7, 27, 18, 42, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'DataJoint: A Simpler Relational Data Model',\n",
       "  'authors': ['Dimitri Yatsenko', 'Edgar Y. Walker', 'Andreas S. Tolias'],\n",
       "  'summary': \"The relational data model offers unrivaled rigor and precision in defining\\ndata structure and querying complex data. Yet the use of relational databases\\nin scientific data pipelines is limited due to their perceived unwieldiness. We\\npropose a simplified and conceptually refined relational data model named\\nDataJoint. The model includes a language for schema definition, a language for\\ndata queries, and diagramming notation for visualizing entities and\\nrelationships among them. The model adheres to the principle of entity\\nnormalization, which requires that all data -- both stored and derived -- must\\nbe represented by well-formed entity sets. DataJoint's data query language is\\nan algebra on entity sets with five operators that provide matching\\ncapabilities to those of other relational query languages with greater clarity\\ndue to entity normalization. Practical implementations of DataJoint have been\\nadopted in neuroscience labs for fluent interaction with scientific data\\npipelines.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 7, 29, 19, 39, 38, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'To Ship or Not to (Function) Ship (Extended version)',\n",
       "  'authors': ['Feilong Liu', 'Niranjan Kamat', 'Spyros Blanas', 'Arnab Nandi'],\n",
       "  'summary': \"Sampling is often used to reduce query latency for interactive big data\\nanalytics. The established parallel data processing paradigm relies on function\\nshipping, where a coordinator dispatches queries to worker nodes and then\\ncollects the results. The commoditization of high-performance networking makes\\ndata shipping possible, where the coordinator directly reads data in the\\nworkers' memory using RDMA while workers process other queries. In this work,\\nwe explore when to use function shipping or data shipping for interactive query\\nprocessing with sampling. Whether function shipping or data shipping should be\\npreferred depends on the amount of data transferred, the current CPU\\nutilization, the sampling method and the number of queries executed over the\\ndata set. The results show that data shipping is up to 6.5x faster when\\nperforming clustered sampling with heavily-utilized workers.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 7, 30, 2, 37, 44, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Interactive Summarization and Exploration of Top Aggregate Query Answers',\n",
       "  'authors': ['Yuhao Wen', 'Xiaodan Zhu', 'Sudeepa Roy', 'Jun Yang'],\n",
       "  'summary': 'We present a system for summarization and interactive exploration of\\nhigh-valued aggregate query answers to make a large set of possible answers\\nmore informative to the user. Our system outputs a set of clusters on the\\nhigh-valued query answers showing their common properties such that the\\nclusters are diverse as much as possible to avoid repeating information, and\\ncover a certain number of top original answers as indicated by the user.\\nFurther, the system facilitates interactive exploration of the query answers by\\nhelping the user (i) choose combinations of parameters for clustering, (ii)\\ninspect the clusters as well as the elements they contain, and (iii) visualize\\nhow changes in parameters affect clustering. We define optimization problems,\\nstudy their complexity, explore properties of the solutions investigating the\\nsemi-lattice structure on the clusters, and propose efficient algorithms and\\noptimizations to achieve these goals. We evaluate our techniques experimentally\\nand discuss our prototype with a graphical user interface that facilitates this\\ninteractive exploration. A user study is conducted to evaluate the usability of\\nour approach.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 7, 31, 2, 31, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Improve3C: Data Cleaning on Consistency and Completeness with Currency',\n",
       "  'authors': ['Xiaoou Ding',\n",
       "   'Hongzhi Wang',\n",
       "   'Jiaxuan Su',\n",
       "   'Jianzhong Li',\n",
       "   'Hong Gao'],\n",
       "  'summary': 'Data quality plays a key role in big data management today. With the\\nexplosive growth of data from a variety of sources, the quality of data is\\nfaced with multiple problems. Motivated by this, we study the multiple data\\nquality improvement on completeness, consistency and currency in this paper.\\nFor the proposed problem, we introduce a 4-step framework, named Improve3C, for\\ndetection and quality improvement on incomplete and inconsistent data without\\ntimestamps. We compute and achieve a relative currency order among records\\nderived from given currency constraints, according to which inconsistent and\\nincomplete data can be repaired effectively considering the temporal impact.\\nFor both effectiveness and efficiency consideration, we carry out inconsistent\\nrepair ahead of incomplete repair. Currency-related consistency distance is\\ndefined to measure the similarity between dirty records and clean ones more\\naccurately. In addition, currency orders are treated as an important feature in\\nthe training process of incompleteness repair. The solution algorithms are\\nintroduced in detail with examples. A thorough experiment on one real-life data\\nand a synthetic one verifies that the proposed method can improve the\\nperformance of dirty data cleaning with multiple quality problems which are\\nhard to be cleaned by the existing approaches effectively.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 7, 31, 18, 48, 32, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Diversification on Big Data in Query Processing',\n",
       "  'authors': ['Meifan Zhang', 'Hongzhi Wang', 'Jianzhong Li', 'Hong Gao'],\n",
       "  'summary': \"Recently, in the area of big data, some popular applications such as web\\nsearch engines and recommendation systems, face the problem to diversify\\nresults during query processing. In this sense, it is both significant and\\nessential to propose methods to deal with big data in order to increase the\\ndiversity of the result set. In this paper, we firstly define a set's diversity\\nand an element's ability to improve the set's overall diversity. Based on these\\ndefinitions, we propose a diversification framework which has good performance\\nin terms of effectiveness and efficiency. Also, this framework has theoretical\\nguarantee on probability of success. Secondly, we design implementation\\nalgorithms based on this framework for both numerical and string data. Thirdly,\\nfor numerical and string data respectively, we carry out extensive experiments\\non real data to verify the performance of our proposed framework, and also\\nperform scalability experiments on synthetic data.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 2, 18, 47, 27, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'On the Fairness of Quality-based Data Markets',\n",
       "  'authors': ['Dan Zhang',\n",
       "   'Hongzhi Wang',\n",
       "   'Xiaoou Ding',\n",
       "   'Yice Zhang',\n",
       "   'Jianzhong Li',\n",
       "   'Hong Gao'],\n",
       "  'summary': 'For data pricing, data quality is a factor that must be considered. To keep\\nthe fairness of data market from the aspect of data quality, we proposed a fair\\ndata market that considers data quality while pricing. To ensure fairness, we\\nfirst design a quality-driven data pricing strategy. Then based on the\\nstrategy, a fairness assurance mechanism for quality-driven data marketplace is\\nproposed. In this mechanism, we ensure that savvy consumers cannot cheat the\\nsystem and users can verify each consumption with Trusted Third Party (TTP)\\nthat they are charged properly. Based on this mechanism, we develop a fair\\nquality-driven data market system. Extensive experiments are performed to\\nverify the effectiveness of proposed techniques. Experimental results show that\\nour quality-driven data pricing strategy could assign a reasonable price to the\\ndata according to data quality and the fairness assurance mechanism could\\neffectively protect quality-driven data pricing from potential cheating.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 5, 14, 13, 29, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The Bases of Association Rules of High Confidence',\n",
       "  'authors': ['Oren Segal',\n",
       "   'Justin Cabot-Miller',\n",
       "   'Kira Adaricheva',\n",
       "   'J. B. Nation',\n",
       "   'Anuar Sharafudinov'],\n",
       "  'summary': 'We develop a new approach for distributed computing of the association rules\\nof high confidence in a binary table. It is derived from the D-basis algorithm\\nin K. Adaricheva and J.B. Nation (TCS 2017), which is performed on multiple\\nsub-tables of a table given by removing several rows at a time. The set of\\nrules is then aggregated using the same approach as the D-basis is retrieved\\nfrom a larger set of implications. This allows to obtain a basis of association\\nrules of high confidence, which can be used for ranking all attributes of the\\ntable with respect to a given fixed attribute using the relevance parameter\\nintroduced in K. Adaricheva et al. (Proceedings of ICFCA-2015). This paper\\nfocuses on the technical implementation of the new algorithm. Some testing\\nresults are performed on transaction data and medical data.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 6, 1, 1, 50, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Learning to Optimize Join Queries With Deep Reinforcement Learning',\n",
       "  'authors': ['Sanjay Krishnan',\n",
       "   'Zongheng Yang',\n",
       "   'Ken Goldberg',\n",
       "   'Joseph Hellerstein',\n",
       "   'Ion Stoica'],\n",
       "  'summary': 'Exhaustive enumeration of all possible join orders is often avoided, and most\\noptimizers leverage heuristics to prune the search space. The design and\\nimplementation of heuristics are well-understood when the cost model is roughly\\nlinear, and we find that these heuristics can be significantly suboptimal when\\nthere are non-linearities in cost. Ideally, instead of a fixed heuristic, we\\nwould want a strategy to guide the search space in a more data-driven\\nway---tailoring the search to a specific dataset and query workload.\\nRecognizing the link between classical Dynamic Programming enumeration methods\\nand recent results in Reinforcement Learning (RL), we propose a new method for\\nlearning optimized join search strategies. We present our RL-based DQ\\noptimizer, which currently optimizes select-project-join blocks. We implement\\nthree versions of DQ to illustrate the ease of integration into existing\\nDBMSes: (1) A version built on top of Apache Calcite, (2) a version integrated\\ninto PostgreSQL, and (3) a version integrated into SparkSQL. Our extensive\\nevaluation shows that DQ achieves plans with optimization costs and query\\nexecution times competitive with the native query optimizer in each system, but\\ncan execute significantly faster after learning (often by orders of magnitude).',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 9, 15, 30, 6, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Optimizing error of high-dimensional statistical queries under differential privacy',\n",
       "  'authors': ['Ryan McKenna',\n",
       "   'Gerome Miklau',\n",
       "   'Michael Hay',\n",
       "   'Ashwin Machanavajjhala'],\n",
       "  'summary': 'Differentially private algorithms for answering sets of predicate counting\\nqueries on a sensitive database have many applications. Organizations that\\ncollect individual-level data, such as statistical agencies and medical\\ninstitutions, use them to safely release summary tabulations. However, existing\\ntechniques are accurate only on a narrow class of query workloads, or are\\nextremely slow, especially when analyzing more than one or two dimensions of\\nthe data. In this work we propose HDMM, a new differentially private algorithm\\nfor answering a workload of predicate counting queries, that is especially\\neffective for higher-dimensional datasets. HDMM represents query workloads\\nusing an implicit matrix representation and exploits this compact\\nrepresentation to efficiently search (a subset of) the space of differentially\\nprivate algorithms for one that answers the input query workload with high\\naccuracy. We empirically show that HDMM can efficiently answer queries with\\nlower error than state-of-the-art techniques on a variety of low and high\\ndimensional datasets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 10, 13, 44, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Ektelo: A Framework for Defining Differentially-Private Computations',\n",
       "  'authors': ['Dan Zhang',\n",
       "   'Ryan McKenna',\n",
       "   'Ios Kotsogiannis',\n",
       "   'George Bissias',\n",
       "   'Michael Hay',\n",
       "   'Ashwin Machanavajjhala',\n",
       "   'Gerome Miklau'],\n",
       "  'summary': 'The adoption of differential privacy is growing but the complexity of\\ndesigning private, efficient and accurate algorithms is still high. We propose\\na novel programming framework and system, Ektelo, for implementing both\\nexisting and new privacy algorithms. For the task of answering linear counting\\nqueries, we show that nearly all existing algorithms can be composed from\\noperators, each conforming to one of a small number of operator classes. While\\npast programming frameworks have helped to ensure the privacy of programs, the\\nnovelty of our framework is its significant support for authoring accurate and\\nefficient (as well as private) programs.\\n  After describing the design and architecture of the Ektelo system, we show\\nthat Ektelo is expressive, allows for safer implementations through code reuse,\\nand that it allows both privacy novices and experts to easily design\\nalgorithms. We demonstrate the use of Ektelo by designing several new\\nstate-of-the-art algorithms.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 10, 14, 18, 40, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Evaluating Datalog via Tree Automata and Cycluits',\n",
       "  'authors': ['Antoine Amarilli',\n",
       "   'Pierre Bourhis',\n",
       "   'Mikaël Monet',\n",
       "   'Pierre Senellart'],\n",
       "  'summary': 'We investigate parameterizations of both database instances and queries that\\nmake query evaluation fixed-parameter tractable in combined complexity. We show\\nthat clique-frontier-guarded Datalog with stratified negation (CFG-Datalog)\\nenjoys bilinear-time evaluation on structures of bounded treewidth for programs\\nof bounded rule size. Such programs capture in particular conjunctive queries\\nwith simplicial decompositions of bounded width, guarded negation fragment\\nqueries of bounded CQ-rank, or two-way regular path queries. Our result is\\nshown by translating to alternating two-way automata, whose semantics is\\ndefined via cyclic provenance circuits (cycluits) that can be tractably\\nevaluated.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 14, 12, 48, 15, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Blockchain Database Application Platform',\n",
       "  'authors': ['Muhammad Muzammal',\n",
       "   'Qiang Qu',\n",
       "   'Bulat Nasrulin',\n",
       "   'Anders Skovsgaard'],\n",
       "  'summary': 'A blockchain is a decentralised linked data structure that is characterised\\nby its inherent resistance to data modification, but it is deficient in search\\nqueries, primarily due to its inferior data formatting. A distributed database\\nis also a decentralised data structure which features quick query processing\\nand well-designed data formatting but suffers from data reliability. In this\\ndemonstration, we showcase a blockchain database application platform developed\\nby integrating the blockchain with the database, i.e. we demonstrate a system\\nthat has the decentralised, distributed and audibility features of the\\nblockchain and quick query processing and well-designed data structure of the\\ndistributed databases. The system features a tamper-resistant, consistent and\\ncost-effective multi-active database and an effective and reliable data-level\\ndisaster recovery backup. The system is demonstrated in practice as a\\nmulti-active database along with the data-level disaster recovery backup\\nfeature.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 15, 17, 44, 25, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Vis4DD: A visualization system that supports Data Quality Visual Assessment',\n",
       "  'authors': ['João Marcelo Borovina Josko', 'João Eduardo Ferreira'],\n",
       "  'summary': 'Data quality assessment process is essential to ensure reliable analytical\\noutcomes. This process depends on human supervision-driven approaches since it\\nis impossible to determine a defect based only on data. Visualization systems\\nbelong to a class of supervised tools that can make data defect pattern\\nvisible. However, their considerable design knowledge encodings and imple-\\nmentations provide little support design to data quality visual assessment. To\\ncover this gap, this work reports the design approach of V is4DD visualization\\nsystem based on patterns of data defects structures and assessment tasks. An\\nexploratory case study used this web-based system to explore which and how\\nvisual-interactive properties facilitate visual detection of data defect.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 15, 13, 47, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'PUG: A Framework and Practical Implementation for Why & Why-Not Provenance (extended version)',\n",
       "  'authors': ['Seokki Lee', 'Bertram Ludaescher', 'Boris Glavic'],\n",
       "  'summary': 'Explaining why an answer is (or is not) returned by a query is important for\\nmany applications including auditing, debugging data and queries, and answering\\nhypothetical questions about data. In this work, we present the first practical\\napproach for answering such questions for queries with negation (first- order\\nqueries). Specifically, we introduce a graph-based provenance model that, while\\nsyntactic in nature, supports reverse reasoning and is proven to encode a wide\\nrange of provenance models from the literature. The implementation of this\\nmodel in our PUG (Provenance Unification through Graphs) system takes a\\nprovenance question and Datalog query as an input and generates a Datalog\\nprogram that computes an explanation, i.e., the part of the provenance that is\\nrelevant to answer the question. Furthermore, we demonstrate how a desirable\\nfactorization of provenance can be achieved by rewriting an input query. We\\nexperimentally evaluate our approach demonstrating its efficiency.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 16, 2, 1, 11, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'FedMark: A Marketplace for Federated Data on the Web',\n",
       "  'authors': ['Tobias Grubenmann',\n",
       "   'Abraham Bernstein',\n",
       "   'Dmitry Moor',\n",
       "   'Sven Seuken'],\n",
       "  'summary': 'The Web of Data (WoD) has experienced a phenomenal growth in the past. This\\ngrowth is mainly fueled by tireless volunteers, government subsidies, and open\\ndata legislations. The majority of commercial data has not made the transition\\nto the WoD, yet. The problem is that it is not clear how publishers of\\ncommercial data can monetize their data in this new setting. Advertisement,\\nwhich is one of the main financial engines of the World Wide Web, cannot be\\napplied to the Web of Data as such unwanted data can easily be filtered out,\\nautomatically. This raises the question how the WoD can (i) maintain its grow\\nwhen subsidies disappear and (ii) give commercial data providers financial\\nincentives to share their wealth of data. In this paper, we propose a\\nmarketplace for the WoD as a solution for this data monetization problem. Our\\napproach allows a customer to transparently buy data from a combination of\\ndifferent providers. To that end, we introduce two different approaches for\\ndeciding which data elements to buy and compare their performance. We also\\nintroduce FedMark, a prototypical implementation of our marketplace that\\nrepresents a first step towards an economically viable WoD beyond subsidies.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 20, 3, 57, 16, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The variable quality of metadata about biological samples used in biomedical experiments',\n",
       "  'authors': ['Rafael S. Gonçalves', 'Mark A. Musen'],\n",
       "  'summary': 'We present an analytical study of the quality of metadata about samples used\\nin biomedical experiments. The metadata under analysis are stored in two\\nwell-known databases: BioSample---a repository managed by the National Center\\nfor Biotechnology Information (NCBI), and BioSamples---a repository managed by\\nthe European Bioinformatics Institute (EBI). We tested whether 11.4M sample\\nmetadata records in the two repositories are populated with values that fulfill\\nthe stated requirements for such values. Our study revealed multiple anomalies\\nin the metadata. Most metadata field names and their values are not\\nstandardized or controlled. Even simple binary or numeric fields are often\\npopulated with inadequate values of different data types. By clustering\\nmetadata field names, we discovered there are often many distinct ways to\\nrepresent the same aspect of a sample. Overall, the metadata we analyzed reveal\\nthat there is a lack of principled mechanisms to enforce and validate metadata\\nrequirements. The significant aberrancies that we found in the metadata are\\nlikely to impede search and secondary use of the associated datasets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 17, 18, 3, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Privacy-Preserving Synthetic Datasets Over Weakly Constrained Domains',\n",
       "  'authors': ['Luke Rodriguez', 'Bill Howe'],\n",
       "  'summary': 'Techniques to deliver privacy-preserving synthetic datasets take a sensitive\\ndataset as input and produce a similar dataset as output while maintaining\\ndifferential privacy. These approaches have the potential to improve data\\nsharing and reuse, but they must be accessible to non-experts and tolerant of\\nrealistic data. Existing approaches make an implicit assumption that the active\\ndomain of the dataset is similar to the global domain, potentially violating\\ndifferential privacy.\\n  In this paper, we present an algorithm for generating differentially private\\nsynthetic data over the large, weakly constrained domains we find in realistic\\nopen data situations. Our algorithm models the unrepresented domain\\nanalytically as a probability distribution to adjust the output and compute\\nnoise, avoiding the need to compute the full domain explicitly. We formulate\\nthe tradeoff between privacy and utility in terms of a \"tolerance for\\nrandomness\" parameter that does not require users to inspect the data to set.\\nFinally, we show that the algorithm produces sensible results on real datasets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 23, 1, 52, 45, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Database-Agnostic Workload Management',\n",
       "  'authors': ['Shrainik Jain', 'Jiaqi Yan', 'Thierry Cruane', 'Bill Howe'],\n",
       "  'summary': 'We present a system to support generalized SQL workload analysis and\\nmanagement for multi-tenant and multi-database platforms. Workload analysis\\napplications are becoming more sophisticated to support database\\nadministration, model user behavior, audit security, and route queries, but the\\nmethods rely on specialized feature engineering, and therefore must be\\ncarefully implemented and reimplemented for each SQL dialect, database system,\\nand application. Meanwhile, the size and complexity of workloads are increasing\\nas systems centralize in the cloud. We model workload analysis and management\\ntasks as variations on query labeling, and propose a system design that can\\nsupport general query labeling routines across multiple applications and\\ndatabase backends. The design relies on the use of learned vector embeddings\\nfor SQL queries as a replacement for application-specific syntactic features,\\nreducing custom code and allowing the use of off-the-shelf machine learning\\nalgorithms for labeling. The key hypothesis, for which we provide evidence in\\nthis paper, is that these learned features can outperform conventional feature\\nengineering on representative machine learning tasks. We present the design of\\na database-agnostic workload management and analytics service, describe\\npotential applications, and show that separating workload representation from\\nlabeling tasks affords new capabilities and can outperform existing solutions\\nfor representative tasks, including workload sampling for index recommendation\\nand user labeling for security audits.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 25, 5, 14, 31, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Rule Module Inheritance with Modification Restrictions',\n",
       "  'authors': ['Felix Burgstaller',\n",
       "   'Bernd Neumayr',\n",
       "   'Emanuel Sallinger',\n",
       "   'Michael Schrefl'],\n",
       "  'summary': 'Adapting rule sets to different settings, yet avoiding uncontrolled\\nproliferation of variations, is a key challenge of rule management. One\\nfundamental concept to foster reuse and simplify adaptation is inheritance.\\nBuilding on rule modules, i.e., rule sets with input and output schema, we\\nformally define inheritance of rule modules by incremental modification in\\nsingle inheritance hierarchies. To avoid uncontrolled proliferation of\\nmodifications, we introduce formal modification restrictions which flexibly\\nregulate the degree to which a child module may be modified in comparison to\\nits parent. As concrete rule language, we employ Datalog+/- which can be\\nregarded a common logical core of many rule languages. We evaluate the approach\\nby a proof-of-concept prototype.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 26, 22, 28, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Monotone Preservation Result for Boolean Queries Expressed as a Containment of Conjunctive Queries',\n",
       "  'authors': ['Dimitri Surinx', 'Jan Van den Bussche'],\n",
       "  'summary': 'When a relational database is queried, the result is normally a relation.\\nSome queries, however, only require a yes/no answer; such queries are often\\ncalled boolean queries. It is customary in database theory to express boolean\\nqueries by testing nonemptiness of query expressions. Another interesting way\\nfor expressing boolean queries are containment statements of the form $Q_1\\n\\\\subseteq Q_2$ where $Q_1$ and $Q_2$ are query expressions. Here, for any input\\ninstance $I$, the boolean query result is $\\\\mathit{true}$ if $Q_1(I)$ is a\\nsubset of $Q_2(I)$ and $\\\\mathit{false}$ otherwise.\\n  In the present paper we will focus on nonemptiness and containment statements\\nabout conjunctive queries. The main goal is to investigate the monotone\\nfragment of the containments of conjunctive queries. In particular, we show a\\npreservation like result for this monotone fragment. That is, we show that, in\\nexpressive power, the monotone containments of conjunctive queries are exactly\\nequal to conjunctive queries under nonemptiness.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 27, 12, 42, 38, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Efficient Data Ingestion and Query Processing for LSM-Based Storage Systems',\n",
       "  'authors': ['Chen Luo', 'Michael J. Carey'],\n",
       "  'summary': 'In recent years, the Log Structured Merge (LSM) tree has been widely adopted\\nby NoSQL and NewSQL systems for its superior write performance. Despite its\\npopularity, however, most existing work has focused on LSM-based key-value\\nstores with only a primary LSM-tree index; auxiliary structures, which are\\ncritical for supporting ad-hoc queries, have received much less attention. In\\nthis paper, we focus on efficient data ingestion and query processing for\\ngeneral-purpose LSM-based storage systems. We first propose and evaluate a\\nseries of optimizations for efficient batched point lookups, significantly\\nimproving the range of applicability of LSM-based secondary indexes. We then\\npresent several new and efficient maintenance strategies for LSM-based storage\\nsystems. Finally, we have implemented and experimentally evaluated the proposed\\ntechniques in the context of the Apache AsterixDB system, and we present the\\nresults here.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 27, 15, 56, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'NeuralCubes: Deep Representations for Visual Data Exploration',\n",
       "  'authors': ['Zhe Wang',\n",
       "   'Dylan Cashman',\n",
       "   'Mingwei Li',\n",
       "   'Jixian Li',\n",
       "   'Matthew Berger',\n",
       "   'Joshua A. Levine',\n",
       "   'Remco Chang',\n",
       "   'Carlos Scheidegger'],\n",
       "  'summary': 'Visual exploration of large multidimensional datasets has seen tremendous\\nprogress in recent years, allowing users to express rich data queries that\\nproduce informative visual summaries, all in real time. Techniques based on\\ndata cubes are some of the most promising approaches. However, these techniques\\nusually require a large memory footprint for large datasets. To tackle this\\nproblem, we present NeuralCubes: neural networks that predict results for\\naggregate queries, similar to data cubes. NeuralCubes learns a function that\\ntakes as input a given query, for instance, a geographic region and temporal\\ninterval, and outputs the result of the query. The learned function serves as a\\nreal-time, low-memory approximator for aggregation queries. NeuralCubes models\\nare small enough to be sent to the client side (e.g. the web browser for a\\nweb-based application) for evaluation, enabling data exploration of large\\ndatasets without database/network connection. We demonstrate the effectiveness\\nof NeuralCubes through extensive experiments on a variety of datasets and\\ndiscuss how NeuralCubes opens up opportunities for new types of visualization\\nand interaction.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 27, 18, 11, 16, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Creating a surrogate commuter network from Australian Bureau of Statistics census data',\n",
       "  'authors': ['Kristopher M. Fair', 'Cameron Zachreson', 'Mikhail Prokopenko'],\n",
       "  'summary': 'Between the 2011 and 2016 national censuses, the Australian Bureau of\\nStatistics changed its anonymity policy compliance system for the distribution\\nof census data. The new method has resulted in dramatic inconsistencies when\\ncomparing low-resolution data to aggregated high-resolution data. Hence,\\naggregated totals do not match true totals, and the mismatch gets worse as the\\ndata resolution gets finer. Here, we address several aspects of this\\ninconsistency with respect to the 2016 usual-residence to place-of-work travel\\ndata. We introduce a re-sampling system that rectifies many of the artifacts\\nintroduced by the new ABS protocol, ensuring a higher level of consistency\\nacross partition sizes. We offer a surrogate high-resolution 2016 commuter\\ndataset that reduces the difference between aggregated and true commuter totals\\nfrom ~34% to only ~7%, which is on the order of the discrepancy across\\npartition resolutions in data from earlier years.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 8, 27, 6, 2, 49, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Chasing Similarity: Distribution-aware Aggregation Scheduling (Extended Version)',\n",
       "  'authors': ['Feilong Liu',\n",
       "   'Ario Salmasi',\n",
       "   'Spyros Blanas',\n",
       "   'Anastasios Sidiropoulos'],\n",
       "  'summary': 'Parallel aggregation is a ubiquitous operation in data analytics that is\\nexpressed as GROUP BY in SQL, reduce in Hadoop, or segment in TensorFlow.\\nParallel aggregation starts with an optional local pre-aggregation step and\\nthen repartitions the intermediate result across the network. While local\\npre-aggregation works well for low-cardinality aggregations, the network\\ncommunication cost remains significant for high-cardinality aggregations even\\nafter local pre-aggregation. The problem is that the repartition-based\\nalgorithm for high-cardinality aggregation does not fully utilize the network.\\n  In this work, we first formulate a mathematical model that captures the\\nperformance of parallel aggregation. We prove that finding optimal aggregation\\nplans from a known data distribution is NP-hard, assuming the Small Set\\nExpansion conjecture. We propose GRASP, a GReedy Aggregation Scheduling\\nProtocol that decomposes parallel aggregation into phases. GRASP is\\ndistribution-aware as it aggregates the most similar partitions in each phase\\nto reduce the transmitted data size in subsequent phases. In addition, GRASP\\ntakes the available network bandwidth into account when scheduling aggregations\\nin each phase to maximize network utilization. The experimental evaluation on\\nreal data shows that GRASP outperforms repartition-based aggregation by 3.5x\\nand LOOM by 2.0x.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 1, 2, 58, 34, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Heterogeneous Replica for Query on Cassandra',\n",
       "  'authors': ['Jialin Qiao', 'Xiangdong Huang', 'Lei Rui', 'Jianmin Wang'],\n",
       "  'summary': 'Cassandra is a popular structured storage system with high-performance,\\nscalability and high availability, and is usually used to store data that has\\nsome sortable attributes. When deploying and configuring Cassandra, it is\\nimportant to design a suitable schema of column families for accelerating the\\ntarget queries. However, one schema is only suitable for a part of queries, and\\nleaves other queries with high latency.\\n  In this paper, we propose a new replica mechanism, called heterogeneous\\nreplica, to reduce the query latency greatly while ensuring high write\\nthroughput and data recovery. With this replica mechanism, different replica\\nhas the same dataset while having different serialization on disk. By\\nimplementing the heterogeneous replica mechanism on Cassandra, we show that the\\nread performance of Cassandra can be improved by two orders of magnitude with\\nTPC-H data set.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 2, 2, 31, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'VStore: A Data Store for Analytics on Large Videos',\n",
       "  'authors': ['Tiantu Xu', 'Luis Materon Botelho', 'Felix Xiaozhu Lin'],\n",
       "  'summary': 'We present VStore, a data store for supporting fast, resource-efficient\\nanalytics over large archival videos. VStore manages video ingestion, storage,\\nretrieval, and consumption. It controls video formats along the video data\\npath. It is challenged by i) the huge combinatorial space of video format\\nknobs; ii) the complex impacts of these knobs and their high profiling cost;\\niii) optimizing for multiple resource types. It explores an idea called\\nbackward derivation of configuration: in the opposite direction along the video\\ndata path, VStore passes the video quantity and quality expected by analytics\\nbackward to retrieval, to storage, and to ingestion. In this process, VStore\\nderives an optimal set of video formats, optimizing for different resources in\\na progressive manner. VStore automatically derives large, complex\\nconfigurations consisting of more than one hundred knobs over tens of video\\nformats. In response to queries, VStore selects video formats catering to the\\nexecuted operators and the target accuracy. It streams video data from disks\\nthrough decoder to operators. It runs queries as fast as 362x of video\\nrealtime.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 3, 15, 31, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Shrinkwrap: Differentially-Private Query Processing in Private Data Federations',\n",
       "  'authors': ['Johes Bater',\n",
       "   'Xi He',\n",
       "   'William Ehrich',\n",
       "   'Ashwin Machanavajjhala',\n",
       "   'Jennie Rogers'],\n",
       "  'summary': 'A private data federation is a set of autonomous databases that share a\\nunified query interface offering in-situ evaluation of SQL queries over the\\nunion of the sensitive data of its members. Owing to privacy concerns, these\\nsystems do not have a trusted data collector that can see all their data and\\ntheir member databases cannot learn about individual records of other engines.\\nFederations currently achieve this goal by evaluating queries obliviously using\\nsecure multiparty computation. This hides the intermediate result cardinality\\nof each query operator by exhaustively padding it. With cascades of such\\noperators, this padding accumulates to a blow-up in the output size of each\\noperator and a proportional loss in query performance. Hence, existing private\\ndata federations do not scale well to complex SQL queries over large datasets.\\n  We introduce Shrinkwrap, a private data federation that offers data owners a\\ndifferentially private view of the data held by others to improve their\\nperformance over oblivious query processing. Shrinkwrap uses computational\\ndifferential privacy to minimize the padding of intermediate query results,\\nachieving up to 35X performance improvement over oblivious query processing.\\nWhen the query needs differentially private output, Shrinkwrap provides a\\ntrade-off between result accuracy and query evaluation performance.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 3, 16, 9, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Split-Correctness in Information Extraction',\n",
       "  'authors': ['Johannes Doleschal',\n",
       "   'Benny Kimelfeld',\n",
       "   'Wim Martens',\n",
       "   'Frank Neven',\n",
       "   'Matthias Niewerth'],\n",
       "  'summary': 'Programs for extracting structured information from text, namely information\\nextractors, often operate separately on document segments obtained from a\\ngeneric splitting operation such as sentences, paragraphs, k-grams, HTTP\\nrequests, and so on. An automated detection of this behavior of extractors,\\nwhich we refer to as split-correctness, would allow text analysis systems to\\ndevise query plans with parallel evaluation on segments for accelerating the\\nprocessing of large documents. Other applications include the incremental\\nevaluation on dynamic content, where re-evaluation of information extractors\\ncan be restricted to revised segments, and debugging, where developers of\\ninformation extractors are informed about potential boundary crossing of\\ndifferent semantic components. We propose a new formal framework for\\nsplit-correctness within the formalism of document spanners. Our analysis\\nstudies the complexity of split-correctness over regular spanners. We also\\ndiscuss different variants of split-correctness, for instance, in the presence\\nof black-box extractors with split constraints.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 8, 11, 3, 40, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Consistent Query Answering for Primary Keys in Logspace',\n",
       "  'authors': ['Paraschos Koutris', 'Jef Wijsen'],\n",
       "  'summary': 'We study the complexity of consistent query answering on databases that may\\nviolate primary key constraints. A repair of such a database is any consistent\\ndatabase that can be obtained by deleting a minimal set of tuples. For every\\nBoolean query q, CERTAINTY(q) is the problem that takes a database as input and\\nasks whether q evaluates to true on every repair. In [KW17], the authors show\\nthat for every self-join-free Boolean conjunctive query q, the problem\\nCERTAINTY(q) is either in P or coNP-complete, and it is decidable which of the\\ntwo cases applies. In this paper, we sharpen this result by showing that for\\nevery self-join-free Boolean conjunctive query q, the problem CERTAINTY(q) is\\neither expressible in symmetric stratified Datalog or coNP-complete. Since\\nsymmetric stratified Datalog is in L, we thus obtain a complexity-theoretic\\ndichotomy between L and coNP-complete. Another new finding of practical\\nimportance is that CERTAINTY(q) is on the logspace side of the dichotomy for\\nqueries q where all join conditions express foreign-to-primary key matches,\\nwhich is undoubtedly the most common type of join condition.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 8, 11, 50, 20, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Probabilistic Blocking with An Application to the Syrian Conflict',\n",
       "  'authors': ['Rebecca C. Steorts', 'Anshumali Shrivastava'],\n",
       "  'summary': 'Entity resolution seeks to merge databases as to remove duplicate entries\\nwhere unique identifiers are typically unknown. We review modern blocking\\napproaches for entity resolution, focusing on those based upon locality\\nsensitive hashing (LSH). First, we introduce $k$-means locality sensitive\\nhashing (KLSH), which is based upon the information retrieval literature and\\nclusters similar records into blocks using a vector-space representation and\\nprojections. Second, we introduce a subquadratic variant of LSH to the\\nliterature, known as Densified One Permutation Hashing (DOPH). Third, we\\npropose a weighted variant of DOPH. We illustrate each method on an application\\nto a subset of the ongoing Syrian conflict, giving a discussion of each method.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 11, 1, 16, 31, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Characterization and extraction of condensed representation of correlated patterns based on formal concept analysis',\n",
       "  'authors': ['Souad Bouasker'],\n",
       "  'summary': 'Correlated pattern mining has increasingly become an important task in data\\nmining since these patterns allow conveying knowledge about meaningful and\\nsurprising relations among data. Frequent correlated patterns were thoroughly\\nstudied in the literature. In this thesis, we propose to benefit from both\\nfrequent correlated as well as rare correlated patterns according to the bond\\ncorrelation measure. We propose to extract a subset without information loss of\\nthe sets of frequent correlated and of rare correlated patterns, this subset is\\ncalled ``Condensed Representation``. In this regard, we are based on the\\nnotions derived from the Formal Concept Analysis FCA, specifically the\\nequivalence classes associated to a closure operator fbond dedicated to the\\nbond measure, to introduce new concise representations of both frequent\\ncorrelated and rare correlated patterns.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 12, 15, 16, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'DPASF: A Flink Library for Streaming Data preprocessing',\n",
       "  'authors': ['Alejandro Alcalde-Barros',\n",
       "   'Diego García-Gil',\n",
       "   'Salvador García',\n",
       "   'Francisco Herrera'],\n",
       "  'summary': 'Data preprocessing techniques are devoted to correct or alleviate errors in\\ndata. Discretization and feature selection are two of the most extended data\\npreprocessing techniques. Although we can find many proposals for static Big\\nData preprocessing, there is little research devoted to the continuous Big Data\\nproblem. Apache Flink is a recent and novel Big Data framework, following the\\nMapReduce paradigm, focused on distributed stream and batch data processing. In\\nthis paper we propose a data stream library for Big Data preprocessing, named\\nDPASF, under Apache Flink. We have implemented six of the most popular data\\npreprocessing algorithms, three for discretization and the rest for feature\\nselection. The algorithms have been tested using two Big Data datasets.\\nExperimental results show that preprocessing can not only reduce the size of\\nthe data, but to maintain or even improve the original accuracy in a short\\ntime. DPASF contains useful algorithms when dealing with Big Data data streams.\\nThe preprocessing algorithms included in the library are able to tackle Big\\nDatasets efficiently and to correct imperfections in the data.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 14, 11, 59, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Assessing and Remedying Coverage for a Given Dataset',\n",
       "  'authors': ['Abolfazl Asudeh', 'Zhongjun Jin', 'H. V. Jagadish'],\n",
       "  'summary': 'Data analysis impacts virtually every aspect of our society today. Often,\\nthis analysis is performed on an existing dataset, possibly collected through a\\nprocess that the data scientists had limited control over. The existing data\\nanalyzed may not include the complete universe, but it is expected to cover the\\ndiversity of items in the universe. Lack of adequate coverage in the dataset\\ncan result in undesirable outcomes such as biased decisions and algorithmic\\nracism, as well as creating vulnerabilities such as opening up room for\\nadversarial attacks.\\n  In this paper, we assess the coverage of a given dataset over multiple\\ncategorical attributes. We first provide efficient techniques for traversing\\nthe combinatorial explosion of value combinations to identify any regions of\\nattribute space not adequately covered by the data. Then, we determine the\\nleast amount of additional data that must be obtained to resolve this lack of\\nadequate coverage. We confirm the value of our proposal through both\\ntheoretical analyses and comprehensive experiments on real data.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 15, 22, 59, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Optimization of Indexing Based on k-Nearest Neighbor Graph for Proximity Search in High-dimensional Data',\n",
       "  'authors': ['Masajiro Iwasaki', 'Daisuke Miyazaki'],\n",
       "  'summary': 'Searching for high-dimensional vector data with high accuracy is an\\ninevitable search technology for various types of data. Graph-based indexes are\\nknown to reduce the query time for high-dimensional data. To further improve\\nthe query time by using graphs, we focused on the indegrees and outdegrees of\\ngraphs. While a sufficient number of incoming edges (indegrees) are\\nindispensable for increasing search accuracy, an excessive number of outgoing\\nedges (outdegrees) should be suppressed so as to not increase the query time.\\nTherefore, we propose three degree-adjustment methods: static degree adjustment\\nof not only outdegrees but also indegrees, dynamic degree adjustment with which\\noutdegrees are determined by the search accuracy users require, and path\\nadjustment to remove edges that have alternative search paths to reduce\\noutdegrees. We also show how to obtain optimal degree-adjustment parameters and\\nthat our methods outperformed previous methods for image and textual data.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 17, 2, 22, 34, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Finding Average Regret Ratio Minimizing Set in Database',\n",
       "  'authors': ['Sepanta Zeighami', 'Raymong Chi-Wing Wong'],\n",
       "  'summary': 'Selecting a certain number of data points (or records) from a database which\\n\"best\" satisfy users\\' expectations is a very prevalent problem with many\\napplications. One application is a hotel booking website showing a certain\\nnumber of hotels on a single page. However, this problem is very challenging\\nsince the selected points should \"collectively\" satisfy the expectation of all\\nusers. Showing a certain number of data points to a single user could decrease\\nthe satisfaction of a user because the user may not be able to see his/her\\nfavorite point which could be found in the original database. In this paper, we\\nwould like to find a set of k points such that on average, the satisfaction\\n(ratio) of a user is maximized. This problem takes into account the probability\\ndistribution of the users and considers the satisfaction (ratio) of all users,\\nwhich is more reasonable in practice, compared with the existing studies that\\nonly consider the worst-case satisfaction (ratio) of the users, which may not\\nreflect the whole population and is not useful in some applications. Motivated\\nby this, in this paper, we propose algorithms for this problem. Finally, we\\nconducted experiments to show the effectiveness and the efficiency of the\\nalgorithms.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 18, 13, 42, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Modeling and In-Database Management of Relational, Data-Aware Processes (Extended Version)',\n",
       "  'authors': ['Diego Calvanese',\n",
       "   'Marco Montali',\n",
       "   'Fabio Patrizi',\n",
       "   'Andrey Rivkin'],\n",
       "  'summary': 'During the last two decades, it has been increasingly acknowledged that the\\nengineering of information systems usually requires a huge effort in\\nintegrating master data and business processes. This has led to a plethora of\\nproposals, both from academia and the industry. However, such approaches\\ntypically come with ad-hoc abstractions to represent and interact with the data\\ncomponent. This has a twofold disadvantage. On the one hand, they cannot be\\nused to effortlessly enrich an existing relational database with dynamics. On\\nthe other hand, they generally do not allow for integrated modelling,\\nverification, and enactment. We attack these two challenges by proposing a\\ndeclarative approach, fully grounded in SQL, that supports the agile modelling\\nof relational data-aware processes directly on top of relational databases. We\\nshow how this approach can be automatically translated into a concrete\\nprocedural SQL dialect, executable directly inside any relational database\\nengine. The translation exploits an in-database representation of process\\nstates that, in turn, is used to handle, at once, process enactment with or\\nwithout logging of the executed instances, as well as process verification. The\\napproach has been implemented in a working prototype.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 18, 13, 57, 49, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Property Graph Type System and Data Definition Language',\n",
       "  'authors': ['Mingxi Wu'],\n",
       "  'summary': 'Property graph manages data by vertices and edges. Each vertex and edge can\\nhave a property map, storing ad hoc attribute and its value. Label can be\\nattached to vertices and edges to group them. While this schema-less\\nmethodology is very flexible for data evolvement and for managing explosive\\ngraph element, it has two shortcomings-- 1) data dependency 2) less\\ncompression. Both problems can be solved by a schema based approach. In this\\npaper, a type system used to model property graph is defined. Based on the type\\nsystem, the associated data definition language (DDL) is proposed and multiple\\ngraph instances created under this type system is discussed.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 20, 5, 58, 48, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'MinJoin: Efficient Edit Similarity Joins via Local Hash Minima',\n",
       "  'authors': ['Haoyu Zhang', 'Qin Zhang'],\n",
       "  'summary': 'We study the problem of computing similarity joins under edit distance on a\\nset of strings. Edit similarity joins is a fundamental problem in databases,\\ndata mining and bioinformatics. It finds important applications in data\\ncleaning and integration, collaborative filtering, genome sequence assembly,\\netc. This problem has attracted significant attention in the past two decades.\\nHowever, all previous algorithms either cannot scale well to long strings and\\nlarge similarity thresholds, or suffer from imperfect accuracy.\\n  In this paper we propose a new algorithm for edit similarity joins using a\\nnovel string partition based approach. We show mathematically that with high\\nprobability our algorithm achieves a perfect accuracy, and runs in linear time\\nplus a data-dependent verification step. Experiments on real world datasets\\nshow that our algorithm significantly outperforms the state-of-the-art\\nalgorithms for edit similarity joins, and achieves perfect accuracy on all the\\ndatasets that we have tested.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 20, 17, 54, 37, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'biggy: An Implementation of Unified Framework for Big Data Management System',\n",
       "  'authors': ['Yao Wu', 'Henan Guan'],\n",
       "  'summary': 'Various tools, softwares and systems are proposed and implemented to tackle\\nthe challenges in big data on different emphases, e.g., data analysis, data\\ntransaction, data query, data storage, data visualization, data privacy. In\\nthis paper, we propose datar, a new prospective and unified framework for Big\\nData Management System (BDMS) from the point of system architecture by\\nleveraging ideas from mainstream computer structure. We introduce five key\\ncomponents of datar by reviewing the cur- rent status of BDMS. Datar features\\nwith configuration chain of pluggable engines, automatic dataflow on job\\npipelines, intelligent self-driving system management and interactive user\\ninterfaces. Moreover, we present biggy as an implementation of datar with\\nmanipulation details demonstrated by four running examples. Evaluations on\\nefficiency and scalability are carried out to show the performance. Our work\\nargues that the envisioned datar is a feasible solution to the unified\\nframework of BDMS, which can manage big data pluggablly, automatically and\\nintelligently with specific functionalities, where specific functionalities\\nrefer to input, storage, computation, control and output of big data.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 22, 15, 53, 22, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards a context-dependent numerical data quality evaluation framework',\n",
       "  'authors': ['Milen S. Marev',\n",
       "   'Ernesto Compatangelo',\n",
       "   'Wamberto Vasconcelos'],\n",
       "  'summary': 'This paper focuses on numeric data, with emphasis on distinct characteristics\\nlike varying significance, unstructured format, mass volume and real-time\\nprocessing. We propose a novel, context-dependent valuation framework\\nspecifically devised to assess quality in numeric datasets. Our framework uses\\neight relevant data quality dimensions, and provide a simple metric to evaluate\\ndataset quality along each dimension. We argue that the proposed set of\\ndimensions and corresponding metrics adequately captures the unique quality\\nantipatterns that are typically associated with numerical data. The\\nintroduction of our framework is part of a wider research effort that aims at\\ndeveloping an articulated numerical data quality improvement approach for Oil\\nand Gas exploration and production workflows that is based on artificial\\nintelligence techniques.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 22, 16, 47, 46, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Selection of BJI configuration: Approach based on minimal transversals',\n",
       "  'authors': ['Issam Ghabry'],\n",
       "  'summary': 'Decision systems deal with a large volume of data stored in new databases\\ncalled data warehouses. Data warehouses are typically modeled by a star schema\\nthat conventionally presents a central fact table and a set of dimension\\ntables. The corresponding queries for this type of model are therefore very\\ncomplex. In order to reduce the cost of executing complex queries, which\\ncontain very expensive joins, the solution envisaged would be to guarantee a\\ngood physical design of the data warehouses. Binary join indexes are very\\nsuitable to reduce the cost of executing these joins. In this work, we proposed\\na binary join index selection approach based on the notion of minimal\\ntransversal. The final configuration obtained is composed of several indexes,\\nwhich make it possible to optimize the execution cost of the query set.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 22, 20, 4, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'VDMS: Efficient Big-Visual-Data Access for Machine Learning Workloads',\n",
       "  'authors': ['Luis Remis',\n",
       "   'Vishakha Gupta-Cledat',\n",
       "   'Christina Strong',\n",
       "   'Ragaad Altarawneh'],\n",
       "  'summary': 'We introduce the Visual Data Management System (VDMS), which enables faster\\naccess to big-visual-data and adds support to visual analytics. This is\\nachieved by searching for relevant visual data via metadata stored as a graph,\\nand enabling faster access to visual data through new machine-friendly storage\\nformats. VDMS differs from existing large scale photo serving, video streaming,\\nand textual big-data management systems due to its primary focus on supporting\\nmachine learning and data analytics pipelines that use visual data (images,\\nvideos, and feature vectors), treating these as first class entities. We\\ndescribe how to use VDMS via its user friendly interface and how it enables\\nrich and efficient vision analytics through a machine learning pipeline for\\nprocessing medical images. We show the improved performance of 2x in complex\\nqueries over a comparable set-up.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 28, 16, 41, 22, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Studio e confronto delle strutture di Apache Spark',\n",
       "  'authors': ['Massimiliano Morrelli'],\n",
       "  'summary': \"English. This document is designed to study the data structures that can be\\nused in the Apache Spark framework and to evaluate the best performing ones to\\nimplement solutions, in particular we will evaluate advantages / disadvantages\\nderiving from the use of Dataset for job creation. The observation of the\\nresults provides further support in evaluating the use of Dataset as an\\nalternative to RDD, in order to understand its strengths and weaknesses. The\\nexamination of the results is possible thanks to specifically designed and\\nimplemented in Java 1.8 language. The execution of the jobs, entrusted to a\\nsuitable distributed environment, will end with the comparison between\\nexecution times and results obtained.\\n  Italiano. Il presente documento nasce allo scopo di studiare le strutture\\ndati utilizzabili nel framework Apache Spark e valutare quelle pi\\\\`u\\nperformanti per implementare soluzioni; valuteremo in articolare i vantaggi /\\nsvantaggi derivanti dall'utilizzo dei Dataset nella progettazione dei job.\\nL'osservazione dei risultati fornisce ulteriore supporto nel valutare\\nl'utilizzo dei Dataset in alternativa a RDD, al fine di comprederne i punti di\\nforza e di debolezza. L'esame dei risultati \\\\`e possibile in virt\\\\`u di due\\ncasi appositamente pensati e implementati in linguaggio Java 1.8. L'esecuzione\\ndei job, affidata a un adeguato ambiente distribuito, si concluder\\\\`a con il\\nconfronto tra tempi di esecuzione e risultati ottenuti.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 10, 29, 11, 26, 17, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Plan-Structured Deep Neural Network Models for Query Performance Prediction',\n",
       "  'authors': ['Ryan Marcus', 'Olga Papaemmanouil'],\n",
       "  'summary': 'Query performance prediction, the task of predicting the latency of a query,\\nis one of the most challenging problem in database management systems. Existing\\napproaches rely on features and performance models engineered by human experts,\\nbut often fail to capture the complex interactions between query operators and\\ninput relations, and generally do not adapt naturally to workload\\ncharacteristics and patterns in query execution plans. In this paper, we argue\\nthat deep learning can be applied to the query performance prediction problem,\\nand we introduce a novel neural network architecture for the task: a\\nplan-structured neural network. Our approach eliminates the need for\\nhuman-crafted feature selection and automatically discovers complex performance\\nmodels both at the operator and query plan level. Our novel neural network\\narchitecture can match the structure of any optimizer-selected query execution\\nplan and predict its latency with high accuracy. We also propose a number of\\noptimizations that reduce training overhead without sacrificing effectiveness.\\nWe evaluated our techniques on various workloads and we demonstrate that our\\nplan-structured neural network can outperform the state-of-the-art in query\\nperformance prediction.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 31, 23, 43, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Incremental Techniques for Large-Scale Dynamic Query Processing',\n",
       "  'authors': ['Iman Elghandour',\n",
       "   'Ahmet Kara',\n",
       "   'Dan Olteanu',\n",
       "   'Stijn Vansummeren'],\n",
       "  'summary': 'Many applications from various disciplines are now required to analyze fast\\nevolving big data in real time. Various approaches for incremental processing\\nof queries have been proposed over the years. Traditional approaches rely on\\nupdating the results of a query when updates are streamed rather than\\nre-computing these queries, and therefore, higher execution performance is\\nexpected. However, they do not perform well for large databases that are\\nupdated at high frequencies. Therefore, new algorithms and approaches have been\\nproposed in the literature to address these challenges by, for instance,\\nreducing the complexity of processing updates. Moreover, many of these\\nalgorithms are now leveraging distributed streaming platforms such as Spark\\nStreaming and Flink. In this tutorial, we briefly discuss legacy approaches for\\nincremental query processing, and then give an overview of the new challenges\\nintroduced due to processing big data streams. We then discuss in detail the\\nrecently proposed algorithms that address some of these challenges. We\\nemphasize the characteristics and algorithmic analysis of various proposed\\napproaches and conclude by discussing future research directions.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 1, 23, 10, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Transparent Concurrency Control: Decoupling Concurrency Control from DBMS',\n",
       "  'authors': ['Ningnan Zhou', 'Xuan Zhou', 'Kian-lee Tan', 'Shan Wang'],\n",
       "  'summary': 'For performance reasons, conventional DBMSes adopt monolithic architectures.\\nA monolithic design cripples the adaptability of a DBMS, making it difficult to\\ncustomize, to meet particular requirements of different applications. In this\\npaper, we propose to completely separate the code of concurrency control (CC)\\nfrom a monolithic DBMS. This allows us to add / remove functionalities or data\\nstructures to / from a DBMS easily, without concerning the issues of data\\nconsistency. As the separation deprives the concurrency controller of the\\nknowledge about data organization and processing, it may incur severe\\nperformance issues. To minimize the performance loss, we devised a two-level CC\\nmechanism. At the operational level, we propose a robust scheduler that\\nguarantees to complete any data operation at a manageable cost. At the\\ntransactional level, the scheduler can utilize data semantics to achieve\\nenhanced performance. Extensive experiments were conducted to demonstrate the\\nfeasibility and effectiveness of our approach.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 2, 1, 18, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Question Answering System Using Graph-Pattern Association Rules (QAGPAR) On YAGO Knowledge Base',\n",
       "  'authors': ['Wahyudi',\n",
       "   'Masayu Leylia Khodra',\n",
       "   'Ary Setijadi Prihatmanto',\n",
       "   'Carmadi Machbub'],\n",
       "  'summary': 'A question answering system (QA System) was developed that uses graph-pattern\\nassociation rules on the YAGO knowledge base. The answer as output of the\\nsystem is provided based on a user question as input. If the answer is missing\\nor unavailable in the database, then graph-pattern association rules are used\\nto get the answer. The architecture of this question answering system is as\\nfollows: question classification, graph component generation, query generation,\\nand query processing. The question answering system uses association graph\\npatterns in a waterfall model. In this paper, the architecture of the system is\\ndescribed, specifically discussing its reasoning and performance capabilities.\\nThe results of this research is that rules with high confidence and correct\\nlogic produce correct answers, and vice versa',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 2, 2, 24, 8, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Automating Software Citation using GitCite',\n",
       "  'authors': ['Leshang Chen', 'Susan Davidson'],\n",
       "  'summary': 'The ability to cite software and give credit to its authors and contributors\\nis increasingly important. While the number of online open-source software\\nrepositories has grown rapidly over the past few years, few are being properly\\ncited when used due to the difficulty of creating appropriate citations and the\\nlack of automated techniques. This paper presents GitCite, a model for software\\ncitation with version control which enables citations to be inferred for any\\nproject component based on a small number of explicit citations attached to\\nsubdirectories/files, and an implementation that integrates with Git and\\nGitHub. The implementation includes a browser extension and a local executable\\ntool, which enable citations to be added/modified/deleted to software project\\nrepositories and managed through functions such as fork/merge/copy.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 3, 18, 43, 25, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Close-reading of Linked Data: a case study in regards to the quality of online authority files',\n",
       "  'authors': ['Ettore Rizza', 'Anne Chardonnens', 'Seth van Hooland'],\n",
       "  'summary': \"More and more cultural institutions use Linked Data principles to share and\\nconnect their collection metadata. In the archival field, initiatives emerge to\\nexploit data contained in archival descriptions and adapt encoding standards to\\nthe semantic web. In this context, online authority files can be used to enrich\\nmetadata. However, relying on a decentralized network of knowledge bases such\\nas Wikidata, DBpedia or even Viaf has its own difficulties. This paper aims to\\noffer a critical view of these linked authority files by adopting a\\nclose-reading approach. Through a practical case study, we intend to identify\\nand illustrate the possibilities and limits of RDF triples compared to\\ninstitutions' less structured metadata.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 6, 12, 33, 7, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Ranked Enumeration of Conjunctive Query Results',\n",
       "  'authors': ['Shaleen Deep', 'Paraschos Koutris'],\n",
       "  'summary': 'We investigate the enumeration of top-k answers for conjunctive queries\\nagainst relational databases according to a given ranking function. The task is\\nto design data structures and algorithms that allow for efficient enumeration\\nafter a preprocessing phase. Our main contribution is a novel priority queue\\nbased algorithm with near-optimal delay and non-trivial space guarantees that\\nare output sensitive and depend on structure of the query. In particular, we\\nexploit certain desirable properties of ranking functions that frequently occur\\nin practice and degree information in the database instance, allowing for\\nefficient enumeration. We introduce the notion of {\\\\em decomposable} and {\\\\em\\ncompatible} ranking functions in conjunction with query decomposition, a\\nproperty that allows for partial aggregation of tuple scores in order to\\nefficiently enumerate the ranked output. We complement the algorithmic results\\nwith lower bounds justifying why certain assumptions about properties of\\nranking functions are necessary and discuss popular conjectures providing\\nevidence for optimality of enumeration delay guarantees. Our results extend and\\nimprove upon a long line of work that has studied ranked enumeration from both\\ntheoretical and practical perspective.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 7, 15, 46, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Efficient Power Theft Detection for Residential Consumers Using Mean Shift Data Mining Knowledge Discovery Process',\n",
       "  'authors': ['Konstantinos Blazakis', 'Georgios Stavrakakis'],\n",
       "  'summary': 'Energy theft constitutes an issue of great importance for electricity\\noperators. The attempt to detect and reduce non-technical losses is a\\nchallenging task due to insufficient inspection methods. With the evolution of\\nadvanced metering infrastructure (AMI) in smart grids, a more complicated\\nstatus quo in energy theft has emerged and many new technologies are being\\nadopted to solve the problem. In order to identify illegal residential\\nconsumers, a computational method of analyzing and identifying electricity\\nconsumption patterns of consumers based on data mining techniques has been\\npresented. Combining principal component analysis (PCA) with mean shift\\nalgorithm for different power theft scenarios, we can now cope with the power\\ntheft detection problem sufficiently. The overall research has shown\\nencouraging results in residential consumers power theft detection that will\\nhelp utilities to improve the reliability, security and operation of power\\nnetwork.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 5, 21, 45, 45, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Probably the Best Itemsets',\n",
       "  'authors': ['Nikolaj Tatti'],\n",
       "  'summary': \"One of the main current challenges in itemset mining is to discover a small\\nset of high-quality itemsets. In this paper we propose a new and general\\napproach for measuring the quality of itemsets. The method is solidly founded\\nin Bayesian statistics and decreases monotonically, allowing for efficient\\ndiscovery of all interesting itemsets. The measure is defined by connecting\\nstatistical models and collections of itemsets. This allows us to score\\nindividual itemsets with the probability of them occuring in random models\\nbuilt on the data.\\n  As a concrete example of this framework we use exponential models. This class\\nof models possesses many desirable properties. Most importantly, Occam's razor\\nin Bayesian model selection provides a defence for the pattern explosion. As\\ngeneral exponential models are infeasible in practice, we use decomposable\\nmodels; a large sub-class for which the measure is solvable. For the actual\\ncomputation of the score we sample models from the posterior distribution using\\nan MCMC approach.\\n  Experimentation on our method demonstrates the measure works in practice and\\nresults in interpretable and insightful itemsets for both synthetic and\\nreal-world data.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 7, 18, 51, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Antidote SQL: Relaxed When Possible, Strict When Necessary',\n",
       "  'authors': ['Pedro Lopes',\n",
       "   'João Sousa',\n",
       "   'Valter Balegas',\n",
       "   'Carla Ferreira',\n",
       "   'Ségio Duarte',\n",
       "   'Annette Bieniusa',\n",
       "   'Rodrigo Rodrigues',\n",
       "   'Nuno Preguiça'],\n",
       "  'summary': 'Geo-replication poses an inherent trade-off between low latency, high\\navailability and strong consistency. While NoSQL databases favor low latency\\nand high availability, relaxing consistency, more recent cloud databases favor\\nstrong consistency and ease of programming, while still providing high\\nscalability. In this paper, we present Antidote SQL, a database system that\\nallows application developers to relax SQL consistency when possible. Unlike\\nNoSQL databases, our approach enforces primary key, foreign key and check SQL\\nconstraints even under relaxed consistency, which is sufficient for\\nguaranteeing the correctness of many applications. To this end, we defined\\nconcurrency semantics for SQL constraints under relaxed consistency and show\\nhow to implement such semantics efficiently. For applications that require\\nstrict SQL consistency, Antidote SQL provides support for such semantics at the\\ncost of requiring coordination among replicas.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 10, 11, 25, 32, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Blockchain Framework for Managing and Monitoring Data in Multi-Site Clinical Trials',\n",
       "  'authors': ['Olivia Choudhury', 'Noor Fairoza', 'Issa Sylla', 'Amar Das'],\n",
       "  'summary': 'The cost of conducting multi-site clinical trials has significantly increased\\nover time, with site monitoring, data management, and amendments being key\\ndrivers. Clinical trial data management approaches typically rely on a central\\ndatabase, and require manual efforts to encode and maintain data capture and\\nreporting requirements. To reduce the administrative burden, time, and effort\\nof ensuring data integrity and privacy in multi-site trials, we propose a novel\\ndata management framework based on permissioned blockchain technology. We\\ndemonstrate how our framework, which uses smart contracts and private channels,\\nenables confidential data communication, protocol enforcement, and and an\\nautomated audit trail. We compare this framework with the traditional data\\nmanagement approach and evaluate its effectiveness in satisfying the major\\nrequirements of multi-site clinical trials. We show that our framework ensures\\nenforcement of IRB-related regulatory requirements across multiple sites and\\nstakeholders.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 11, 16, 34, 44, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Generalized Lineage-Aware Temporal Windows: Supporting Outer and Anti Joins in Temporal-Probabilistic Databases',\n",
       "  'authors': ['Katerina Papaioannou', 'Martin Theobald', 'Michael Böhlen'],\n",
       "  'summary': 'The result of a temporal-probabilistic (TP) join with negation includes, at\\neach time point, the probability with which a tuple of a positive relation\\n${\\\\bf p}$ matches none of the tuples in a negative relation ${\\\\bf n}$, for a\\ngiven join condition $\\\\theta$. TP outer and anti joins thus resemble the\\ncharacteristics of relational outer and anti joins also in the case when there\\nexist time points at which input tuples from ${\\\\bf p}$ have non-zero\\nprobabilities to be $true$ and input tuples from ${\\\\bf n}$ have non-zero\\nprobabilities to be $false$, respectively. For the computation of TP joins with\\nnegation, we introduce generalized lineage-aware temporal windows, a mechanism\\nthat binds an output interval to the lineages of all the matching valid tuples\\nof each input relation. We group the windows of two TP relations into three\\ndisjoint sets based on the way attributes, lineage expressions and intervals\\nare produced. We compute all windows in an incremental manner, and we show that\\npipelined computations allow for the direct integration of our approach into\\nPostgreSQL. We thereby alleviate the prevalent redundancies in the interval\\ncomputations of existing approaches, which is proven by an extensive\\nexperimental evaluation with real-world datasets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 12, 13, 38, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'SaGe: Web Preemption for Public SPARQL Query Services',\n",
       "  'authors': ['Thomas Minier', 'Hala Skaf-Molli', 'Pascal Molli'],\n",
       "  'summary': 'To provide stable and responsive public SPARQL query services, data providers\\nenforce quotas on server usage. Queries which exceed these quotas are\\ninterrupted and deliver partial results. Such interruption is not an issue if\\nit is possible to resume queries execution afterward. Unfortunately, there is\\nno preemption model for the Web that allows for suspending and resuming SPARQL\\nqueries. In this paper, we propose SaGe: a SPARQL query engine based on Web\\npreemption. SaGe allows SPARQL queries to be suspended by the Web server after\\na fixed time quantum and resumed upon client request. Web preemption is\\ntractable only if its cost in time is negligible compared to the time quantum.\\nThe challenge is to support the full SPARQL query language while keeping the\\ncost of preemption negligible. Experimental results demonstrate that SaGe\\noutperforms existing SPARQL query processing approaches by several orders of\\nmagnitude in term of the average total query execution time and the time for\\nfirst results.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 13, 8, 53, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'GrapAL: Connecting the Dots in Scientific Literature',\n",
       "  'authors': ['Christine Betts', 'Joanna Power', 'Waleed Ammar'],\n",
       "  'summary': 'We introduce GrapAL (Graph database of Academic Literature), a versatile tool\\nfor exploring and investigating a knowledge base of scientific literature, that\\nwas semi-automatically constructed using NLP methods. GrapAL satisfies a\\nvariety of use cases and information needs requested by researchers. At the\\ncore of GrapAL is a Neo4j graph database with an intuitive schema and a simple\\nquery language. In this paper, we describe the basic elements of GrapAL, how to\\nuse it, and several use cases such as finding experts on a given topic for peer\\nreviewing, discovering indirect connections between biomedical entities and\\ncomputing citation-based metrics. We open source the demo code to help other\\nresearchers develop applications that build on GrapAL.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 14, 0, 7, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Schema Validation and Evolution for Graph Databases',\n",
       "  'authors': ['Angela Bonifati',\n",
       "   'Peter Furniss',\n",
       "   'Alastair Green',\n",
       "   'Russ Harmer',\n",
       "   'Eugenia Oshurko',\n",
       "   'Hannes Voigt'],\n",
       "  'summary': 'Despite the maturity of commercial graph databases, little consensus has been\\nreached so far on the standardization of data definition languages (DDLs) for\\nproperty graphs (PG). The discussion on the characteristics of PG schemas is\\nongoing in many standardization and community groups. Although some basic\\naspects of a schema are already present in Neo4j 3.5, like in most commercial\\ngraph databases, full support is missing allowing to constraint property graphs\\nwith more or less flexibility. In this paper, we focus on two different\\nperspectives from which a PG schema should be considered, as being descriptive\\nor prescriptive, and we show how it would be possible to switch from one to\\nanother as the application under development gains more stability. Apart from\\nproposing concise schema DDL inspired by Cypher syntax, we show how schema\\nvalidation can be enforced through homomorphisms between PG schemas and PG\\ninstances; and how schema evolution can be described through the use of graph\\nrewriting operations. Our prototypical implementation demonstrates feasibility\\nand shows the need of offering high-level query primitives to accommodate\\nflexible graph schema requirements as showcased in our work.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 18, 7, 17, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards Semantic Big Graph Analytics for Cross-Domain Knowledge Discovery',\n",
       "  'authors': ['Feichen Shen'],\n",
       "  'summary': \"In recent years, the size of big linked data has grown rapidly and this\\nnumber is still rising. Big linked data and knowledge bases come from different\\ndomains such as life sciences, publications, media, social web, and so on.\\nHowever, with the rapid increasing of data, it is very challenging for people\\nto acquire a comprehensive collection of cross domain knowledge to meet their\\nneeds. Under this circumstance, it is extremely difficult for people without\\nexpertise to extract knowledge from various domains. Therefore, nowadays human\\nlimited knowledge can't feed the high requirement for discovering large amount\\nof cross domain knowledge. In this research, we present a big graph analytics\\nframework aims at addressing this issue by providing semantic methods to\\nfacilitate the management of big graph data from close domains in order to\\ndiscover cross domain knowledge in a more accurate and efficient way.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 20, 18, 23, 37, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An IDEA: An Ingestion Framework for Data Enrichment in AsterixDB',\n",
       "  'authors': ['Xikui Wang', 'Michael J. Carey'],\n",
       "  'summary': 'Big Data today is being generated at an unprecedented rate from various\\nsources such as sensors, applications, and devices, and it often needs to be\\nenriched based on other reference information to support complex analytical\\nqueries. Depending on the use case, the enrichment operations can be compiled\\ncode, declarative queries, or machine learning models with different\\ncomplexities. For enrichments that will be frequently used in the future, it\\ncan be advantageous to push their computation into the ingestion pipeline so\\nthat they can be stored (and queried) together with the data. In some cases,\\nthe referenced information may change over time, so the ingestion pipeline\\nshould be able to adapt to such changes to guarantee the currency and/or\\ncorrectness of the enrichment results.\\n  In this paper, we present a new data ingestion framework that supports data\\ningestion at scale, enrichments requiring complex operations, and adaptiveness\\nto reference data changes. We explain how this framework has been built on top\\nof Apache AsterixDB and investigate its performance at scale under various\\nworkloads.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 21, 21, 23, 5, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Capuchin: Causal Database Repair for Algorithmic Fairness',\n",
       "  'authors': ['Babak Salimi', 'Luke Rodriguez', 'Bill Howe', 'Dan Suciu'],\n",
       "  'summary': \"Fairness is increasingly recognized as a critical component of machine\\nlearning systems. However, it is the underlying data on which these systems are\\ntrained that often reflect discrimination, suggesting a database repair\\nproblem. Existing treatments of fairness rely on statistical correlations that\\ncan be fooled by statistical anomalies, such as Simpson's paradox. Proposals\\nfor causality-based definitions of fairness can correctly model some of these\\nsituations, but they require specification of the underlying causal models. In\\nthis paper, we formalize the situation as a database repair problem, proving\\nsufficient conditions for fair classifiers in terms of admissible variables as\\nopposed to a complete causal model. We show that these conditions correctly\\ncapture subtle fairness violations. We then use these conditions as the basis\\nfor database repair algorithms that provide provable fairness guarantees about\\nclassifiers trained on their training labels. We evaluate our algorithms on\\nreal data, demonstrating improvement over the state of the art on multiple\\nfairness metrics proposed in the literature while retaining high utility.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 21, 22, 13, 29, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'How I Learned to Stop Worrying and Love Re-optimization',\n",
       "  'authors': ['Matthew Perron',\n",
       "   'Zeyuan Shang',\n",
       "   'Tim Kraska',\n",
       "   'Michael Stonebraker'],\n",
       "  'summary': 'Cost-based query optimizers remain one of the most important components of\\ndatabase management systems for analytic workloads. Though modern optimizers\\nselect plans close to optimal performance in the common case, a small number of\\nqueries are an order of magnitude slower than they could be. In this paper we\\ninvestigate why this is still the case, despite decades of improvements to cost\\nmodels, plan enumeration, and cardinality estimation. We demonstrate why we\\nbelieve that a re-optimization mechanism is likely the most cost-effective way\\nto improve end-to-end query performance. We find that even a simple\\nre-optimization scheme can improve the latency of many poorly performing\\nqueries. We demonstrate that re-optimization improves the end-to-end latency of\\nthe top 20 longest running queries in the Join Order Benchmark by 27%,\\nrealizing most of the benefit of perfect cardinality estimation.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 21, 22, 33, 52, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Parsing Gigabytes of JSON per Second',\n",
       "  'authors': ['Geoff Langdale', 'Daniel Lemire'],\n",
       "  'summary': 'JavaScript Object Notation or JSON is a ubiquitous data exchange format on\\nthe Web. Ingesting JSON documents can become a performance bottleneck due to\\nthe sheer volume of data. We are thus motivated to make JSON parsing as fast as\\npossible.\\n  Despite the maturity of the problem of JSON parsing, we show that substantial\\nspeedups are possible. We present the first standard-compliant JSON parser to\\nprocess gigabytes of data per second on a single core, using commodity\\nprocessors. We can use a quarter or fewer instructions than a state-of-the-art\\nreference parser like RapidJSON. Unlike other validating parsers, our software\\n(simdjson) makes extensive use of Single Instruction, Multiple Data (SIMD)\\ninstructions. To ensure reproducibility, simdjson is freely available as\\nopen-source software under a liberal license.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 22, 0, 24, 1, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Utility-driven Data Analytics on Uncertain Data',\n",
       "  'authors': ['Wensheng Gan',\n",
       "   'Jerry Chun-Wei Lin',\n",
       "   'Han-Chieh Chao',\n",
       "   'Athanasios V. Vasilakos',\n",
       "   'Philip S. Yu'],\n",
       "  'summary': 'Modern Internet of Things (IoT) applications generate massive amounts of\\ndata, much of it in the form of objects/items of readings, events, and log\\nentries. Specifically, most of the objects in these IoT data contain rich\\nembedded information (e.g., frequency and uncertainty) and different level of\\nimportance (e.g., unit utility of items, interestingness, cost, risk, or\\nweight). Many existing approaches in data mining and analytics have limitations\\nsuch as only the binary attribute is considered within a transaction, as well\\nas all the objects/items having equal weights or importance. To solve these\\ndrawbacks, a novel utility-driven data analytics algorithm named HUPNU is\\npresented, to extract High-Utility patterns by considering both Positive and\\nNegative unit utilities from Uncertain data. The qualified high-utility\\npatterns can be effectively discovered for risk prediction, manufacturing\\nmanagement, decision-making, among others. By using the developed vertical\\nProbability-Utility list with the Positive-and-Negative utilities structure, as\\nwell as several effective pruning strategies. Experiments showed that the\\ndeveloped HUPNU approach performed great in mining the qualified patterns\\nefficiently and effectively.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 25, 19, 43, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Detecting Data Errors with Statistical Constraints',\n",
       "  'authors': ['Jing Nathan Yan',\n",
       "   'Oliver Schulte',\n",
       "   'Jiannan Wang',\n",
       "   'Reynold Cheng'],\n",
       "  'summary': \"A powerful approach to detecting erroneous data is to check which potentially\\ndirty data records are incompatible with a user's domain knowledge. Previous\\napproaches allow the user to specify domain knowledge in the form of logical\\nconstraints (e.g., functional dependency and denial constraints). We extend the\\nconstraint-based approach by introducing a novel class of statistical\\nconstraints (SCs). An SC treats each column as a random variable, and enforces\\nan independence or dependence relationship between two (or a few) random\\nvariables. Statistical constraints are expressive, allowing the user to specify\\na wide range of domain knowledge, beyond traditional integrity constraints.\\nFurthermore, they work harmoniously with downstream statistical modeling. We\\ndevelop CODED, an SC-Oriented Data Error Detection system that supports three\\nkey tasks: (1) Checking whether an SC is violated or not on a given dataset,\\n(2) Identify the top-k records that contribute the most to the violation of an\\nSC, and (3) Checking whether a set of input SCs have conflicts or not. We\\npresent effective solutions for each task. Experiments on synthetic and\\nreal-world data illustrate how SCs apply to error detection, and provide\\nevidence that CODED performs better than state-of-the-art approaches.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 26, 2, 44, 20, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Decentralized Evolution and Consolidation of RDF Graphs',\n",
       "  'authors': ['Natanael Arndt', 'Michael Martin'],\n",
       "  'summary': 'The World Wide Web and the Semantic Web are designed as a network of\\ndistributed services and datasets. In this network and its genesis,\\ncollaboration played and still plays a crucial role. But currently we only have\\ncentral collaboration solutions for RDF data, such as SPARQL endpoints and wiki\\nsystems, while decentralized solutions can enable applications for many more\\nuse-cases. Inspired by a successful distributed source code management\\nmethodology in software engineering a framework to support distributed\\nevolution is proposed. The system is based on Git and provides distributed\\ncollaboration on RDF graphs. This paper covers the formal expression of the\\nevolution and consolidation of distributed datasets, the synchronization, as\\nwell as other supporting operations.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 27, 11, 58, 7, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Evaluation of Frequent Itemset Mining Platforms using Apriori and FP-Growth Algorithm',\n",
       "  'authors': ['Ravi Ranjan', 'Aditi Sharma'],\n",
       "  'summary': 'With the overwhelming amount of complex and heterogeneous data pouring from\\nany-where, any-time, and any-device, there is undeniably an era of Big Data.\\nThe emergence of the Big Data as a disruptive technology for next generation of\\nintelligent systems, has brought many issues of how to extract and make use of\\nthe knowledge obtained from the data within short times, limited budget and\\nunder high rates of data generation. Companies are recognizing that big data\\ncan be used to make more accurate predictions, and can be used to enhance the\\nbusiness with the help of appropriate association rule mining algorithm. To\\nhelp these organizations, with which software and algorithm is more appropriate\\nfor them depending on their dataset, we compared the most famous three\\nMapReduce based software Hadoop, Spark, Flink on two widely used algorithms\\nApriori and Fp-Growth on different scales of dataset.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 2, 28, 10, 36, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Exploring Reproducibility and FAIR Principles in Data Science Using Ecological Niche Modeling as a Case Study',\n",
       "  'authors': ['Maria Luiza Mondelli',\n",
       "   'A. Townsend Peterson',\n",
       "   'Luiz M. R. Gadelha Jr'],\n",
       "  'summary': 'Reproducibility is a fundamental requirement of the scientific process since\\nit enables outcomes to be replicated and verified. Computational scientific\\nexperiments can benefit from improved reproducibility for many reasons,\\nincluding validation of results and reuse by other scientists. However,\\ndesigning reproducible experiments remains a challenge and hence the need for\\ndeveloping methodologies and tools that can support this process. Here, we\\npropose a conceptual model for reproducibility to specify its main attributes\\nand properties, along with a framework that allows for computational\\nexperiments to be findable, accessible, interoperable, and reusable. We present\\na case study in ecological niche modeling to demonstrate and evaluate the\\nimplementation of this framework.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 31, 19, 43, 5, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'DeepDB: Learn from Data, not from Queries!',\n",
       "  'authors': ['Benjamin Hilprecht',\n",
       "   'Andreas Schmidt',\n",
       "   'Moritz Kulessa',\n",
       "   'Alejandro Molina',\n",
       "   'Kristian Kersting',\n",
       "   'Carsten Binnig'],\n",
       "  'summary': 'The typical approach for learned DBMS components is to capture the behavior\\nby running a representative set of queries and use the observations to train a\\nmachine learning model. This workload-driven approach, however, has two major\\ndownsides. First, collecting the training data can be very expensive, since all\\nqueries need to be executed on potentially large databases. Second, training\\ndata has to be recollected when the workload and the data changes. To overcome\\nthese limitations, we take a different route: we propose to learn a pure\\ndata-driven model that can be used for different tasks such as query answering\\nor cardinality estimation. This data-driven model also supports ad-hoc queries\\nand updates of the data without the need of full retraining when the workload\\nor data changes. Indeed, one may now expect that this comes at a price of lower\\naccuracy since workload-driven models can make use of more information.\\nHowever, this is not the case. The results of our empirical evaluation\\ndemonstrate that our data-driven approach not only provides better accuracy\\nthan state-of-the-art learned components but also generalizes better to unseen\\nqueries.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 9, 2, 8, 59, 17, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Blended Integrated Open Data: dados abertos públicos integrados',\n",
       "  'authors': ['Fabiola Santore',\n",
       "   'Lucas F. Oliveira',\n",
       "   'Rafael de Paulo Dias',\n",
       "   'Henrique V. Ehrenfried',\n",
       "   'Alessandro Elias',\n",
       "   'Diego Pasqualin',\n",
       "   'Luis C. E. de Bona',\n",
       "   'Marcos Didonet Del Fabro',\n",
       "   'Marcos Sunye'],\n",
       "  'summary': 'While several public institutions provide its data openly, the effort\\nrequired to access, integrate and query this data is too high, reducing the\\namount of possible dataset users. The Blended Integrated Open Data (BIOD)\\nproject has as objective to ease the access to public Open Data. It integrates\\nand makes available more than 300Gb of data, containing billions of records\\nfrom different Open Data Sets, allowing to query over them, and thus to\\nretrieve related information from originally disconnected data sets. This paper\\npresents the set of open data available, how to access it and how produce new\\ncompatible data to improve the existing data set.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 9, 2, 14, 45, 6, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Approximate Query Service on Autonomous IoT Cameras',\n",
       "  'authors': ['Mengwei Xu',\n",
       "   'Xiwen Zhang',\n",
       "   'Yunxin Liu',\n",
       "   'Gang Huang',\n",
       "   'Xuanzhe Liu',\n",
       "   'Felix Xiaozhu Lin'],\n",
       "  'summary': \"Elf is a runtime for an energy-constrained camera to continuously summarize\\nvideo scenes as approximate object counts. Elf's novelty centers on planning\\nthe camera's count actions under energy constraint. (1) Elf explores the rich\\naction space spanned by the number of sample image frames and the choice of\\nper-frame object counters; it unifies errors from both sources into one single\\nbounded error. (2) To decide count actions at run time, Elf employs a\\nlearning-based planner, jointly optimizing for past and future videos without\\ndelaying result materialization. Tested with more than 1,000 hours of videos\\nand under realistic energy constraints, Elf continuously generates object\\ncounts within only 11% of the true counts on average. Alongside the counts, Elf\\npresents narrow errors shown to be bounded and up to 3.4x smaller than\\ncompetitive baselines. At a higher level, Elf makes a case for advancing the\\ngeographic frontier of video analytics.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 9, 2, 19, 45, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Los Angeles Metro Bus Data Analysis Using GPS Trajectory and Schedule Data (Demo Paper)',\n",
       "  'authors': ['Kien Nguyen',\n",
       "   'Jingyun Yang',\n",
       "   'Yijun Lin',\n",
       "   'Jianfa Lin',\n",
       "   'Yao-Yi Chiang',\n",
       "   'Cyrus Shahabi'],\n",
       "  'summary': 'With the widespread installation of location-enabled devices on public\\ntransportation, public vehicles are generating massive amounts of trajectory\\ndata in real time. However, using these trajectory data for meaningful analysis\\nrequires careful considerations in storing, managing, processing, and\\nvisualizing the data. Using the location data of the Los Angeles Metro bus\\nsystem, along with publicly available bus schedule data, we conduct a data\\nprocessing and analyses study to measure the performance of the public\\ntransportation system in Los Angeles utilizing a number of metrics including\\ntravel-time reliability, on-time performance, bus bunching, and travel-time\\nestimation. We demonstrate the visualization of the data analysis results\\nthrough an interactive web-based application. The developed algorithms and\\nsystem provide powerful tools to detect issues and improve the efficiency of\\npublic transportation systems.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 9, 3, 5, 1, 44, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Finding Maximal Non-Redundant Association Rules in Tennis Data',\n",
       "  'authors': ['Daniel Weidner', 'Martin Atzmueller', 'Dietmar Seipel'],\n",
       "  'summary': 'The concept of association rules is well--known in data mining. But often\\nredundancy and subsumption are not considered, and standard approaches produce\\nthousands or even millions of resulting association rules. Without further\\ninformation or post--mining approaches, this huge number of rules is typically\\nuseless for the domain specialist -- which is an instance of the infamous\\npattern explosion problem. In this work, we present a new definition of\\nredundancy and subsumption based on the confidence and the support of the rules\\nand propose post-- mining to prune a set of association rules. In a case study,\\nwe apply our method to association rules mined from spatio--temporal data. The\\ndata represent the trajectories of the ball in tennis matches -- more\\nprecisely, the points/times the tennis ball hits the ground. The goal is to\\nanalyze the strategies of the players and to try to improve their performance\\nby looking at the resulting association rules. The proposed approach is\\ngeneral, and can also be applied to other spatio--temporal data with a similar\\nstructure.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 9, 3, 7, 28, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Algebraic Property Graphs',\n",
       "  'authors': ['Joshua Shinavier', 'Ryan Wisnesky', 'Joshua G. Meyers'],\n",
       "  'summary': 'We present a case study in applied category theory written from the point of\\nview of an applied domain: the formalization of the widely-used property graphs\\ndata model in an enterprise setting using elementary constructions from type\\ntheory and category theory, including limit and co-limit sketches. Observing\\nthat algebraic data types are a common foundation of most of the enterprise\\nschema languages we deal with in practice, for graph data or otherwise, we\\nintroduce a type theory for algebraic property graphs wherein the types denote\\nboth algebraic data types in the sense of functional programming and join-union\\nE/R diagrams in the sense of database theory. We also provide theoretical\\nfoundations for graph transformation along schema mappings with by-construction\\nguarantees of semantic consistency. Our data model originated as a\\nformalization of a data integration toolkit developed at Uber which carries\\ndata and schemas along composable mappings between data interchange languages\\nsuch as Apache Avro, Apache Thrift, and Protocol Buffers, and graph languages\\nincluding RDF with OWL or SHACL-based schemas.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 9, 11, 7, 29, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Selecting Data to Clean for Fact Checking: Minimizing Uncertainty vs. Maximizing Surprise',\n",
       "  'authors': ['Stavros Sintos', 'Pankaj K. Agarwal', 'Jun Yang'],\n",
       "  'summary': 'We study the optimization problem of selecting numerical quantities to clean\\nin order to fact-check claims based on such data. Oftentimes, such claims are\\ntechnically correct, but they can still mislead for two reasons. First, data\\nmay contain uncertainty and errors. Second, data can be \"fished\" to advance\\nparticular positions. In practice, fact-checkers cannot afford to clean all\\ndata and must choose to clean what \"matters the most\" to checking a claim. We\\nexplore alternative definitions of what \"matters the most\": one is to ascertain\\nclaim qualities (by minimizing uncertainty in these measures), while an\\nalternative is just to counter the claim (by maximizing the probability of\\nfinding a counterargument). We show whether the two objectives align with each\\nother, with important implications on when fact-checkers should exercise care\\nin selective data cleaning, to avoid potential bias introduced by their desire\\nto counter claims. We develop efficient algorithms for solving the various\\nvariants of the optimization problem, showing significant improvements over\\nnaive solutions. The problem is particularly challenging because the objectives\\nin the fact-checking context are complex, non-linear functions over data. We\\nobtain results that generalize to a large class of functions, with potential\\napplications beyond fact-checking.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 9, 11, 21, 25, 40, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'DBPal: Weak Supervision for Learning a Natural Language Interface to Databases',\n",
       "  'authors': ['Nathaniel Weir',\n",
       "   'Andrew Crotty',\n",
       "   'Alex Galakatos',\n",
       "   'Amir Ilkhechi',\n",
       "   'Shekar Ramaswamy',\n",
       "   'Rohin Bhushan',\n",
       "   'Ugur Cetintemel',\n",
       "   'Prasetya Utama',\n",
       "   'Nadja Geisler',\n",
       "   'Benjamin Hättasch',\n",
       "   'Steffen Eger',\n",
       "   'Carsten Binnig'],\n",
       "  'summary': 'This paper describes DBPal, a new system to translate natural language\\nutterances into SQL statements using a neural machine translation model. While\\nother recent approaches use neural machine translation to implement a Natural\\nLanguage Interface to Databases (NLIDB), existing techniques rely on supervised\\nlearning with manually curated training data, which results in substantial\\noverhead for supporting each new database schema. In order to avoid this issue,\\nDBPal implements a novel training pipeline based on weak supervision that\\nsynthesizes all training data from a given database schema. In our evaluation,\\nwe show that DBPal can outperform existing rule-based NLIDBs while achieving\\ncomparable performance to other NLIDBs that leverage deep neural network models\\nwithout relying on manually curated training data for every new database\\nschema.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 9, 11, 22, 1, 58, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Harmonise and integrate heterogeneous areal data with the R package arealDB',\n",
       "  'authors': ['Steffen Ehrmann', 'Ralf Seppelt', 'Carsten Meyer'],\n",
       "  'summary': 'Many relevant applications in the environmental and socioeconomic sciences\\nuse areal data, such as biodiversity checklists, agricultural statistics, or\\nsocioeconomic surveys. For applications that surpass the spatial, temporal or\\nthematic scope of any single data source, data must be integrated from several\\nheterogeneous sources. Inconsistent concepts, definitions, or messy data tables\\nmake this a tedious and error-prone process. To date, a dedicated tool to\\naddress these challenges is still lacking. Here, we introduce the R package\\narealDB that integrates heterogeneous areal data and associated geometries into\\na consistent database, in an easy-to-use workflow. It is useful for harmonising\\nlanguage and semantics of variables, relating data to geometries, and\\ndocumenting metadata and provenance. We illustrate the functionality by\\nintegrating two disparate datasets (Brazil, USA) on the harvested area of\\nsoybean. arealDB promises quality-improvements to downstream scientific,\\nmonitoring, and management applications but also substantial time-savings to\\ndatabase collation efforts.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 9, 14, 15, 19, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'MVDLite: a Fast Validation Algorithm for Model View Definition Rules',\n",
       "  'authors': ['Han Liu',\n",
       "   'Ge Gao',\n",
       "   'Hehua Zhang',\n",
       "   'Yu-Shen Liu',\n",
       "   'Yan Song',\n",
       "   'Ming Gu'],\n",
       "  'summary': 'Model View Definition (MVD) is the standard methodology to define the data\\nexchange requirements and rule constraints for Building Information Models\\n(BIMs). In this paper, the MVDLite algorithm is proposed for the fast\\nvalidation of MVD rules. A \"rule chain\" structure is introduced to combine the\\ndata templates, constraint statements, and logical interconnections in an input\\nmvdXML ruleset, which leads to fast filtering of data nodes through the rule\\nchain. By establishing the correspondence of each prefix of the rule chain with\\na string, the deep-caching strategy further improves efficiency. The\\noutperforming experimental results show that our algorithm significantly\\nreduces the running time of MVD validation on large real-world BIMs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 9, 16, 5, 39, 49, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Leyenda: An Adaptive, Hybrid Sorting Algorithm for Large Scale Data with Limited Memory',\n",
       "  'authors': ['Yuanjing Shi', 'Zhaoxing Li'],\n",
       "  'summary': \"Sorting is the one of the fundamental tasks of modern data management\\nsystems. With Disk I/O being the most-accused performance bottleneck and more\\ncomputation-intensive workloads, it has come to our attention that in\\nheterogeneous environment, performance bottleneck may vary among different\\ninfrastructure. As a result, sort kernels need to be adaptive to changing\\nhardware conditions. In this paper, we propose Leyenda, a hybrid, parallel and\\nefficient Radix Most-Significant-Bit (MSB) MergeSort algorithm, with\\nutilization of local thread-level CPU cache and efficient disk/memory I/O.\\nLeyenda is capable of performing either internal or external sort efficiently,\\nbased on different I/O and processing conditions. We benchmarked Leyenda with\\nthree different workloads from Sort Benchmark, targeting three unique use\\ncases, including internal, partially in-memory and external sort, and we found\\nLeyenda to outperform GNU's parallel in-memory quick/merge sort implementations\\nby up to three times. Leyenda is also ranked the second best external sort\\nalgorithm on ACM 2019 SIGMOD programming contest and forth overall.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 9, 17, 18, 10, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Metadata Systems for Data Lakes: Models and Features',\n",
       "  'authors': ['Pegdwendé Sawadogo',\n",
       "   'Etienne Scholly',\n",
       "   'Cécile Favre',\n",
       "   'Eric Ferey',\n",
       "   'Sabine Loudcher',\n",
       "   'Jérôme Darmont'],\n",
       "  'summary': 'Over the past decade, the data lake concept has emerged as an alternative to\\ndata warehouses for storing and analyzing big data. A data lake allows storing\\ndata without any predefined schema. Therefore, data querying and analysis\\ndepend on a metadata system that must be efficient and comprehensive. However,\\nmetadata management in data lakes remains a current issue and the criteria for\\nevaluating its effectiveness are more or less nonexistent.In this paper, we\\nintroduce MEDAL, a generic, graph-based model for metadata management in data\\nlakes. We also propose evaluation criteria for data lake metadata systems\\nthrough a list of expected features. Eventually, we show that our approach is\\nmore comprehensive than existing metadata systems.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 9, 20, 9, 6, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Rule-Based Relational XML Access Control Model in the Presence of Authorization Conflicts',\n",
       "  'authors': ['Ali Alwehaibi', 'Mustafa Atay'],\n",
       "  'summary': 'There is considerable amount of sensitive XML data stored in relational\\ndatabases. It is a challenge to enforce node level fine-grained authorization\\npolicies for XML data stored in relational databases which typically support\\ntable and column level access control. Moreover, it is common to have\\nconflicting authorization policies over the hierarchical nested structure of\\nXML data. There are a couple of XML access control models for relational XML\\ndatabases proposed in the literature. However, to our best knowledge, none of\\nthem discussed handling authorization conflicts with conditions in the domain\\nof relational XML databases. Therefore, we believe that there is a need to\\ndefine and incorporate effective fine-grained XML authorization models with\\nconflict handling mechanisms in the presence of conditions into relational XML\\ndatabases. We address this issue in this study.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 9, 24, 17, 16, 40, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Skyline Queries Over Incomplete Data Streams (Technical Report)',\n",
       "  'authors': ['Weilong Ren', 'Xiang Lian', 'Kambiz Ghazinour'],\n",
       "  'summary': 'Nowadays, efficient and effective processing over massive stream data has\\nattracted much attention from the database community, which are useful in many\\nreal applications such as sensor data monitoring, network intrusion detection,\\nand so on. In practice, due to the malfunction of sensing devices or imperfect\\ndata collection techniques, real-world stream data may often contain missing or\\nincomplete data attributes. In this paper, we will formalize and tackle a novel\\nand important problem, named skyline query over incomplete data stream\\n(Sky-iDS), which retrieves skyline objects (in the presence of missing\\nattributes) with high confidences from incomplete data stream. In order to\\ntackle the Sky-iDS problem, we will design efficient approaches to impute\\nmissing attributes of objects from incomplete data stream via differential\\ndependency (DD) rules. We will propose effective pruning strategies to reduce\\nthe search space of the Sky-iDS problem, devise cost-model-based index\\nstructures to facilitate the data imputation and skyline computation at the\\nsame time, and integrate our proposed techniques into an efficient Sky-iDS\\nquery answering algorithm. Extensive experiments have been conducted to confirm\\nthe efficiency and effectiveness of our Sky-iDS processing approach over both\\nreal and synthetic data sets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 9, 24, 23, 17, 29, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Discovering Process Models from Uncertain Event Data',\n",
       "  'authors': ['Marco Pegoraro',\n",
       "   'Merih Seran Uysal',\n",
       "   'Wil M. P. van der Aalst'],\n",
       "  'summary': 'Modern information systems are able to collect event data in the form of\\nevent logs. Process mining techniques allow to discover a model from event\\ndata, to check the conformance of an event log against a reference model, and\\nto perform further process-centric analyses. In this paper, we consider\\nuncertain event logs, where data is recorded together with explicit uncertainty\\ninformation. We describe a technique to discover a directly-follows graph from\\nsuch event data which retains information about the uncertainty in the process.\\nWe then present experimental results of performing inductive mining over the\\ndirectly-follows graph to obtain models representing the certain and uncertain\\npart of the process.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 9, 20, 8, 42, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'SaGe: Preemptive Query Execution for High Data Availability on the Web',\n",
       "  'authors': ['Thomas Minier', 'Hala Skaf-Molli', 'Pascal Molli'],\n",
       "  'summary': 'Semantic Web applications require querying available RDF Data with high\\nperformance and reliability. However, ensuring both data availability and\\nperformant SPARQL query execution in the context of public SPARQL servers are\\nchallenging problems. Queries could have arbitrary execution time and unknown\\narrival rates. In this paper, we propose SaGe, a preemptive server-side SPARQL\\nquery engine. SaGe relies on a preemptable physical query execution plan and\\npreemptable physical operators. SaGe stops query execution after a given slice\\nof time, saves the state of the plan and sends the saved plan back to the\\nclient with retrieved results. Later, the client can continue the query\\nexecution by resubmitting the saved plan to the server. By ensuring a fair\\nquery execution, SaGe maintains server availability and provides high query\\nthroughput. Experimental results demonstrate that SaGe outperforms the state of\\nthe art SPARQL query engines in terms of query throughput, query timeout and\\nanswer completeness.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 6, 1, 7, 56, 22, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Quality-Assured Synchronized Task Assignment in Crowdsourcing',\n",
       "  'authors': ['Jiayang Tu', 'Peng Cheng', 'Lei Chen'],\n",
       "  'summary': 'With the rapid development of crowdsourcing platforms that aggregate the\\nintelligence of Internet workers, crowdsourcing has been widely utilized to\\naddress problems that require human cognitive abilities. Considering great\\ndynamics of worker arrival and departure, it is of vital importance to design a\\ntask assignment scheme to adaptively select the most beneficial tasks for the\\navailable workers. In this paper, in order to make the most efficient\\nutilization of the worker labor and balance the accuracy of answers and the\\noverall latency, we a) develop a parameter estimation model that assists in\\nestimating worker expertise, question easiness and answer confidence; b)\\npropose a \\\\textit{quality-assured synchronized task assignment scheme} that\\nexecutes in batches and maximizes the number of potentially completed questions\\n(MCQ) within each batch. We prove that MCQ problem is NP-hard and present two\\ngreedy approximation solutions to address the problem. The effectiveness and\\nefficiency of the approximation solutions are further evaluated through\\nextensive experiments on synthetic and real datasets. The experimental results\\nshow that the accuracy and the overall latency of the MCQ approaches outperform\\nthe existing online task assignment algorithms in the synchronized task\\nassignment scenario.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 6, 2, 14, 1, 49, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Native Directly Follows Operator',\n",
       "  'authors': ['Alifah Syamsiyah',\n",
       "   'Boudewijn F. van Dongen',\n",
       "   'Remco M. Dijkman'],\n",
       "  'summary': 'Typical legacy information systems store data in relational databases.\\nProcess mining is a research discipline that analyzes this data to obtain\\ninsights into processes. Many different process mining techniques can be\\napplied to data. In current techniques, an XES event log serves as a basis for\\nanalysis. However, because of the static characteristic of an XES event log, we\\nneed to create one XES file for each process mining question, which leads to\\noverhead and inflexibility. As an alternative, people attempt to perform\\nprocess mining directly on the data source using so-called intermediate\\nstructures. In previous work, we investigated methods to build intermediate\\nstructures on source data by executing a basic SQL query on the database.\\nHowever, the nested form in the SQL query can cause performance issues on the\\ndatabase side. Therefore, in this paper, we propose a native SQL operator for\\ndirect process discovery on relational databases. We define a native operator\\nfor the simplest form of the intermediate structure, called the \"directly\\nfollows relation\". This approach has been evaluated with big event data and the\\nexperimental results show that it performs faster than the state-of-the-art of\\ndatabase approaches.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 6, 5, 12, 42, 45, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Curator: Provenance Management for Modern Distributed Systems',\n",
       "  'authors': ['Warren Smith', 'Thomas Moyer', 'Charles Munson'],\n",
       "  'summary': 'Data provenance is a valuable tool for protecting and troubleshooting\\ndistributed systems. Careful design of the provenance components reduces the\\nimpact on the design, implementation, and operation of the distributed system.\\nIn this paper, we present Curator, a provenance management toolkit that can be\\neasily integrated with microservice-based systems and other modern distributed\\nsystems. This paper describes the design of Curator and discusses how we have\\nused Curator to add provenance to distributed systems. We find that our\\napproach results in no changes to the design of these distributed systems and\\nminimal additional code and dependencies to manage. In addition, Curator uses\\nthe same scalable infrastructure as the distributed system and can therefore\\nscale with the distributed system.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 6, 6, 14, 45, 46, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'PubMed Labs: An experimental platform for improving biomedical literature search',\n",
       "  'authors': ['Nicolas Fiorini',\n",
       "   'Kathi Canese',\n",
       "   'Rostyslav Bryzgunov',\n",
       "   'Ievgeniia Radetska',\n",
       "   'Asta Gindulyte',\n",
       "   'Martin Latterner',\n",
       "   'Vadim Miller',\n",
       "   'Maxim Osipov',\n",
       "   'Michael Kholodov',\n",
       "   'Grisha Starchenko',\n",
       "   'Evgeny Kireev',\n",
       "   'Zhiyong Lu'],\n",
       "  'summary': 'PubMed is a freely accessible system for searching the biomedical literature,\\nwith approximately 2.5 million users worldwide on an average workday. We have\\nrecently developed PubMed Labs (www.pubmed.gov/labs), an experimental platform\\nfor users to test new features/tools and provide feedback, which enables us to\\nmake more informed decisions about potential changes to improve the search\\nquality and overall usability of PubMed. In doing so, we hope to better meet\\nour user needs in an era of information overload. Another novel aspect of\\nPubMed Labs lies in its mobile-first and responsive layout, which offers better\\nsupport for accessing PubMed on the increasingly popular use of mobile and\\nsmall-screen devices. Currently, PubMed Labs only includes a core subset of\\nPubMed functionalities, e.g. search, facets. We encourage users to test PubMed\\nLabs and share their experience with us, based on which we expect to\\ncontinuously improve PubMed Labs with more advanced features and better user\\nexperience.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 6, 11, 14, 10, 1, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Performance evaluation for CRUD operations in asynchronously replicated document oriented database',\n",
       "  'authors': ['Ciprian-Octavian Truică',\n",
       "   'Florin Rădulescu',\n",
       "   'Alexandru Boicea',\n",
       "   'Ion Bucur'],\n",
       "  'summary': 'NoSQL databases are becoming increasingly popular as more developers seek new\\nways for storing information. The popularity of these databases has risen due\\nto their flexibility and scalability needed in domains like Big Data and Cloud\\nComputing. This paper examines asynchronous replication, one of the key\\nfeatures for a scalable and flexible system. Three of the most popular\\nDocument-Oriented Databases, MongoDB, CouchDB, and Couchbase, are examined. For\\ntesting, the execution time for CRUD operations for a single database instance\\nand for a distributed environment with two nodes is taken into account and the\\nresults are compared with tests outcomes obtained for three relational database\\nmanagement systems: Microsoft SQL Server, MySQL, and PostgreSQL.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 6, 12, 20, 38, 56, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards Semantically Enhanced Data Understanding',\n",
       "  'authors': ['Markus Schröder',\n",
       "   'Christian Jilek',\n",
       "   'Jörn Hees',\n",
       "   'Andreas Dengel'],\n",
       "  'summary': 'In the field of machine learning, data understanding is the practice of\\ngetting initial insights in unknown datasets. Such knowledge-intensive tasks\\nrequire a lot of documentation, which is necessary for data scientists to grasp\\nthe meaning of the data. Usually, documentation is separate from the data in\\nvarious external documents, diagrams, spreadsheets and tools which causes\\nconsiderable look up overhead. Moreover, other supporting applications are not\\nable to consume and utilize such unstructured data. That is why we propose a\\nmethodology that uses a single semantic model that interlinks data with its\\ndocumentation. Hence, data scientists are able to directly look up the\\nconnected information about the data by simply following links. Equally, they\\ncan browse the documentation which always refers to the data. Furthermore, the\\nmodel can be used by other approaches providing additional support, like\\nsearching, comparing, integrating or visualizing data. To showcase our approach\\nwe also demonstrate an early prototype.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 6, 13, 11, 19, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Crowd-Powered Data Mining',\n",
       "  'authors': ['Chengliang Chai',\n",
       "   'Ju Fan',\n",
       "   'Guoliang Li',\n",
       "   'Jiannan Wang',\n",
       "   'Yudian Zheng'],\n",
       "  'summary': 'Many data mining tasks cannot be completely addressed by auto- mated\\nprocesses, such as sentiment analysis and image classification. Crowdsourcing\\nis an effective way to harness the human cognitive ability to process these\\nmachine-hard tasks. Thanks to public crowdsourcing platforms, e.g., Amazon\\nMechanical Turk and Crowd- Flower, we can easily involve hundreds of thousands\\nof ordinary workers (i.e., the crowd) to address these machine-hard tasks. In\\nthis tutorial, we will survey and synthesize a wide spectrum of existing\\nstudies on crowd-powered data mining. We first give an overview of\\ncrowdsourcing, and then summarize the fundamental techniques, including quality\\ncontrol, cost control, and latency control, which must be considered in\\ncrowdsourced data mining. Next we review crowd-powered data mining operations,\\nincluding classification, clustering, pattern mining, machine learning using\\nthe crowd (including deep learning, transfer learning and semi-supervised\\nlearning) and knowledge discovery. Finally, we provide the emerging challenges\\nin crowdsourced data mining.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 6, 13, 12, 4, 20, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Efficient Handling of SPARQL OPTIONAL for OBDA (Extended Version)',\n",
       "  'authors': ['Guohui Xiao',\n",
       "   'Roman Kontchakov',\n",
       "   'Benjamin Cogrel',\n",
       "   'Diego Calvanese',\n",
       "   'Elena Botoeva'],\n",
       "  'summary': 'OPTIONAL is a key feature in SPARQL for dealing with missing information.\\nWhile this operator is used extensively, it is also known for its complexity,\\nwhich can make efficient evaluation of queries with OPTIONAL challenging. We\\ntackle this problem in the Ontology-Based Data Access (OBDA) setting, where the\\ndata is stored in a SQL relational database and exposed as a virtual RDF graph\\nby means of an R2RML mapping. We start with a succinct translation of a SPARQL\\nfragment into SQL. It fully respects bag semantics and three-valued logic and\\nrelies on the extensive use of the LEFT JOIN operator and COALESCE function. We\\nthen propose optimisation techniques for reducing the size and improving the\\nstructure of generated SQL queries. Our optimisations capture interactions\\nbetween JOIN, LEFT JOIN, COALESCE and integrity constraints such as attribute\\nnullability, uniqueness and foreign key constraints. Finally, we empirically\\nverify effectiveness of our techniques on the BSBM OBDA benchmark.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 6, 15, 11, 48, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'TrQuery: An Embedding-based Framework for Recommanding SPARQL Queries',\n",
       "  'authors': ['Lijing Zhang', 'Xiaowang Zhang', 'Zhiyong Feng'],\n",
       "  'summary': 'In this paper, we present an embedding-based framework (TrQuery) for\\nrecommending solutions of a SPARQL query, including approximate solutions when\\nexact querying solutions are not available due to incompleteness or\\ninconsistencies of real-world RDF data. Within this framework, embedding is\\napplied to score solutions together with edit distance so that we could obtain\\nmore fine-grained recommendations than those recommendations via edit distance.\\nFor instance, graphs of two querying solutions with a similar structure can be\\ndistinguished in our proposed framework while the edit distance depending on\\nstructural difference becomes unable. To this end, we propose a novel score\\nmodel built on vector space generated in embedding system to compute the\\nsimilarity between an approximate subgraph matching and a whole graph matching.\\nFinally, we evaluate our approach on large RDF datasets DBpedia and YAGO, and\\nexperimental results show that TrQuery exhibits an excellent behavior in terms\\nof both effectiveness and efficiency.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 6, 16, 8, 17, 8, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Itemsets of interest for negative association rules',\n",
       "  'authors': ['Hyeok Kong', 'Dokjun An', 'Jihyang Ri'],\n",
       "  'summary': 'So far, most of association rule minings have considered about positive\\nassociation rules based on frequent itemsets in databases[2,5-7], but they have\\nnot considered the problem of mining negative association rules correlated with\\nfrequent and infrequent itemsets. Negative association rule mining is much more\\ndifficult than positive association rule mining because it needs infrequent\\nitemsets, and only the rare association rule mining which is a kind of negative\\nassociation rule minings has been studied. This paper presents a mathematical\\nmodel to mine positive and negative association rules precisely, for which in a\\npoint of view that negation of a frequent itemset is an infrequent itemset, we\\nmake clear the importance of the problem of mining negative association rules\\nbased on certain infrequent itemsets and study on what conditions infrequent\\nitemsets of interest should satisfy for negative association rules.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 6, 19, 7, 50, 48, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Developing a Temporal Bibliographic Data Set for Entity Resolution',\n",
       "  'authors': ['Yichen Hu', 'Qing Wang', 'Peter Christen'],\n",
       "  'summary': 'Entity resolution is the process of identifying groups of records within or\\nacross data sets where each group represents a real-world entity. Novel\\ntechniques that consider temporal features to improve the quality of entity\\nresolution have recently attracted significant attention. However, there are\\ncurrently no large data sets available that contain both temporal information\\nas well as ground truth information to evaluate the quality of temporal entity\\nresolution approaches. In this paper, we describe the preparation of a temporal\\ndata set based on author profiles extracted from the Digital Bibliography and\\nLibrary Project (DBLP). We completed missing links between publications and\\nauthor profiles in the DBLP data set using the DBLP public API. We then used\\nthe Microsoft Academic Graph (MAG) to link temporal affiliation information for\\nDBLP authors. We selected around 80K (1%) of author profiles that cover 2\\nmillion (50%) publications using information in DBLP such as alternative author\\nnames and personal web profile to improve the reliability of the resulting\\nground truth, while at the same time keeping the data set challenging for\\ntemporal entity resolution research.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 6, 20, 2, 14, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Searching of interesting itemsets for negative association rules',\n",
       "  'authors': ['Hyeok Kong', 'Dokjun An', 'Douk Han'],\n",
       "  'summary': 'In this paper, we propose an algorithm of searching for both positive and\\nnegative itemsets of interest which should be given at the first stage for\\npositive and negative association rules mining. Traditional association rule\\nmining algorithms extract positive association rules based on frequent\\nitemsets, for which the frequent itemsets, i.e. only positive itemsets of\\ninterest are searched. Further, there are useful itemsets among the frequent\\nitemsets pruned from the traditional algorithms to reduce the search space, for\\nmining of negative association rules. Therefore, the traditional algorithms\\nhave not come true to find negative itemsets needed in mining of negative\\nassociation rules. Our new algorithm to search for both positive and negative\\nitemsets of interest prepares preconditions for mining of all positive and\\nnegative association rules.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 6, 20, 12, 24, 20, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Novel Selectivity Estimation Strategy for Modern DBMS',\n",
       "  'authors': ['Jun Hyung Shin'],\n",
       "  'summary': 'Selectivity estimation is important in query optimization, however accurate\\nestimation is difficult when predicates are complex. Instead of existing\\ndatabase synopses and statistics not helpful for such cases, we introduce a new\\napproach to compute the exact selectivity by running an aggregate query during\\nthe optimization phase. Exact selectivity can be achieved without significant\\noverhead for in-memory and GPU-accelerated databases by adding extra query\\nexecution calls. We implement a selection push-down extension based on the\\nnovel selectivity estimation strategy in the MapD database system. Our approach\\nrecords constant and less than 30 millisecond overheads in any circumstances\\nwhile running on GPU. The novel strategy successfully generates better query\\nexecution plans which result in performance improvement up to 4.8 times from\\nTPC-H benchmark SF-50 queries and 7.3 times from star schema benchmark SF-80\\nqueries.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 6, 21, 18, 34, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Tensor Based Data Model for Polystore: An Application to Social Networks Data',\n",
       "  'authors': ['Eric Leclercq', 'Marinette Savonnet'],\n",
       "  'summary': 'In this article, we show how the mathematical object tensor can be used to\\nbuild a multi-paradigm model for the storage of social data in data warehouses.\\nFrom an architectural point of view, our approach allows to link different\\nstorage systems (polystore) and limits the impact of ETL tools performing model\\ntransformations required to feed different analysis algorithms. Therefore,\\nsystems can take advantage of multiple data models both in terms of query\\nexecution performance and the semantic expressiveness of data representation.\\nThe proposed model allows to reach the logical independence between data and\\nprograms implementing analysis algorithms. With a concrete case study on\\nmessage virality on Twitter during the French presidential election of 2017, we\\nhighlight some of the contributions of our model.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 6, 26, 13, 33, 20, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Skyline Diagram: Efficient Space Partitioning for Skyline Queries',\n",
       "  'authors': ['Jinfei Liu',\n",
       "   'Juncheng Yang',\n",
       "   'Li Xiong',\n",
       "   'Jian Pei',\n",
       "   'Jun Luo',\n",
       "   'Yuzhang Guo',\n",
       "   'Shuaicheng Ma',\n",
       "   'Chenglin Fan'],\n",
       "  'summary': 'Skyline queries are important in many application domains. In this paper, we\\npropose a novel structure Skyline Diagram, which given a set of points,\\npartitions the plane into a set of regions, referred to as skyline polyominos.\\nAll query points in the same skyline polyomino have the same skyline query\\nresults. Similar to $k^{th}$-order Voronoi diagram commonly used to facilitate\\n$k$ nearest neighbor ($k$NN) queries, skyline diagram can be used to facilitate\\nskyline queries and many other applications. However, it may be computationally\\nexpensive to build the skyline diagram. By exploiting some interesting\\nproperties of skyline, we present several efficient algorithms for building the\\ndiagram with respect to three kinds of skyline queries, quadrant, global, and\\ndynamic skylines. In addition, we propose an approximate skyline diagram which\\ncan significantly reduce the space cost. Experimental results on both real and\\nsynthetic datasets show that our algorithms are efficient and scalable.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 12, 4, 20, 15, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Mapping RDF Graphs to Property Graphs',\n",
       "  'authors': ['Shota Matsumoto', 'Ryota Yamanaka', 'Hirokazu Chiba'],\n",
       "  'summary': 'Increasing amounts of scientific and social data are published in the\\nResource Description Framework (RDF). Although the RDF data can be queried\\nusing the SPARQL language, even the SPARQL-based operation has a limitation in\\nimplementing traversal or analytical algorithms. Recently, a variety of graph\\ndatabase implementations dedicated to analyses on the property graph model have\\nemerged. However, the RDF model and the property graph model are not\\ninteroperable. Here, we developed a framework based on the Graph to Graph\\nMapping Language (G2GML) for mapping RDF graphs to property graphs to make the\\nmost of accumulated RDF data. Using this framework, graph data described in the\\nRDF model can be converted to the property graph model and can be loaded to\\nseveral graph database engines for further analysis. Future works include\\nimplementing and utilizing graph algorithms to make the most of the accumulated\\ndata in various analytical engines.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 12, 5, 3, 22, 37, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'vChain: Enabling Verifiable Boolean Range Queries over Blockchain Databases',\n",
       "  'authors': ['Cheng Xu', 'Ce Zhang', 'Jianliang Xu'],\n",
       "  'summary': \"Blockchains have recently been under the spotlight due to the boom of\\ncryptocurrencies and decentralized applications. There is an increasing demand\\nfor querying the data stored in a blockchain database. To ensure query\\nintegrity, the user can maintain the entire blockchain database and query the\\ndata locally. However, this approach is not economic, if not infeasible,\\nbecause of the blockchain's huge data size and considerable maintenance costs.\\nIn this paper, we take the first step toward investigating the problem of\\nverifiable query processing over blockchain databases. We propose a novel\\nframework, called vChain, that alleviates the storage and computing costs of\\nthe user and employs verifiable queries to guarantee the results' integrity. To\\nsupport verifiable Boolean range queries, we propose an accumulator-based\\nauthenticated data structure that enables dynamic aggregation over arbitrary\\nquery attributes. Two new indexes are further developed to aggregate\\nintra-block and inter-block data records for efficient query verification. We\\nalso propose an inverted prefix tree structure to accelerate the processing of\\na large number of subscription queries simultaneously. Security analysis and\\nempirical study validate the robustness and practicality of the proposed\\ntechniques.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 12, 6, 7, 33, 37, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'On the Enumeration Complexity of Unions of Conjunctive Queries',\n",
       "  'authors': ['Nofar Carmeli', 'Markus Kröll'],\n",
       "  'summary': 'We study the enumeration complexity of Unions of Conjunctive Queries(UCQs).\\nWe aim to identify the UCQs that are tractable in the sense that the answer\\ntuples can be enumerated with a linear preprocessing phase and a constant delay\\nbetween every successive tuples. It has been established that, in the absence\\nof self-joins and under conventional complexity assumptions, the CQs that admit\\nsuch an evaluation are precisely the free-connex ones. A union of tractable CQs\\nis always tractable. We generalize the notion of free-connexity from CQs to\\nUCQs, thus showing that some unions containing intractable CQs are, in fact,\\ntractable. Interestingly, some unions consisting of only intractable CQs are\\ntractable too. We show how to use the techniques presented in this article also\\nin settings where the database contains cardinality dependencies (including\\nfunctional dependencies and key constraints) or when the UCQs contain\\ndisequalities. The question of finding a full characterization of the\\ntractability of UCQs remains open. Nevertheless, we prove that for several\\nclasses of queries, free-connexity fully captures the tractable UCQs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 12, 10, 14, 39, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Semantic Width of Conjunctive Queries and Constraint Satisfaction Problems',\n",
       "  'authors': ['Georg Gottlob', 'Matthias Lanzinger', 'Reinhard Pichler'],\n",
       "  'summary': \"Answering Conjunctive Queries (CQs) and solving Constraint Satisfaction\\nProblems (CSPs) are arguably among the most fundamental tasks in Computer\\nScience. They are classical NP-complete problems. Consequently, the search for\\ntractable fragments of these problems has received a lot of research interest\\nover the decades. This research has traditionally progressed along three\\northogonal threads. a) Reformulating queries into simpler, equivalent, queries\\n(semantic optimization) b) Bounding answer sizes based on structural properties\\nof the query c) Decomposing the query in such a way that global consistency\\nfollows from local consistency. Much progress has been made by various works\\nthat connect two of these threads. Bounded answer sizes and decompositions have\\nbeen shown to be tightly connected through the important notions of fractional\\nhypertree width and, more recently, submodular width. recent papers by\\nBarcel\\\\'o et al. study decompositions up to generalized hypertree width under\\nsemantic optimization. In this work, we connect all three of these threads by\\nintroducing a general notion of semantic width and investigating semantic\\nversions of fractional hypertree width, adaptive width, submodular width and\\nthe fractional cover number.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 12, 11, 11, 1, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The Empusa code generator: bridging the gap between the intended and the actual content of RDF resources',\n",
       "  'authors': ['Jesse C. J. van Dam',\n",
       "   'Jasper J. Koehorst',\n",
       "   'Peter J. Schaap',\n",
       "   'Maria Suarez-Diez'],\n",
       "  'summary': 'The RDF data model facilitates integration of diverse data available in\\nstructured and semi-structured formats. To obtain an RDF graph with a low\\namount of errors and internal redundancy, the chosen ontology must be\\nconsistently applied. However, with each addition of new diverse data the\\nontology must evolve thereby increasing its complexity, which could lead to\\naccumulation of unintended erroneous composites. Thus, there is a need for a\\ngatekeeping system that compares the intended content described in the ontology\\nwith the actual content of the resource.\\n  Here we present Empusa, a tool that has been developed to facilitate the\\ncreation of composite RDF resources from disparate sources. Empusa can be used\\nto convert a schema into an associated application programming interface (API)\\nthat can be used to perform data consistency checks and generates Markdown\\ndocumentation to make persistent URLs resolvable. In this way, the use of\\nEmpusa ensures consistency within and between the ontology (OWL), the Shape\\nExpressions (ShEx) describing the graph structure, and the content of the\\nresource.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 12, 11, 13, 25, 19, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Helix: Holistic Optimization for Accelerating Iterative Machine Learning',\n",
       "  'authors': ['Doris Xin',\n",
       "   'Stephen Macke',\n",
       "   'Litian Ma',\n",
       "   'Jialin Liu',\n",
       "   'Shuchen Song',\n",
       "   'Aditya Parameswaran'],\n",
       "  'summary': 'Machine learning workflow development is a process of trial-and-error:\\ndevelopers iterate on workflows by testing out small modifications until the\\ndesired accuracy is achieved. Unfortunately, existing machine learning systems\\nfocus narrowly on model training---a small fraction of the overall development\\ntime---and neglect to address iterative development. We propose Helix, a\\nmachine learning system that optimizes the execution across\\niterations---intelligently caching and reusing, or recomputing intermediates as\\nappropriate. Helix captures a wide variety of application needs within its\\nScala DSL, with succinct syntax defining unified processes for data\\npreprocessing, model specification, and learning. We demonstrate that the reuse\\nproblem can be cast as a Max-Flow problem, while the caching problem is\\nNP-Hard. We develop effective lightweight heuristics for the latter. Empirical\\nevaluation shows that Helix is not only able to handle a wide variety of use\\ncases in one unified workflow but also much faster, providing run time\\nreductions of up to 19x over state-of-the-art systems, such as DeepDive or\\nKeystoneML, on four real-world applications in natural language processing,\\ncomputer vision, social and natural sciences.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 12, 14, 2, 32, 45, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data Provenance for Sport',\n",
       "  'authors': ['Andrew J. Simmons',\n",
       "   'Scott Barnett',\n",
       "   'Simon Vajda',\n",
       "   'Rajesh Vasa'],\n",
       "  'summary': 'Data analysts often discover irregularities in their underlying dataset,\\nwhich need to be traced back to the original source and corrected. Standards\\nfor representing data provenance (i.e. the origins of the data), such as the\\nW3C PROV standard, can assist with this process, however require a mapping\\nbetween abstract provenance concepts and the domain of use in order to apply\\nthem effectively. We propose a custom notation for expressing provenance of\\ninformation in the sport performance analysis domain, and map our notation to\\nconcepts in the W3C PROV standard where possible. We evaluate the functionality\\nof W3C PROV (without specialisations) and the VisTrails workflow manager\\n(without extensions), and find that as is, neither are able to fully capture\\nsport performance analysis workflows, notably due to limitations surrounding\\ncapture of automated and manual activities respectively. Furthermore, their\\nnotations suffer from ineffective use of visual design space, and present\\npotential usability issues as their terminology is unlikely to match that of\\nsport practitioners. Our findings suggest that one-size-fits-all provenance and\\nworkflow systems are a poor fit in practice, and that their notation and\\nfunctionality need to be optimised for the domain of use.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 12, 14, 7, 41, 25, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'LSM-based Storage Techniques: A Survey',\n",
       "  'authors': ['Chen Luo', 'Michael J. Carey'],\n",
       "  'summary': 'Recently, the Log-Structured Merge-tree (LSM-tree) has been widely adopted\\nfor use in the storage layer of modern NoSQL systems. Because of this, there\\nhave been a large number of research efforts, from both the database community\\nand the operating systems community, that try to improve various aspects of\\nLSM-trees. In this paper, we provide a survey of recent research efforts on\\nLSM-trees so that readers can learn the state-of-the-art in LSM-based storage\\ntechniques. We provide a general taxonomy to classify the literature of\\nLSM-trees, survey the efforts in detail, and discuss their strengths and\\ntrade-offs. We further survey several representative LSM-based open-source\\nNoSQL systems and discuss some potential future research directions resulting\\nfrom the survey.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 12, 18, 17, 48, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'DeepLens: Towards a Visual Data Management System',\n",
       "  'authors': ['Sanjay Krishnan', 'Adam Dziedzic', 'Aaron J. Elmore'],\n",
       "  'summary': 'Advances in deep learning have greatly widened the scope of automatic\\ncomputer vision algorithms and enable users to ask questions directly about the\\ncontent in images and video. This paper explores the necessary steps towards a\\nfuture Visual Data Management System (VDMS), where the predictions of such deep\\nlearning models are stored, managed, queried, and indexed. We propose a query\\nand data model that disentangles the neural network models used, the query\\nworkload, and the data source semantics from the query processing layer. Our\\nsystem, DeepLens, is based on dataflow query processing systems and this\\nresearch prototype presents initial experiments to elicit important open\\nresearch questions in visual analytics systems. One of our main conclusions is\\nthat any future \"declarative\" VDMS will have to revisit query optimization and\\nautomated physical design from a unified perspective of performance and\\naccuracy tradeoffs. Physical design and query optimization choices can not only\\nchange performance by orders of magnitude, they can potentially affect the\\naccuracy of results.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 12, 18, 19, 25, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Demonstration of a Multiresolution Schema Mapping System',\n",
       "  'authors': ['Zhongjun Jin',\n",
       "   'Christopher Baik',\n",
       "   'Michael Cafarella',\n",
       "   'H. V. Jagadish',\n",
       "   'Yuze Lou'],\n",
       "  'summary': 'Enterprise databases usually contain large and complex schemas. Authoring\\ncomplete schema mapping queries in this case requires deep knowledge about the\\nsource and target schemas and is thereby very challenging to programmers.\\nSample-driven schema mapping allows the user to describe the schema mapping\\nusing data records. However, real data records are still harder to specify than\\nother useful insights about the desired schema mapping the user might have. In\\nthis project, we develop a schema mapping system, PRISM, that enables\\nmultiresolution schema mapping. The end user is not limited to providing\\nhigh-resolution constraints like exact data records but may also provide\\nconstraints of various resolutions, like incomplete data records, value ranges,\\nand data types. This new interaction paradigm gives the user more flexibility\\nin describing the desired schema mapping. This demonstration showcases how to\\nuse PRISM for schema mapping in a real database.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 12, 18, 21, 58, 19, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Speeding-up the Verification Phase of Set Similarity Joins in the GPGPU paradigm',\n",
       "  'authors': ['Christos Bellas', 'Anastasios Gounaris'],\n",
       "  'summary': 'We investigate the problem of exact set similarity joins using a co-process\\nCPU-GPU scheme. The state-of-the-art CPU solutions split the wok in two main\\nphases. First, filtering and index building takes place to reduce the candidate\\nsets to be compared as much as possible; then the pairs are compared to verify\\nwhether they should become part of the result. We investigate in-depth\\nsolutions for transferring the second, so-called verification phase, to the GPU\\naddressing several challenges regarding the data serialization and layout, the\\nthread management and the techniques to compare sets of tokens. Using real\\ndatasets, we provide concrete experimental proofs that our solutions have\\nreached their maximum potential, since they totally overlap verification with\\nCPU tasks, and manage to yield significant speed-ups, up to 2.6X in our cases.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 12, 21, 14, 24, 56, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Partitioned Data Security on Outsourced Sensitive and Non-sensitive Data',\n",
       "  'authors': ['Sharad Mehrotra',\n",
       "   'Shantanu Sharma',\n",
       "   'Jeffrey D. Ullman',\n",
       "   'Anurag Mishra'],\n",
       "  'summary': 'Despite extensive research on cryptography, secure and efficient query\\nprocessing over outsourced data remains an open challenge. This paper continues\\nalong the emerging trend in secure data processing that recognizes that the\\nentire dataset may not be sensitive, and hence, non-sensitivity of data can be\\nexploited to overcome limitations of existing encryption-based approaches. We\\npropose a new secure approach, entitled query binning (QB) that allows\\nnon-sensitive parts of the data to be outsourced in clear-text while\\nguaranteeing that no information is leaked by the joint processing of\\nnon-sensitive data (in clear-text) and sensitive data (in encrypted form). QB\\nmaps a query to a set of queries over the sensitive and non-sensitive data in a\\nway that no leakage will occur due to the joint processing over sensitive and\\nnon-sensitive data. Interestingly, in addition to improve performance, we show\\nthat QB actually strengthens the security of the underlying cryptographic\\ntechnique by preventing size, frequency-count, and workload-skew attacks.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 12, 20, 3, 6, 17, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'TaxoGen: Unsupervised Topic Taxonomy Construction by Adaptive Term Embedding and Clustering',\n",
       "  'authors': ['Chao Zhang',\n",
       "   'Fangbo Tao',\n",
       "   'Xiusi Chen',\n",
       "   'Jiaming Shen',\n",
       "   'Meng Jiang',\n",
       "   'Brian Sadler',\n",
       "   'Michelle Vanni',\n",
       "   'Jiawei Han'],\n",
       "  'summary': 'Taxonomy construction is not only a fundamental task for semantic analysis of\\ntext corpora, but also an important step for applications such as information\\nfiltering, recommendation, and Web search. Existing pattern-based methods\\nextract hypernym-hyponym term pairs and then organize these pairs into a\\ntaxonomy. However, by considering each term as an independent concept node,\\nthey overlook the topical proximity and the semantic correlations among terms.\\nIn this paper, we propose a method for constructing topic taxonomies, wherein\\nevery node represents a conceptual topic and is defined as a cluster of\\nsemantically coherent concept terms. Our method, TaxoGen, uses term embeddings\\nand hierarchical clustering to construct a topic taxonomy in a recursive\\nfashion. To ensure the quality of the recursive process, it consists of: (1) an\\nadaptive spherical clustering module for allocating terms to proper levels when\\nsplitting a coarse topic into fine-grained ones; (2) a local embedding module\\nfor learning term embeddings that maintain strong discriminative power at\\ndifferent levels of the taxonomy. Our experiments on two real datasets\\ndemonstrate the effectiveness of TaxoGen compared with baseline methods.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 12, 22, 16, 11, 17, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Explaining Aggregates for Exploratory Analytics',\n",
       "  'authors': ['Fotis Savva',\n",
       "   'Christos Anagnostopoulos',\n",
       "   'Peter Triantafillou'],\n",
       "  'summary': \"Analysts wishing to explore multivariate data spaces, typically pose queries\\ninvolving selection operators, i.e., range or radius queries, which define data\\nsubspaces of possible interest and then use aggregation functions, the results\\nof which determine their exploratory analytics interests. However, such\\naggregate query (AQ) results are simple scalars and as such, convey limited\\ninformation about the queried subspaces for exploratory analysis. We address\\nthis shortcoming aiding analysts to explore and understand data subspaces by\\ncontributing a novel explanation mechanism coined XAXA: eXplaining Aggregates\\nfor eXploratory Analytics. XAXA's novel AQ explanations are represented using\\nfunctions obtained by a three-fold joint optimization problem. Explanations\\nassume the form of a set of parametric piecewise-linear functions acquired\\nthrough a statistical learning model. A key feature of the proposed solution is\\nthat model training is performed by only monitoring AQs and their answers\\non-line. In XAXA, explanations for future AQs can be computed without any\\ndatabase (DB) access and can be used to further explore the queried data\\nsubspaces, without issuing any more queries to the DB. We evaluate the\\nexplanation accuracy and efficiency of XAXA through theoretically grounded\\nmetrics over real-world and synthetic datasets and query workloads.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 12, 29, 11, 43, 32, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Fast Algorithm for K-Truss Discovery on Public-Private Graphs',\n",
       "  'authors': ['Soroush Ebadian', 'Xin Huang'],\n",
       "  'summary': 'In public-private graphs, users share one public graph and have their own\\nprivate graphs. A private graph consists of personal private contacts that only\\ncan be visible to its owner, e.g., hidden friend lists on Facebook and secret\\nfollowing on Sina Weibo. However, existing public-private analytic algorithms\\nhave not yet investigated the dense subgraph discovery of k-truss, where each\\nedge is contained in at least k-2 triangles. This paper aims at finding k-truss\\nefficiently in public-private graphs. The core of our solution is a novel\\nalgorithm to update k-truss with node insertions. We develop a\\nclassification-based hybrid strategy of node insertions and edge insertions to\\nincrementally compute k-truss in public-private graphs. Extensive experiments\\nvalidate the superiority of our proposed algorithms against state-of-the-art\\nmethods on real-world datasets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 1, 3, 31, 6, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Enriching Ontology-based Data Access with Provenance (Extended Version)',\n",
       "  'authors': ['Diego Calvanese',\n",
       "   'Davide Lanti',\n",
       "   'Ana Ozaki',\n",
       "   'Rafael Penaloza',\n",
       "   'Guohui Xiao'],\n",
       "  'summary': 'Ontology-based data access (OBDA) is a popular paradigm for querying\\nheterogeneous data sources by connecting them through mappings to an ontology.\\nIn OBDA, it is often difficult to reconstruct why a tuple occurs in the answer\\nof a query. We address this challenge by enriching OBDA with provenance\\nsemirings, taking inspiration from database theory. In particular, we\\ninvestigate the problems of (i) deciding whether a provenance annotated OBDA\\ninstance entails a provenance annotated conjunctive query, and (ii) computing a\\npolynomial representing the provenance of a query entailed by a provenance\\nannotated OBDA instance. Differently from pure databases, in our case these\\npolynomials may be infinite. To regain finiteness, we consider idempotent\\nsemirings, and study the complexity in the case of DL-Lite ontologies. We\\nimplement Task (ii) in a state-of-the-art OBDA system and show the practical\\nfeasibility of the approach through an extensive evaluation against two popular\\nbenchmarks.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 1, 8, 15, 48, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Efficient Algorithms for Densest Subgraph Discovery',\n",
       "  'authors': ['Yixiang Fang',\n",
       "   'Kaiqiang Yu',\n",
       "   'Reynold Cheng',\n",
       "   'Laks V. S. Lakshmanan',\n",
       "   'Xuemin Lin'],\n",
       "  'summary': 'Densest subgraph discovery (DSD) is a fundamental problem in graph mining. It\\nhas been studied for decades, and is widely used in various areas, including\\nnetwork science, biological analysis, and graph databases. Given a graph G, DSD\\naims to find a subgraph D of G with the highest density (e.g., the number of\\nedges over the number of vertices in D). Because DSD is difficult to solve, we\\npropose a new solution paradigm in this paper. Our main observation is that a\\ndensest subgraph can be accurately found through a k-core (a kind of dense\\nsubgraph of G), with theoretical guarantees. Based on this intuition, we\\ndevelop efficient exact and approximation solutions for DSD. Moreover, our\\nsolutions are able to find the densest subgraphs for a wide range of graph\\ndensity definitions, including clique-based and general pattern-based density.\\nWe have performed extensive experimental evaluation on eleven real datasets.\\nOur results show that our algorithms are up to four orders of magnitude faster\\nthan existing approaches.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 2, 4, 28, 6, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Evaluating Geospatial RDF stores Using the Benchmark Geographica 2',\n",
       "  'authors': ['Theofilos Ioannidis',\n",
       "   'George Garbis',\n",
       "   'Kostis Kyzirakos',\n",
       "   'Konstantina Bereta',\n",
       "   'Manolis Koubarakis'],\n",
       "  'summary': 'Since 2007, geospatial extensions of SPARQL, like GeoSPARQL and stSPARQL,\\nhave been defined and corresponding geospatial RDF stores have been\\nimplemented. In addition, some work on developing benchmarks for evaluating\\ngeospatial RDF stores has been carried out. In this paper, we revisit the\\nGeographica benchmark defined by our group in 2013 which uses both real world\\nand synthetic data to test the performance and functionality of geospatial RDF\\nstores. We present Geographica 2, a new version of the benchmark which extends\\nGeographica by adding one more workload, extending our existing workloads and\\nevaluating 5 more RDF stores. Using three different real workloads, Geographica\\n2 tests the efficiency of primitive spatial functions in RDF stores and the\\nperformance of the RDF stores in real use case scenarios, a more detailed\\nevaluation is performed using a synthetic workload and the scalability of the\\nRDF stores is stressed with the scalability workload. In total eight systems\\nare evaluated out of which six adequately support GeoSPARQL and two offer\\nlimited spatial support.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 5, 10, 51, 25, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An Effective Algorithm for Learning Single Occurrence Regular Expressions with Interleaving',\n",
       "  'authors': ['Yeting Li', 'Haiming Chen', 'Xiaolan Zhang', 'Lingqi Zhang'],\n",
       "  'summary': 'The advantages offered by the presence of a schema are numerous. However,\\nmany XML documents in practice are not accompanied by a (valid) schema, making\\nschema inference an attractive research problem. The fundamental task in XML\\nschema learning is inferring restricted subclasses of regular expressions. Most\\nprevious work either lacks support for interleaving or only has limited support\\nfor interleaving. In this paper, we first propose a new subclass Single\\nOccurrence Regular Expressions with Interleaving (SOIRE), which has\\nunrestricted support for interleaving. Then, based on single occurrence\\nautomaton and maximum independent set, we propose an algorithm iSOIRE to infer\\nSOIREs. Finally, we further conduct a series of experiments on real datasets to\\nevaluate the effectiveness of our work, comparing with both ongoing learning\\nalgorithms in academia and industrial tools in real-world. The results reveal\\nthe practicability of SOIRE and the effectiveness of iSOIRE, showing the high\\npreciseness and conciseness of our work.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 5, 15, 32, 22, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Increasing Transparent and Accountable Use of Data by Quantifying the Actual Privacy Risk in Interactive Record Linkage',\n",
       "  'authors': ['Qinbo Li', \"Adam G. D'Souza\", 'Cason Schmit', 'Hye-Chung Kum'],\n",
       "  'summary': 'Record linkage refers to the task of integrating data from two or more\\ndatabases without a common identifier. MINDFIRL (MInimum Necessary Disclosure\\nFor Interactive Record Linkage) is a software system that demonstrates the\\ntradeoff between utility and privacy in interactive record linkage. Due to the\\nneed to access personally identifiable information (PII) to accurately assess\\nwhether different records refer to the same person in heterogeneous databases,\\nprivacy is a major concern in interactive record linkage. MINDFIRL supports\\ninteractive record linkage while minimizing the privacy risk by (1) using\\npseudonyms to separate the identifying information from the sensitive\\ninformation, (2) dynamically disclosing only the minimum necessary information\\nincrementally, as needed on-demand at the point of decision, and (3) quantifies\\nthe risk due to the needed information disclosure to support transparency, the\\nreasoning, communication, and decisions on the privacy and utility trade off.\\nIn this paper we present an overview of the MINDFIRL system and the\\nk-Anonymized Privacy Risk (KAPR) score used to measure the privacy risk based\\non the disclosed information. We prove that KAPR score is a norm meeting all\\nthe desirable properties for a risk score for interactive record linkage.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 7, 22, 0, 8, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Countrywide Traffic Accident Dataset',\n",
       "  'authors': ['Sobhan Moosavi',\n",
       "   'Mohammad Hossein Samavatian',\n",
       "   'Srinivasan Parthasarathy',\n",
       "   'Rajiv Ramnath'],\n",
       "  'summary': 'Reducing traffic accidents is an important public safety challenge. However,\\nthe majority of studies on traffic accident analysis and prediction have used\\nsmall-scale datasets with limited coverage, which limits their impact and\\napplicability; and existing large-scale datasets are either private, old, or do\\nnot include important contextual information such as environmental stimuli\\n(weather, points-of-interest, etc.). In order to help the research community\\naddress these shortcomings we have - through a comprehensive process of data\\ncollection, integration, and augmentation - created a large-scale publicly\\navailable database of accident information named US-Accidents. US-Accidents\\ncurrently contains data about $2.25$ million instances of traffic accidents\\nthat took place within the contiguous United States, and over the last three\\nyears. Each accident record consists of a variety of intrinsic and contextual\\nattributes such as location, time, natural language description, weather,\\nperiod-of-day, and points-of-interest. We present this dataset in this paper,\\nalong with a wide range of insights gleaned from this dataset with respect to\\nthe spatiotemporal characteristics of accidents. The dataset is publicly\\navailable at https://smoosavi.org/datasets/us_accidents.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 12, 22, 26, 6, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Scalable Community Detection over Geo-Social Network',\n",
       "  'authors': ['Xiuwen Zheng', 'Qiyu Liu', 'Amarnath Gupta'],\n",
       "  'summary': 'We consider a community finding problem called Co-located Community Detection\\n(CCD) over geo-social networks, which retrieves communities that satisfy both\\nhigh structural tightness and spatial closeness constraints. To provide a\\nsolution that benefits from existing studies on community detection, we\\ndecouple the spatial constraint from graph structural constraint and propose a\\nuniform CCD framework which gives users the freedom to choose customized\\nmeasurements for social cohesiveness (e.g., $k$-core or $k$-truss). For the\\nspatial closeness constraint, we apply the bounded radius spatial constraint\\nand develop an exact algorithm together with effective pruning rules. To\\nfurther improve the efficiency and make our framework scale to a very large\\nscale of data, we propose a near-linear time approximation algorithm with a\\nconstant approximation ratio ($\\\\sqrt{2}$). We conduct extensive experiments on\\nboth synthetic and real-world datasets to demonstrate the efficiency and\\neffectiveness of our algorithms.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 13, 6, 50, 2, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Books Recommendation Approach Based on Online Bookstore Data',\n",
       "  'authors': ['Xinyu Wei', 'Jiahui Chen', 'Jing Chen', 'Bernie Liu'],\n",
       "  'summary': 'In the era of information explosion, facing complex information, it is\\ndifficult for users to choose the information of interest, and businesses also\\nneed detailed information on ways to let the ad stand out. By this time, it is\\nrecommended that a good way. We firstly by using random interviews,\\nsimulations, asking experts, summarizes methods outlined the main factors\\naffecting the scores of books that users drew. In order to further illustrate\\nthe impact of these factors, we also by combining the AHP consistency test,\\nthen fuzzy evaluation method, empowered each factor, influencing factors and\\nthe degree of influence come. For the second question, predict user evaluation\\nof the listed books from the predict annex. First, given the books Annex\\nlabels, user data extraction scorebooks and mathematical analysis of data\\nobtained from SPSS user preferences and then use software to nearest neighbor\\nanalysis to result in predicted value.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 15, 12, 32, 17, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Scalable Distributed Subtrajectory Clustering',\n",
       "  'authors': ['Panagiotis Tampakis',\n",
       "   'Nikos Pelekis',\n",
       "   'Christos Doulkeridis',\n",
       "   'Yannis Theodoridis'],\n",
       "  'summary': 'Trajectory clustering is an important operation of knowledge discovery from\\nmobility data. Especially nowadays, the need for performing advanced analytic\\noperations over massively produced data, such as mobility traces, in efficient\\nand scalable ways is imperative. However, discovering clusters of complete\\ntrajectories can overlook significant patterns that exist only for a small\\nportion of their lifespan. In this paper, we address the problem of Distributed\\nSubtrajectory Clustering in an efficient and highly scalable way. The problem\\nis challenging because the subtrajectories to be clustered are not known in\\nadvance, but they need to be discovered dynamically based on adjacent\\nsubtrajectories in space and time. Towards this objective, we split the\\noriginal problem to three sub-problems, namely Subtrajectory Join, Trajectory\\nSegmentation and Clustering and Outlier Detection, and deal with each one in a\\ndistributed fashion by utilizing the MapReduce programming model. The\\nefficiency and the effectiveness of our solution is demonstrated experimentally\\nover a synthetic and two large real datasets from the maritime and urban\\ndomains and through comparison with two state of the art subtrajectory\\nclustering algorithms.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 17, 11, 15, 21, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Low-resource Deep Entity Resolution with Transfer and Active Learning',\n",
       "  'authors': ['Jungo Kasai',\n",
       "   'Kun Qian',\n",
       "   'Sairam Gurajada',\n",
       "   'Yunyao Li',\n",
       "   'Lucian Popa'],\n",
       "  'summary': 'Entity resolution (ER) is the task of identifying different representations\\nof the same real-world entities across databases. It is a key step for\\nknowledge base creation and text mining. Recent adaptation of deep learning\\nmethods for ER mitigates the need for dataset-specific feature engineering by\\nconstructing distributed representations of entity records. While these methods\\nachieve state-of-the-art performance over benchmark data, they require large\\namounts of labeled data, which are typically unavailable in realistic ER\\napplications. In this paper, we develop a deep learning-based method that\\ntargets low-resource settings for ER through a novel combination of transfer\\nlearning and active learning. We design an architecture that allows us to learn\\na transferable model from a high-resource setting to a low-resource one. To\\nfurther adapt to the target dataset, we incorporate active learning that\\ncarefully selects a few informative examples to fine-tune the transferred\\nmodel. Empirical evaluation demonstrates that our method achieves comparable,\\nif not better, performance compared to state-of-the-art learning-based methods\\nwhile using an order of magnitude fewer labels.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 17, 20, 33, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Observing LOD using Equivalent Set Graphs: it is mostly flat and sparsely linked',\n",
       "  'authors': ['Luigi Asprino',\n",
       "   'Wouter Beek',\n",
       "   'Paolo Ciancarini',\n",
       "   'Frank van Harmelen',\n",
       "   'Valentina Presutti'],\n",
       "  'summary': 'This paper presents an empirical study aiming at understanding the modeling\\nstyle and the overall semantic structure of Linked Open Data. We observe how\\nclasses, properties and individuals are used in practice. We also investigate\\nhow hierarchies of concepts are structured, and how much they are linked. In\\naddition to discussing the results, this paper contributes (i) a conceptual\\nframework, including a set of metrics, which generalises over the observable\\nconstructs; (ii) an open source implementation that facilitates its application\\nto other Linked Data knowledge graphs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 19, 13, 48, 8, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Efficient privacy preservation of big data for accurate data mining',\n",
       "  'authors': ['M. A. P. Chamikara',\n",
       "   'P. Bertok',\n",
       "   'D. Liu',\n",
       "   'S. Camtepe',\n",
       "   'I. Khalil'],\n",
       "  'summary': 'Computing technologies pervade physical spaces and human lives, and produce a\\nvast amount of data that is available for analysis. However, there is a growing\\nconcern that potentially sensitive data may become public if the collected data\\nare not appropriately sanitized before being released for investigation.\\nAlthough there are more than a few privacy-preserving methods available, they\\nare not efficient, scalable or have problems with data utility, and/or privacy.\\nThis paper addresses these issues by proposing an efficient and scalable\\nnonreversible perturbation algorithm, PABIDOT, for privacy preservation of big\\ndata via optimal geometric transformations. PABIDOT was tested for efficiency,\\nscalability, resistance, and accuracy using nine datasets and five\\nclassification algorithms. Experiments show that PABIDOT excels in execution\\nspeed, scalability, attack resistance and accuracy in large-scale\\nprivacy-preserving data classification when compared with two other, related\\nprivacy-preserving algorithms.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 19, 15, 23, 41, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Layered Aggregate Engine for Analytics Workloads',\n",
       "  'authors': ['Maximilian Schleich',\n",
       "   'Dan Olteanu',\n",
       "   'Mahmoud Abo Khamis',\n",
       "   'Hung Q. Ngo',\n",
       "   'XuanLong Nguyen'],\n",
       "  'summary': 'This paper introduces LMFAO (Layered Multiple Functional Aggregate\\nOptimization), an in-memory optimization and execution engine for batches of\\naggregates over the input database. The primary motivation for this work stems\\nfrom the observation that for a variety of analytics over databases, their\\ndata-intensive tasks can be decomposed into group-by aggregates over the join\\nof the input database relations. We exemplify the versatility and\\ncompetitiveness of LMFAO for a handful of widely used analytics: learning ridge\\nlinear regression, classification trees, regression trees, and the structure of\\nBayesian networks using Chow-Liu trees; and data cubes used for exploration in\\ndata warehousing.\\n  LMFAO consists of several layers of logical and code optimizations that\\nsystematically exploit sharing of computation, parallelism, and code\\nspecialization.\\n  We conducted two types of performance benchmarks. In experiments with four\\ndatasets, LMFAO outperforms by several orders of magnitude on one hand, a\\ncommercial database system and MonetDB for computing batches of aggregates, and\\non the other hand, TensorFlow, Scikit, R, and AC/DC for learning a variety of\\nmodels over databases.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 20, 15, 20, 20, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Database Meets Deep Learning: Challenges and Opportunities',\n",
       "  'authors': ['Wei Wang',\n",
       "   'Meihui Zhang',\n",
       "   'Gang Chen',\n",
       "   'H. V. Jagadish',\n",
       "   'Beng Chin Ooi',\n",
       "   'Kian-Lee Tan'],\n",
       "  'summary': 'Deep learning has recently become very popular on account of its incredible\\nsuccess in many complex data-driven applications, such as image classification\\nand speech recognition. The database community has worked on data-driven\\napplications for many years, and therefore should be playing a lead role in\\nsupporting this new wave. However, databases and deep learning are different in\\nterms of both techniques and applications. In this paper, we discuss research\\nproblems at the intersection of the two fields. In particular, we discuss\\npossible improvements for deep learning systems from a database perspective,\\nand analyze database applications that may benefit from deep learning\\ntechniques.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 21, 7, 26, 31, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Comparative Survey of Recent Natural Language Interfaces for Databases',\n",
       "  'authors': ['Katrin Affolter', 'Kurt Stockinger', 'Abraham Bernstein'],\n",
       "  'summary': 'Over the last few years natural language interfaces (NLI) for databases have\\ngained significant traction both in academia and industry. These systems use\\nvery different approaches as described in recent survey papers. However, these\\nsystems have not been systematically compared against a set of benchmark\\nquestions in order to rigorously evaluate their functionalities and expressive\\npower.\\n  In this paper, we give an overview over 24 recently developed NLIs for\\ndatabases. Each of the systems is evaluated using a curated list of ten sample\\nquestions to show their strengths and weaknesses. We categorize the NLIs into\\nfour groups based on the methodology they are using: keyword-, pattern-,\\nparsing-, and grammar-based NLI. Overall, we learned that keyword-based systems\\nare enough to answer simple questions. To solve more complex questions\\ninvolving subqueries, the system needs to apply some sort of parsing to\\nidentify structural dependencies. Grammar-based systems are overall the most\\npowerful ones, but are highly dependent on their manually designed rules. In\\naddition to providing a systematic analysis of the major systems, we derive\\nlessons learned that are vital for designing NLIs that can answer a wide range\\nof user questions.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 21, 7, 49, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Explainable Fact Checking with Probabilistic Answer Set Programming',\n",
       "  'authors': ['Naser Ahmadi',\n",
       "   'Joohyung Lee',\n",
       "   'Paolo Papotti',\n",
       "   'Mohammed Saeed'],\n",
       "  'summary': 'One challenge in fact checking is the ability to improve the transparency of\\nthe decision. We present a fact checking method that uses reference information\\nin knowledge graphs (KGs) to assess claims and explain its decisions. KGs\\ncontain a formal representation of knowledge with semantic descriptions of\\nentities and their relationships. We exploit such rich semantics to produce\\ninterpretable explanations for the fact checking output. As information in a KG\\nis inevitably incomplete, we rely on logical rule discovery and on Web text\\nmining to gather the evidence to assess a given claim. Uncertain rules and\\nfacts are turned into logical programs and the checking task is modeled as an\\ninference problem in a probabilistic extension of answer set programs.\\nExperiments show that the probabilistic inference enables the efficient\\nlabeling of claims with interpretable explanations, and the quality of the\\nresults is higher than state of the art baselines.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 21, 15, 35, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'On Performance Stability in LSM-based Storage Systems (Extended Version)',\n",
       "  'authors': ['Chen Luo', 'Michael J. Carey'],\n",
       "  'summary': 'The Log-Structured Merge-Tree (LSM-tree) has been widely adopted for use in\\nmodern NoSQL systems for its superior write performance. Despite the popularity\\nof LSM-trees, they have been criticized for suffering from write stalls and\\nlarge performance variances due to the inherent mismatch between their fast\\nin-memory writes and slow background I/O operations. In this paper, we use a\\nsimple yet effective two-phase experimental approach to evaluate write stalls\\nfor various LSM-tree designs. We further explore the design choices of LSM\\nmerge schedulers to minimize write stalls given an I/O bandwidth budget. We\\nhave conducted extensive experiments in the context of the Apache AsterixDB\\nsystem and we present the results here.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 23, 22, 57, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Bag Query Containment and Information Theory',\n",
       "  'authors': ['Mahmoud Abo Khamis',\n",
       "   'Phokion G. Kolaitis',\n",
       "   'Hung Q. Ngo',\n",
       "   'Dan Suciu'],\n",
       "  'summary': 'The query containment problem is a fundamental algorithmic problem in data\\nmanagement. While this problem is well understood under set semantics, it is by\\nfar less understood under bag semantics. In particular, it is a long-standing\\nopen question whether or not the conjunctive query containment problem under\\nbag semantics is decidable. We unveil tight connections between information\\ntheory and the conjunctive query containment under bag semantics. These\\nconnections are established using information inequalities, which are\\nconsidered to be the laws of information theory. Our first main result asserts\\nthat deciding the validity of maxima of information inequalities is many-one\\nequivalent to the restricted case of conjunctive query containment in which the\\ncontaining query is acyclic; thus, either both these problems are decidable or\\nboth are undecidable. Our second main result identifies a new decidable case of\\nthe conjunctive query containment problem under bag semantics. Specifically, we\\ngive an exponential time algorithm for conjunctive query containment under bag\\nsemantics, provided the containing query is chordal and admits a simple\\njunction tree.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 24, 5, 20, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Datalog Materialisation in Distributed RDF Stores with Dynamic Data Exchange',\n",
       "  'authors': ['Temitope Ajileye', 'Boris Motik', 'Ian Horrocks'],\n",
       "  'summary': 'Several centralised RDF systems support datalog reasoning by precomputing and\\nstoring all logically implied triples using the wellknown seminaive algorithm.\\nLarge RDF datasets often exceed the capacity of centralised RDF systems, and a\\ncommon solution is to distribute the datasets in a cluster of shared-nothing\\nservers. While numerous distributed query answering techniques are known,\\ndistributed seminaive evaluation of arbitrary datalog rules is less understood.\\nIn fact, most distributed RDF stores either support no reasoning or can handle\\nonly limited datalog fragments. In this paper we extend the dynamic data\\nexchange approach for distributed query answering by Potter et al. [12] to a\\nreasoning algorithm that can handle arbitrary rules while preserving important\\nproperties such as nonrepetition of inferences. We also show empirically that\\nour algorithm scales well to very large RDF datasets',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 24, 22, 52, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Survey and Experimental Analysis of Distributed Subgraph Matching',\n",
       "  'authors': ['Longbin Lai',\n",
       "   'Zhu Qing',\n",
       "   'Zhengyi Yang',\n",
       "   'Xin Jin',\n",
       "   'Zhengmin Lai',\n",
       "   'Ran Wang',\n",
       "   'Kongzhang Hao',\n",
       "   'Xuemin Lin',\n",
       "   'Lu Qin',\n",
       "   'Wenjie Zhang',\n",
       "   'Ying Zhang',\n",
       "   'Zhengping Qian',\n",
       "   'Jingren Zhou'],\n",
       "  'summary': 'Recently there emerge many distributed algorithms that aim at solving\\nsubgraph matching at scale. Existing algorithm-level comparisons failed to\\nprovide a systematic view to the pros and cons of each algorithm mainly due to\\nthe intertwining of strategy and optimization. In this paper, we identify four\\nstrategies and three general-purpose optimizations from representative\\nstate-of-the-art works. We implement the four strategies with the optimizations\\nbased on the common Timely dataflow system for systematic strategy-level\\ncomparison. Our implementation covers all representation algorithms. We conduct\\nextensive experiments for both unlabelled matching and labelled matching to\\nanalyze the performance of distributed subgraph matching under various\\nsettings, which is finally summarized as a practical guide.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 27, 9, 38, 46, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Pruned Landmark Labeling Meets Vertex Centric Computation: A Surprisingly Happy Marriage!',\n",
       "  'authors': ['Ruoming Jin',\n",
       "   'Zhen Peng',\n",
       "   'Wendell Wu',\n",
       "   'Feodor Dragan',\n",
       "   'Gagan Agrawal',\n",
       "   'Bin Ren'],\n",
       "  'summary': 'In this paper, we study how the Pruned Landmark Labeling (PPL) algorithm can\\nbe parallelized in a scalable fashion, producing the same results as the\\nsequential algorithm. More specifically, we parallelize using a Vertex-Centric\\n(VC) computational model on a modern SIMD powered multicore architecture. We\\ndesign a new VC-PLL algorithm that resolves the apparent mismatch between the\\ninherent sequential dependence of the PLL algorithm and the Vertex- Centric\\n(VC) computing model. Furthermore, we introduce a novel batch execution model\\nfor VC computation and the BVC-PLL algorithm to reduce the computational\\ninefficiency in VC-PLL. Quite surprisingly, the theoretical analysis reveals\\nthat under a reasonable assumption, BVC-PLL has lower computational and memory\\naccess costs than PLL and indicates it may run faster than PLL as a sequential\\nalgorithm. We also demonstrate how BVC-PLL algorithm can be extended to handle\\ndirected graphs and weighted graphs and how it can utilize the hierarchical\\nparallelism on a modern parallel computing architecture. Extensive experiments\\non real-world graphs not only show the sequential BVC-PLL can run more than two\\ntimes faster than the original PLL, but also demonstrates its parallel\\nefficiency and scalability.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 28, 2, 19, 19, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Semantic-Rich Similarity Measure in Heterogeneous Information Networks',\n",
       "  'authors': ['Yu Zhou', 'Jianbin Huang', 'Heli Sun'],\n",
       "  'summary': 'Measuring the similarities between objects in information networks has\\nfundamental importance in recommendation systems, clustering and web search.\\nThe existing metrics depend on the meta path or meta structure specified by\\nusers. In this paper, we propose a stratified meta structure based similarity\\n$SMSS$ in heterogeneous information networks. The stratified meta structure can\\nbe constructed automatically and capture rich semantics. Then, we define the\\ncommuting matrix of the stratified meta structure by virtue of the commuting\\nmatrices of meta paths and meta structures. As a result, $SMSS$ is defined by\\nvirtue of these commuting matrices. Experimental evaluations show that the\\nproposed $SMSS$ on the whole outperforms the state-of-the-art metrics in terms\\nof ranking and clustering.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 2, 1, 22, 48, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Graph Pattern Matching for Dynamic Team Formation',\n",
       "  'authors': ['Shuai Ma',\n",
       "   'Jia Li',\n",
       "   'Chunming Hu',\n",
       "   'Xudong Liu',\n",
       "   'Jinpeng Huai'],\n",
       "  'summary': 'Finding a list of k teams of experts, referred to as top-k team formation,\\nwith the required skills and high collaboration compatibility has been\\nextensively studied. However, existing methods have not considered the specific\\ncollaboration relationships among different team members, i.e., structural\\nconstraints, which are typically needed in practice. In this study, we first\\npropose a novel graph pattern matching approach for top-k team formation, which\\nincorporates both structural constraints and capacity bounds. Second, we\\nformulate and study the dynamic top-k team formation problem due to the growing\\nneed of a dynamic environment. Third, we develop an unified incremental\\napproach, together with an optimization technique, to handle continuous pattern\\nand data updates, separately and simultaneously, which has not been explored\\nbefore. Finally, using real-life and synthetic data, we conduct an extensive\\nexperimental study to show the effectiveness and efficiency of our graph\\npattern matching approach for (dynamic) top-k team formation.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 3, 14, 24, 8, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Eliciting Worker Preference for Task Completion',\n",
       "  'authors': ['Mohammadreza Esfandiari',\n",
       "   'Senjuti Basu Roy',\n",
       "   'Sihem Amer-Yahia'],\n",
       "  'summary': 'Current crowdsourcing platforms provide little support for worker feedback.\\nWorkers are sometimes invited to post free text describing their experience and\\npreferences in completing tasks. They can also use forums such as Turker\\nNation1 to exchange preferences on tasks and requesters. In fact, crowdsourcing\\nplatforms rely heavily on observing workers and inferring their preferences\\nimplicitly. In this work, we believe that asking workers to indicate their\\npreferences explicitly improve their experience in task completion and hence,\\nthe quality of their contributions. Explicit elicitation can indeed help to\\nbuild more accurate worker models for task completion that captures the\\nevolving nature of worker preferences. We design a worker model whose accuracy\\nis improved iteratively by requesting preferences for task factors such as\\nrequired skills, task payment, and task relevance. We propose a generic\\nframework, develop efficient solutions in realistic scenarios, and run\\nextensive experiments that show the benefit of explicit preference elicitation\\nover implicit ones with statistical significance.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 10, 3, 55, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Focus: Querying Large Video Datasets with Low Latency and Low Cost',\n",
       "  'authors': ['Kevin Hsieh',\n",
       "   'Ganesh Ananthanarayanan',\n",
       "   'Peter Bodik',\n",
       "   'Paramvir Bahl',\n",
       "   'Matthai Philipose',\n",
       "   'Phillip B. Gibbons',\n",
       "   'Onur Mutlu'],\n",
       "  'summary': 'Large volumes of videos are continuously recorded from cameras deployed for\\ntraffic control and surveillance with the goal of answering \"after the fact\"\\nqueries: identify video frames with objects of certain classes (cars, bags)\\nfrom many days of recorded video. While advancements in convolutional neural\\nnetworks (CNNs) have enabled answering such queries with high accuracy, they\\nare too expensive and slow. We build Focus, a system for low-latency and\\nlow-cost querying on large video datasets. Focus uses cheap ingestion\\ntechniques to index the videos by the objects occurring in them. At\\ningest-time, it uses compression and video-specific specialization of CNNs.\\nFocus handles the lower accuracy of the cheap CNNs by judiciously leveraging\\nexpensive CNNs at query-time. To reduce query time latency, we cluster similar\\nobjects and hence avoid redundant processing. Using experiments on video\\nstreams from traffic, surveillance and news channels, we see that Focus uses\\n58X fewer GPU cycles than running expensive ingest processors and is 37X faster\\nthan processing all the video at query time.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 10, 18, 52, 25, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Cobra: A Framework for Cost Based Rewriting of Database Applications',\n",
       "  'authors': ['K. Venkatesh Emani', 'S. Sudarshan'],\n",
       "  'summary': 'Database applications are typically written using a mixture of imperative\\nlanguages and declarative frameworks for data processing. Application logic\\ngets distributed across the declarative and imperative parts of a program.\\nOften, there is more than one way to implement the same program, whose\\nefficiency may depend on a number of parameters. In this paper, we propose a\\nframework that automatically generates all equivalent alternatives of a given\\nprogram using a given set of program transformations, and chooses the least\\ncost alternative. We use the concept of program regions as an algebraic\\nabstraction of a program and extend the Volcano/Cascades framework for\\noptimization of algebraic expressions, to optimize programs. We illustrate the\\nuse of our framework for optimizing database applications. We show through\\nexperimental results, that our framework has wide applicability in real world\\napplications and provides significant performance benefits.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 15, 17, 58, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An Integration-Oriented Ontology to Govern Evolution in Big Data Ecosystems',\n",
       "  'authors': ['Sergi Nadal',\n",
       "   'Oscar Romero',\n",
       "   'Alberto Abelló',\n",
       "   'Panos Vassiliadis',\n",
       "   'Stijn Vansummeren'],\n",
       "  'summary': 'Big Data architectures allow to flexibly store and process heterogeneous\\ndata, from multiple sources, in their original format. The structure of those\\ndata, commonly supplied by means of REST APIs, is continuously evolving. Thus\\ndata analysts need to adapt their analytical processes after each API release.\\nThis gets more challenging when performing an integrated or historical\\nanalysis. To cope with such complexity, in this paper, we present the Big Data\\nIntegration ontology, the core construct to govern the data integration process\\nunder schema evolution by systematically annotating it with information\\nregarding the schema of the sources. We present a query rewriting algorithm\\nthat, using the annotated ontology, converts queries posed over the ontology to\\nqueries over the sources. To cope with syntactic evolution in the sources, we\\npresent an algorithm that semi-automatically adapts the ontology upon new\\nreleases. This guarantees ontology-mediated queries to correctly retrieve data\\nfrom the most recent schema version as well as correctness in historical\\nqueries. A functional and performance evaluation on real-world APIs is\\nperformed to validate our approach.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 16, 8, 55, 41, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'One-Pass Trajectory Simplification Using the Synchronous Euclidean Distance',\n",
       "  'authors': ['Xuelian Lin',\n",
       "   'Jiahao Jiang',\n",
       "   'Shuai Ma',\n",
       "   'Yimeng Zuo',\n",
       "   'Chunming Hu'],\n",
       "  'summary': 'Various mobile devices have been used to collect, store and transmit\\ntremendous trajectory data, and it is known that raw trajectory data seriously\\nwastes the storage, network band and computing resource. To attack this issue,\\none-pass line simplification (LS) algorithms have are been developed, by\\ncompressing data points in a trajectory to a set of continuous line segments.\\nHowever, these algorithms adopt the perpendicular Euclidean distance, and none\\nof them uses the synchronous Euclidean distance (SED), and cannot support\\nspatio-temporal queries. To do this, we develop two one-pass error bounded\\ntrajectory simplification algorithms (CISED-S and CISED-W) using SED, based on\\na novel spatio-temporal cone intersection technique. Using four real-life\\ntrajectory datasets, we experimentally show that our approaches are both\\nefficient and effective. In terms of running time, algorithms CISED-S and\\nCISED-W are on average 3 times faster than SQUISH-E (the most efficient\\nexisting LS algorithm using SED). In terms of compression ratios, algorithms\\nCISED-S and CISED-W are comparable with and 19.6% better than DPSED (the most\\neffective existing LS algorithm using SED) on average, respectively, and are\\n21.1% and 42.4% better than SQUISH-E on average, respectively.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 12, 9, 36, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Query2Vec: An Evaluation of NLP Techniques for Generalized Workload Analytics',\n",
       "  'authors': ['Shrainik Jain', 'Bill Howe', 'Jiaqi Yan', 'Thierry Cruanes'],\n",
       "  'summary': 'We consider methods for learning vector representations of SQL queries to\\nsupport generalized workload analytics tasks, including workload summarization\\nfor index selection and predicting queries that will trigger memory errors. We\\nconsider vector representations of both raw SQL text and optimized query plans,\\nand evaluate these methods on synthetic and real SQL workloads. We find that\\ngeneral algorithms based on vector representations can outperform existing\\napproaches that rely on specialized features. For index recommendation, we\\ncluster the vector representations to compress large workloads with no loss in\\nperformance from the recommended index. For error prediction, we train a\\nclassifier over learned vectors that can automatically relate subtle syntactic\\npatterns with specific errors raised during query execution. Surprisingly, we\\nalso find that these methods enable transfer learning, where a model trained on\\none SQL corpus can be applied to an unrelated corpus and still enable good\\nperformance. We find that these general approaches, when trained on a large\\ncorpus of SQL queries, provides a robust foundation for a variety of workload\\nanalysis tasks and database features, without requiring application-specific\\nfeature engineering.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 17, 10, 21, 49, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Computing Possible and Certain Answers over Order-Incomplete Data',\n",
       "  'authors': ['Antoine Amarilli',\n",
       "   'Mouhamadou Lamine Ba',\n",
       "   'Daniel Deutch',\n",
       "   'Pierre Senellart'],\n",
       "  'summary': 'This paper studies the complexity of query evaluation for databases whose\\nrelations are partially ordered; the problem commonly arises when combining or\\ntransforming ordered data from multiple sources. We focus on queries in a\\nuseful fragment of SQL, namely positive relational algebra with aggregates,\\nwhose bag semantics we extend to the partially ordered setting. Our semantics\\nleads to the study of two main computational problems: the possibility and\\ncertainty of query answers. We show that these problems are respectively\\nNP-complete and coNP-complete, but identify tractable cases depending on the\\nquery operators or input partial orders. We further introduce a duplicate\\nelimination operator and study its effect on the complexity results.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 19, 13, 20, 56, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'RAQ: Relationship-Aware Graph Querying in Large Networks',\n",
       "  'authors': ['Jithin Vachery',\n",
       "   'Akhil Arora',\n",
       "   'Sayan Ranu',\n",
       "   'Arnab Bhattacharya'],\n",
       "  'summary': 'The phenomenal growth of graph data from a wide variety of real-world\\napplications has rendered graph querying to be a problem of paramount\\nimportance. Traditional techniques use structural as well as node similarities\\nto find matches of a given query graph in a (large) target graph. However,\\nalmost all existing techniques have tacitly ignored the presence of\\nrelationships in graphs, which are usually encoded through interactions between\\nnode and edge labels. In this paper, we propose RAQ -- Relationship-Aware Graph\\nQuerying, to mitigate this gap. Given a query graph, RAQ identifies the $k$\\nbest matching subgraphs of the target graph that encode similar relationships\\nas in the query graph. To assess the utility of RAQ as a graph querying\\nparadigm for knowledge discovery and exploration tasks, we perform a user\\nsurvey on the Internet Movie Database (IMDb), where an overwhelming 86% of the\\n170 surveyed users preferred the relationship-aware match over traditional\\ngraph querying. The need to perform subgraph isomorphism renders RAQ NP-hard.\\nThe querying is made practical through beam stack search. Extensive experiments\\non multiple real-world graph datasets demonstrate RAQ to be effective,\\nefficient, and scalable.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 19, 13, 50, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'PRESTO: Probabilistic Cardinality Estimation for RDF Queries Based on Subgraph Overlapping',\n",
       "  'authors': ['Xin Wang',\n",
       "   'Eugene Siow',\n",
       "   'Aastha Madaan',\n",
       "   'Thanassis Tiropanis'],\n",
       "  'summary': 'In query optimisation accurate cardinality estimation is essential for\\nfinding optimal query plans. It is especially challenging for RDF due to the\\nlack of explicit schema and the excessive occurrence of joins in RDF queries.\\nExisting approaches typically collect statistics based on the counts of triples\\nand estimate the cardinality of a query as the product of its join components,\\nwhere errors can accumulate even when the estimation of each component is\\naccurate. As opposed to existing methods, we propose PRESTO, a cardinality\\nestimation method that is based on the counts of subgraphs instead of triples\\nand uses a probabilistic method to estimate cardinalities of RDF queries as a\\nwhole. PRESTO avoids some major issues of existing approaches and is able to\\naccurately estimate arbitrary queries under a bound memory constraint. We\\nevaluate PRESTO with YAGO and show that PRESTO is more accurate for both simple\\nand complex queries.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 19, 14, 11, 45, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An Efficient Density-based Clustering Algorithm for Higher-Dimensional Data',\n",
       "  'authors': ['Thapana Boonchoo', 'Xiang Ao', 'Qing He'],\n",
       "  'summary': 'DBSCAN is a typically used clustering algorithm due to its clustering ability\\nfor arbitrarily-shaped clusters and its robustness to outliers. Generally, the\\ncomplexity of DBSCAN is O(n^2) in the worst case, and it practically becomes\\nmore severe in higher dimension. Grid-based DBSCAN is one of the recent\\nimproved algorithms aiming at facilitating efficiency. However, the performance\\nof grid-based DBSCAN still suffers from two problems: neighbour explosion and\\nredundancies in merging, which make the algorithms infeasible in\\nhigh-dimensional space. In this paper, we propose a novel algorithm named GDPAM\\nattempting to extend Grid-based DBSCAN to higher data dimension. In GDPAM, a\\nbitmap indexing is utilized to manage non-empty grids so that the neighbour\\ngrid queries can be performed efficiently. Furthermore, we adopt an efficient\\nunion-find algorithm to maintain the clustering information in order to reduce\\nredundancies in the merging. The experimental results on both real-world and\\nsynthetic datasets demonstrate that the proposed algorithm outperforms the\\nstate-of-the-art exact/approximate DBSCAN and suggests a good scalability.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 22, 6, 35, 17, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'ACGreGate: A Framework for Practical Access Control for Applications using Weakly Consistent Databases',\n",
       "  'authors': ['Mathias Weber', 'Annette Bieniusa'],\n",
       "  'summary': 'Scalable and highly available systems often require data stores that offer\\nweaker consistency guarantees than traditional relational databases systems.\\nThe correctness of these applications highly depends on the resilience of the\\napplication model against data inconsistencies. In particular regarding\\napplication security, it is difficult to determine which inconsistencies can be\\ntolerated and which might lead to security breaches.\\n  In this paper, we discuss the problem of how to develop an access control\\nlayer for applications using weakly consistent data stores without loosing the\\nperformance benefits gained by using weaker consistency models. We present\\nACGreGate, a Java framework for implementing correct access control layers for\\napplications using weakly consistent data stores. Under certain requirements on\\nthe data store, ACGreGate ensures that the access control layer operates\\ncorrectly with respect to dynamically adaptable security policies. We used\\nACGreGate to implement the access control layer of a student management system.\\nThis case study shows that practically useful security policies can be\\nimplemented with the framework incurring little overhead. A comparison with a\\nsetup using a centralized server shows the benefits of using ACGreGate for\\nscalability of the service to geo-scale.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 22, 9, 23, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'TritanDB: Time-series Rapid Internet of Things Analytics',\n",
       "  'authors': ['Eugene Siow', 'Thanassis Tiropanis', 'Xin Wang', 'Wendy Hall'],\n",
       "  'summary': 'The efficient management of data is an important prerequisite for realising\\nthe potential of the Internet of Things (IoT). Two issues given the large\\nvolume of structured time-series IoT data are, addressing the difficulties of\\ndata integration between heterogeneous Things and improving ingestion and query\\nperformance across databases on both resource-constrained Things and in the\\ncloud. In this paper, we examine the structure of public IoT data and discover\\nthat the majority exhibit unique flat, wide and numerical characteristics with\\na mix of evenly and unevenly-spaced time-series. We investigate the advances in\\ntime-series databases for telemetry data and combine these findings with\\nmicrobenchmarks to determine the best compression techniques and storage data\\nstructures to inform the design of a novel solution optimised for IoT data. A\\nquery translation method with low overhead even on resource-constrained Things\\nallows us to utilise rich data models like the Resource Description Framework\\n(RDF) for interoperability and data integration on top of the optimised\\nstorage. Our solution, TritanDB, shows an order of magnitude performance\\nimprovement across both Things and cloud hardware on many state-of-the-art\\ndatabases within IoT scenarios. Finally, we describe how TritanDB supports\\nvarious analyses of IoT time-series data like forecasting.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 24, 12, 10, 46, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Time Constrained Continuous Subgraph Search over Streaming Graphs',\n",
       "  'authors': ['Youhuan Li', 'Lei Zou', 'M. Tamer Ozsu', 'Dongyan Zhao'],\n",
       "  'summary': 'The growing popularity of dynamic applications such as social networks\\nprovides a promising way to detect valuable information in real time. Efficient\\nanalysis over high-speed data from dynamic applications is of great\\nsignificance. Data from these dynamic applications can be easily modeled as\\nstreaming graph. In this paper, we study the subgraph (isomorphism) search over\\nstreaming graph data that obeys timing order constraints over the occurrence of\\nedges in the stream. We propose a data structure and algorithm to efficiently\\nanswer subgraph search and introduce optimizations to greatly reduce the space\\ncost, and propose concurrency management to improve system throughput.\\nExtensive experiments on real network traffic data and synthetic social\\nstreaming data confirms the efficiency and effectiveness of our solution.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 28, 14, 43, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Killing Two Birds with One Stone -- Querying Property Graphs using SPARQL via GREMLINATOR',\n",
       "  'authors': ['Harsh Thakkar',\n",
       "   'Dharmen Punjani',\n",
       "   'Jens Lehmann',\n",
       "   'Sören Auer'],\n",
       "  'summary': 'Knowledge graphs have become popular over the past decade and frequently rely\\non the Resource Description Framework (RDF) or Property Graph (PG) databases as\\ndata models. However, the query languages for these two data models -- SPARQL\\nfor RDF and the PG traversal language Gremlin -- are lacking interoperability.\\nWe present Gremlinator, the first translator from SPARQL -- the W3C\\nstandardized language for RDF -- and Gremlin -- a popular property graph\\ntraversal language. Gremlinator translates SPARQL queries to Gremlin path\\ntraversals for executing graph pattern matching queries over graph databases.\\nThis allows a user, who is well versed in SPARQL, to access and query a wide\\nvariety of Graph Data Management Systems (DMSs) avoiding the steep learning\\ncurve for adapting to a new Graph Query Language (GQL). Gremlin is a graph\\ncomputing system-agnostic traversal language (covering both OLTP graph database\\nor OLAP graph processors), making it a desirable choice for supporting\\ninteroperability for querying Graph DMSs. Gremlinator currently supports the\\ntranslation of a subset of SPARQL 1.0, specifically the SPARQL SELECT queries.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 25, 23, 15, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Estimating the Cardinality of Conjunctive Queries over RDF Data Using Graph Summarisation',\n",
       "  'authors': ['Giorgio Stefanoni', 'Boris Motik', 'Egor V. Kostylev'],\n",
       "  'summary': 'Estimating the cardinality (i.e., the number of answers) of conjunctive\\nqueries is particularly difficult in RDF systems: queries over RDF data are\\nnavigational and thus tend to involve many joins. We present a new, principled\\ncardinality estimation technique based on graph summarisation. We interpret a\\nsummary of an RDF graph using a possible world semantics and formalise the\\nestimation problem as computing the expected cardinality over all RDF graphs\\nrepresented by the summary, and we present a closed-form formula for computing\\nthe expectation of arbitrary queries. We also discuss approaches to RDF graph\\nsummarisation. Finally, we show empirically that our cardinality technique is\\nmore accurate and more consistent, often by orders of magnitude, than the state\\nof the art.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 29, 16, 48, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Automatically Leveraging MapReduce Frameworks for Data-Intensive Applications',\n",
       "  'authors': ['Maaz Bin Safeer Ahmad', 'Alvin Cheung'],\n",
       "  'summary': 'MapReduce is a popular programming paradigm for developing large-scale,\\ndata-intensive computation. Many frameworks that implement this paradigm have\\nrecently been developed. To leverage these frameworks, however, developers must\\nbecome familiar with their APIs and rewrite existing code. Casper is a new tool\\nthat automatically translates sequential Java programs into the MapReduce\\nparadigm. Casper identifies potential code fragments to rewrite and translates\\nthem in two steps: (1) Casper uses program synthesis to search for a program\\nsummary (i.e., a functional specification) of each code fragment. The summary\\nis expressed using a high-level intermediate language resembling the MapReduce\\nparadigm and verified to be semantically equivalent to the original using a\\ntheorem prover. (2) Casper generates executable code from the summary, using\\neither the Hadoop, Spark, or Flink API. We evaluated Casper by automatically\\nconverting real-world, sequential Java benchmarks to MapReduce. The resulting\\nbenchmarks perform up to 48.2x faster compared to the original.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 1, 30, 0, 2, 57, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Anomaly Detection in Log Data using Graph Databases and Machine Learning to Defend Advanced Persistent Threats',\n",
       "  'authors': ['Timo Schindler'],\n",
       "  'summary': 'Advanced Persistent Threats (APTs) are a main impendence in cyber security of\\ncomputer networks. In 2015, a successful breach remains undetected 146 days on\\naverage, reported by [Fi16].With our work we demonstrate a feasible and fast\\nway to analyse real world log data to detect breaches or breach attempts. By\\nadapting well-known kill chain mechanisms and a combine of a time series\\ndatabase and an abstracted graph approach, it is possible to create flexible\\nattack profiles. Using this approach, it can be demonstrated that the graph\\nanalysis successfully detects simulated attacks by analysing the log data of a\\nsimulated computer network. Considering another source for log data, the\\nframework is capable to deliver sufficient performance for analysing real-world\\ndata in short time. By using the computing power of the graph database it is\\npossible to identify the attacker and furthermore it is feasible to detect\\nother affected system components. We believe to significantly reduce the\\ndetection time of breaches with this approach and react fast to new attack\\nvectors.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 2, 1, 12, 17, 54, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Distributed Clustering Algorithm for Spatial Data Mining',\n",
       "  'authors': ['Malika Bendechache', 'M-Tahar Kechadi'],\n",
       "  'summary': 'Distributed data mining techniques and mainly distributed clustering are\\nwidely used in the last decade because they deal with very large and\\nheterogeneous datasets which cannot be gathered centrally. Current distributed\\nclustering approaches are normally generating global models by aggregating\\nlocal results that are obtained on each site. While this approach mines the\\ndatasets on their locations the aggregation phase is complex, which may produce\\nincorrect and ambiguous global clusters and therefore incorrect knowledge. In\\nthis paper we propose a new clustering approach for very large spatial datasets\\nthat are heterogeneous and distributed. The approach is based on K-means\\nAlgorithm but it generates the number of global clusters dynamically. Moreover,\\nthis approach uses an elaborated aggregation phase. The aggregation phase is\\ndesigned in such a way that the overall process is efficient in time and memory\\nallocation. Preliminary results show that the proposed approach produces high\\nquality results and scales up well. We also compared it to two popular\\nclustering algorithms and show that this approach is much more efficient.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 2, 1, 14, 41, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Hierarchical Aggregation Approach for Distributed clustering of spatial datasets',\n",
       "  'authors': ['Malika Bendechache', 'Nhien-An Le-Khac', 'M-Tahar Kechadi'],\n",
       "  'summary': 'In this paper, we present a new approach of distributed clustering for\\nspatial datasets, based on an innovative and efficient aggregation technique.\\nThis distributed approach consists of two phases: 1) local clustering phase,\\nwhere each node performs a clustering on its local data, 2) aggregation phase,\\nwhere the local clusters are aggregated to produce global clusters. This\\napproach is characterised by the fact that the local clusters are represented\\nin a simple and efficient way. And The aggregation phase is designed in such a\\nway that the final clusters are compact and accurate while the overall process\\nis efficient in both response time and memory allocation. We evaluated the\\napproach with different datasets and compared it to well-known clustering\\ntechniques. The experimental results show that our approach is very promising\\nand outperforms all those algorithms',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 2, 1, 12, 50, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Axiomatic Foundations and Algorithms for Deciding Semantic Equivalences of SQL Queries',\n",
       "  'authors': ['Shumo Chu',\n",
       "   'Brendan Murphy',\n",
       "   'Jared Roesch',\n",
       "   'Alvin Cheung',\n",
       "   'Dan Suciu'],\n",
       "  'summary': \"Deciding the equivalence of SQL queries is a fundamental problem in data\\nmanagement. As prior work has mainly focused on studying the theoretical\\nlimitations of the problem, very few implementations for checking such\\nequivalences exist. In this paper, we present a new formalism and\\nimplementation for reasoning about the equivalences of SQL queries. Our\\nformalism, U-semiring, extends SQL's semiring semantics with unbounded\\nsummation and duplicate elimination. U-semiring is defined using only very few\\naxioms and can thus be easily implemented using proof assistants such as Coq\\nfor automated query reasoning. Yet, they are sufficient enough to enable us\\nreason about sophisticated SQL queries that are evaluated over bags and sets,\\nalong with various integrity constraints. To evaluate the effectiveness of\\nU-semiring, we have used it to formally verify 39 query rewrite rules from both\\nclassical data management research papers and real-world SQL engines, where\\nmany of them have never been proven correct before.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 2, 6, 21, 40, 50, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Beyond Markov Logic: Efficient Mining of Prediction Rules in Large Graphs',\n",
       "  'authors': ['Tommaso Soru',\n",
       "   'André Valdestilhas',\n",
       "   'Edgard Marx',\n",
       "   'Axel-Cyrille Ngonga Ngomo'],\n",
       "  'summary': 'Graph representations of large knowledge bases may comprise billions of\\nedges. Usually built upon human-generated ontologies, several knowledge bases\\ndo not feature declared ontological rules and are far from being complete.\\nCurrent rule mining approaches rely on schemata or store the graph in-memory,\\nwhich can be unfeasible for large graphs. In this paper, we introduce\\nHornConcerto, an algorithm to discover Horn clauses in large graphs without the\\nneed of a schema. Using a standard fact-based confidence score, we can mine\\nclose Horn rules having an arbitrary body size. We show that our method can\\noutperform existing approaches in terms of runtime and memory consumption and\\nmine high-quality rules for the link prediction task, achieving\\nstate-of-the-art results on a widely-used benchmark. Moreover, we find that\\nrules alone can perform inference significantly faster than embedding-based\\nmethods and achieve accuracies on link prediction comparable to\\nresource-demanding approaches such as Markov Logic Networks.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 2, 10, 18, 46, 54, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Notable Characteristics Search through Knowledge Graphs',\n",
       "  'authors': ['Davide Mottin',\n",
       "   'Bastian Grasnick',\n",
       "   'Axel Kroschk',\n",
       "   'Patrick Siegler',\n",
       "   'Emmanuel Mueller'],\n",
       "  'summary': 'Query answering routinely employs knowledge graphs to assist the user in the\\nsearch process. Given a knowledge graph that represents entities and\\nrelationships among them, one aims at complementing the search with intuitive\\nbut effective mechanisms. In particular, we focus on the comparison of two or\\nmore entities and the detection of unexpected, surprising properties, called\\nnotable characteristics. Such characteristics provide intuitive explanations of\\nthe peculiarities of the selected entities with respect to similar entities. We\\npropose a solid probabilistic approach that first retrieves entity nodes\\nsimilar to the query nodes provided by the user, and then exploits\\ndistributional properties to understand whether a certain attribute is\\ninteresting or not. Our preliminary experiments demonstrate the solidity of our\\napproach and show that we are able to discover notable characteristics that are\\nindeed interesting and relevant for the user.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 2, 12, 14, 24, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'First-order queries on classes of structures with bounded expansion',\n",
       "  'authors': ['Wojtek Kazana', 'Luc Segoufin'],\n",
       "  'summary': 'We consider the evaluation of first-order queries over classes of databases\\nwith bounded expansion. The notion of bounded expansion is fairly broad and\\ngeneralizes bounded degree, bounded treewidth and exclusion of at least one\\nminor. It was known that over a class of databases with bounded expansion,\\nfirst-order sentences could be evaluated in time linear in the size of the\\ndatabase. We give a different proof of this result. Moreover, we show that\\nanswers to first-order queries can be enumerated with constant delay after a\\nlinear time preprocessing. We also show that counting the number of answers to\\na query can be done in time linear in the size of the database.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 2, 13, 13, 29, 50, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'PRoST: Distributed Execution of SPARQL Queries Using Mixed Partitioning Strategies',\n",
       "  'authors': ['Matteo Cossu', 'Michael Färber', 'Georg Lausen'],\n",
       "  'summary': 'The rapidly growing size of RDF graphs in recent years necessitates\\ndistributed storage and parallel processing strategies. To obtain efficient\\nquery processing using computer clusters a wide variety of different approaches\\nhave been proposed. Related to the approach presented in the current paper are\\nsystems built on top of Hadoop HDFS, for example using Apache Accumulo or using\\nApache Spark. We present a new RDF store called PRoST (Partitioned RDF on Spark\\nTables) based on Apache Spark. PRoST introduces an innovative strategy that\\ncombines the Vertical Partitioning approach with the Property Table, two\\npreexisting models for storing RDF datasets. We demonstrate that our proposal\\noutperforms state-of-the-art systems w.r.t. the runtime for a wide range of\\nquery types and without any extensive precomputing phase.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 2, 16, 11, 25, 15, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data Consistency Simulation Tool for NoSQL Database Systems',\n",
       "  'authors': ['Nazim Faour'],\n",
       "  'summary': 'Various data consistency levels have an important part in the integrity of\\ndata and also affect performance especially the data that is replicated many\\ntimes across or over the cluster. Based on BASE and the theorem of CAP\\ntradeoffs, most systems of NoSQL have more relaxed consistency guarantees than\\nanother kind of databases which implement ACID. Most systems of NoSQL gave\\ndifferent methods to adjust a required level of consistency to ensure the\\nminimal numbering of the replicas accepted in each operation. Simulations are\\nalways depending on a simplified model and ignore many details and facts about\\nthe real system. Therefore, a simulation can only work as an estimation or an\\nexplanation vehicle for observed behavior. So to create simulation tool, I have\\nto characterize a model, identify influence factors and simply implement that\\ndepending on a (modeled) workload. In this paper, I have a model of simulation\\nto measure the consistency of the data and to detect the data consistency\\nviolations in simulated network partition settings. So workloads are needed\\nwith the set of users who make requests and then put the results for analysis.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 2, 22, 14, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Cuttlefish: A Lightweight Primitive for Adaptive Query Processing',\n",
       "  'authors': ['Tomer Kaftan',\n",
       "   'Magdalena Balazinska',\n",
       "   'Alvin Cheung',\n",
       "   'Johannes Gehrke'],\n",
       "  'summary': \"Modern data processing applications execute increasingly sophisticated\\nanalysis that requires operations beyond traditional relational algebra. As a\\nresult, operators in query plans grow in diversity and complexity. Designing\\nquery optimizer rules and cost models to choose physical operators for all of\\nthese novel logical operators is impractical. To address this challenge, we\\ndevelop Cuttlefish, a new primitive for adaptively processing online query\\nplans that explores candidate physical operator instances during query\\nexecution and exploits the fastest ones using multi-armed bandit reinforcement\\nlearning techniques. We prototype Cuttlefish in Apache Spark and adaptively\\nchoose operators for image convolution, regular expression matching, and\\nrelational joins. Our experiments show Cuttlefish-based adaptive convolution\\nand regular expression operators can reach 72-99% of the throughput of an\\nall-knowing oracle that always selects the optimal algorithm, even when\\nindividual physical operators are up to 105x slower than the optimal.\\nAdditionally, Cuttlefish achieves join throughput improvements of up to 7.5x\\ncompared with Spark SQL's query optimizer.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 2, 26, 6, 50, 43, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Adaptive Geospatial Joins for Modern Hardware',\n",
       "  'authors': ['Andreas Kipf',\n",
       "   'Harald Lang',\n",
       "   'Varun Pandey',\n",
       "   'Raul Alexandru Persa',\n",
       "   'Peter Boncz',\n",
       "   'Thomas Neumann',\n",
       "   'Alfons Kemper'],\n",
       "  'summary': \"Geospatial joins are a core building block of connected mobility\\napplications. An especially challenging problem are joins between streaming\\npoints and static polygons. Since points are not known beforehand, they cannot\\nbe indexed. Nevertheless, points need to be mapped to polygons with low\\nlatencies to enable real-time feedback.\\n  We present an adaptive geospatial join that uses true hit filtering to avoid\\nexpensive geometric computations in most cases. Our technique uses a\\nquadtree-based hierarchical grid to approximate polygons and stores these\\napproximations in a specialized radix tree. We emphasize on an approximate\\nversion of our algorithm that guarantees a user-defined precision. The exact\\nversion of our algorithm can adapt to the expected point distribution by\\nrefining the index. We optimized our implementation for modern hardware\\narchitectures with wide SIMD vector processing units, including Intel's brand\\nnew Knights Landing. Overall, our approach can perform up to two orders of\\nmagnitude faster than existing techniques.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 2, 26, 18, 11, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'All nearest neighbor calculation based on Delaunay graphs',\n",
       "  'authors': ['Nasrin Mazaheri Soudani', 'Ali Karami'],\n",
       "  'summary': 'When we have two data sets and want to find the nearest neighbour of each\\npoint in the first dataset among points in the second one, we need the all\\nnearest neighbour operator. This is an operator in spatial databases that has\\nmany application in different fields such as GIS and VLSI circuit design.\\nExisting algorithms for calculating this operator assume that there is no pre\\ncomputation on these data sets. These algorithms has o(n*m*d) time complexity\\nwhere n and m are the number of points in two data sets and d is the dimension\\nof data points. With assumption of some pre computation on data sets algorithms\\nwith lower time complexity can be obtained. One of the most common pre\\ncomputation on spatial data is Delaunay graphs. In the Delaunay graph of a data\\nset each point is linked to its nearest neighbours. In this paper, we introduce\\nan algorithm for computing the all nearest neighbour operator on spatial data\\nsets based on their Delaunay graphs. The performance of this algorithm is\\ncompared with one of the best existing algorithms for computing ANN operator in\\nterms of CPU time and the number of IOs. The experimental results show that\\nthis algorithm has better performance than the other.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 2, 26, 20, 32, 5, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Apache Calcite: A Foundational Framework for Optimized Query Processing Over Heterogeneous Data Sources',\n",
       "  'authors': ['Edmon Begoli',\n",
       "   'Jesús Camacho Rodríguez',\n",
       "   'Julian Hyde',\n",
       "   'Michael J. Mior',\n",
       "   'Daniel Lemire'],\n",
       "  'summary': \"Apache Calcite is a foundational software framework that provides query\\nprocessing, optimization, and query language support to many popular\\nopen-source data processing systems such as Apache Hive, Apache Storm, Apache\\nFlink, Druid, and MapD. Calcite's architecture consists of a modular and\\nextensible query optimizer with hundreds of built-in optimization rules, a\\nquery processor capable of processing a variety of query languages, an adapter\\narchitecture designed for extensibility, and support for heterogeneous data\\nmodels and stores (relational, semi-structured, streaming, and geospatial).\\nThis flexible, embeddable, and extensible architecture is what makes Calcite an\\nattractive choice for adoption in big-data frameworks. It is an active project\\nthat continues to introduce support for the new types of data sources, query\\nlanguages, and approaches to query processing and optimization.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 2, 28, 2, 10, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'VizRec: A framework for secure data exploration via visual representation',\n",
       "  'authors': ['Lorenzo De Stefani',\n",
       "   'Leonhard F. Spiegelberg',\n",
       "   'Tim Kraska',\n",
       "   'Eli Upfal'],\n",
       "  'summary': 'Visual representations of data (visualizations) are tools of great importance\\nand widespread use in data analytics as they provide users visual insight to\\npatterns in the observed data in a simple and effective way. However, since\\nvisualizations tools are applied to sample data, there is a a risk of\\nvisualizing random fluctuations in the sample rather than a true pattern in the\\ndata. This problem is even more significant when visualization is used to\\nidentify interesting patterns among many possible possibilities, or to identify\\nan interesting deviation in a pair of observations among many possible pairs,\\nas commonly done in visual recommendation systems.\\n  We present VizRec, a framework for improving the performance of visual\\nrecommendation systems by quantifying the statistical significance of\\nrecommended visualizations. The proposed methodology allows to control the\\nprobability of misleading visual recommendations using both classical\\nstatistical testing procedures and a novel application of the Vapnik\\nChervonenkis (VC) dimension method which is a fundamental concept in\\nstatistical learning theory.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 11, 1, 19, 35, 11, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data Integration for Supporting Biomedical Knowledge Graph Creation at Large-Scale',\n",
       "  'authors': ['Samaneh Jozashoori', 'Tatiana Novikova', 'Maria-Esther Vidal'],\n",
       "  'summary': 'In recent years, following FAIR and open data principles, the number of\\navailable big data including biomedical data has been increased exponentially.\\nIn order to extract knowledge, these data should be curated, integrated, and\\nsemantically described. Accordingly, several semantic integration techniques\\nhave been developed; albeit effective, they may suffer from scalability in\\nterms of different properties of big data. Even scaled-up approaches may be\\nhighly costly because tasks of semantification, curation and integration are\\nperformed independently. In order to overcome these issues, we devise ConMap, a\\nsemantic integration approach which exploits knowledge encoded in ontology in\\norder to describe mapping rules to perform these tasks at the same time.\\nExperimental results performed on different data sets suggest that ConMap can\\nsignificantly reduce the time required for knowledge graph creation by up to\\n70\\\\% of the time that is consumed following a traditional approach.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 11, 5, 13, 16, 54, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'STAR: Scaling Transactions through Asymmetric Replication',\n",
       "  'authors': ['Yi Lu', 'Xiangyao Yu', 'Samuel Madden'],\n",
       "  'summary': 'In this paper, we present STAR, a new distributed in-memory database with\\nasymmetric replication. By employing a single-node non-partitioned architecture\\nfor some replicas and a partitioned architecture for other replicas, STAR is\\nable to efficiently run both highly partitionable workloads and workloads that\\ninvolve cross-partition transactions. The key idea is a new phase-switching\\nalgorithm where the execution of single-partition and cross-partition\\ntransactions is separated. In the partitioned phase, single-partition\\ntransactions are run on multiple machines in parallel to exploit more\\nconcurrency. In the single-master phase, mastership for the entire database is\\nswitched to a single designated master node, which can execute these\\ntransactions without the use of expensive coordination protocols like two-phase\\ncommit. Because the master node has a full copy of the database, this\\nphase-switching can be done at negligible cost. Our experiments on two popular\\nbenchmarks (YCSB and TPC-C) show that high availability via replication can\\ncoexist with fast serializable transaction execution in distributed in-memory\\ndatabases, with STAR outperforming systems that employ conventional concurrency\\ncontrol and replication algorithms by up to one order of magnitude.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 11, 5, 22, 20, 25, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Meet Cyrus - The Query by Voice Mobile Assistant for the Tutoring and Formative Assessment of SQL Learners',\n",
       "  'authors': ['Josue Espinosa Godinez', 'Hasan M. Jamil'],\n",
       "  'summary': \"Being declarative, SQL stands a better chance at being the programming\\nlanguage for conceptual computing next to natural language programming. We\\nexamine the possibility of using SQL as a back-end for natural language\\ndatabase programming. Distinctly from keyword based SQL querying, keyword\\ndependence and SQL's table structure constraints are significantly less\\npronounced in our approach. We present a mobile device voice query interface,\\ncalled Cyrus, to arbitrary relational databases. Cyrus supports a large type of\\nquery classes, sufficient for an entry level database class. Cyrus is also\\napplication independent, allows test database adaptation, and not limited to\\nspecific sets of keywords or natural language sentence structures. It's\\ncooperative error reporting is more intuitive, and iOS based mobile platform is\\nalso more accessible compared to most contemporary mobile and voice enabled\\nsystems.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 11, 9, 23, 55, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The Impact of Timestamp Granularity in Optimistic Concurrency Control',\n",
       "  'authors': ['Yihe Huang',\n",
       "   'Hao Bai',\n",
       "   'Eddie Kohler',\n",
       "   'Barbara Liskov',\n",
       "   'Liuba Shrira'],\n",
       "  'summary': \"Optimistic concurrency control (OCC) can exploit the strengths of parallel\\nhardware to provide excellent performance for uncontended transactions, and is\\npopular in high-performance in-memory databases and transactional systems. But\\nat high contention levels, OCC is susceptible to frequent aborts, leading to\\nwasted work and degraded performance. Contention managers, mixed\\noptimistic/pessimistic concurrency control algorithms, and novel\\noptimistic-inspired concurrency control algorithms, such as TicToc, aim to\\naddress this problem, but these mechanisms introduce sometimes-high overheads\\nof their own. We show that in real-world benchmarks, traditional OCC can\\noutperform these alternative mechanisms by simply adding fine-grained version\\ntimestamps (using different timestamps for disjoint components of each record).\\nWith fine-grained timestamps, OCC gets 1.14x TicToc's throughput in TPC-C at\\n128 cores (previous work reported TicToc having 1.8x higher throughput than OCC\\nat 80 hyperthreads). Our study shows that timestamp granularity has a greater\\nimpact than previously thought on the performance of transaction processing\\nsystems, and should not be overlooked in the push for faster concurrency\\ncontrol schemes.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 11, 12, 19, 15, 27, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'PanJoin: A Partition-based Adaptive Stream Join',\n",
       "  'authors': ['Fei Pan', 'Hans-Arno Jacobsen'],\n",
       "  'summary': 'In stream processing, stream join is one of the critical sources of\\nperformance bottlenecks. The sliding-window-based stream join provides a\\nprecise result but consumes considerable computational resources. The current\\nsolutions lack support for the join predicates on large windows. These\\nalgorithms and their hardware accelerators are either limited to equi-join or\\nuse a nested loop join to process all the requests.\\n  In this paper, we present a new algorithm called PanJoin which has high\\nthroughput on large windows and supports both equi-join and non-equi-join.\\nPanJoin implements three new data structures to reduce computations during the\\nprobing phase of stream join. We also implement the most hardware-friendly data\\nstructure, called BI-Sort, on FPGA. Our evaluation shows that PanJoin\\noutperforms several recently proposed stream join methods by more than 1000x,\\nand it also adapts well to highly skewed data.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 11, 13, 2, 1, 32, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Personal Names Popularity Estimation and its Application to Record Linkage',\n",
       "  'authors': ['Ksenia Zhagorina', 'Pavel Braslavski', 'Vladimir Gusev'],\n",
       "  'summary': 'This study deals with a fairly simply formulated problem -- how to estimate\\nthe number of people bearing the same full name in a large population.\\nEstimation of name popularity can leverage personal name matching in databases\\nand be of interest for many other domains. A distinctive feature of large\\ncollections of names is that they contain a large number of unique items, which\\nis challenging for statistical modeling. We investigate a number of statistical\\ntechniques and also propose a simple yet effective method aimed at obtaining\\nmore accurate count estimates. In our experiments we use a dataset containing\\nabout 20 million name occurrences that correspond to about 13 million\\nreal-world persons. We perform a thorough evaluation of the name count\\nestimation methods and a record linkage experiment guided by name popularity\\nestimates. Obtained results suggest that theoretically informed approaches\\noutperform simple heuristics and can be useful in a variety of applications.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 11, 13, 15, 26, 30, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Privacy Preserving Utility Mining: A Survey',\n",
       "  'authors': ['Wensheng Gan',\n",
       "   'Jerry Chun-Wei Lin',\n",
       "   'Han-Chieh Chao',\n",
       "   'Shyue-Liang Wang',\n",
       "   'Philip S. Yu'],\n",
       "  'summary': 'In big data era, the collected data usually contains rich information and\\nhidden knowledge. Utility-oriented pattern mining and analytics have shown a\\npowerful ability to explore these ubiquitous data, which may be collected from\\nvarious fields and applications, such as market basket analysis, retail,\\nclick-stream analysis, medical analysis, and bioinformatics. However, analysis\\nof these data with sensitive private information raises privacy concerns. To\\nachieve better trade-off between utility maximizing and privacy preserving,\\nPrivacy-Preserving Utility Mining (PPUM) has become a critical issue in recent\\nyears. In this paper, we provide a comprehensive overview of PPUM. We first\\npresent the background of utility mining, privacy-preserving data mining and\\nPPUM, then introduce the related preliminaries and problem formulation of PPUM,\\nas well as some key evaluation criteria for PPUM. In particular, we present and\\ndiscuss the current state-of-the-art PPUM algorithms, as well as their\\nadvantages and deficiencies in detail. Finally, we highlight and discuss some\\ntechnical challenges and open directions for future research on PPUM.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 11, 18, 19, 58, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'StarStar Models: Process Analysis on top of Databases',\n",
       "  'authors': ['Alessandro Berti', 'Wil van der Aalst'],\n",
       "  'summary': 'Much time in process mining projects is spent on finding and understanding\\ndata sources and extracting the event data needed. As a result, only a fraction\\nof time is spent actually applying techniques to discover, control and predict\\nthe business process. Moreover, there is a lack of techniques to display\\nrelationships on top of databases without the need to express a complex query\\nto get the required information. In this paper, a novel modeling technique that\\nworks on top of databases is presented. This technique is able to show a\\nmultigraph representing activities inferred from database events, connected\\nwith edges that are annotated with frequency and performance information. The\\nrepresentation may be the entry point to apply advanced process mining\\ntechniques that work on classic event logs, as the model provides a simple way\\nto retrieve a classic event log from a specified piece of model. Comparison\\nwith similar techniques and an empirical evaluation are provided.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 11, 20, 9, 32, 11, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'HyperBench: A Benchmark and Tool for Hypergraphs and Empirical Findings',\n",
       "  'authors': ['Wolfgang Fischl',\n",
       "   'Georg Gottlob',\n",
       "   'Davide M. Longo',\n",
       "   'Reinhard Pichler'],\n",
       "  'summary': 'To cope with the intractability of answering Conjunctive Queries (CQs) and\\nsolving Constraint Satisfaction Problems (CSPs), several notions of hypergraph\\ndecompositions have been proposed -- giving rise to different notions of width,\\nnoticeably, plain, generalized, and fractional hypertree width (hw, ghw, and\\nfhw). Given the increasing interest in using such decomposition methods in\\npractice, a publicly accessible repository of decomposition software, as well\\nas a large set of benchmarks, and a web-accessible workbench for inserting,\\nanalysing, and retrieving hypergraphs are called for.\\n  We address this need by providing (i) concrete implementations of hypergraph\\ndecompositions (including new practical algorithms), (ii) a new, comprehensive\\nbenchmark of hypergraphs stemming from disparate CQ and CSP collections, and\\n(iii) HyperBench, our new web-inter\\\\-face for accessing the benchmark and the\\nresults of our analyses. In addition, we describe a number of actual\\nexperiments we carried out with this new infrastructure.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 11, 20, 11, 18, 22, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data Context Informed Data Wrangling',\n",
       "  'authors': ['Martin Koehler',\n",
       "   'Alex Bogatu',\n",
       "   'Cristina Civili',\n",
       "   'Nikolaos Konstantinou',\n",
       "   'Edward Abel',\n",
       "   'Alvaro A. A. Fernandes',\n",
       "   'John Keane',\n",
       "   'Leonid Libkin',\n",
       "   'Norman W. Paton'],\n",
       "  'summary': 'The process of preparing potentially large and complex data sets for further\\nanalysis or manual examination is often called data wrangling. In classical\\nwarehousing environments, the steps in such a process have been carried out\\nusing Extract-Transform-Load platforms, with significant manual involvement in\\nspecifying, configuring or tuning many of them. Cost-effective data wrangling\\nprocesses need to ensure that data wrangling steps benefit from automation\\nwherever possible. In this paper, we define a methodology to fully automate an\\nend-to-end data wrangling process incorporating data context, which associates\\nportions of a target schema with potentially spurious extensional data of types\\nthat are commonly available. Instance-based evidence together with data\\nprofiling paves the way to inform automation in several steps within the\\nwrangling process, specifically, matching, mapping validation, value format\\ntransformation, and data repair. The approach is evaluated with real estate\\ndata showing substantial improvements in the results of automated wrangling.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 11, 22, 17, 34, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Enabling Efficient Updates in KV Storage via Hashing: Design and Performance Evaluation',\n",
       "  'authors': ['Yongkun Li',\n",
       "   'Helen H. W. Chan',\n",
       "   'Patrick P. C. Lee',\n",
       "   'Yinlong Xu'],\n",
       "  'summary': 'Persistent key-value (KV) stores mostly build on the Log-Structured Merge\\n(LSM) tree for high write performance, yet the LSM-tree suffers from the\\ninherently high I/O amplification. KV separation mitigates I/O amplification by\\nstoring only keys in the LSM-tree and values in separate storage. However, the\\ncurrent KV separation design remains inefficient under update-intensive\\nworkloads due to its high garbage collection (GC) overhead in value storage. We\\npropose HashKV, which aims for high update performance atop KV separation under\\nupdate-intensive workloads. HashKV uses hash-based data grouping, which\\ndeterministically maps values to storage space so as to make both updates and\\nGC efficient. We further relax the restriction of such deterministic mappings\\nvia simple but useful design extensions. We extensively evaluate various design\\naspects of HashKV. We show that HashKV achieves 4.6x update throughput and\\n53.4% less write traffic compared to the current KV separation design. In\\naddition, we demonstrate that we can integrate the design of HashKV with\\nstate-of-the-art KV stores and improve their respective performance.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 11, 25, 12, 52, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Adaptive Wavelet Clustering for Highly Noisy Data',\n",
       "  'authors': ['Zengjian Chen',\n",
       "   'Jiayi Liu',\n",
       "   'Yihe Deng',\n",
       "   'Kun He',\n",
       "   'John E. Hopcroft'],\n",
       "  'summary': 'In this paper we make progress on the unsupervised task of mining arbitrarily\\nshaped clusters in highly noisy datasets, which is a task present in many\\nreal-world applications. Based on the fundamental work that first applies a\\nwavelet transform to data clustering, we propose an adaptive clustering\\nalgorithm, denoted as AdaWave, which exhibits favorable characteristics for\\nclustering. By a self-adaptive thresholding technique, AdaWave is parameter\\nfree and can handle data in various situations. It is deterministic, fast in\\nlinear time, order-insensitive, shape-insensitive, robust to highly noisy data,\\nand requires no pre-knowledge on data models. Moreover, AdaWave inherits the\\nability from the wavelet transform to cluster data in different resolutions. We\\nadopt the \"grid labeling\" data structure to drastically reduce the memory\\nconsumption of the wavelet transform so that AdaWave can be used for relatively\\nhigh dimensional data. Experiments on synthetic as well as natural datasets\\ndemonstrate the effectiveness and efficiency of our proposed method.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 11, 27, 3, 5, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Frequency Scaling based Performance Indicator Framework for Big Data Systems',\n",
       "  'authors': ['Chen Yang',\n",
       "   'Zhihui Du',\n",
       "   'Xiaofeng Meng',\n",
       "   'Yongjie Du',\n",
       "   'Zhiqiang Duan'],\n",
       "  'summary': 'It is important for big data systems to identify their performance\\nbottleneck. However, the popular indicators such as resource utilizations, are\\noften misleading and incomparable with each other. In this paper, a novel\\nindicator framework which can directly compare the impact of different\\nindicators with each other is proposed to identify and analyze the performance\\nbottleneck efficiently. A methodology which can construct the indicator from\\nthe performance change with the CPU frequency scaling is described. Spark is\\nused as an example of a big data system and two typical SQL benchmarks are used\\nas the workloads to evaluate the proposed method. Experimental results show\\nthat the proposed method is accurate compared with the resource utilization\\nmethod and easy to implement compared with some white-box method. Meanwhile,\\nthe analysis with our indicators lead to some interesting findings and valuable\\nperformance optimization suggestions for big data systems.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 11, 27, 6, 34, 57, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Efficiently Charting RDF',\n",
       "  'authors': ['Oren Kalinsky',\n",
       "   'Oren Mishali',\n",
       "   'Aidan Hogan',\n",
       "   'Yoav Etsion',\n",
       "   'Benny Kimelfeld'],\n",
       "  'summary': 'We propose a visual query language for interactively exploring large-scale\\nknowledge graphs. Starting from an overview, the user explores bar charts\\nthrough three interactions: class expansion, property expansion, and\\nsubject/object expansion. A major challenge faced is performance: a\\nstate-of-the-art SPARQL engine may require tens of minutes to compute the\\nmultiway join, grouping and counting required to render a bar chart. A\\npromising alternative is to apply approximation through online aggregation,\\ntrading precision for performance. However, state-of-the-art online aggregation\\nalgorithms such as Wander Join have two limitations for our exploration\\nscenario: (1) a high number of rejected paths slows the convergence of the\\ncount estimations, and (2) no unbiased estimator exists for counts under the\\ndistinct operator. We thus devise a specialized algorithm for online\\naggregation that augments Wander Join with exact partial computations to reduce\\nthe number of rejected paths encountered, as well as a novel estimator that we\\nprove to be unbiased in the case of the distinct operator. In an experimental\\nstudy with random interactions exploring two large-scale knowledge graphs, our\\nalgorithm shows a clear reduction in error with respect to computation time\\nversus Wander Join.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 11, 27, 13, 11, 16, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Approximate Evaluation of Label-Constrained Reachability Queries',\n",
       "  'authors': ['Stefania Dumbrava',\n",
       "   'Angela Bonifati',\n",
       "   'Amaia Nazabal Ruiz Diaz',\n",
       "   'Romain Vuillemot'],\n",
       "  'summary': 'The current surge of interest in graph-based data models mirrors the usage of\\nincreasingly complex reachability queries, as witnessed by recent analytical\\nstudies on real-world graph query logs. Despite the maturity of graph DBMS\\ncapabilities, complex label-constrained reachability queries, along with their\\ncorresponding aggregate versions, remain difficult to evaluate. In this paper,\\nwe focus on the approximate evaluation of counting label-constrained\\nreachability queries. We offer a human-explainable solution to graph\\nApproximate Query Processing (AQP). This consists of a summarization algorithm\\n(GRASP), as well as of a custom visualization plug-in, which allows users to\\nexplore the obtained summaries. We prove that the problem of node group\\nminimization, associated to the creation of GRASP summaries, is NP-complete.\\nNonetheless, our GRASP summaries are reasonably small in practice, even for\\nlarge graph instances, and guarantee approximate graph query answering, paired\\nwith controllable error estimates. We experimentally gauge the scalability and\\nefficiency of our GRASP algorithm, and verify the accuracy and error estimation\\nof the graph AQP module. To the best of our knowledge, ours is the first system\\ncapable of handling visualization-driven approximate graph analytics for\\ncomplex label-constrained reachability queries.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 11, 28, 13, 51, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Chiller: Contention-centric Transaction Execution and Data Partitioning for Modern Networks',\n",
       "  'authors': ['Erfan Zamanian', 'Julian Shun', 'Carsten Binnig', 'Tim Kraska'],\n",
       "  'summary': 'Distributed transactions on high-overhead TCP/IP-based networks were\\nconventionally considered to be prohibitively expensive and thus were avoided\\nat all costs. To that end, the primary goal of almost any existing partitioning\\nscheme is to minimize the number of cross-partition transactions. However, with\\nthe new generation of fast RDMA-enabled networks, this assumption is no longer\\nvalid. In fact, recent work has shown that distributed databases can scale even\\nwhen the majority of transactions are cross-partition. In this paper, we first\\nmake the case that the new bottleneck which hinders truly scalable transaction\\nprocessing in modern RDMA-enabled databases is data contention, and that\\noptimizing for data contention leads to different partitioning layouts than\\noptimizing for the number of distributed transactions. We then present Chiller,\\na new approach to data partitioning and transaction execution, which aims to\\nminimize data contention for both local and distributed transactions. Finally,\\nwe evaluate Chiller using various workloads, and show that our partitioning and\\nexecution strategy outperforms traditional partitioning techniques which try to\\navoid distributed transactions, by up to a factor of 2.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 11, 29, 14, 45, 48, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Verity: Blockchains to Detect Insider Attacks in DBMS',\n",
       "  'authors': ['Shubham S. Srivastava',\n",
       "   'Medha Atre',\n",
       "   'Shubham Sharma',\n",
       "   'Rahul Gupta',\n",
       "   'Sandeep K. Shukla'],\n",
       "  'summary': \"Integrity and security of the data in database systems are typically\\nmaintained with access control policies and firewalls. However, insider attacks\\n-- where someone with an intimate knowledge of the system and administrative\\nprivileges tampers with the data -- pose a unique challenge. Measures like\\nappend only logging prove to be insufficient because an attacker with\\nadministrative privileges can alter logs and login records to eliminate the\\ntrace of attack, thus making insider attacks hard to detect.\\n  In this paper, we propose Verity -- first of a kind system to the best of our\\nknowledge. Verity serves as a dataless framework by which any blockchain\\nnetwork can be used to store fixed-length metadata about tuples from any SQL\\ndatabase, without complete migration of the database. Verity uses a formalism\\nfor parsing SQL queries and query results to check the respective tuples'\\nintegrity using blockchains to detect insider attacks. We have implemented our\\ntechnique using Hyperledger Fabric, Composer REST API, and SQLite database.\\nUsing TPC-H data and SQL queries of varying complexity and types, our\\nexperiments demonstrate that any overhead of integrity checking remains\\nconstant per tuple in a query's results, and scales linearly.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 2, 1, 2, 56, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Approximate Computation for Big Data Analytics',\n",
       "  'authors': ['Shuai Ma', 'Jinpeng Huai'],\n",
       "  'summary': \"Over the past a few years, research and development has made significant\\nprogresses on big data analytics. A fundamental issue for big data analytics is\\nthe efficiency. If the optimal solution is unable to attain or not required or\\nhas a price to high to pay, it is reasonable to sacrifice optimality with a\\n`good' feasible solution that can be computed efficiently. Existing\\napproximation techniques can be in general classified into approximation\\nalgorithms, approximate query processing for aggregate SQL queries and\\napproximation computing for multiple layers of the system stack. In this\\narticle, we systematically introduce approximate computation, i.e., query\\napproximation and data approximation, for efficiency and effectiveness big data\\nanalytics. We first explain the idea and rationale of query approximation, and\\nshow efficiency can be obtained with high effectiveness in practice with three\\nanalytic tasks: graph pattern matching, trajectory compression and dense\\nsubgraph computation. We then explain the idea and rationale of data\\napproximation, and show efficiency can be obtained even without sacrificing for\\neffectiveness in practice with three analytic tasks: shortest paths/distances,\\nnetwork anomaly detection and link prediction.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 2, 1, 38, 30, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Une nouvelle approche de complétion des valeurs manquantes dans les bases de données',\n",
       "  'authors': ['Leila Ben Othman'],\n",
       "  'summary': \"When tackling real-life datasets, it is common to face the existence of\\nscrambled missing values within data. Considered as 'dirty data', usually it is\\nremoved during a pre-processing step. Starting from the fact that 'making up\\nthis missing data is better than throwing out it away', we present a new\\napproach trying to complete missing data. The main singularity of the\\nintroduced approach is that it sheds light on a fruitful synergy between\\ngeneric basis of association rules and the topic of missing values handling. In\\nfact, beyond interesting compactness rate, such generic association rules make\\nit possible to get a considerable reduction of conflicts during the completion\\nstep. A new metric called 'Robustness' is also introduced, and aims to select\\nthe robust association rule for the completion of a missing value whenever a\\nconflict appears. Carried out experiments on benchmark datasets confirm the\\nsoundness of our approach. Thus, it reduces conflict during the completion step\\nwhile offering a high percentage of correct completion accuracy.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 3, 10, 30, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Improving Suppression to Reduce Disclosure Risk and Enhance Data Utility',\n",
       "  'authors': ['Marmar Orooji', 'Gerald M. Knapp'],\n",
       "  'summary': 'In Privacy Preserving Data Publishing, various privacy models have been\\ndeveloped for employing anonymization operations on sensitive individual level\\ndatasets, in order to publish the data for public access while preserving the\\nprivacy of individuals in the dataset. However, there is always a trade-off\\nbetween preserving privacy and data utility; the more changes we make on the\\nconfidential dataset to reduce disclosure risk, the more information the data\\nloses and the less data utility it preserves. The optimum privacy technique is\\nthe one that results in a dataset with minimum disclosure risk and maximum data\\nutility. In this paper, we propose an improved suppression method, which\\nreduces the disclosure risk and enhances the data utility by targeting the\\nhighest risk records and keeping other records intact. We have shown the\\neffectiveness of our approach through an experiment on a real-world\\nconfidential dataset.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 2, 18, 48, 34, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Dataset search: a survey',\n",
       "  'authors': ['Adriane Chapman',\n",
       "   'Elena Simperl',\n",
       "   'Laura Koesten',\n",
       "   'George Konstantinidis',\n",
       "   'Luis-Daniel Ibáñez-Gonzalez',\n",
       "   'Emilia Kacprzak',\n",
       "   'Paul Groth'],\n",
       "  'summary': 'Generating value from data requires the ability to find, access and make\\nsense of datasets. There are many efforts underway to encourage data sharing\\nand reuse, from scientific publishers asking authors to submit data alongside\\nmanuscripts to data marketplaces, open data portals and data communities.\\nGoogle recently beta released a search service for datasets, which allows users\\nto discover data stored in various online repositories via keyword queries.\\nThese developments foreshadow an emerging research field around dataset search\\nor retrieval that broadly encompasses frameworks, methods and tools that help\\nmatch a user data need against a collection of datasets. Here, we survey the\\nstate of the art of research and commercial systems in dataset retrieval. We\\nidentify what makes dataset search a research field in its own right, with\\nunique challenges and methods and highlight open problems. We look at\\napproaches and implementations from related areas dataset search is drawing\\nupon, including information retrieval, databases, entity-centric and tabular\\nsearch in order to identify possible paths to resolve these open problems as\\nwell as immediate next steps that will take the field forward.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 3, 14, 6, 13, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Exact Selectivity Computation for Modern In-Memory Database Query Optimization',\n",
       "  'authors': ['Jun Hyung Shin', 'Florin Rusu', 'Alex Suhan'],\n",
       "  'summary': 'Selectivity estimation remains a critical task in query optimization even\\nafter decades of research and industrial development. Optimizers rely on\\naccurate selectivities when generating execution plans. They maintain a large\\nrange of statistical synopses for efficiently estimating selectivities.\\nNonetheless, small errors -- propagated exponentially -- can lead to severely\\nsub-optimal plans---especially, for complex predicates. Database systems for\\nmodern computing architectures rely on extensive in-memory processing supported\\nby massive multithread parallelism and vectorized instructions. However, they\\nmaintain the same synopses approach to query optimization as traditional\\ndisk-based databases. We introduce a novel query optimization paradigm for\\nin-memory and GPU-accelerated databases based on \\\\textit{exact selectivity\\ncomputation (ESC)}. The central idea in ESC is to compute selectivities exactly\\nthrough queries during query optimization. In order to make the process\\nefficient, we propose several optimizations targeting the selection and\\nmaterialization of tables and predicates to which ESC is applied. We implement\\nESC in the MapD open-source database system. Experiments on the TPC-H and SSB\\nbenchmarks show that ESC records constant and less than 30 milliseconds\\noverhead when running on GPU and generates improved query execution plans that\\nare as much as 32X faster.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 6, 2, 20, 57, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Looking Back at Postgres',\n",
       "  'authors': ['Joseph M. Hellerstein'],\n",
       "  'summary': 'This is a recollection of the UC Berkeley Postgres project, which was led by\\nMike Stonebraker from the mid-1980\\'s to the mid-1990\\'s. The article was\\nsolicited for Stonebraker\\'s Turing Award book, as one of many\\npersonal/historical recollections. As a result it focuses on Stonebraker\\'s\\ndesign ideas and leadership. But Stonebraker was never a coder, and he stayed\\nout of the way of his development team. The Postgres codebase was the work of a\\nteam of brilliant students and the occasional university \"staff programmers\"\\nwho had little more experience (and only slightly more compensation) than the\\nstudents. I was lucky to join that team as a student during the latter years of\\nthe project. I got helpful input on this writeup from some of the more senior\\nstudents on the project, but any errors or omissions are mine. If you spot any\\nsuch, please contact me and I will try to fix them.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 7, 18, 59, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Popular SQL Server Database Encryption Choices',\n",
       "  'authors': ['Sourav Mukherjee'],\n",
       "  'summary': \"This article gives an overview of different database encryption choices in\\nSQL Server. Which one works best in which situation. In today's world Data is\\nmore crucial than the expensive hardware cost. No one wants their personal data\\nto be comprised. Same for business houses as well and they also do not want\\ntheir data to be inappropriately handled to go out of the business. To help\\nprotect the public rights and safety, recently this year, the European Union\\nhad come up with strict rules and regulation of GDPR (General Data Protection\\nRegulation).\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 7, 2, 53, 49, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Solving linear programs on factorized databases',\n",
       "  'authors': ['Florent Capelli',\n",
       "   'Nicolas Crosetti',\n",
       "   'Joachim Niehren',\n",
       "   'Jan Ramon'],\n",
       "  'summary': 'A typical workflow for solving a linear programming problem is to first write\\na linear program parametrized by the data in a language such as Math GNU Prog\\nor AMPL then call the solver on this program while providing the data. When the\\ndata is extracted using a query on a database, this approach ignores the\\nunderlying structure of the answer set which may result in a blow-up of the\\nsize of the linear program if the answer set is big. In this paper, we study\\nthe problem of solving linear programming problems whose variables are the\\nanswers to a conjunctive query. We show that one can exploit the structure of\\nthe query to rewrite the linear program so that its size depends only on the\\nsize of the database and not on the size of the answer set. More precisely, we\\ngive a generic way of rewriting a linear program whose variables are the tuples\\nin Q(D) for a conjunctive query Q and a database D into a linear program having\\na number of variables that only depends on the size of a factorized\\nrepresentation of Q(D), which can be much smaller when the fractional hypertree\\nwidth of Q is bounded.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 11, 16, 22, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'All-Instances Restricted Chase Termination',\n",
       "  'authors': ['Tomasz Gogacz', 'Jerzy Marcinkowski', 'Andreas Pieris'],\n",
       "  'summary': 'The chase procedure is a fundamental algorithmic tool in database theory with\\na variety of applications. A key problem concerning the chase procedure is\\nall-instances termination: for a given set of tuple-generating dependencies\\n(TGDs), is it the case that the chase terminates for every input database? In\\nview of the fact that this problem is undecidable, it is natural to ask whether\\nknown well-behaved classes of TGDs ensure decidability. We consider here the\\nmain paradigms that led to robust TGD-based formalisms, that is, guardedness\\nand stickiness. Although all-instances termination is well-understood for the\\noblivious version of the chase, the more subtle case of the restricted (a.k.a.\\nthe standard) chase is rather unexplored. We show that all-instances restricted\\nchase termination for guarded and sticky single-head TGDs is decidable.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 12, 20, 51, 58, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Exploring Communities in Large Profiled Graphs',\n",
       "  'authors': ['Yankai Chen',\n",
       "   'Yixiang Fang',\n",
       "   'Reynold Cheng',\n",
       "   'Yun Li',\n",
       "   'Xiaojun Chen',\n",
       "   'Jie Zhang'],\n",
       "  'summary': 'Given a graph $G$ and a vertex $q\\\\in G$, the community search (CS) problem\\naims to efficiently find a subgraph of $G$ whose vertices are closely related\\nto $q$. Communities are prevalent in social and biological networks, and can be\\nused in product advertisement and social event recommendation. In this paper,\\nwe study profiled community search (PCS), where CS is performed on a profiled\\ngraph. This is a graph in which each vertex has labels arranged in a\\nhierarchical manner. Extensive experiments show that PCS can identify\\ncommunities with themes that are common to their vertices, and is more\\neffective than existing CS approaches. As a naive solution for PCS is highly\\nexpensive, we have also developed a tree index, which facilitate efficient and\\nonline solutions for PCS.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 16, 13, 38, 12, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data Quality Measures and Data Cleansing for Research Information Systems',\n",
       "  'authors': ['Otmane Azeroual', 'Gunter Saake', 'Mohammad Abuosba'],\n",
       "  'summary': 'The collection, transfer and integration of research information into\\ndifferent research Information systems can result in different data errors that\\ncan have a variety of negative effects on data quality. In order to detect\\nerrors at an early stage and treat them efficiently, it is necessary to\\ndetermine the clean-up measures and the new techniques of data cleansing for\\nquality improvement in research institutions. Thereby an adequate and reliable\\nbasis for decision-making using an RIS is provided, and confidence in a given\\ndataset increased. In this paper, possible measures and the new techniques of\\ndata cleansing for improving and increasing the data quality in research\\ninformation systems will be presented and how these are to be applied to the\\nResearch information.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 18, 12, 59, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An Experimental Study of the Treewidth of Real-World Graph Data (Extended Version)',\n",
       "  'authors': ['Silviu Maniu', 'Pierre Senellart', 'Suraj Jog'],\n",
       "  'summary': 'Treewidth is a parameter that measures how tree-like a relational instance\\nis, and whether it can reasonably be decomposed into a tree. Many computation\\ntasks are known to be tractable on databases of small treewidth, but computing\\nthe treewidth of a given instance is intractable. This article is the first\\nlarge-scale experimental study of treewidth and tree decompositions of\\nreal-world database instances (25 datasets from 8 different domains, with sizes\\nranging from a few thousand to a few million vertices). The goal is to\\ndetermine which data, if any, can benefit of the wealth of algorithms for\\ndatabases of small treewidth. For each dataset, we obtain upper and lower bound\\nestimations of their treewidth, and study the properties of their tree\\ndecompositions. We show in particular that, even when treewidth is high, using\\npartial tree decompositions can result in data structures that can assist\\nalgorithms.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 21, 10, 35, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Predictive Indexing',\n",
       "  'authors': ['Joy Arulraj', 'Ran Xian', 'Lin Ma', 'Andrew Pavlo'],\n",
       "  'summary': \"There has been considerable research on automated index tuning in database\\nmanagement systems (DBMSs). But the majority of these solutions tune the index\\nconfiguration by retrospectively making computationally expensive physical\\ndesign changes all at once. Such changes degrade the DBMS's performance during\\nthe process, and have reduced utility during subsequent query processing due to\\nthe delay between a workload shift and the associated change. A better approach\\nis to generate small changes that tune the physical design over time, forecast\\nthe utility of these changes, and apply them ahead of time to maximize their\\nimpact.\\n  This paper presents predictive indexing that continuously improves a\\ndatabase's physical design using lightweight physical design changes. It uses a\\nmachine learning model to forecast the utility of these changes, and\\ncontinuously refines the index configuration of the database to handle evolving\\nworkloads. We introduce a lightweight hybrid scan operator with which a DBMS\\ncan make use of partially-built indexes for query processing. Our evaluation\\nshows that predictive indexing improves the throughput of a DBMS by 3.5--5.2x\\ncompared to other state-of-the-art indexing approaches. We demonstrate that\\npredictive indexing works seamlessly with other lightweight automated physical\\ndesign tuning methods.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 21, 20, 16, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Just-in-Time Index Compilation',\n",
       "  'authors': ['Darshana Balakrishnan', 'Lukasz Ziarek', 'Oliver Kennedy'],\n",
       "  'summary': 'Creating or modifying a primary index is a time-consuming process, as the\\nindex typically needs to be rebuilt from scratch. In this paper, we explore a\\nmore graceful \"just-in-time\" approach to index reorganization, where small\\nchanges are dynamically applied in the background. To enable this type of\\nreorganization, we formalize a composable organizational grammar, expressive\\nenough to capture instances of not only existing index structures, but\\narbitrary hybrids as well. We introduce an algebra of rewrite rules for such\\nstructures, and a framework for defining and optimizing policies for\\njust-in-time rewriting. Our experimental analysis shows that the resulting\\nindex structure is flexible enough to adapt to a variety of performance goals,\\nwhile also remaining competitive with existing structures like the C++ standard\\ntemplate library map.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 22, 22, 19, 17, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Concentration of Measure Approach to Database De-anonymization',\n",
       "  'authors': ['Farhad Shirani', 'Siddharth Garg', 'Elza Erkip'],\n",
       "  'summary': 'In this paper, matching of correlated high-dimensional databases is\\ninvestigated. A stochastic database model is considered where the correlation\\namong the database entries is governed by an arbitrary joint distribution.\\nConcentration of measure theorems such as typicality and laws of large numbers\\nare used to develop a database matching scheme and derive necessary conditions\\nfor successful matching. Furthermore, it is shown that these conditions are\\ntight through a converse result which characterizes a set of distributions on\\nthe database entries for which reliable matching is not possible. The necessary\\nand sufficient conditions for reliable matching are evaluated in the cases when\\nthe database entries are independent and identically distributed as well as\\nunder Markovian database models.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 23, 0, 21, 48, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Fast and Robust Distributed Subgraph Enumeration',\n",
       "  'authors': ['Xuguang Ren', 'Junhu Wang', 'Wook-Shin Han', 'Jeffrey Xu Yu'],\n",
       "  'summary': 'We study the classic subgraph enumeration problem under distributed settings.\\nExisting solutions either suffer from severe memory crisis or rely on large\\nindexes, which makes them impractical for very large graphs. Most of them\\nfollow a synchronous model where the performance is often bottlenecked by the\\nmachine with the worst performance. Motivated by this, in this paper, we\\npropose RADS, a Robust Asynchronous Distributed Subgraph enumeration system.\\nRADS first identifies results that can be found using single-machine\\nalgorithms. This strategy not only improves the overall performance but also\\nreduces network communication and memory cost. Moreover, RADS employs a novel\\nregion-grouped multi-round expand verify & filter framework which does not need\\nto shuffle and exchange the intermediate results, nor does it need to replicate\\na large part of the data graph in each machine. This feature not only reduces\\nnetwork communication cost and memory usage, but also allows us to adopt simple\\nstrategies for memory control and load balancing, making it more robust.\\nSeveral heuristics are also used in RADS to further improve the performance.\\nOur experiments verified the superiority of RADS to state-of-the-art subgraph\\nenumeration approaches.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 23, 6, 49, 2, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Benchmarking Time Series Databases with IoTDB-Benchmark for IoT Scenarios',\n",
       "  'authors': ['Rui Liu', 'Jun Yuan'],\n",
       "  'summary': 'With the wide application of time series databases (TSDBs) in big data fields\\nlike cluster monitoring and industrial IoT, there have been developed a number\\nof TSDBs for time series data management. Different TSDBs have test reports\\ncomparing themselves with other databases to show their advantages, but the\\ncomparisons are typically based on their own tools without using a common\\nwell-recognized test framework. To the best of our knowledge, there is no\\nmature TSDB benchmark either. With the goal of establishing a standard of\\nevaluating TSDB systems, we present the IoTDB-Benchmark framework, specifically\\ndesigned for TSDB and IoT application scenarios. We pay close attention to some\\nspecial data ingestion scenarios and summarize 10 basic queries types. We use\\nthis benchmark to compare four TSDB systems: InfluxDB, OpenTSDB, KairosDB and\\nTimescaleDB. Our benchmark framework/tool not only measures performance metrics\\nbut also takes system resource consumption into consideration.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 24, 9, 40, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'HRDBMS: Combining the Best of Modern and Traditional Relational Databases',\n",
       "  'authors': ['Jason Arnold', 'Boris Glavic', 'Ioan Raicu'],\n",
       "  'summary': 'HRDBMS is a novel distributed relational database that uses a hybrid model\\ncombining the best of traditional distributed relational databases and Big Data\\nanalytics platforms such as Hive. This allows HRDBMS to leverage years worth of\\nresearch regarding query optimization, while also taking advantage of the\\nscalability of Big Data platforms. The system uses an execution framework that\\nis tailored for relational processing, thus addressing some of the performance\\nchallenges of running SQL on top of platforms such as MapReduce and Spark.\\nThese include excessive materialization of intermediate results, lack of a\\nglobal cost-based optimization, unnecessary sorting, lack of index support, no\\nstatistics, no support for DML and ACID, and excessive communication caused by\\nthe rigid communication patterns enforced by these platforms.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 24, 22, 20, 46, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Flexible Operator Embeddings via Deep Learning',\n",
       "  'authors': ['Ryan Marcus', 'Olga Papaemmanouil'],\n",
       "  'summary': \"Integrating machine learning into the internals of database management\\nsystems requires significant feature engineering, a human effort-intensive\\nprocess to determine the best way to represent the pieces of information that\\nare relevant to a task. In addition to being labor intensive, the process of\\nhand-engineering features must generally be repeated for each data management\\ntask, and may make assumptions about the underlying database that are not\\nuniversally true. We introduce flexible operator embeddings, a deep learning\\ntechnique for automatically transforming query operators into feature vectors\\nthat are useful for a multiple data management tasks and is custom-tailored to\\nthe underlying database. Our approach works by taking advantage of an\\noperator's context, resulting in a neural network that quickly transforms\\nsparse representations of query operators into dense, information-rich feature\\nvectors. Experimentally, we show that our flexible operator embeddings perform\\nwell across a number of data management tasks, using both synthetic and\\nreal-world datasets.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 25, 21, 33, 50, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Learning of High Dengue Incidence with Clustering and FP-Growth Algorithm using WHO Historical Data',\n",
       "  'authors': ['Franz Stewart V. Dizon',\n",
       "   'Stephen Kyle R. Farinas',\n",
       "   'Reynaldo John Tristan H. Mahinay Jr.',\n",
       "   'Harry S. Pardo',\n",
       "   'Cecil Jose A. Delfinado'],\n",
       "  'summary': 'This paper applies FP-Growth algorithm in mining fuzzy association rules for\\na prediction system of dengue. The system mines its rules through input of\\nhistoric predictor variables for dengue. The rules will be used to build a\\nrule-based classifier to predict the dengue incidence for the next month for\\nthe years 2001-2006 in the Philippines. The FP-Growth Algorithm was compared to\\nApriori Algorithm by Sensitivity, Specificity, PPV, NPV, execution time and\\nmemory usage. The results showed that FP-Growth Algorithm is significantly\\nbetter in execution time, numerically better in memory and comparable in\\nSensitivity, Specificity, PPV and NPV to Apriori Algorithm.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 1, 12, 3, 36, 1, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'SCAR: Strong Consistency using Asynchronous Replication with Minimal Coordination',\n",
       "  'authors': ['Yi Lu', 'Xiangyao Yu', 'Samuel Madden'],\n",
       "  'summary': \"Data replication is crucial in modern distributed systems as a means to\\nprovide high availability. Many techniques have been proposed to utilize\\nreplicas to improve a system's performance, often requiring expensive\\ncoordination or sacrificing consistency. In this paper, we present SCAR, a new\\ndistributed and replicated in-memory database that allows serializable\\ntransactions to read from backup replicas with minimal coordination. SCAR works\\nby assigning logical timestamps to database records so that a transaction can\\nsafely read from a backup replica without coordinating with the primary\\nreplica, because the records cannot be changed up to a certain logical time. In\\naddition, we propose two optimization techniques, timestamp synchronization and\\nparallel locking and validation, to further reduce coordination. We show that\\nSCAR outperforms systems with conventional concurrency control algorithms and\\nreplication strategies by up to a factor of 2 on three popular benchmarks. We\\nalso demonstrate that SCAR achieves higher throughput by running under reduced\\nisolation levels and detects concurrency anomalies in real time.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 1, 17, 31, 13, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Parallel Index-based Stream Join on a Multicore CPU',\n",
       "  'authors': ['Amirhesam Shahvarani', 'Hans-Arno Jacobsen'],\n",
       "  'summary': 'There is increasing interest in using multicore processors to accelerate\\nstream processing. For example, indexing sliding window content to enhance the\\nperformance of streaming queries is greatly improved by utilizing the\\ncomputational capabilities of a multicore processor. However, designing an\\neffective concurrency control mechanism that addresses the problem of\\nconcurrent indexing in highly dynamic settings remains a challenge. In this\\npaper, we introduce an index data structure, called the Partitioned In-memory\\nMerge-Tree, to address the challenges that arise when indexing highly dynamic\\ndata, which are common in streaming settings. To complement the index, we\\ndesign an algorithm to realize a parallel index-based stream join that exploits\\nthe computational power of multicore processors. Our experiments using an\\nocta-core processor show that our parallel stream join achieves up to 5.5 times\\nhigher throughput than a single-threaded approach.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 1, 18, 25, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'HISTEX (HISTory EXerciser) : A tool for testing the implementation of Isolation Levels of Relational Database Management Systems',\n",
       "  'authors': ['Dimitrios Liarokapis', 'Elizabeth ONeil', 'Patrick ONeil'],\n",
       "  'summary': 'We present a multi-process application called HISTEX (HISTory EXerciser),\\nwhich executes input histories in a generic transactional notation on\\ncommercial DBMS platforms. HISTEX could be used to discover potential errors in\\nthe implementation of Isolation Levels by Relational Database Management\\nSystems or cases where a system behaves over restrictively. It can also be used\\nfor performance measurements related to database workloads executing on real\\ndatabase systems instead of simulated environments. HISTEX has been implemented\\nin C by utilizing Embedded SQL. However, many of its ideas could be\\nreincarnated in new implementations that could rely on other database\\nconnectivity paradigms such as JDBC, JPA etc. We expect that by presenting some\\nof the ideas behind its development we could re-invigorate some fresh interest\\nand involvement in the research community regarding such tools.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 2, 16, 32, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Opportunistic View Materialization with Deep Reinforcement Learning',\n",
       "  'authors': ['Xi Liang', 'Aaron J. Elmore', 'Sanjay Krishnan'],\n",
       "  'summary': 'Carefully selected materialized views can greatly improve the performance of\\nOLAP workloads. We study using deep reinforcement learning to learn adaptive\\nview materialization and eviction policies. Our insight is that such selection\\npolicies can be effectively trained with an asynchronous RL algorithm, that\\nruns paired counter-factual experiments during system idle times to evaluate\\nthe incremental value of persisting certain views. Such a strategy obviates the\\nneed for accurate cardinality estimation or hand-designed scoring heuristics.\\nWe focus on inner-join views and modeling effects in a main-memory, OLAP\\nsystem. Our research prototype system, called DQM, is implemented in SparkSQL\\nand we experiment on several workloads including the Join Order Benchmark and\\nthe TPC-DS workload. Results suggest that: (1) DQM can outperform heuristic\\nwhen their assumptions are not satisfied by the workload or there are temporal\\neffects like period maintenance, (2) even with the cost of learning, DQM is\\nmore adaptive to changes in the workload, and (3) DQM is broadly applicable to\\ndifferent workloads and skews.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 4, 16, 54, 56, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Voyageur: An Experiential Travel Search Engine',\n",
       "  'authors': ['Sara Evensen',\n",
       "   'Aaron Feng',\n",
       "   'Alon Halevy',\n",
       "   'Jinfeng Li',\n",
       "   'Vivian Li',\n",
       "   'Yuliang Li',\n",
       "   'Huining Liu',\n",
       "   'George Mihaila',\n",
       "   'John Morales',\n",
       "   'Natalie Nuno',\n",
       "   'Ekaterina Pavlovic',\n",
       "   'Wang-Chiew Tan',\n",
       "   'Xiaolan Wang'],\n",
       "  'summary': 'We describe Voyageur, which is an application of experiential search to the\\ndomain of travel. Unlike traditional search engines for online services,\\nexperiential search focuses on the experiential aspects of the service under\\nconsideration. In particular, Voyageur needs to handle queries for subjective\\naspects of the service (e.g., quiet hotel, friendly staff) and combine these\\nwith objective attributes, such as price and location. Voyageur also highlights\\ninteresting facts and tips about the services the user is considering to\\nprovide them with further insights into their choices.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 4, 19, 25, 15, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Optimizing Subgraph Queries by Combining Binary and Worst-Case Optimal Joins',\n",
       "  'authors': ['Amine Mhedhbi', 'Semih Salihoglu'],\n",
       "  'summary': 'We study the problem of optimizing subgraph queries using the new worst-case\\noptimal join plans. Worst-case optimal plans evaluate queries by matching one\\nquery vertex at a time using multiway intersections. The core problem in\\noptimizing worst-case optimal plans is to pick an ordering of the query\\nvertices to match. We design a cost-based optimizer that (i) picks efficient\\nquery vertex orderings for worst-case optimal plans; and (ii) generates hybrid\\nplans that mix traditional binary joins with worst-case optimal style multiway\\nintersections. Our cost metric combines the cost of binary joins with a new\\ncost metric called intersection-cost. The plan space of our optimizer contains\\nplans that are not in the plan spaces based on tree decompositions from prior\\nwork. In addition to our optimizer, we describe an adaptive technique that\\nchanges the orderings of the worst-case optimal sub-plans during query\\nexecution. We demonstrate the effectiveness of the plans our optimizer picks\\nand adaptive technique through extensive experiments. Our optimizer is\\nintegrated into the Graphflow DBMS.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 5, 21, 57, 11, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Scheduling OLTP Transactions via Machine Learning',\n",
       "  'authors': ['Yangjun Sheng',\n",
       "   'Anthony Tomasic',\n",
       "   'Tieying Zhang',\n",
       "   'Andrew Pavlo'],\n",
       "  'summary': 'Current main memory database system architectures are still challenged by\\nhigh contention workloads and this challenge will continue to grow as the\\nnumber of cores in processors continues to increase. These systems schedule\\ntransactions randomly across cores to maximize concurrency and to produce a\\nuniform load across cores. Scheduling never considers potential conflicts.\\nPerformance could be improved if scheduling balanced between concurrency to\\nmaximize throughput and scheduling transactions linearly to avoid conflicts. In\\nthis paper, we present the design of several intelligent transaction scheduling\\nalgorithms that consider both potential transaction conflicts and concurrency.\\nTo incorporate reasoning about transaction conflicts, we develop a supervised\\nmachine learning model that estimates the probability of conflict. This model\\nis incorporated into several scheduling algorithms. In addition, we integrate\\nan unsupervised machine learning algorithm into an intelligent scheduling\\nalgorithm. We then empirically measure the performance impact of different\\nscheduling algorithms on OLTP and social networking workloads. Our results show\\nthat, with appropriate settings, intelligent scheduling can increase throughput\\nby 54% and reduce abort rate by 80% on a 20-core machine, relative to random\\nscheduling. In summary, the paper provides preliminary evidence that\\nintelligent scheduling significantly improves DBMS performance.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 7, 15, 27, 30, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Automated data validation: an industrial experience report',\n",
       "  'authors': ['Lei Zhang',\n",
       "   'Sean Howard',\n",
       "   'Tom Montpool',\n",
       "   'Jessica Moore',\n",
       "   'Krittika Mahajan',\n",
       "   'Andriy Miranskyy'],\n",
       "  'summary': 'There has been a massive explosion of data generated by customers and\\nretained by companies in the last decade. However, there is a significant\\nmismatch between the increasing volume of data and the lack of automation\\nmethods and tools. The lack of best practices in data science programming may\\nlead to software quality degradation, release schedule slippage, and budget\\noverruns. To mitigate these concerns, we would like to bring software\\nengineering best practices into data science. Specifically, we focus on\\nautomated data validation in the data preparation phase of the software\\ndevelopment life cycle.\\n  This paper studies a real-world industrial case and applies software\\nengineering best practices to develop an automated test harness called RESTORE.\\nWe release RESTORE as an open-source R package. Our experience report, done on\\nthe geodemographic data, shows that RESTORE enables efficient and effective\\ndetection of errors injected during the data preparation phase. RESTORE also\\nsignificantly reduced the cost of testing. We hope that the community benefits\\nfrom the open-source project and the practical advice based on our experience.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 8, 21, 47, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Transparency, Fairness, Data Protection, Neutrality: Data Management Challenges in the Face of New Regulation',\n",
       "  'authors': ['Serge Abiteboul', 'Julia Stoyanovich'],\n",
       "  'summary': \"The data revolution continues to transform every sector of science, industry\\nand government. Due to the incredible impact of data-driven technology on\\nsociety, we are becoming increasingly aware of the imperative to use data and\\nalgorithms responsibly -- in accordance with laws and ethical norms. In this\\narticle we discuss three recent regulatory frameworks: the European Union's\\nGeneral Data Protection Regulation (GDPR), the New York City Automated\\nDecisions Systems (ADS) Law, and the Net Neutrality principle, that aim to\\nprotect the rights of individuals who are impacted by data collection and\\nanalysis. These frameworks are prominent examples of a global trend:\\nGovernments are starting to recognize the need to regulate data-driven\\nalgorithmic technology.\\n  Our goal in this paper is to bring these regulatory frameworks to the\\nattention of the data management community, and to underscore the technical\\nchallenges they raise and which we, as a community, are well-equipped to\\naddress. The main take-away of this article is that legal and ethical norms\\ncannot be incorporated into data-driven systems as an afterthought. Rather, we\\nmust think in terms of responsibility by design, viewing it as a systems\\nrequirement.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 8, 22, 12, 12, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'RadegastXDB - Prototype of Native XML Database Management System: Technical Report',\n",
       "  'authors': ['Petr Lukáš', 'Radim Bača', 'Michal Krátký'],\n",
       "  'summary': 'A lot of advances in the processing of XML data have been proposed in last\\ntwo decades. There were many approaches focused on the efficient processing of\\ntwig pattern queries (TPQ). However, including the TPQ into an XQuery compiler\\nis not a straightforward task and current XML DBMSs process XQueries without\\nany TPQ detection. In this paper, we demonstrate our prototype of a native XML\\nDBMS called RadegastXDB that uses a TPQ detection to accelerate structural\\nXQueries. Such a detection allows us to utilize state-of-the-art TPQ processing\\nalgorithms. Our experiments show that, for the structural queries, these\\nalgorithms and state-of-the-art XML indexing techniques make our prototype\\nfaster than all of the current XML DBMSs, especially for large data\\ncollections. We also show that using the same techniques is also efficient for\\nthe processing of queries with value predicates.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 9, 8, 18, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Hybrid Data Cleaning Framework using Markov Logic Networks',\n",
       "  'authors': ['Yunjun Gao',\n",
       "   'Congcong Ge',\n",
       "   'Xiaoye Miao',\n",
       "   'Haobo Wang',\n",
       "   'Bin Yao',\n",
       "   'Qing Li'],\n",
       "  'summary': 'With the increase of dirty data, data cleaning turns into a crux of data\\nanalysis. Most of the existing algorithms rely on either qualitative techniques\\n(e.g., data rules) or quantitative ones (e.g., statistical methods). In this\\npaper, we present a novel hybrid data cleaning framework on top of Markov logic\\nnetworks (MLNs), termed as MLNClean, which is capable of cleaning both\\nschema-level and instance-level errors. MLNClean mainly consists of two\\ncleaning stages, namely, first cleaning multiple data versions separately (each\\nof which corresponds to one data rule), and then deriving the final clean data\\nbased on multiple data versions. Moreover, we propose a series of\\ntechniques/concepts, e.g., the MLN index, the concepts of reliability score and\\nfusion score, to facilitate the cleaning process. Extensive experimental\\nresults on both real and synthetic datasets demonstrate the superiority of\\nMLNClean to the state-of-the-art approach in terms of both accuracy and\\nefficiency.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 14, 6, 8, 46, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Adding Value by Combining Business and Sensor Data: An Industry 4.0 Use Case',\n",
       "  'authors': ['Guenter Hesse',\n",
       "   'Christoph Matthies',\n",
       "   'Werner Sinzig',\n",
       "   'Matthias Uflacker'],\n",
       "  'summary': \"Industry 4.0 and the Internet of Things are recent developments that have\\nlead to the creation of new kinds of manufacturing data. Linking this new kind\\nof sensor data to traditional business information is crucial for enterprises\\nto take advantage of the data's full potential. In this paper, we present a\\ndemo which allows experiencing this data integration, both vertically between\\ntechnical and business contexts and horizontally along the value chain. The\\ntool simulates a manufacturing company, continuously producing both business\\nand sensor data, and supports issuing ad-hoc queries that answer specific\\nquestions related to the business. In order to adapt to different environments,\\nusers can configure sensor characteristics to their needs.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 15, 10, 46, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'ExplainIt! -- A declarative root-cause analysis engine for time series data (extended version)',\n",
       "  'authors': ['Vimalkumar Jeyakumar',\n",
       "   'Omid Madani',\n",
       "   'Ali Parandeh',\n",
       "   'Ashutosh Kulshreshtha',\n",
       "   'Weifei Zeng',\n",
       "   'Navindra Yadav'],\n",
       "  'summary': 'We present ExplainIt!, a declarative, unsupervised root-cause analysis engine\\nthat uses time series monitoring data from large complex systems such as data\\ncentres. ExplainIt! empowers operators to succinctly specify a large number of\\ncausal hypotheses to search for causes of interesting events. ExplainIt! then\\nranks these hypotheses, reducing the number of causal dependencies from\\nhundreds of thousands to a handful for human understanding. We show how a\\ndeclarative language, such as SQL, can be effective in declaratively\\nenumerating hypotheses that probe the structure of an unknown probabilistic\\ngraphical causal model of the underlying system. Our thesis is that databases\\nare in a unique position to enable users to rapidly explore the possible causal\\nmechanisms in data collected from diverse sources. We empirically demonstrate\\nhow ExplainIt! had helped us resolve over 30 performance issues in a commercial\\nproduct since late 2014, of which we discuss a few cases in detail.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 19, 17, 42, 5, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Indexes in Microsoft SQL Server',\n",
       "  'authors': ['Sourav Mukherjee'],\n",
       "  'summary': 'Indexes are the best apposite choice for quickly retrieving the records. This\\nis nothing but cutting down the number of Disk IO. Instead of scanning the\\ncomplete table for the results, we can decrease the number of IO\\'s or page\\nfetches using index structures such as B-Trees or Hash Indexes to retrieve the\\ndata faster. The most convenient way to consider an index is to think like a\\ndictionary. It has words and its corresponding definitions against those words.\\nThe dictionary will have an index on \"word\" because when we open a dictionary\\nand we want to fetch its corresponding word quickly, then find its definition.\\nThe dictionary generally contains just a single index - an index ordered by\\nword. When we modify any record and change the corresponding value of an\\nindexed column in a clustered index, the database might require moving the\\nentire row into a separately new position to maintain the rows in the sorted\\norder. This action is essentially turned into an update query into a DELETE\\nfollowed by an INSERT, and it decreases the performance of the query. The\\nclustered index in the table can often be available on the primary key or a\\nforeign key column because key values usually do not modify once a record is\\ninjected into the database.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 20, 3, 55, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Reliability Maximization in Uncertain Graphs',\n",
       "  'authors': ['Xiangyu Ke',\n",
       "   'Arijit Khan',\n",
       "   'Mohammad Al Hasan',\n",
       "   'Rojin Rezvansangsari'],\n",
       "  'summary': 'Network reliability measures the probability that a target node is reachable\\nfrom a source node in an uncertain graph, i.e., a graph where every edge is\\nassociated with a probability of existence. In this paper, we investigate the\\nnovel and fundamental problem of adding a small number of edges in the\\nuncertain network for maximizing the reliability between a given pair of nodes.\\nWe study the NP-hardness and the approximation hardness of our problem, and\\ndesign effective, scalable solutions. Furthermore, we consider extended\\nversions of our problem (e.g., multiple source and target nodes can be provided\\nas input) to support and demonstrate a wider family of queries and\\napplications, including sensor network reliability maximization and social\\ninfluence maximization. Experimental results validate the effectiveness and\\nefficiency of the proposed algorithms.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 20, 16, 15, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Repairing mappings under policy views',\n",
       "  'authors': ['Angela Bonifati', 'Ugo Comignani', 'Efthymia Tsamoura'],\n",
       "  'summary': 'The problem of data exchange involves a source schema, a target schema and a\\nset of mappings from transforming the data between the two schemas. We study\\nthe problem of data exchange in the presence of privacy restrictions on the\\nsource. The privacy restrictions are expressed as a set of policy views\\nrepresenting the information that is safe to expose over all instances of the\\nsource. We propose a protocol that provides formal privacy guarantees and is\\ndata-independent, i.e., if certain criteria are met, then the protocol\\nguarantees that the mappings leak no sensitive information independently of the\\ndata that lies in the source. We also propose an algorithm for repairing an\\ninput mapping w.r.t. a set of policy views, in cases where the input mapping\\nleaks sensitive information. The empirical evaluation of our work shows that\\nthe proposed algorithm is quite efficient, repairing sets of 300 s-t tgds in an\\naverage time of 5s on a commodity machine. To the best of our knowledge, our\\nwork is the first one that studies the problems of exchanging data and\\nrepairing mappings under such privacy restrictions. Furthermore, our work is\\nthe first to provide practical algorithms for a logical privacy-preservation\\nparadigm, described as an open research challenge in previous work on this\\narea.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 21, 21, 28, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Approximate Query Processing using Deep Generative Models',\n",
       "  'authors': ['Saravanan Thirumuruganathan',\n",
       "   'Shohedul Hasan',\n",
       "   'Nick Koudas',\n",
       "   'Gautam Das'],\n",
       "  'summary': 'Data is generated at an unprecedented rate surpassing our ability to analyze\\nthem. The database community has pioneered many novel techniques for\\nApproximate Query Processing (AQP) that could give approximate results in a\\nfraction of time needed for computing exact results. In this work, we explore\\nthe usage of deep learning (DL) for answering aggregate queries specifically\\nfor interactive applications such as data exploration and visualization. We use\\ndeep generative models, an unsupervised learning based approach, to learn the\\ndata distribution faithfully such that aggregate queries could be answered\\napproximately by generating samples from the learned model. The model is often\\ncompact - few hundred KBs - so that arbitrary AQP queries could be answered on\\nthe client side without contacting the database server. Our other contributions\\ninclude identifying model bias and minimizing it through a rejection sampling\\nbased approach and an algorithm to build model ensembles for AQP for improved\\naccuracy. Our extensive experiments show that our proposed approach can provide\\nanswers with high accuracy and low latency.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 24, 15, 21, 19, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Apache Hive: From MapReduce to Enterprise-grade Big Data Warehousing',\n",
       "  'authors': ['Jesús Camacho-Rodríguez',\n",
       "   'Ashutosh Chauhan',\n",
       "   'Alan Gates',\n",
       "   'Eugene Koifman',\n",
       "   \"Owen O'Malley\",\n",
       "   'Vineet Garg',\n",
       "   'Zoltan Haindrich',\n",
       "   'Sergey Shelukhin',\n",
       "   'Prasanth Jayachandran',\n",
       "   'Siddharth Seth',\n",
       "   'Deepak Jaiswal',\n",
       "   'Slim Bouguerra',\n",
       "   'Nishant Bangarwa',\n",
       "   'Sankar Hariappan',\n",
       "   'Anishek Agarwal',\n",
       "   'Jason Dere',\n",
       "   'Daniel Dai',\n",
       "   'Thejas Nair',\n",
       "   'Nita Dembla',\n",
       "   'Gopal Vijayaraghavan',\n",
       "   'Günther Hagleitner'],\n",
       "  'summary': \"Apache Hive is an open-source relational database system for analytic\\nbig-data workloads. In this paper we describe the key innovations on the\\njourney from batch tool to fully fledged enterprise data warehousing system. We\\npresent a hybrid architecture that combines traditional MPP techniques with\\nmore recent big data and cloud concepts to achieve the scale and performance\\nrequired by today's analytic applications. We explore the system by detailing\\nenhancements along four main axis: Transactions, optimizer, runtime, and\\nfederation. We then provide experimental results to demonstrate the performance\\nof the system for typical workloads and conclude with a look at the community\\nroadmap.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 26, 15, 53, 49, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Lineage-Aware Temporal Windows: Supporting Set Operations in Temporal-Probabilistic Databases',\n",
       "  'authors': ['Katerina Papaioannou', 'Martin Theobald', 'Michael H. Böhlen'],\n",
       "  'summary': 'In temporal-probabilistic (TP) databases, the combination of the temporal and\\nthe probabilistic dimension adds significant overhead to the computation of set\\noperations. Although set queries are guaranteed to yield linearly sized output\\nrelations, existing solutions exhibit quadratic runtime complexity. They suffer\\nfrom redundant interval comparisons and additional joins for the formation of\\nlineage expressions. In this paper, we formally define the semantics of set\\noperations in TP databases and study their properties. For their efficient\\ncomputation, we introduce the lineage-aware temporal window, a mechanism that\\ndirectly binds intervals with lineage expressions. We suggest the lineage-aware\\nwindow advancer (LAWA) for producing the windows of two TP relations in\\nlinearithmic time, and we implement all TP set operations based on LAWA. By\\nexploiting the flexibility of lineage-aware temporal windows, we perform direct\\nfiltering of irrelevant intervals and finalization of output lineage\\nexpressions and thus guarantee that no additional computational cost or buffer\\nspace is needed. A series of experiments over both synthetic and real-world\\ndatasets show that (a) our approach has predictable performance, depending only\\non the input size and not on the number of time intervals per fact or their\\noverlap, and that (b) it outperforms state-of-the-art approaches in both\\ntemporal and probabilistic databases.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 1, 15, 17, 21, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'SharPer: Sharding Permissioned Blockchains Over Network Clusters',\n",
       "  'authors': ['Mohammad Javad Amiri', 'Divyakant Agrawal', 'Amr El Abbadi'],\n",
       "  'summary': 'Scalability is one of the main roadblocks to business adoption of blockchain\\nsystems. Despite recent intensive research on using sharding techniques to\\nenhance the scalability of blockchain systems, existing solutions do not\\nefficiently address cross-shard transactions. In this paper, we introduce\\nSharPer, a permissioned blockchain system that improves scalability by\\nclustering (partitioning) the nodes and assigning different data shards to\\ndifferent clusters where each data shard is replicated on the nodes of a\\ncluster. SharPer supports both intra-shard and cross-shard transactions and\\nprocesses intra-shard transactions of different clusters as well as cross-shard\\ntransactions with non-overlapping clusters simultaneously. In SharPer, the\\nblockchain ledger is formed as a directed acyclic graph where each cluster\\nmaintains only a view of the ledger. SharPer also incorporates a flattened\\nprotocol to establish consensus among clusters on the order of cross-shard\\ntransactions. The experimental results reveal the efficiency of SharPer in\\nterms of performance and scalability especially in workloads with a low\\npercentage of cross-shard transactions.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 2, 3, 51, 37, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Blueprint for Interoperable Blockchains',\n",
       "  'authors': ['Tien Tuan Anh Dinh', 'Anwitaman Datta', 'Beng Chin Ooi'],\n",
       "  'summary': 'Research in blockchain systems has mainly focused on improving security and\\nbridging the performance gaps between blockchains and databases. Despite many\\npromising results, we observe a worrying trend that the blockchain landscape is\\nfragmented in which many systems exist in silos. Apart from a handful of\\ngeneral-purpose blockchains, such as Ethereum or Hyperledger Fabric, there are\\nhundreds of others designed for specific applications and typically do not talk\\nto each other. In this paper, we describe our vision of interoperable\\nblockchains. We argue that supporting interaction among different blockchains\\nrequires overcoming challenges that go beyond data standardization. The\\nunderlying problem is to allow smart contracts running in different blockchains\\nto communicate. We discuss three open problems: access control, general\\ncross-chain transactions, and cross-chain communication. We describe partial\\nsolutions to some of these problems in the literature. Finally, we propose a\\nnovel design to overcome these challenges.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 2, 14, 44, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'E2FM: an encrypted and compressed full-text index for collections of genomic sequences',\n",
       "  'authors': ['Ferdinando Montecuollo',\n",
       "   'Giovannni Schmid',\n",
       "   'Roberto Tagliaferri'],\n",
       "  'summary': 'Next Generation Sequencing (NGS) platforms and, more generally,\\nhigh-throughput technologies are giving rise to an exponential growth in the\\nsize of nucleotide sequence databases. Moreover, many emerging applications of\\nnucleotide datasets -- as those related to personalized medicine -- require the\\ncompliance with regulations about the storage and processing of sensitive data.\\nWe have designed and carefully engineered E2FM-index, a new full-text index in\\nminute space which was optimized for compressing and encrypting nucleotide\\nsequence collections in FASTA format and for performing fast pattern-search\\nqueries. E2FM-index allows to build self-indexes which occupy till to 1/20 of\\nthe storage required by the input FASTA file, thus permitting to save about 95%\\nof storage when indexing collections of highly similar sequences; moreover, it\\ncan exactly search the built indexes for patterns in times ranging from few\\nmilliseconds to a few hundreds milliseconds, depending on pattern length.\\nSupplementary material and supporting datasets are available through\\nBioinformatics Online and https://figshare.com/s/6246ee9c1bd730a8bf6e.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 10, 15, 19, 19, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'LISA: Towards Learned DNA Sequence Search',\n",
       "  'authors': ['Darryl Ho',\n",
       "   'Jialin Ding',\n",
       "   'Sanchit Misra',\n",
       "   'Nesime Tatbul',\n",
       "   'Vikram Nathan',\n",
       "   'Vasimuddin Md',\n",
       "   'Tim Kraska'],\n",
       "  'summary': 'Next-generation sequencing (NGS) technologies have enabled affordable\\nsequencing of billions of short DNA fragments at high throughput, paving the\\nway for population-scale genomics. Genomics data analytics at this scale\\nrequires overcoming performance bottlenecks, such as searching for short DNA\\nsequences over long reference sequences. In this paper, we introduce LISA\\n(Learned Indexes for Sequence Analysis), a novel learning-based approach to DNA\\nsequence search. As a first proof of concept, we focus on accelerating one of\\nthe most essential flavors of the problem, called exact search. LISA builds on\\nand extends FM-index, which is the state-of-the-art technique widely deployed\\nin genomics tool-chains. Initial experiments with human genome datasets\\nindicate that LISA achieves up to a factor of 4X performance speedup against\\nits traditional counterpart.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 10, 17, 41, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'LiveGraph: A Transactional Graph Storage System with Purely Sequential Adjacency List Scans',\n",
       "  'authors': ['Xiaowei Zhu',\n",
       "   'Guanyu Feng',\n",
       "   'Marco Serafini',\n",
       "   'Xiaosong Ma',\n",
       "   'Jiping Yu',\n",
       "   'Lei Xie',\n",
       "   'Ashraf Aboulnaga',\n",
       "   'Wenguang Chen'],\n",
       "  'summary': \"The specific characteristics of graph workloads make it hard to design a\\none-size-fits-all graph storage system. Systems that support transactional\\nupdates use data structures with poor data locality, which limits the\\nefficiency of analytical workloads or even simple edge scans. Other systems run\\ngraph analytics workloads efficiently, but cannot properly support\\ntransactions.\\n  This paper presents LiveGraph, a graph storage system that outperforms both\\nthe best graph transactional systems and the best systems for real-time graph\\nanalytics on fresh data. LiveGraph does that by ensuring that adjacency list\\nscans, a key operation in graph workloads, are purely sequential: they never\\nrequire random accesses even in presence of concurrent transactions. This is\\nachieved by combining a novel graph-aware data structure, the Transactional\\nEdge Log (TEL), together with a concurrency control mechanism that leverages\\nTEL's data layout. Our evaluation shows that LiveGraph significantly\\noutperforms state-of-the-art (graph) database solutions on both transactional\\nand real-time analytical workloads.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 13, 15, 57, 13, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Semantic Guided and Response Times Bounded Top-k Similarity Search over Knowledge Graphs',\n",
       "  'authors': ['Yuxiang Wang',\n",
       "   'Arijit Khan',\n",
       "   'Tianxing Wu',\n",
       "   'Jiahui Jin',\n",
       "   'Haijiang Yan'],\n",
       "  'summary': \"Recently, graph query is widely adopted for querying knowledge graphs. Given\\na query graph $G_Q$, the graph query finds subgraphs in a knowledge graph $G$\\nthat exactly or approximately match $G_Q$. We face two challenges on graph\\nquery: (1) the structural gap between $G_Q$ and the predefined schema in $G$\\ncauses mismatch with query graph, (2) users cannot view the answers until the\\ngraph query terminates, leading to a longer system response time (SRT). In this\\npaper, we propose a semantic-guided and response-time-bounded graph query to\\nreturn the top-k answers effectively and efficiently. We leverage a knowledge\\ngraph embedding model to build the semantic graph $SG_Q$, and we define the\\npath semantic similarity ($pss$) over $SG_Q$ as the metric to evaluate the\\nanswer's quality. Then, we propose an A* semantic search on $SG_Q$ to find the\\ntop-k answers with the greatest $pss$ via a heuristic $pss$ estimation.\\nFurthermore, we make an approximate optimization on A* semantic search to allow\\nusers to trade off the effectiveness for SRT within a user-specific time bound.\\nExtensive experiments over real datasets confirm the effectiveness and\\nefficiency of our solution.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 15, 8, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'On foundational aspects of RDF and SPARQL',\n",
       "  'authors': ['Dominique Duval', 'Rachid Echahed', 'Frederic Prost'],\n",
       "  'summary': 'We consider the recommendations of the World Wide Web Consortium (W3C) about\\nthe Resource Description Framework (RDF) and the associated query language\\nSPARQL. We propose a new formal framework based on category theory which\\nprovides clear and concise formal definitions of the main basic features of RDF\\nand SPARQL. We propose to define the notions of RDF graphs as well as SPARQL\\nbasic graph patterns as objects of some nested categories. This allows one to\\nclarify, in particular, the role of blank nodes. Furthermore, we consider basic\\nSPARQL CONSTRUCT and SELECT queries and formalize their operational semantics\\nfollowing a novel algebraic graph transformation approach called POIM.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 16, 8, 21, 57, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Service Wrapper: a system for converting web data into web services',\n",
       "  'authors': ['Naibo Wang',\n",
       "   'Zhiling Luo',\n",
       "   'Xiya Lyu',\n",
       "   'Zitong Yang',\n",
       "   'Jianwei Yin'],\n",
       "  'summary': 'Web services are widely used in many areas via callable APIs, however, data\\nare not always available in this way. We always need to get some data from web\\npages whose structure is not in order. Many developers use web data extraction\\nmethods to generate wrappers to get useful contents from websites and convert\\nthem into well-structured files. These methods, however, are designed\\nspecifically for professional wrapper program developers and not friendly to\\nusers without expertise in this domain. In this work, we construct a service\\nwrapper system to convert available data in web pages into web services.\\nAdditionally, a set of algorithms are introduced to solve problems in the whole\\nconversion process. People can use our system to convert web data into web\\nservices with fool-style operations and invoke these services by one simple\\nstep, which greatly expands the use of web data. Our cases show the ease of\\nuse, high availability, and stability of our system.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 17, 9, 29, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'MV-PBT: Multi-Version Index for Large Datasets and HTAP Workloads',\n",
       "  'authors': ['Christian Riegger',\n",
       "   'Tobias Vincon',\n",
       "   'Robert Gottstein',\n",
       "   'Ilia Petrov'],\n",
       "  'summary': \"Modern mixed (HTAP) workloads execute fast update-transactions and\\nlong-running analytical queries on the same dataset and system. In\\nmulti-version (MVCC) systems, such workloads result in many short-lived\\nversions and long version-chains as well as in increased and frequent\\nmaintenance overhead. Consequently, the index pressure increases significantly.\\nFirstly, the frequent modifications cause frequent creation of new versions,\\nyielding a surge in index maintenance overhead. Secondly and more importantly,\\nindex-scans incur extra I/O overhead to determine, which of the resulting\\ntuple-versions are visible to the executing transaction (visibility-check) as\\ncurrent designs only store version/timestamp information in the base table --\\nnot in the index. Such index-only visibility-check is critical for HTAP\\nworkloads on large datasets. In this paper we propose the Multi-Version\\nPartitioned B-Tree (MV-PBT) as a version-aware index structure, supporting\\nindex-only visibility checks and flash-friendly I/O patterns. The experimental\\nevaluation indicates a 2x improvement for analytical queries and 15% higher\\ntransactional throughput under HTAP workloads (CH-Benchmark). MV-PBT offers 40%\\nhigher transactional throughput compared to WiredTiger's LSM-Tree\\nimplementation under YCSB.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 17, 16, 51, 44, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An LSM-based Tuple Compaction Framework for Apache AsterixDB (Extended Version)',\n",
       "  'authors': ['Wail Y. Alkowaileet', 'Sattam Alsubaiee', 'Michael J. Carey'],\n",
       "  'summary': 'Document database systems store self-describing semi-structured records, such\\nas JSON, \"as-is\" without requiring the users to pre-define a schema. This\\nprovides users with the flexibility to change the structure of incoming records\\nwithout worrying about taking the system offline or hindering the performance\\nof currently running queries. However, the flexibility of such systems does not\\nfree. The large amount of redundancy in the records can introduce an\\nunnecessary storage overhead and impact query performance.\\n  Our focus in this paper is to address the storage overhead issue by\\nintroducing a tuple compactor framework that infers and extracts the schema\\nfrom self-describing semi-structured records during the data ingestion. As many\\nprominent document stores, such as MongoDB and Couchbase, adopt Log Structured\\nMerge (LSM) trees in their storage engines, our framework exploits LSM\\nlifecycle events to piggyback the schema inference and extraction operations.\\nWe have implemented and empirically evaluated our approach to measure its\\nimpact on storage, data ingestion, and query performance in the context of\\nApache AsterixDB.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 17, 22, 13, 40, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Effective Discovery of Meaningful Outlier Relationships',\n",
       "  'authors': ['Aline Bessa',\n",
       "   'Juliana Freire',\n",
       "   'Divesh Srivastava',\n",
       "   'Tamraparni Dasu'],\n",
       "  'summary': 'We propose PODS (Predictable Outliers in Data-trendS), a method that, given a\\ncollection of temporal data sets, derives data-driven explanations for outliers\\nby identifying meaningful relationships between them. First, we formalize the\\nnotion of meaningfulness, which so far has been informally framed in terms of\\nexplainability. Next, since outliers are rare and it is difficult to determine\\nwhether their relationships are meaningful, we develop a new criterion that\\ndoes so by checking if these relationships could have been predicted from\\nnon-outliers, i.e., if we could see the outlier relationships coming. Finally,\\nsearching for meaningful outlier relationships between every pair of data sets\\nin a large data collection is computationally infeasible. To address that, we\\npropose an indexing strategy that prunes irrelevant comparisons across data\\nsets, making the approach scalable. We present the results of an experimental\\nevaluation using real data sets and different baselines, which demonstrates the\\neffectiveness, robustness, and scalability of our approach.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 19, 2, 3, 15, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Exploiting Data Skew for Improved Query Performance',\n",
       "  'authors': ['Wangda Zhang', 'Kenneth A. Ross'],\n",
       "  'summary': 'Analytic queries enable sophisticated large-scale data analysis within many\\ncommercial, scientific and medical domains today. Data skew is a ubiquitous\\nfeature of these real-world domains. In a retail database, some products are\\ntypically much more popular than others. In a text database, word frequencies\\nfollow a Zipf distribution with a small number of very common words, and a long\\ntail of infrequent words. In a geographic database, some regions have much\\nhigher populations (and data measurements) than others. Current systems do not\\nmake the most of caches for exploiting skew. In particular, a whole cache line\\nmay remain cache resident even though only a small part of the cache line\\ncorresponds to a popular data item. In this paper, we propose a novel index\\nstructure for repositioning data items to concentrate popular items into the\\nsame cache lines. The net result is better spatial locality, and better\\nutilization of limited cache resources. We develop a theoretical model for\\nanalyzing the cache behavior, and implement database operators that are\\nefficient in the presence of skew. Our experiments on real and synthetic data\\nshow that exploiting skew can significantly improve in-memory query\\nperformance. In some cases, our techniques can speed up queries by over an\\norder of magnitude.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 22, 16, 5, 2, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Integrating Information About Entities Progressively',\n",
       "  'authors': ['Ben McCamish',\n",
       "   'Christopher Buss',\n",
       "   'Arash Termehchy',\n",
       "   'David Maier'],\n",
       "  'summary': 'Users often have to integrate information about entities from multiple data\\nsources. This task is challenging as each data source may represent information\\nabout the same entity in a distinct form, e.g., each data source may use a\\ndifferent name for the same person. Currently, data from different\\nrepresentations are translated into a unified one via lengthy and costly expert\\nattention and tuning. Such methods cannot scale to the rapidly increasing\\nnumber and variety of available data sources. We demonstrate ProgMap, a\\nentity-matching framework in which data sources learn to collaborate and\\nintegrate information about entities on-demand and with minimal expert\\nintervention. The data sources leverage user feedback to improve the accuracy\\nof their collaboration and results. ProgMap also has techniques to reduce the\\namount of required user feedback to achieve effective matchings.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 22, 22, 40, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Toward Co-existing Database Schemas based on Bidirectional Transformation',\n",
       "  'authors': ['Jumpei Tanaka',\n",
       "   'Van-Dang Tran',\n",
       "   'Hiroyuki kato',\n",
       "   'Zhenjiang Hu'],\n",
       "  'summary': 'According to strong demands for rapid and reliable software delivery,\\nco-existing database schema versions with multiple application versions are\\nreality to contribute them. Current database management systems do not support\\nco-existing schema versions in one database. Although a design of co-existing\\nschema based on updatable view tables was previously proposed, its flexibility\\nis limited due to pre-defined several restrictions to achieve data\\nsynchronization among schemas and handling independent unsynchronized data in\\neach schema. In this preliminary report, we present a new approach for\\nco-existing schemas based on bidirectional transformation. We explain the\\nrequired properties to realize co-existing schemas, bidirectionality and\\ntotality. We show that the co-existing schemas can be implemented\\nsystematically by applying putback-based bidirectional transformation to\\nsatisfy both the bidirectionality and the totality. While the bidirectionality\\ncan be satisfied by applying bidirectional transformation, to satisfy the\\ntotality, extra functions need to be introduced. How to derive these extra\\nfunctions is presented.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 24, 7, 50, 56, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Toward a view-based data cleaning architecture',\n",
       "  'authors': ['Toshiyuki Shimizu', 'Hiroki Omori', 'Masatoshi Yoshikawa'],\n",
       "  'summary': 'Big data analysis has become an active area of study with the growth of\\nmachine learning techniques. To properly analyze data, it is important to\\nmaintain high-quality data. Thus, research on data cleaning is also important.\\nIt is difficult to automatically detect and correct inconsistent values for\\ndata requiring expert knowledge or data created by many contributors, such as\\nintegrated data from heterogeneous data sources. An example of such data is\\nmetadata for scientific datasets, which should be confirmed by data managers\\nwhile handling the data. To support the efficient cleaning of data by data\\nmanagers, we propose a data cleaning architecture in which data managers\\ninteractively browse and correct portions of data through views. In this paper,\\nwe explain our view-based data cleaning architecture and discuss some remaining\\nissues.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 24, 11, 52, 13, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Typical Snapshots Selection for Shortest Path Query in Dynamic Road Networks',\n",
       "  'authors': ['Mengxuan Zhang', 'Lei Li', 'Wen Hua', 'Xiaofang Zhou'],\n",
       "  'summary': 'Finding the shortest paths in road network is an important query in our life\\nnowadays, and various index structures are constructed to speed up the query\\nanswering. However, these indexes can hardly work in real-life scenario because\\nthe traffic condition changes dynamically, which makes the pathfinding slower\\nthan in the static environment. In order to speed up path query answering in\\nthe dynamic road network, we propose a framework to support these indexes.\\nFirstly, we view the dynamic graph as a series of static snapshots. After that,\\nwe propose two kinds of methods to select the typical snapshots. The first kind\\nis time-based and it only considers the temporal information. The second\\ncategory is the graph representation-based, which considers more insights:\\nedge-based that captures the road continuity, and vertex-based that reflects\\nthe region traffic fluctuation. Finally, we propose the snapshot matching to\\nfind the most similar typical snapshot for the current traffic condition and\\nuse its index to answer the query directly. Extensive experiments on real-life\\nroad network and traffic conditions validate the effectiveness of our approach.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 27, 13, 28, 13, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Survey on Map-Matching Algorithms',\n",
       "  'authors': ['Pingfu Chao', 'Yehong Xu', 'Wen Hua', 'Xiaofang Zhou'],\n",
       "  'summary': 'The map-matching is an essential preprocessing step for most of the\\ntrajectory-based applications. Although it has been an active topic for more\\nthan two decades and, driven by the emerging applications, is still under\\ndevelopment. There is a lack of categorisation of existing solutions recently\\nand analysis for future research directions. In this paper, we review the\\ncurrent status of the map-matching problem and survey the existing algorithms.\\nWe propose a new categorisation of the solutions according to their\\nmap-matching models and working scenarios. In addition, we experimentally\\ncompare three representative methods from different categories to reveal how\\nmatching model affects the performance. Besides, the experiments are conducted\\non multiple real datasets with different settings to demonstrate the influence\\nof other factors in map-matching problem, like the trajectory quality, data\\ncompression and matching latency.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 29, 3, 26, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Informal Data Transformation Considered Harmful',\n",
       "  'authors': ['Eric Daimler', 'Ryan Wisnesky'],\n",
       "  'summary': 'In this paper we take the common position that AI systems are limited more by\\nthe integrity of the data they are learning from than the sophistication of\\ntheir algorithms, and we take the uncommon position that the solution to\\nachieving better data integrity in the enterprise is not to clean and validate\\ndata ex-post-facto whenever needed (the so-called data lake approach to data\\nmanagement, which can lead to data scientists spending 80% of their time\\ncleaning data), but rather to formally and automatically guarantee that data\\nintegrity is preserved as it transformed (migrated, integrated, composed,\\nqueried, viewed, etc) throughout the enterprise, so that data and programs that\\ndepend on that data need not constantly be re-validated for every particular\\nuse.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 2, 6, 26, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'RDF 1.1: Knowledge Representation and Data Integration Language for the Web',\n",
       "  'authors': ['Dominik Tomaszuk', 'David Hyland-Wood'],\n",
       "  'summary': \"Resource Description Framework (RDF) can seen as a solution in today's\\nlandscape of knowledge representation research. An RDF language has symmetrical\\nfeatures because subjects and objects in triples can be interchangeably used.\\nMoreover, the regularity and symmetry of the RDF language allow knowledge\\nrepresentation that is easily processed by machines, and because its structure\\nis similar to natural languages, it is reasonably readable for people. RDF\\nprovides some useful features for generalized knowledge representation. Its\\ndistributed nature, due to its identifier grounding in IRIs, naturally scales\\nto the size of the Web. However, its use is often hidden from view and is,\\ntherefore, one of the less well-known of the knowledge representation\\nframeworks. Therefore, we summarise RDF v1.0 and v1.1 to broaden its audience\\nwithin the knowledge representation community. This article reviews current\\napproaches, tools, and applications for mapping from relational databases to\\nRDF and from XML to RDF. We discuss RDF serializations, including formats with\\nsupport for multiple graphs and we analyze RDF compression proposals. Finally,\\nwe present a summarized formal definition of RDF 1.1 that provides additional\\ninsights into the modeling of reification, blank nodes, and entailments.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 2, 13, 44, 19, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Statistical Detection of Collective Data Fraud',\n",
       "  'authors': ['Ruoyu Wang',\n",
       "   'Xiaobo Hu',\n",
       "   'Daniel Sun',\n",
       "   'Guoqiang Li',\n",
       "   'Raymond Wong',\n",
       "   'Shiping Chen',\n",
       "   'Jianquan Liu'],\n",
       "  'summary': 'Statistical divergence is widely applied in multimedia processing, basically\\ndue to regularity and interpretable features displayed in data. However, in a\\nbroader range of data realm, these advantages may no longer be feasible, and\\ntherefore a more general approach is required. In data detection, statistical\\ndivergence can be used as a similarity measurement based on collective\\nfeatures. In this paper, we present a collective detection technique based on\\nstatistical divergence. The technique extracts distribution similarities among\\ndata collections, and then uses the statistical divergence to detect collective\\nanomalies. Evaluation shows that it is applicable in the real world.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 3, 2, 5, 8, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Distributed Nonblocking Commit Protocols for Many-Party Cross-Blockchain Transactions',\n",
       "  'authors': ['Xinying Wang',\n",
       "   'Olamide Timothy Tawose',\n",
       "   'Feng Yan',\n",
       "   'Dongfang Zhao'],\n",
       "  'summary': \"The interoperability across multiple blockchains would play a critical role\\nin future blockchain-based data management paradigm. Existing techniques either\\nwork only for two blockchains or requires a centralized component to govern the\\ncross-blockchain transaction execution, neither of which would meet the\\nscalability requirement. This paper proposes a new distributed commit protocol,\\nnamely \\\\textit{cross-blockchain transaction} (CBT), for conducting transactions\\nacross an arbitrary number of blockchains without any centralized component.\\nThe key idea of CBT is to extend the two-phase commit protocol with a heartbeat\\nmechanism to ensure the liveness of CBT without introducing additional nodes or\\nblockchains. We have implemented CBT and compared it to the state-of-the-art\\nprotocols, demonstrating CBT's low overhead (3.6\\\\% between two blockchains,\\nless than $1\\\\%$ among 32 or more blockchains) and high scalability (linear\\nscalability on up to 64-blockchain transactions). In addition, we developed a\\ngraphic user interface for users to virtually monitor the status of the\\ncross-blockchain transactions.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 5, 5, 58, 41, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Clustering based Privacy Preserving of Big Data using Fuzzification and Anonymization Operation',\n",
       "  'authors': ['Saira Khan',\n",
       "   'Khalid Iqbal',\n",
       "   'Safi Faizullah',\n",
       "   'Muhammad Fahad',\n",
       "   'Jawad Ali',\n",
       "   'Waqas Ahmed'],\n",
       "  'summary': 'Big Data is used by data miner for analysis purpose which may contain\\nsensitive information. During the procedures it raises certain privacy\\nchallenges for researchers. The existing privacy preserving methods use\\ndifferent algorithms that results into limitation of data reconstruction while\\nsecuring the sensitive data. This paper presents a clustering based privacy\\npreservation probabilistic model of big data to secure sensitive\\ninformation..model to attain minimum perturbation and maximum privacy. In our\\nmodel, sensitive information is secured after identifying the sensitive data\\nfrom data clusters to modify or generalize it.The resulting dataset is analysed\\nto calculate the accuracy level of our model in terms of hidden data, lossed\\ndata as result of reconstruction. Extensive experiements are carried out in\\norder to demonstrate the results of our proposed model. Clustering based\\nPrivacy preservation of individual data in big data with minimum perturbation\\nand successful reconstruction highlights the significance of our model in\\naddition to the use of standard performance evaluation measures.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 6, 11, 31, 12, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Monte Carlo Tree Search for Generating Interactive Data Analysis Interfaces',\n",
       "  'authors': ['Yiru Chen', 'Eugene Wu'],\n",
       "  'summary': 'Interactive tools like user interfaces help democratize data access for\\nend-users by hiding underlying programming details and exposing the necessary\\nwidget interface to users. Since customized interfaces are costly to build,\\nautomated interface generation is desirable. SQL is the dominant way to analyze\\ndata and there already exists logs to analyze data. Previous work proposed a\\nsyntactic approach to analyze structural changes in SQL query logs and\\nautomatically generates a set of widgets to express the changes. However, they\\ndo not consider layout usability and the sequential order of queries in the\\nlog. We propose to adopt Monte Carlo Tree Search(MCTS) to search for the\\noptimal interface that accounts for hierarchical layout as well as the\\nusability in terms of how easy to express the query log.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 7, 6, 1, 30, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data Structure Primitives on Persistent Memory: An Evaluation',\n",
       "  'authors': ['Philipp Götze', 'Arun Kumar Tharanatha', 'Kai-Uwe Sattler'],\n",
       "  'summary': 'Persistent Memory (PMem), as already available, e.g., with Intel Optane DC\\nPersistent Memory, represents a very promising, next-generation memory solution\\nwith a significant impact on database architectures. Several data structures\\nfor this new technology and its properties have already been proposed. However,\\nprimarily only complete structures are presented and evaluated. Thus, the\\nimplications of the individual ideas and PMem features are concealed.\\nTherefore, in this paper, we disassemble the structures presented so far,\\nidentify their underlying design primitives, and assign them to appropriate\\ndesign goals regarding PMem. As a result of our comprehensive experiments on\\nreal PM hardware, we can reveal the trade-offs of the primitives for various\\naccess patterns. This allowed us to pinpoint their best use cases as well as\\nvulnerabilities. Besides our general insights regarding PMem-based data\\nstructure design, we also discovered new combinations not examined in the\\nliterature so far.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 7, 16, 56, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Extracting Multiple Viewpoint Models from Relational Databases',\n",
       "  'authors': ['Alessandro Berti', 'Wil van der Aalst'],\n",
       "  'summary': 'Much time in process mining projects is spent on finding and understanding\\ndata sources and extracting the event data needed. As a result, only a fraction\\nof time is spent actually applying techniques to discover, control and predict\\nthe business process. Moreover, current process mining techniques assume a\\nsingle case notion. However, in reallife processes often different case notions\\nare intertwined. For example, events of the same order handling process may\\nrefer to customers, orders, order lines, deliveries, and payments. Therefore,\\nwe propose to use Multiple Viewpoint (MVP) models that relate events through\\nobjects and that relate activities through classes. The required event data are\\nmuch closer to existing relational databases. MVP models provide a holistic\\nview on the process, but also allow for the extraction of classical event logs\\nusing different viewpoints. This way existing process mining techniques can be\\nused for each viewpoint without the need for new data extractions and\\ntransformations. We provide a toolchain allowing for the discovery of MVP\\nmodels (annotated with performance and frequency information) from relational\\ndatabases. Moreover, we demonstrate that classical process mining techniques\\ncan be applied to any selected viewpoint.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 8, 15, 15, 32, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'GeoCMS : Towards a Geo-Tagged Media Management System',\n",
       "  'authors': ['Jang You Park', 'YongHee Jung', 'Wei Ding', 'Kwang Woo Nam'],\n",
       "  'summary': \"In this paper, we propose the design and implementation of the new geotagged\\nmedia management system. A large amount of daily geo-tagged media data\\ngenerated by user's smart phone, mobile device, dash cam and camera. Geotagged\\nmedia, such as geovideos and geophotos, can be captured with spatial temporal\\ninformation such as time, location, visible area, camera direction, moving\\ndirection and visible distance information. Due to the increase in geo-tagged\\nmultimedia data, the researches for efficient managing and mining geo-tagged\\nmultimedia are newly expected to be a new area in database and data mining.\\nThis paper proposes a geo-tagged media management system, so called Open\\nGeoCMS(Geotagged media Contents Management System). Open GeoCMS is a new\\nframework to manage geotagged media data on the web. Our framework supports\\nvarious types which are for moving point, moving photo - a sequence of photos\\nby a drone, moving double and moving video. Also, GeoCMS has the label viewer\\nand editor system for photos and videos. The Open GeoCMS have been developed as\\nan open source system.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 10, 2, 25, 58, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Case Study on Visualizing Large Spatial Datasets in a Web-based Map Viewer',\n",
       "  'authors': ['Alejandro Cortiñas', 'Miguel R. Luaces', 'Tirso V. Rodeiro'],\n",
       "  'summary': 'Lately, many companies are using Mobile Workforce Management technologies\\ncombined with information collected by sensors from mobile devices in order to\\nimprove their business processes. Even for small companies, the information\\nthat needs to be handled grows at a high rate, and most of the data collected\\nhave a geographic dimension. Being able to visualize this data in real-time\\nwithin a map viewer is a very important deal for these companies. In this paper\\nwe focus on this topic, presenting a case study on visualizing large spatial\\ndatasets. Particularly, since most of the Mobile Workforce Management software\\nis web-based, we propose a solution suitable for this environment.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 14, 17, 9, 34, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An Efficient and Wear-Leveling-Aware Frequent-Pattern Mining on Non-Volatile Memory',\n",
       "  'authors': ['Jiaqi Dong',\n",
       "   'Runyu Zhang',\n",
       "   'Chaoshu Yang',\n",
       "   'Yujuan Tan',\n",
       "   'Duo Liu'],\n",
       "  'summary': 'Frequent-pattern mining is a common approach to reveal the valuable hidden\\ntrends behind data. However, existing frequent-pattern mining algorithms are\\ndesigned for DRAM, instead of persistent memories (PMs), which can lead to\\nsevere performance and energy overhead due to the utterly different\\ncharacteristics between DRAM and PMs when they are running on PMs. In this\\npaper, we propose an efficient and Wear-leveling-aware Frequent-Pattern Mining\\nscheme, WFPM, to solve this problem. The proposed WFPM is evaluated by a series\\nof experiments based on realistic datasets from diversified application\\nscenarios, where WFPM achieves 32.0% performance improvement and prolongs the\\nNVM lifetime of header table by 7.4x over the EvFP-Tree.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 15, 7, 21, 11, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Complete and Sufficient Spatial Domination of Multidimensional Rectangles',\n",
       "  'authors': ['Tobias Emrich',\n",
       "   'Hans-Peter Kriegel',\n",
       "   'Andreas Züfle',\n",
       "   'Peer Kröger',\n",
       "   'Matthias Renz'],\n",
       "  'summary': 'Rectangles are used to approximate objects, or sets of objects, in a plethora\\nof applications, systems and index structures. Many tasks, such as nearest\\nneighbor search and similarity ranking, require to decide if objects in one\\nrectangle A may, must, or must not be closer to objects in a second rectangle\\nB, than objects in a third rectangle R. To decide this relation of \"Spatial\\nDomination\" it can be shown that using minimum and maximum distances it is\\noften impossible to detect spatial domination. This spatial gem provides a\\nnecessary and sufficient decision criterion for spatial domination that can be\\ncomputed efficiently even in higher dimensional space. In addition, this\\nspatial gem provides an example, pseudocode and an implementation in Python.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 15, 22, 24, 40, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Query Results over Ongoing Databases that Remain Valid as Time Passes By (Extended Version)',\n",
       "  'authors': ['Yvonne Mülle', 'Michael H. Böhlen'],\n",
       "  'summary': 'Ongoing time point now is used to state that a tuple is valid from the start\\npoint onward. For database systems ongoing time points have far-reaching\\nimplications since they change continuously as time passes by. State-of-the-art\\napproaches deal with ongoing time points by instantiating them to the reference\\ntime. The instantiation yields query results that are only valid at the chosen\\ntime and get invalidated as time passes by. We propose a solution that keeps\\nongoing time points uninstantiated during query processing. We do so by\\nevaluating predicates and functions at all possible reference times. This\\nrenders query results independent of a specific reference time and yields\\nresults that remain valid as time passes by. As query results, we propose\\nongoing relations that include a reference time attribute. The value of the\\nreference time attribute is restricted by predicates and functions on ongoing\\nattributes. We describe and evaluate an efficient implementation of ongoing\\ndata types and operations in PostgreSQL.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 16, 10, 7, 25, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Generative Datalog with Continuous Distributions',\n",
       "  'authors': ['Martin Grohe',\n",
       "   'Benjamin Lucien Kaminski',\n",
       "   'Joost-Pieter Katoen',\n",
       "   'Peter Lindner'],\n",
       "  'summary': 'Arguing for the need to combine declarative and probabilistic programming,\\nB\\\\\\'ar\\\\\\'any et al. (TODS 2017) recently introduced a probabilistic extension of\\nDatalog as a \"purely declarative probabilistic programming language.\" We\\nrevisit this language and propose a more principled approach towards defining\\nits semantics based on stochastic kernels and Markov processes - standard\\nnotions from probability theory. This allows us to extend the semantics to\\ncontinuous probability distributions, thereby settling an open problem posed by\\nB\\\\\\'ar\\\\\\'any et al.\\n  We show that our semantics is fairly robust, allowing both parallel execution\\nand arbitrary chase orders when evaluating a program. We cast our semantics in\\nthe framework of infinite probabilistic databases (Grohe and Lindner, ICDT\\n2020), and show that the semantics remains meaningful even when the input of a\\nprobabilistic Datalog program is an arbitrary probabilistic database.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 17, 15, 2, 21, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Siamese Graph Neural Networks for Data Integration',\n",
       "  'authors': ['Evgeny Krivosheev',\n",
       "   'Mattia Atzeni',\n",
       "   'Katsiaryna Mirylenka',\n",
       "   'Paolo Scotton',\n",
       "   'Fabio Casati'],\n",
       "  'summary': 'Data integration has been studied extensively for decades and approached from\\ndifferent angles. However, this domain still remains largely rule-driven and\\nlacks universal automation. Recent development in machine learning and in\\nparticular deep learning has opened the way to more general and more efficient\\nsolutions to data integration problems. In this work, we propose a general\\napproach to modeling and integrating entities from structured data, such as\\nrelational databases, as well as unstructured sources, such as free text from\\nnews articles. Our approach is designed to explicitly model and leverage\\nrelations between entities, thereby using all available information and\\npreserving as much context as possible. This is achieved by combining siamese\\nand graph neural networks to propagate information between connected entities\\nand support high scalability. We evaluate our method on the task of integrating\\ndata about business entities, and we demonstrate that it outperforms standard\\nrule-based systems, as well as other deep learning approaches that do not use\\ngraph-based representations.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 17, 21, 51, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'RCELF: A Residual-based Approach for Influence Maximization Problem',\n",
       "  'authors': ['Xinxun Zeng', 'Shiqi Zhang', 'Bo Tang'],\n",
       "  'summary': 'Influence Maximization Problem (IMP) is selecting a seed set of nodes in the\\nsocial network to spread the influence as widely as possible. It has many\\napplications in multiple domains, e.g., viral marketing is frequently used for\\nnew products or activities advertisements. While it is a classic and\\nwell-studied problem in computer science, unfortunately, all those proposed\\ntechniques are compromising among time efficiency, memory consumption, and\\nresult quality. In this paper, we conduct comprehensive experimental studies on\\nthe state-of-the-art IMP approximate approaches to reveal the underlying\\ntrade-off strategies. Interestingly, we find that even the state-of-the-art\\napproaches are impractical when the propagation probability of the network have\\nbeen taken into consideration. With the findings of existing approaches, we\\npropose a novel residual-based approach (i.e., RCELF) for IMP, which i)\\novercomes the deficiencies of existing approximate approaches, and ii) provides\\ntheoretical guaranteed results with high efficiency in both time- and space-\\nperspectives. We demonstrate the superiority of our proposal by extensive\\nexperimental evaluation on real datasets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 18, 9, 9, 34, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'AI Data Wrangling with Associative Arrays',\n",
       "  'authors': ['Jeremy Kepner',\n",
       "   'Vijay Gadepally',\n",
       "   'Hayden Jananthan',\n",
       "   'Lauren Milechin',\n",
       "   'Siddharth Samsi'],\n",
       "  'summary': 'The AI revolution is data driven. AI \"data wrangling\" is the process by which\\nunusable data is transformed to support AI algorithm development (training) and\\ndeployment (inference). Significant time is devoted to translating diverse data\\nrepresentations supporting the many query and analysis steps found in an AI\\npipeline. Rigorous mathematical representations of these data enables data\\ntranslation and analysis optimization within and across steps. Associative\\narray algebra provides a mathematical foundation that naturally describes the\\ntabular structures and set mathematics that are the basis of databases.\\nLikewise, the matrix operations and corresponding inference/training\\ncalculations used by neural networks are also well described by associative\\narrays. More surprisingly, a general denormalized form of hierarchical formats,\\nsuch as XML and JSON, can be readily constructed. Finally, pivot tables, which\\nare among the most widely used data analysis tools, naturally emerge from\\nassociative array constructors. A common foundation in associative arrays\\nprovides interoperability guarantees, proving that their operations are linear\\nsystems with rigorous mathematical properties, such as, associativity,\\ncommutativity, and distributivity that are critical to reordering\\noptimizations.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 18, 22, 11, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Fides: Managing Data on Untrusted Infrastructure',\n",
       "  'authors': ['Sujaya Maiyya',\n",
       "   'Danny Hyun Bum Cho',\n",
       "   'Divyakant Agrawal',\n",
       "   'Amr El Abbadi'],\n",
       "  'summary': 'Significant amounts of data are currently being stored and managed on\\nthird-party servers. It is impractical for many small scale enterprises to own\\ntheir private datacenters, hence renting third-party servers is a viable\\nsolution for such businesses. But the increasing number of malicious attacks,\\nboth internal and external, as well as buggy software on third-party servers is\\ncausing clients to lose their trust in these external infrastructures. While\\nsmall enterprises cannot avoid using external infrastructures, they need the\\nright set of protocols to manage their data on untrusted infrastructures. In\\nthis paper, we propose TFCommit, a novel atomic commitment protocol that\\nexecutes transactions on data stored across multiple untrusted servers. To our\\nknowledge, TFCommit is the first atomic commitment protocol to execute\\ntransactions in an untrusted environment without using expensive Byzantine\\nreplication. Using TFCommit, we propose an auditable data management system,\\nFides, residing completely on untrustworthy infrastructure. As an auditable\\nsystem, Fides guarantees the detection of potentially malicious failures\\noccurring on untrusted servers using tamper-resistant logs with the support of\\ncryptographic techniques. The experimental evaluation demonstrates the\\nscalability and the relatively low overhead of our approach that allows\\nexecuting transactions on untrusted infrastructure.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 20, 1, 20, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Graph Generators: State of the Art and Open Challenges',\n",
       "  'authors': ['Angela Bonifati',\n",
       "   'Irena Holubová',\n",
       "   'Arnau Prat-Pérez',\n",
       "   'Sherif Sakr'],\n",
       "  'summary': 'The abundance of interconnected data has fueled the design and implementation\\nof graph generators reproducing real-world linking properties, or gauging the\\neffectiveness of graph algorithms, techniques and applications manipulating\\nthese data. We consider graph generation across multiple subfields, such as\\nSemantic Web, graph databases, social networks, and community detection, along\\nwith general graphs. Despite the disparate requirements of modern graph\\ngenerators throughout these communities, we analyze them under a common\\numbrella, reaching out the functionalities, the practical usage, and their\\nsupported operations. We argue that this classification is serving the need of\\nproviding scientists, researchers and practitioners with the right data\\ngenerator at hand for their work. This survey provides a comprehensive overview\\nof the state-of-the-art graph generators by focusing on those that are\\npertinent and suitable for several data-intensive tasks. Finally, we discuss\\nopen challenges and missing requirements of current graph generators along with\\ntheir future extensions to new emerging fields.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 22, 8, 11, 6, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards context in large scale biomedical knowledge graphs',\n",
       "  'authors': ['Jens Dörpinghaus',\n",
       "   'Andreas Stefan',\n",
       "   'Bruce Schultz',\n",
       "   'Marc Jacobs'],\n",
       "  'summary': 'Contextual information is widely considered for NLP and knowledge discovery\\nin life sciences since it highly influences the exact meaning of natural\\nlanguage. The scientific challenge is not only to extract such context data,\\nbut also to store this data for further query and discovery approaches. Here,\\nwe propose a multiple step knowledge graph approach using labeled property\\ngraphs based on polyglot persistence systems to utilize context data for\\ncontext mining, graph queries, knowledge discovery and extraction. We introduce\\nthe graph-theoretic foundation for a general context concept within semantic\\nnetworks and show a proof-of-concept based on biomedical literature and text\\nmining. Our test system contains a knowledge graph derived from the entirety of\\nPubMed and SCAIView data and is enriched with text mining data and domain\\nspecific language data using BEL. Here, context is a more general concept than\\nannotations. This dense graph has more than 71M nodes and 850M relationships.\\nWe discuss the impact of this novel approach with 27 real world use cases\\nrepresented by graph queries.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 23, 7, 33, 2, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Adaptive Low-level Storage of Very Large Knowledge Graphs',\n",
       "  'authors': ['Jacopo Urbani', 'Ceriel Jacobs'],\n",
       "  'summary': 'The increasing availability and usage of Knowledge Graphs (KGs) on the Web\\ncalls for scalable and general-purpose solutions to store this type of data\\nstructures. We propose Trident, a novel storage architecture for very large KGs\\non centralized systems. Trident uses several interlinked data structures to\\nprovide fast access to nodes and edges, with the physical storage changing\\ndepending on the topology of the graph to reduce the memory footprint. In\\ncontrast to single architectures designed for single tasks, our approach offers\\nan interface with few low-level and general-purpose primitives that can be used\\nto implement tasks like SPARQL query answering, reasoning, or graph analytics.\\nOur experiments show that Trident can handle graphs with 10^11 edges using\\ninexpensive hardware, delivering competitive performance on multiple workloads.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 24, 16, 33, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Estimating Descriptors for Large Graphs',\n",
       "  'authors': ['Zohair Raza Hassan',\n",
       "   'Mudassir Shabbir',\n",
       "   'Imdadullah Khan',\n",
       "   'Waseem Abbas'],\n",
       "  'summary': 'Embedding networks into a fixed dimensional feature space, while preserving\\nits essential structural properties is a fundamental task in graph analytics.\\nThese feature vectors (graph descriptors) are used to measure the pairwise\\nsimilarity between graphs. This enables applying data mining algorithms (e.g\\nclassification, clustering, or anomaly detection) on graph-structured data\\nwhich have numerous applications in multiple domains. State-of-the-art\\nalgorithms for computing descriptors require the entire graph to be in memory,\\nentailing a huge memory footprint, and thus do not scale well to increasing\\nsizes of real-world networks. In this work, we propose streaming algorithms to\\nefficiently approximate descriptors by estimating counts of sub-graphs of order\\n$k\\\\leq 4$, and thereby devise extensions of two existing graph comparison\\nparadigms: the Graphlet Kernel and NetSimile. Our algorithms require a single\\nscan over the edge stream, have space complexity that is a fraction of the\\ninput size, and approximate embeddings via a simple sampling scheme. Our design\\nexploits the trade-off between available memory and estimation accuracy to\\nprovide a method that works well for limited memory requirements. We perform\\nextensive experiments on real-world networks and demonstrate that our\\nalgorithms scale well to massive graphs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 28, 13, 3, 58, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Query-Sequence Optimization on a Reconfigurable Hardware-Accelerated System',\n",
       "  'authors': ['Lekshmi B. G.', 'Andreas Becher', 'Klaus Meyer-Wegener'],\n",
       "  'summary': 'Hardware acceleration of database query processing can be done with the help\\nof FPGAs. In particular, they are partially reconfigurable during runtime,\\nwhich allows for the runtime adaption of the hardware to a variety of queries.\\nReconfiguration itself, however, takes some time. As the affected area of the\\nFPGA is not available for computations during the reconfiguration, avoiding\\nsome of the reconfigurations can improve overall performance. This paper\\npresents optimizations based on query sequences, which reduces the impact of\\nthe reconfigurations. Knowledge of coming queries is used to (I) speculatively\\nstart reconfiguration already when a query is still running and (II) avoid\\noverwriting of reconfigurable regions that will be used again in subsequent\\nqueries. We evaluate our optimizations with a calibrated model and measurements\\nfor various parameter values. Improvements in execution time of up to 21% can\\nbe obtained even with sequences of only two queries',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 29, 8, 23, 13, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Scalable Framework for Quality Assessment of RDF Datasets',\n",
       "  'authors': ['Gezim Sejdiu', 'Anisa Rula', 'Jens Lehmann', 'Hajira Jabeen'],\n",
       "  'summary': 'Over the last years, Linked Data has grown continuously. Today, we count more\\nthan 10,000 datasets being available online following Linked Data standards.\\nThese standards allow data to be machine readable and inter-operable.\\nNevertheless, many applications, such as data integration, search, and\\ninterlinking, cannot take full advantage of Linked Data if it is of low\\nquality. There exist a few approaches for the quality assessment of Linked\\nData, but their performance degrades with the increase in data size and quickly\\ngrows beyond the capabilities of a single machine. In this paper, we present\\nDistQualityAssessment -- an open source implementation of quality assessment of\\nlarge RDF datasets that can scale out to a cluster of machines. This is the\\nfirst distributed, in-memory approach for computing different quality metrics\\nfor large RDF datasets using Apache Spark. We also provide a quality assessment\\npattern that can be used to generate new scalable metrics that can be applied\\nto big data. The work presented here is integrated with the SANSA framework and\\nhas been applied to at least three use cases beyond the SANSA community. The\\nresults show that our approach is more generic, efficient, and scalable as\\ncompared to previously proposed approaches.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 1, 29, 21, 30, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Novel Approach for Generating SPARQL Queries from RDF Graphs',\n",
       "  'authors': ['Emna Jabri'],\n",
       "  'summary': \"This work is done as part of a research master's thesis project. The goal is\\nto generate SPARQL queries based on user-supplied keywords to query RDF graphs.\\nTo do this, we first transformed the input ontology into an RDF graph that\\nreflects the semantics represented in the ontology. Subsequently, we stored\\nthis RDF graph in the Neo4j graphical database to ensure efficient and\\npersistent management of RDF data. At the time of the interrogation, we studied\\nthe different possible and desired interpretations of the request originally\\nmade by the user. We have also proposed to carry out a sort of transformation\\nbetween the two query languages SPARQL and Cypher, which is specific to Neo4j.\\nThis allows us to implement the architecture of our system over a wide variety\\nof BD-RDFs providing their query languages, without changing any of the other\\ncomponents of the system. Finally, we tested and evaluated our tool using\\ndifferent test bases, and it turned out that our tool is comprehensive,\\neffective, and powerful enough.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 30, 18, 28, 49, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'TASM: A Tile-Based Storage Manager for Video Analytics',\n",
       "  'authors': ['Maureen Daum',\n",
       "   'Brandon Haynes',\n",
       "   'Dong He',\n",
       "   'Amrita Mazumdar',\n",
       "   'Magdalena Balazinska'],\n",
       "  'summary': 'Modern video data management systems store videos as a single encoded file,\\nwhich significantly limits possible storage level optimizations. We design,\\nimplement, and evaluate TASM, a new tile-based storage manager for video data.\\nTASM uses a feature in modern video codecs called \"tiles\" that enables spatial\\nrandom access into encoded videos. TASM physically tunes stored videos by\\noptimizing their tile layouts given the video content and a query workload.\\nAdditionally, TASM dynamically tunes that layout in response to changes in the\\nquery workload or if the query workload and video contents are incrementally\\ndiscovered. Finally, TASM also produces efficient initial tile layouts for\\nnewly ingested videos. We demonstrate that TASM can speed up subframe selection\\nqueries by an average of over 50% and up to 94%. TASM can also improve the\\nthroughput of the full scan phase of object detection queries by up to 2X.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 4, 15, 40, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Efficient Semi-External Depth-First Search',\n",
       "  'authors': ['Xiaolong Wan', 'Hongzhi Wang'],\n",
       "  'summary': 'As the sizes of graphs grow rapidly, currently many real-world graphs can\\nhardly be loaded in the main memory. It becomes a hot topic to compute\\ndepth-first search (DFS) results, i.e., depth-first order or DFS-Tree, on\\nsemi-external memory model. Semi-external algorithms assume the main memory\\ncould at least hold a spanning tree T of a graph G, and gradually restructure T\\ninto a DFS-Tree, which is non-trivial. In this paper, we present a\\ncomprehensive study of semi-external DFS problem. Based on our theoretical\\nanalysis of its main challenge, we introduce a new semi-external DFS algorithm,\\nnamed EP-DFS, with a lightweight index N+-index. Unlike traditional algorithms,\\nwe focus on addressing such complex problem efficiently not only with less\\nI/Os, but also with simpler CPU calculation (implementation-friendly) and less\\nrandom I/O accesses (key-to-efficiency). Extensive experimental evaluation is\\nconducted on both synthetic and real graphs. The experimental results confirm\\nthat our EP-DFS algorithm significantly outperforms existing algorithms.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 5, 1, 55, 22, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'J-Logic: a Logic for Querying JSON',\n",
       "  'authors': ['Jan Hidders', 'Jan Paredaens', 'Jan Van den Bussche'],\n",
       "  'summary': \"We propose a logical framework, based on Datalog, to study the foundations of\\nquerying JSON data. The main feature of our approach, which we call J-Logic, is\\nthe emphasis on paths. Paths are sequences of keys and are used to access the\\ntree structure of nested JSON objects. J-Logic also features ``packing'' as a\\nmeans to generate a new key from a path or subpath. J-Logic with recursion is\\ncomputationally complete, but many queries can be expressed without recursion,\\nsuch as deep equality. We give a necessary condition for queries to be\\nexpressible without recursion. Most of our results focus on the deterministic\\nnature of JSON objects as partial functions from keys to values. Predicates\\ndefined by J-Logic programs may not properly describe objects, however.\\nNevertheless we show that every object-to-object transformation in J-Logic can\\nbe defined using only objects in intermediate results. Moreover we show that it\\nis decidable whether a positive, nonrecursive J-Logic program always returns an\\nobject when given objects as inputs. Regarding packing, we show that packing is\\nunnecessary if the output does not require new keys. Finally, we show the\\ndecidability of query containment for positive, nonrecursive J-Logic programs.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 7, 21, 56, 29, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An Ontology for the Materials Design Domain',\n",
       "  'authors': ['Huanyu Li', 'Rickard Armiento', 'Patrick Lambrix'],\n",
       "  'summary': 'In the materials design domain, much of the data from materials calculations\\nare stored in different heterogeneous databases. Materials databases usually\\nhave different data models. Therefore, the users have to face the challenges to\\nfind the data from adequate sources and integrate data from multiple sources.\\nOntologies and ontology-based techniques can address such problems as the\\nformal representation of domain knowledge can make data more available and\\ninteroperable among different systems. In this paper, we introduce the\\nMaterials Design Ontology (MDO), which defines concepts and relations to cover\\nknowledge in the field of materials design. MDO is designed using domain\\nknowledge in materials science (especially in solid-state physics), and is\\nguided by the data from several databases in the materials design field. We\\nshow the application of the MDO to materials data retrieved from well-known\\nmaterials databases.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 13, 20, 32, 7, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Categorical anomaly detection in heterogeneous data using minimum description length clustering',\n",
       "  'authors': ['James Cheney',\n",
       "   'Xavier Gombau',\n",
       "   'Ghita Berrada',\n",
       "   'Sidahmed Benabderrahmane'],\n",
       "  'summary': 'Fast and effective unsupervised anomaly detection algorithms have been\\nproposed for categorical data based on the minimum description length (MDL)\\nprinciple. However, they can be ineffective when detecting anomalies in\\nheterogeneous datasets representing a mixture of different sources, such as\\nsecurity scenarios in which system and user processes have distinct behavior\\npatterns. We propose a meta-algorithm for enhancing any MDL-based anomaly\\ndetection model to deal with heterogeneous data by fitting a mixture model to\\nthe data, via a variant of k-means clustering. Our experimental results show\\nthat using a discrete mixture model provides competitive performance relative\\nto two previous anomaly detection algorithms, while mixtures of more\\nsophisticated models yield further gains, on both synthetic datasets and\\nrealistic datasets from a security scenario.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 14, 14, 48, 37, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'NeuroCard: One Cardinality Estimator for All Tables',\n",
       "  'authors': ['Zongheng Yang',\n",
       "   'Amog Kamsetty',\n",
       "   'Sifei Luan',\n",
       "   'Eric Liang',\n",
       "   'Yan Duan',\n",
       "   'Xi Chen',\n",
       "   'Ion Stoica'],\n",
       "  'summary': 'Query optimizers rely on accurate cardinality estimates to produce good\\nexecution plans. Despite decades of research, existing cardinality estimators\\nare inaccurate for complex queries, due to making lossy modeling assumptions\\nand not capturing inter-table correlations. In this work, we show that it is\\npossible to learn the correlations across all tables in a database without any\\nindependence assumptions. We present NeuroCard, a join cardinality estimator\\nthat builds a single neural density estimator over an entire database.\\nLeveraging join sampling and modern deep autoregressive models, NeuroCard makes\\nno inter-table or inter-column independence assumptions in its probabilistic\\nmodeling. NeuroCard achieves orders of magnitude higher accuracy than the best\\nprior methods (a new state-of-the-art result of 8.5$\\\\times$ maximum error on\\nJOB-light), scales to dozens of tables, while being compact in space (several\\nMBs) and efficient to construct or update (seconds to minutes).',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 15, 3, 21, 46, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Sorting-based Interactive Regret Minimization',\n",
       "  'authors': ['Jiping Zheng', 'Chen Chen'],\n",
       "  'summary': 'As an important tool for multi-criteria decision making in database systems,\\nthe regret minimization query is shown to have the merits of top-k and skyline\\nqueries: it controls the output size while does not need users to provide any\\npreferences. Existing researches verify that the regret ratio can be much\\ndecreased when interaction is available. In this paper, we study how to enhance\\ncurrent interactive regret minimization query by sorting mechanism. Instead of\\nselecting the most favorite point from the displayed points for each\\ninteraction round, users sort the displayed data points and send the results to\\nthe system. By introducing sorting mechanism, for each round of interaction the\\nutility space explored will be shrunk to some extent. Further the candidate\\npoints selection for following rounds of interaction will be narrowed to\\nsmaller data spaces thus the number of interaction rounds will be reduced. We\\npropose two effective sorting-based algorithms namely Sorting-Simplex and\\nSorting-Random to find the maximum utility point based on Simplex method and\\nrandomly selection strategy respectively. Experiments on synthetic and real\\ndatasets verify our Sorting-Simplex and Sorting-Random algorithms outperform\\ncurrent state-of-art ones.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 19, 4, 8, 8, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Improving Locality Sensitive Hashing by Efficiently Finding Projected Nearest Neighbors',\n",
       "  'authors': ['Omid Jafari', 'Parth Nagarkar', 'Jonathan Montaño'],\n",
       "  'summary': 'Similarity search in high-dimensional spaces is an important task for many\\nmultimedia applications. Due to the notorious curse of dimensionality,\\napproximate nearest neighbor techniques are preferred over exact searching\\ntechniques since they can return good enough results at a much better speed.\\nLocality Sensitive Hashing (LSH) is a very popular random hashing technique for\\nfinding approximate nearest neighbors. Existing state-of-the-art Locality\\nSensitive Hashing techniques that focus on improving performance of the overall\\nprocess, mainly focus on minimizing the total number of IOs while sacrificing\\nthe overall processing time. The main time-consuming process in LSH techniques\\nis the process of finding neighboring points in projected spaces. We present a\\nnovel index structure called radius-optimized Locality Sensitive Hashing\\n(roLSH). With the help of sampling techniques and Neural Networks, we present\\ntwo techniques to find neighboring points in projected spaces efficiently,\\nwithout sacrificing the accuracy of the results. Our extensive experimental\\nanalysis on real datasets shows the performance benefit of roLSH over existing\\nstate-of-the-art LSH techniques.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 19, 17, 46, 30, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The Lernaean Hydra of Data Series Similarity Search: An Experimental Evaluation of the State of the Art',\n",
       "  'authors': ['Karima Echihabi',\n",
       "   'Kostas Zoumpatianos',\n",
       "   'Themis Palpanas',\n",
       "   'Houda Benbrahim'],\n",
       "  'summary': 'Increasingly large data series collections are becoming commonplace across\\nmany different domains and applications. A key operation in the analysis of\\ndata series collections is similarity search, which has attracted lots of\\nattention and effort over the past two decades. Even though several relevant\\napproaches have been proposed in the literature, none of the existing studies\\nprovides a detailed evaluation against the available alternatives. The lack of\\ncomparative results is further exacerbated by the non-standard use of\\nterminology, which has led to confusion and misconceptions. In this paper, we\\nprovide definitions for the different flavors of similarity search that have\\nbeen studied in the past, and present the first systematic experimental\\nevaluation of the efficiency of data series similarity search techniques. Based\\non the experimental results, we describe the strengths and weaknesses of each\\napproach and give recommendations for the best approach to use under typical\\nuse cases. Finally, by identifying the shortcomings of each method, our\\nfindings lay the ground for solid further developments in the field.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 20, 1, 4, 27, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Return of the Lernaean Hydra: Experimental Evaluation of Data Series Approximate Similarity Search',\n",
       "  'authors': ['Karima Echihabi',\n",
       "   'Kostas Zoumpatianos',\n",
       "   'Themis Palpanas',\n",
       "   'Houda Benbrahim'],\n",
       "  'summary': 'Data series are a special type of multidimensional data present in numerous\\ndomains, where similarity search is a key operation that has been extensively\\nstudied in the data series literature. In parallel, the multidimensional\\ncommunity has studied approximate similarity search techniques. We propose a\\ntaxonomy of similarity search techniques that reconciles the terminology used\\nin these two domains, we describe modifications to data series indexing\\ntechniques enabling them to answer approximate similarity queries with quality\\nguarantees, and we conduct a thorough experimental evaluation to compare\\napproximate similarity search techniques under a unified framework, on\\nsynthetic and real datasets in memory and on disk. Although data series differ\\nfrom generic multidimensional vectors (series usually exhibit correlation\\nbetween neighboring values), our results show that data series techniques\\nanswer approximate %similarity queries with strong guarantees and an excellent\\nempirical performance, on data series and vectors alike. These techniques\\noutperform the state-of-the-art approximate techniques for vectors when\\noperating on disk, and remain competitive in memory.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 20, 1, 27, 49, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Benchmarking Learned Indexes',\n",
       "  'authors': ['Ryan Marcus',\n",
       "   'Andreas Kipf',\n",
       "   'Alexander van Renen',\n",
       "   'Mihail Stoian',\n",
       "   'Sanchit Misra',\n",
       "   'Alfons Kemper',\n",
       "   'Thomas Neumann',\n",
       "   'Tim Kraska'],\n",
       "  'summary': 'Recent advancements in learned index structures propose replacing existing\\nindex structures, like B-Trees, with approximate learned models. In this work,\\nwe present a unified benchmark that compares well-tuned implementations of\\nthree learned index structures against several state-of-the-art \"traditional\"\\nbaselines. Using four real-world datasets, we demonstrate that learned index\\nstructures can indeed outperform non-learned indexes in read-only in-memory\\nworkloads over a dense array. We also investigate the impact of caching,\\npipelining, dataset size, and key size. We study the performance profile of\\nlearned index structures, and build an explanation for why learned models\\nachieve such good performance. Finally, we investigate other important\\nproperties of learned index structures, such as their performance in\\nmulti-threaded systems and their build times.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 23, 7, 37, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'PRIPEL: Privacy-Preserving Event Log Publishing Including Contextual Information',\n",
       "  'authors': ['Stephan A. Fahrenkrog-Petersen',\n",
       "   'Han van der Aa',\n",
       "   'Matthias Weidlich'],\n",
       "  'summary': \"Event logs capture the execution of business processes in terms of executed\\nactivities and their execution context. Since logs contain potentially\\nsensitive information about the individuals involved in the process, they\\nshould be pre-processed before being published to preserve the individuals'\\nprivacy. However, existing techniques for such pre-processing are limited to a\\nprocess' control-flow and neglect contextual information, such as attribute\\nvalues and durations. This thus precludes any form of process analysis that\\ninvolves contextual factors. To bridge this gap, we introduce PRIPEL, a\\nframework for privacy-aware event log publishing. Compared to existing work,\\nPRIPEL takes a fundamentally different angle and ensures privacy on the level\\nof individual cases instead of the complete log. This way, contextual\\ninformation as well as the long tail process behaviour are preserved, which\\nenables the application of a rich set of process analysis techniques. We\\ndemonstrate the feasibility of our framework in a case study with a real-world\\nevent log.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 23, 9, 28, 40, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Coconut Palm: Static and Streaming Data Series Exploration Now in your Palm',\n",
       "  'authors': ['Haridimos Kondylakis',\n",
       "   'Niv Dayan',\n",
       "   'Kostas Zoumpatianos',\n",
       "   'Themis Palpanas'],\n",
       "  'summary': 'Many modern applications produce massive streams of data series and maintain\\nthem in indexes to be able to explore them through nearest neighbor search.\\nExisting data series indexes, however, are expensive to operate as they issue\\nmany random I/Os to storage. To address this problem, we recently proposed\\nCoconut, a new infrastructure that organizes data series based on a new\\nsortable format. In this way, Coconut is able to leverage state-of-the-art\\nindexing techniques that rely on sorting for the first time to build, maintain\\nand query data series indexes using fast sequential I/Os. In this\\ndemonstration, we present Coconut Palm, a new exploration tool that allows to\\ninteractively combine different indexing techniques from within the Coconut\\ninfrastructure and to thereby seamlessly explore data series from across\\nvarious scientific domains. We highlight the rich indexing design choices that\\nCoconut opens up, and we present a new recommender tool that allows users to\\nintelligently navigate them for both static and streaming data exploration\\nscenarios.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 20, 2, 25, 34, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Tsunami: A Learned Multi-dimensional Index for Correlated Data and Skewed Workloads',\n",
       "  'authors': ['Jialin Ding',\n",
       "   'Vikram Nathan',\n",
       "   'Mohammad Alizadeh',\n",
       "   'Tim Kraska'],\n",
       "  'summary': 'Filtering data based on predicates is one of the most fundamental operations\\nfor any modern data warehouse. Techniques to accelerate the execution of filter\\nexpressions include clustered indexes, specialized sort orders (e.g., Z-order),\\nmulti-dimensional indexes, and, for high selectivity queries, secondary\\nindexes. However, these schemes are hard to tune and their performance is\\ninconsistent. Recent work on learned multi-dimensional indexes has introduced\\nthe idea of automatically optimizing an index for a particular dataset and\\nworkload. However, the performance of that work suffers in the presence of\\ncorrelated data and skewed query workloads, both of which are common in real\\napplications. In this paper, we introduce Tsunami, which addresses these\\nlimitations to achieve up to 6X faster query performance and up to 8X smaller\\nindex size than existing learned multi-dimensional indexes, in addition to up\\nto 11X faster query performance and 170X smaller index size than\\noptimally-tuned traditional indexes.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 23, 19, 25, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'SPIDER: Selective Plotting of Interconnected Data and Entity Relations',\n",
       "  'authors': ['Pranav Addepalli',\n",
       "   'Eric Wu',\n",
       "   'Douglas Bossart',\n",
       "   'Christina Lin',\n",
       "   'Allistar Smith'],\n",
       "  'summary': 'Intelligence analysts have long struggled with an abundance of data that must\\nbe investigated on a daily basis. In the U.S. Army, this activity involves\\nreconciling information from various sources, a process that has been automated\\nto a certain extent, but which remains highly manual. To promote automation, a\\nsemantic analysis prototype was designed to aid in the intelligence analysis\\nprocess. This tool, called Selective Plotting of Interconnected Data and Entity\\nRelations (SPIDER), extracts entities and their relationships from text in\\norder to streamline investigations. SPIDER is a web application that can be\\nremotely-accessed via a web browser, and has three major components: (1) a Java\\nAPI that reads documents, extracts entities and relationships using Stanford\\nCoreNLP, (2) a Neo4j graph database that stores entities, relationships, and\\nproperties; (3) a JavaScript-based SigmaJS visualization tool for displaying\\nthe graph on the browser. SPIDER can scale document analysis to thousands of\\nfiles for quick visualization, making the intelligence analysis process more\\nefficient, and allowing military leadership quicker insights into a vast array\\nof potentially-hidden knowledge.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 25, 13, 59, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Hands-off Model Integration in Spatial Index Structures',\n",
       "  'authors': ['Ali Hadian', 'Ankit Kumar', 'Thomas Heinis'],\n",
       "  'summary': \"Spatial indexes are crucial for the analysis of the increasing amounts of\\nspatial data, for example generated through IoT applications. The plethora of\\nindexes that has been developed in recent decades has primarily been optimised\\nfor disk. With increasing amounts of memory even on commodity machines,\\nhowever, moving them to main memory is an option. Doing so opens up the\\nopportunity to use additional optimizations that are only amenable to main\\nmemory. In this paper we thus explore the opportunity to use light-weight\\nmachine learning models to accelerate queries on spatial indexes. We do so by\\nexploring the potential of using interpolation and similar techniques on the\\nR-tree, arguably the most broadly used spatial index. As we show in our\\nexperimental analysis, the query execution time can be reduced by up to 60%\\nwhile simultaneously shrinking the index's memory footprint by over 90%\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 29, 22, 5, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Lachesis: Automatic Partitioning for UDF-Centric Analytics',\n",
       "  'authors': ['Jia Zou',\n",
       "   'Amitabh Das',\n",
       "   'Pratik Barhate',\n",
       "   'Arun Iyengar',\n",
       "   'Binhang Yuan',\n",
       "   'Dimitrije Jankov',\n",
       "   'Chris Jermaine'],\n",
       "  'summary': \"Persistent partitioning is effective in avoiding expensive shuffling\\noperations. However it remains a significant challenge to automate this process\\nfor Big Data analytics workloads that extensively use user defined functions\\n(UDFs), where sub-computations are hard to be reused for partitionings compared\\nto relational applications. In addition, functional dependency that is widely\\nutilized for partitioning selection is often unavailable in the unstructured\\ndata that is ubiquitous in UDF-centric analytics. We propose the Lachesis\\nsystem, which represents UDF-centric workloads as workflows of analyzable and\\nreusable sub-computations. Lachesis further adopts a deep reinforcement\\nlearning model to infer which sub-computations should be used to partition the\\nunderlying data. This analysis is then applied to automatically optimize the\\nstorage of the data across applications to improve the performance and users'\\nproductivity.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 30, 4, 49, 44, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Hierarchical Graph Matching Network for Graph Similarity Computation',\n",
       "  'authors': ['Haibo Xiu',\n",
       "   'Xiao Yan',\n",
       "   'Xiaoqiang Wang',\n",
       "   'James Cheng',\n",
       "   'Lei Cao'],\n",
       "  'summary': 'Graph edit distance / similarity is widely used in many tasks, such as graph\\nsimilarity search, binary function analysis, and graph clustering. However,\\ncomputing the exact graph edit distance (GED) or maximum common subgraph (MCS)\\nbetween two graphs is known to be NP-hard. In this paper, we propose the\\nhierarchical graph matching network (HGMN), which learns to compute graph\\nsimilarity from data. HGMN is motivated by the observation that two similar\\ngraphs should also be similar when they are compressed into more compact\\ngraphs. HGMN utilizes multiple stages of hierarchical clustering to organize a\\ngraph into successively more compact graphs. At each stage, the earth mover\\ndistance (EMD) is adopted to obtain a one-to-one mapping between the nodes in\\ntwo graphs (on which graph similarity is to be computed), and a correlation\\nmatrix is also derived from the embeddings of the nodes in the two graphs. The\\ncorrelation matrices from all stages are used as input for a convolutional\\nneural network (CNN), which is trained to predict graph similarity by\\nminimizing the mean squared error (MSE). Experimental evaluation on 4 datasets\\nin different domains and 4 performance metrics shows that HGMN consistently\\noutperforms existing baselines in the accuracy of graph similarity\\napproximation.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 6, 30, 6, 16, 19, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'GraphQL Live Querying with DynamoDB',\n",
       "  'authors': ['Austin Silveria'],\n",
       "  'summary': 'We present a method of implementing GraphQL live queries at the database\\nlevel. Our DynamoDB simulation in Go mimics a distributed key-value store and\\nimplements live queries to expose possible pitfalls. Two key components for\\nimplementing live queries are storing fields selected in a live query and\\ndetermining which object fields have been updated in each database write. A\\nstream(key, fields) request to the system contains fields to include in the\\nlive query stream and on subsequent put(key, object) operations, the database\\nasynchronously determines which fields were updated and pushes a new query view\\nto the stream if those fields overlap with the stream() request. Following a\\ndiscussion of our implementation, we explore motivations for using live queries\\nsuch as simplifying software communication, minimizing data transfer, and\\nenabling real-time data and describe an architecture for building software with\\nGraphQL and live queries.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 1, 0, 15, 44, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Privacy-Aware Data Cleaning-as-a-Service (Extended Version)',\n",
       "  'authors': ['Yu Huang', 'Mostafa Milani', 'Fei Chiang'],\n",
       "  'summary': 'Data cleaning is a pervasive problem for organizations as they try to reap\\nvalue from their data. Recent advances in networking and cloud computing\\ntechnology have fueled a new computing paradigm called Database-as-a-Service,\\nwhere data management tasks are outsourced to large service providers. In this\\npaper, we consider a Data Cleaning-as-a-Service model that allows a client to\\ninteract with a data cleaning provider who hosts curated, and sensitive data.\\nWe present PACAS: a Privacy-Aware data Cleaning-As-a-Service model that\\nfacilitates interaction between the parties with client query requests for\\ndata, and a service provider using a data pricing scheme that computes prices\\naccording to data sensitivity. We propose new extensions to the model to define\\ngeneralized data repairs that obfuscate sensitive data to allow data sharing\\nbetween the client and service provider. We present a new semantic distance\\nmeasure to quantify the utility of such repairs, and we re-define the notion of\\nconsistency in the presence of generalized values. The PACAS model uses\\n(X,Y,L)-anonymity that extends existing data publishing techniques to consider\\nthe semantics in the data while protecting sensitive values. Our evaluation\\nover real data show that PACAS safeguards semantically related sensitive\\nvalues, and provides lower repair errors compared to existing privacy-aware\\ncleaning techniques.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 2, 0, 33, 43, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Knowledge Translation: Extended Technical Report',\n",
       "  'authors': ['Bahar Ghadiri Bashardoost',\n",
       "   'Renée J. Miller',\n",
       "   'Kelly Lyons',\n",
       "   'Fatemeh Nargesian'],\n",
       "  'summary': \"We introduce Kensho, a tool for generating mapping rules between two\\nKnowledge Bases (KBs). To create the mapping rules, Kensho starts with a set of\\ncorrespondences and enriches them with additional semantic information\\nautomatically identified from the structure and constraints of the KBs. Our\\napproach works in two phases. In the first phase, semantic associations between\\nresources of each KB are captured. In the second phase, mapping rules are\\ngenerated by interpreting the correspondences in a way that respects the\\ndiscovered semantic associations among elements of each KB. Kensho's mapping\\nrules are expressed using SPARQL queries and can be used directly to exchange\\nknowledge from source to target. Kensho is able to automatically rank the\\ngenerated mapping rules using a set of heuristics. We present an experimental\\nevaluation of Kensho and assess our mapping generation and ranking strategies\\nusing more than 50 synthesized and real world settings, chosen to showcase some\\nof the most important applications of knowledge translation. In addition, we\\nuse three existing benchmarks to demonstrate Kensho's ability to deal with\\ndifferent mapping scenarios.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 3, 21, 37, 38, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Rule-based Language for Application Integration',\n",
       "  'authors': ['Daniel Ritter', 'Jan Broß'],\n",
       "  'summary': 'Although message-based (business) application integration is based on\\norchestrated message flows, current modeling languages exclusively cover (parts\\nof) the control flow, while under-specifying the data flow. Especially for more\\ndata-intensive integration scenarios, this fact adds to the inherent data\\nprocessing weakness in conventional integration systems.\\n  We argue that with a more data-centric integration language and a relational\\nlogic based implementation of integration semantics, optimizations from the\\ndata management domain(e.g., data partitioning, parallelization) can be\\ncombined with common integration processing (e.g., scatter/gather,\\nsplitter/gather). With the Logic Integration Language (LiLa) we redefine\\nintegration logic tailored for data-intensive processing and propose a novel\\napproach to data-centric integration modeling, from which we derive the\\ncontrol-and data flow and apply them to a conventional integration system.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 9, 13, 2, 12, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards Integrated and Open COVID-19 Data',\n",
       "  'authors': ['Georgios M. Santipantakis',\n",
       "   'George A. Vouros',\n",
       "   'Christos Doulkeridis'],\n",
       "  'summary': 'Motivated by the global unrest related to the COVID-19 pandemic, we present a\\nsystem prototype for ontology-based, integration of national data published\\nfrom various countries. COVID-related data is published from different\\nauthorities, in different formats, at varying spatio-temporal granularity, and\\nirregularly. Consequently, this hinders the joint data exploration and\\nexploitation, which could lead scientists to acquire important insights,\\nwithout having to deal with the cumbersome task of data acquisition and\\nintegration. Motivated by this shortcoming, we propose an approach for data\\nacquisition, ontology-based data representation, and data transformation to\\nRDF, which also enables interlinking with other publicly available data\\nsources. Currently, data coming from the following European countries has been\\nsuccessfully integrated: Austria, Belgium, France, Germany, Greece, Italy, and\\nSweden. The knowledge base is automatically being updated, and it is available\\nto the public through a SPARQL endpoint and a direct download link.\\nFurthermore, we showcase how data integration enables spatio-temporal data\\nanalysis and knowledge discovery, by means of meaningful queries that would not\\nbe feasible to process otherwise.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 1, 23, 37, 31, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Sampling Based Approximate Skyline Calculation on Big Data',\n",
       "  'authors': ['Xingxing Xiao', 'Jianzhong Li'],\n",
       "  'summary': 'The existing algorithms for processing skyline queries cannot adapt to big\\ndata. This paper proposes two approximate skyline algorithms based on sampling.\\nThe first algorithm obtains a fixed size sample and computes the approximate\\nskyline on the sample. The error of the first algorithm is relatively small in\\nmost cases, and is almost independent of the input relation size. The second\\nalgorithm returns an $(\\\\epsilon,\\\\delta)$-approximation for the exact skyline.\\nThe size of sample required by the second algorithm can be regarded as a\\nconstant relative to the input relation size, so is the running time.\\nExperiments verify the error analysis of the first algorithm and show that the\\nsecond algorithm is much faster than the existing skyline algorithms.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 12, 4, 37, 48, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The network footprint of replication in popular DBMSs',\n",
       "  'authors': ['Muhammad Karam Shehzad',\n",
       "   'Jam Muhammad Yousif',\n",
       "   'Muhammad Saqib Ilyas',\n",
       "   'Adnan Iqbal'],\n",
       "  'summary': 'Database replication is an important component of reliable, disaster tolerant\\nand highly available distributed systems. However, data replication also causes\\ncommunication and processing overhead. Quantification of these overheads is\\ncrucial in choosing a suitable DBMS form several available options and capacity\\nplanning. In this paper, we present results from a comparative empirical\\nanalysis of replication activities of three commonly used DBMSs - MySQL,\\nPostgreSQL and Cassandra under text as well as image traffic. In our\\nexperiments, the total traffic with two replicas (which is the norm) was as\\nmuch as $300$\\\\% higher than the total traffic with no replica. Furthermore,\\nactivation of the compression option for replication traffic, built into MySQL,\\nreduced the total network traffic by as much as $20$\\\\%. We also found that\\naverage CPU utilization and memory utilization were not impacted by the number\\nof replicas or the dataset.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 12, 8, 26, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards Querying in Decentralized Environments with Privacy-Preserving Aggregation',\n",
       "  'authors': ['Ruben Taelman', 'Simon Steyskal', 'Sabrina Kirrane'],\n",
       "  'summary': 'The Web is a ubiquitous economic, educational, and collaborative space.\\nHowever, it also serves as a haven for personal information harvesting.\\nExisting decentralised Web-based ecosystems, such as Solid, aim to combat\\npersonal data exploitation on the Web by enabling individuals to manage their\\ndata in the personal data store of their choice. Since personal data in these\\ndecentralised ecosystems are distributed across many sources, there is a need\\nfor techniques to support efficient privacy-preserving query execution over\\npersonal data stores. Towards this end, in this position paper we present a\\nframework for efficient privacy preserving federated querying, and highlight\\nopen research challenges and opportunities. The overarching goal being to\\nprovide a means to position future research into privacy-preserving querying\\nwithin decentralised environments.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 14, 9, 41, 10, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Automatic Storage Structure Selection for hybrid Workload',\n",
       "  'authors': ['Hongzhi Wang', 'Yan Wei', 'Hao Yan'],\n",
       "  'summary': 'In the use of database systems, the design of the storage engine and data\\nmodel directly affects the performance of the database when performing queries.\\nTherefore, the users of the database need to select the storage engine and\\ndesign data model according to the workload encountered. However, in a hybrid\\nworkload, the query set of the database is dynamically changing, and the design\\nof its optimal storage structure is also changing. Motivated by this, we\\npropose an automatic storage structure selection system based on learning cost,\\nwhich is used to dynamically select the optimal storage structure of the\\ndatabase under hybrid workloads. In the system, we introduce a machine learning\\nmethod to build a cost model for the storage engine, and a column-oriented data\\nlayout generation algorithm. Experimental results show that the proposed system\\ncan choose the optimal combination of storage engine and data model according\\nto the current workload, which greatly improves the performance of the default\\nstorage structure. And the system is designed to be compatible with different\\nstorage engines for easy use in practical applications.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 15, 3, 42, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'DeepSampling: Selectivity Estimation with Predicted Error and Response Time',\n",
       "  'authors': ['Tin Vu', 'Ahmed Eldawy'],\n",
       "  'summary': 'The rapid growth of spatial data urges the research community to find\\nefficient processing techniques for interactive queries on large volumes of\\ndata. Approximate Query Processing (AQP) is the most prominent technique that\\ncan provide real-time answer for ad-hoc queries based on a random sample.\\nUnfortunately, existing AQP methods provide an answer without providing any\\naccuracy metrics due to the complex relationship between the sample size, the\\nquery parameters, the data distribution, and the result accuracy. This paper\\nproposes DeepSampling, a deep-learning-based model that predicts the accuracy\\nof a sample-based AQP algorithm, specially selectivity estimation, given the\\nsample size, the input distribution, and query parameters. The model can also\\nbe reversed to measure the sample size that would produce a desired accuracy.\\nDeepSampling is the first system that provides a reliable tool for existing\\nspatial databases to control the accuracy of AQP.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 16, 3, 23, 1, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Benchmarking database performance for genomic data',\n",
       "  'authors': ['Matloob Khushi'],\n",
       "  'summary': 'Genomic regions represent features such as gene annotations, transcription\\nfactor binding sites and epigenetic modifications. Performing various genomic\\noperations such as identifying overlapping/non-overlapping regions or nearest\\ngene annotations are common research needs. The data can be saved in a database\\nsystem for easy management, however, there is no comprehensive database\\nbuilt-in algorithm at present to identify overlapping regions. Therefore I have\\ndeveloped a region-mapping (RegMap) SQL-based algorithm to perform genomic\\noperations and have benchmarked the performance of different databases.\\nBenchmarking identified that PostgreSQL extracts overlapping regions much\\nfaster than MySQL. Insertion and data uploads in PostgreSQL were also better,\\nalthough general searching capability of both databases was almost equivalent.\\nIn addition, using the algorithm pair-wise, overlaps of >1000 datasets of\\ntranscription factor binding sites and histone marks, collected from previous\\npublications, were reported and it was found that HNF4G significantly\\nco-locates with cohesin subunit STAG1 (SA1).',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 16, 4, 8, 38, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'SECODA: Segmentation- and Combination-Based Detection of Anomalies',\n",
       "  'authors': ['Ralph Foorthuis'],\n",
       "  'summary': 'This study introduces SECODA, a novel general-purpose unsupervised\\nnon-parametric anomaly detection algorithm for datasets containing continuous\\nand categorical attributes. The method is guaranteed to identify cases with\\nunique or sparse combinations of attribute values. Continuous attributes are\\ndiscretized repeatedly in order to correctly determine the frequency of such\\nvalue combinations. The concept of constellations, exponentially increasing\\nweights and discretization cut points, as well as a pruning heuristic are used\\nto detect anomalies with an optimal number of iterations. Moreover, the\\nalgorithm has a low memory imprint and its runtime performance scales linearly\\nwith the size of the dataset. An evaluation with simulated and real-life\\ndatasets shows that this algorithm is able to identify many different types of\\nanomalies, including complex multidimensional instances. An evaluation in terms\\nof a data quality use case with a real dataset demonstrates that SECODA can\\nbring relevant and practical value to real-world settings.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 16, 10, 3, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'SDM-RDFizer: An RML Interpreter for the Efficient Creation of RDF Knowledge Graphs',\n",
       "  'authors': ['Enrique Iglesias',\n",
       "   'Samaneh Jozashoori',\n",
       "   'David Chaves-Fraga',\n",
       "   'Diego Collarana',\n",
       "   'Maria-Esther Vidal'],\n",
       "  'summary': 'In recent years, the amount of data has increased exponentially, and\\nknowledge graphs have gained attention as data structures to integrate data and\\nknowledge harvested from myriad data sources. However, data complexity issues\\nlike large volume, high-duplicate rate, and heterogeneity usually characterize\\nthese data sources, being required data management tools able to address the\\nimpact negatively of these issues on the knowledge graph creation process. In\\nthis paper, we propose the SDM-RDFizer, an interpreter of the RDF Mapping\\nLanguage (RML), to transform raw data in various formats into an RDF knowledge\\ngraph. SDM-RDFizer implements novel algorithms to execute the logical operators\\nbetween mappings in RML, allowing thus to scale up to complex scenarios where\\ndata is not only broad but has a high-duplication rate. We empirically evaluate\\nthe SDM-RDFizer performance against diverse testbeds with diverse\\nconfigurations of data volume, duplicates, and heterogeneity. The observed\\nresults indicate that SDM-RDFizer is two orders of magnitude faster than state\\nof the art, thus, meaning that SDM-RDFizer an interoperable and scalable\\nsolution for knowledge graph creation. SDM-RDFizer is publicly available as a\\nresource through a Github repository and a DOI.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 17, 9, 32, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards Inferring Queries from Simple and Partial Provenance Examples',\n",
       "  'authors': ['Amir Gilad', 'Yuval Moskovitch'],\n",
       "  'summary': 'The field of query-by-example aims at inferring queries from output examples\\ngiven by non-expert users, by finding the underlying logic that binds the\\nexamples. However, for a very small set of examples, it is difficult to\\ncorrectly infer such logic. To bridge this gap, previous work suggested\\nattaching explanations to each output example, modeled as provenance, allowing\\nusers to explain the reason behind their choice of example. In this paper, we\\nexplore the problem of inferring queries from a few output examples and\\nintuitive explanations. We propose a two step framework: (1) convert the\\nexplanations into (partial) provenance and (2) infer a query that generates the\\noutput examples using a novel algorithm that employs a graph based approach.\\nThis framework is suitable for non-experts as it does not require the\\nspecification of the provenance in its entirety or an understanding of its\\nstructure. We show promising initial experimental results of our approach.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 20, 14, 22, 20, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Spitz: A Verifiable Database System',\n",
       "  'authors': ['Meihui Zhang', 'Zhongle Xie', 'Cong Yue', 'Ziyue Zhong'],\n",
       "  'summary': 'Databases in the past have helped businesses maintain and extract insights\\nfrom their data. Today, it is common for a business to involve multiple\\nindependent, distrustful parties. This trend towards decentralization\\nintroduces a new and important requirement to databases: the integrity of the\\ndata, the history, and the execution must be protected. In other words, there\\nis a need for a new class of database systems whose integrity can be verified\\n(or verifiable databases).\\n  In this paper, we identify the requirements and the design challenges of\\nverifiable databases.We observe that the main challenges come from the need to\\nbalance data immutability, tamper evidence, and performance. We first consider\\napproaches that extend existing OLTP and OLAP systems with support for\\nverification. We next examine a clean-slate approach, by describing a new\\nsystem, Spitz, specifically designed for efficiently supporting immutable and\\ntamper-evident transaction management. We conduct a preliminary performance\\nstudy of both approaches against a baseline system, and provide insights on\\ntheir performance.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 21, 2, 16, 12, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Approximate Partition Selection for Big-Data Workloads using Summary Statistics',\n",
       "  'authors': ['Kexin Rong',\n",
       "   'Yao Lu',\n",
       "   'Peter Bailis',\n",
       "   'Srikanth Kandula',\n",
       "   'Philip Levis'],\n",
       "  'summary': 'Many big-data clusters store data in large partitions that support access at\\na coarse, partition-level granularity. As a result, approximate query\\nprocessing via row-level sampling is inefficient, often requiring reads of many\\npartitions. In this work, we seek to answer queries quickly and approximately\\nby reading a subset of the data partitions and combining partial answers in a\\nweighted manner without modifying the data layout. We illustrate how to\\nefficiently perform this query processing using a set of pre-computed summary\\nstatistics, which inform the choice of partitions and weights. We develop novel\\nmeans of using the statistics to assess the similarity and importance of\\npartitions. Our experiments on several datasets and data layouts demonstrate\\nthat to achieve the same relative error compared to uniform partition sampling,\\nour techniques offer from 2.7$\\\\times$ to $70\\\\times$ reduction in the number of\\npartitions read, and the statistics stored per partition require fewer than\\n100KB.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 24, 17, 15, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Replicability and Reproducibility of a Schema Evolution Study in Embedded Databases',\n",
       "  'authors': ['Dimitri Braininger',\n",
       "   'Wolfgang Mauerer',\n",
       "   'Stefanie Scherzinger'],\n",
       "  'summary': 'Ascertaining the feasibility of independent falsification or repetition of\\npublished results is vital to the scientific process, and replication or\\nreproduction experiments are routinely performed in many disciplines.\\nUnfortunately, such studies are only scarcely available in database research,\\nwith few papers dedicated to re-evaluating published results. In this paper, we\\nconduct a case study on replicating and reproducing a study on schema evolution\\nin embedded databases. We obtain exact results for one out of four database\\napplications studied, and come close in two further cases. By reporting\\nresults, efforts, and obstacles encountered, we hope to increase appreciation\\nfor the substantial efforts required to ensure reproducibility. By discussing\\nminutiae details required for reproducible work, we argue that such important,\\nbut often ignored components of scientific work should receive more credit in\\nthe evaluation of future research.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 25, 10, 14, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'On the complexity of query containment and computing certain answers in the presence of ACs',\n",
       "  'authors': ['Foto N. Afrati', 'Matthew Damigos'],\n",
       "  'summary': 'We often add arithmetic to extend the expressiveness of query languages and\\nstudy the complexity of problems such as testing query containment and finding\\ncertain answers in the framework of answering queries using views. When adding\\narithmetic comparisons, the complexity of such problems is higher than the\\ncomplexity of their counterparts without them. It has been observed that we can\\nachieve lower complexity if we restrict some of the comparisons in the\\ncontaining query to be closed or open semi-interval comparisons. Here, focusing\\na) on the problem of containment for conjunctive queries with arithmetic\\ncomparisons (CQAC queries, for short), we prove upper bounds on its\\ncomputational complexity and b) on the problem of computing certain answers, we\\nfind large classes of CQAC queries and views where this problem is polynomial.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 25, 13, 26, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Table2Charts: Recommending Charts by Learning Shared Table Representations',\n",
       "  'authors': ['Mengyu Zhou',\n",
       "   'Qingtao Li',\n",
       "   'Xinyi He',\n",
       "   'Yuejiang Li',\n",
       "   'Yibo Liu',\n",
       "   'Wei Ji',\n",
       "   'Shi Han',\n",
       "   'Yining Chen',\n",
       "   'Daxin Jiang',\n",
       "   'Dongmei Zhang'],\n",
       "  'summary': 'It is common for people to create different types of charts to explore a\\nmulti-dimensional dataset (table). However, to recommend commonly composed\\ncharts in real world, one should take the challenges of efficiency, imbalanced\\ndata and table context into consideration. In this paper, we propose\\nTable2Charts framework which learns common patterns from a large corpus of\\n(table, charts) pairs. Based on deep Q-learning with copying mechanism and\\nheuristic searching, Table2Charts does table-to-sequence generation, where each\\nsequence follows a chart template. On a large spreadsheet corpus with 165k\\ntables and 266k charts, we show that Table2Charts could learn a shared\\nrepresentation of table fields so that recommendation tasks on different chart\\ntypes could mutually enhance each other. Table2Charts outperforms other chart\\nrecommendation systems in both multi-type task (with doubled recall numbers\\nR@3=0.61 and R@1=0.43) and human evaluations.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 24, 15, 6, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Automatic Integration Issues of Tabular Data for On-Line Analysis Processing',\n",
       "  'authors': ['Yuzhao Yang',\n",
       "   'Jérôme Darmont',\n",
       "   'Franck Ravat',\n",
       "   'Olivier Teste'],\n",
       "  'summary': 'Companies and individuals produce numerous tabular data. The objective of\\nthis position paper is to draw up the challenges posed by the automatic\\nintegration of data in the form of tables so that they can be cross-analyzed.\\nWe provide a first automatic solution for the integration of such tabular data\\nto allow On-Line Analysis Processing. To fulfil this task, features of tabular\\ndata should be analyzed and the challenge of automatic multidimensional schema\\ngeneration should be addressed. Hence, we propose a typology of tabular data\\nand discuss our idea of an automatic solution.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 26, 6, 58, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The Impact of Discretization Method on the Detection of Six Types of Anomalies in Datasets',\n",
       "  'authors': ['Ralph Foorthuis'],\n",
       "  'summary': 'Anomaly detection is the process of identifying cases, or groups of cases,\\nthat are in some way unusual and do not fit the general patterns present in the\\ndataset. Numerous algorithms use discretization of numerical data in their\\ndetection processes. This study investigates the effect of the discretization\\nmethod on the unsupervised detection of each of the six anomaly types\\nacknowledged in a recent typology of data anomalies. To this end, experiments\\nare conducted with various datasets and SECODA, a general-purpose algorithm for\\nunsupervised non-parametric anomaly detection in datasets with numerical and\\ncategorical attributes. This algorithm employs discretization of continuous\\nattributes, exponentially increasing weights and discretization cut points, and\\na pruning heuristic to detect anomalies with an optimal number of iterations.\\nThe results demonstrate that standard SECODA can detect all six types, but that\\ndifferent discretization methods favor the discovery of certain anomaly types.\\nThe main findings also hold for other detection techniques using\\ndiscretization.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 27, 18, 43, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Cache-Efficient Sweeping-Based Interval Joins for Extended Allen Relation Predicates (Extended Version)',\n",
       "  'authors': ['Danila Piatov', 'Sven Helmer', 'Anton Dignös', 'Fabio Persia'],\n",
       "  'summary': \"We develop a family of efficient plane-sweeping interval join algorithms that\\ncan evaluate a wide range of interval predicates such as Allen's relationships\\nand parameterized relationships. Our technique is based on a framework,\\ncomponents of which can be flexibly combined in different manners to support\\nthe required interval relation. In temporal databases, our algorithms can\\nexploit a well-known and flexible access method, the Timeline Index, thus\\nexpanding the set of operations it supports even further. Additionally,\\nemploying a compact data structure, the gapless hash map, we utilize the CPU\\ncache efficiently. In an experimental evaluation, we show that our approach is\\nseveral times faster and scales better than state-of-the-art techniques, while\\nbeing much better suited for real-time event processing.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 28, 14, 6, 10, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Relational Data Synthesis using Generative Adversarial Networks: A Design Space Exploration',\n",
       "  'authors': ['Ju Fan',\n",
       "   'Tongyu Liu',\n",
       "   'Guoliang Li',\n",
       "   'Junyou Chen',\n",
       "   'Yuwei Shen',\n",
       "   'Xiaoyong Du'],\n",
       "  'summary': \"The proliferation of big data has brought an urgent demand for\\nprivacy-preserving data publishing. Traditional solutions to this demand have\\nlimitations on effectively balancing the tradeoff between privacy and utility\\nof the released data. Thus, the database community and machine learning\\ncommunity have recently studied a new problem of relational data synthesis\\nusing generative adversarial networks (GAN) and proposed various algorithms.\\nHowever, these algorithms are not compared under the same framework and thus it\\nis hard for practitioners to understand GAN's benefits and limitations. To\\nbridge the gaps, we conduct so far the most comprehensive experimental study\\nthat investigates applying GAN to relational data synthesis. We introduce a\\nunified GAN-based framework and define a space of design solutions for each\\ncomponent in the framework, including neural network architectures and training\\nstrategies. We conduct extensive experiments to explore the design space and\\ncompare with traditional data synthesis approaches. Through extensive\\nexperiments, we find that GAN is very promising for relational data synthesis,\\nand provide guidance for selecting appropriate design solutions. We also point\\nout limitations of GAN and identify future research directions.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 28, 17, 41, 11, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Batching and Matching for Food Delivery in Dynamic Road Networks',\n",
       "  'authors': ['Manas Joshi',\n",
       "   'Arshdeep Singh',\n",
       "   'Sayan Ranu',\n",
       "   'Amitabha Bagchi',\n",
       "   'Priyank Karia',\n",
       "   'Puneet Kala'],\n",
       "  'summary': 'Given a stream of food orders and available delivery vehicles, how should\\norders be assigned to vehicles so that the delivery time is minimized? Several\\ndecisions have to be made: (1) assignment of orders to vehicles, (2) grouping\\norders into batches to cope with limited vehicle availability, and (3) adapting\\nto dynamic positions of delivery vehicles. We show that the minimization\\nproblem is not only NP-hard but inapproximable in polynomial time. To mitigate\\nthis computational bottleneck, we develop an algorithm called FoodMatch, which\\nmaps the vehicle assignment problem to that of minimum weight perfect matching\\non a bipartite graph. To further reduce the quadratic construction cost of the\\nbipartite graph, we deploy best-first search to only compute a subgraph that is\\nhighly likely to contain the minimum matching. The solution quality is further\\nenhanced by reducing batching to a graph clustering problem and anticipating\\ndynamic positions of vehicles through angular distance. Extensive experiments\\non food-delivery data from large metropolitan cities establish that FoodMatch\\nis substantially better than baseline strategies on a number of metrics, while\\nbeing efficient enough to handle real-world workloads.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 29, 3, 42, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'VALMOD: A Suite for Easy and Exact Detection of Variable Length Motifs in Data Series',\n",
       "  'authors': ['Michele Linardi', 'Yan Zhu', 'Themis Palpanas', 'Eamonn Keogh'],\n",
       "  'summary': 'Data series motif discovery represents one of the most useful primitives for\\ndata series mining, with applications to many domains, such as robotics,\\nentomology, seismology, medicine, and climatology, and others. The\\nstate-of-the-art motif discovery tools still require the user to provide the\\nmotif length. Yet, in several cases, the choice of motif length is critical for\\ntheir detection. Unfortunately, the obvious brute-force solution, which tests\\nall lengths within a given range, is computationally untenable, and does not\\nprovide any support for ranking motifs at different resolutions (i.e.,\\nlengths). We demonstrate VALMOD, our scalable motif discovery algorithm that\\nefficiently finds all motifs in a given range of lengths, and outputs a\\nlength-invariant ranking of motifs. Furthermore, we support the analysis\\nprocess by means of a newly proposed meta-data structure that helps the user to\\nselect the most promising pattern length. This demo aims at illustrating in\\ndetail the steps of the proposed approach, showcasing how our algorithm and\\ncorresponding graphical insights enable users to efficiently identify the\\ncorrect motifs. (Paper published in ACM Sigmod Conference 2018.)',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 31, 8, 44, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Matrix Profile Goes MAD: Variable-Length Motif And Discord Discovery in Data Series',\n",
       "  'authors': ['Michele Linardi', 'Yan Zhu', 'Themis Palpanas', 'Eamonn Keogh'],\n",
       "  'summary': 'In the last fifteen years, data series motif and discord discovery have\\nemerged as two useful and well-used primitives for data series mining, with\\napplications to many domains, including robotics, entomology, seismology,\\nmedicine, and climatology. Nevertheless, the state-of-the-art motif and discord\\ndiscovery tools still require the user to provide the relative length. Yet, in\\nseveral cases, the choice of length is critical and unforgiving. Unfortunately,\\nthe obvious brute-force solution, which tests all lengths within a given range,\\nis computationally untenable. In this work, we introduce a new framework, which\\nprovides an exact and scalable motif and discord discovery algorithm that\\nefficiently finds all motifs and discords in a given range of lengths. We\\nevaluate our approach with five diverse real datasets, and demonstrate that it\\nis up to 20 times faster than the state-of-the-art. Our results also show that\\nremoving the unrealistic assumption that the user knows the correct length, can\\noften produce more intuitive and actionable results, which could have otherwise\\nbeen missed. (Paper published in Data Mining and Knowledge Discovery Journal -\\n2020)',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 31, 9, 19, 58, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'FunMap: Efficient Execution of Functional Mappings for Knowledge Graph Creation',\n",
       "  'authors': ['Samaneh Jozashoori',\n",
       "   'David Chaves-Fraga',\n",
       "   'Enrique Iglesias',\n",
       "   'Maria-Esther Vidal',\n",
       "   'Oscar Corcho'],\n",
       "  'summary': 'Data has exponentially grown in the last years, and knowledge graphs\\nconstitute powerful formalisms to integrate a myriad of existing data sources.\\nTransformation functions -- specified with function-based mapping languages\\nlike FunUL and RML+FnO -- can be applied to overcome interoperability issues\\nacross heterogeneous data sources. However, the absence of engines to\\nefficiently execute these mapping languages hinders their global adoption. We\\npropose FunMap, an interpreter of function-based mapping languages; it relies\\non a set of lossless rewriting rules to push down and materialize the execution\\nof functions in initial steps of knowledge graph creation. Although applicable\\nto any function-based mapping language that supports joins between mapping\\nrules, FunMap feasibility is shown on RML+FnO. FunMap reduces data redundancy,\\ne.g., duplicates and unused attributes, and converts RML+FnO mappings into a\\nset of equivalent rules executable on RML-compliant engines. We evaluate FunMap\\nperformance over real-world testbeds from the biomedical domain. The results\\nindicate that FunMap reduces the execution time of RML-compliant engines by up\\nto a factor of 18, furnishing, thus, a scalable solution for knowledge graph\\ncreation.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 31, 10, 48, 41, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Proposed DBMS for OTT platforms in line with new age requirements',\n",
       "  'authors': ['Khushi Shah',\n",
       "   'Aryan Shah',\n",
       "   'Charmi Shah',\n",
       "   'Devansh Shah',\n",
       "   'Mustafa Africawala',\n",
       "   'Rushabh Shah',\n",
       "   'Nishant Doshi'],\n",
       "  'summary': 'Database management has become an enormous tool for on-demand content\\ndistribution services, proffering required information and providing custom\\nservices to the user. Also plays a major role for the platforms to manage their\\ndata in such a way that data redundancy is minimized. This paper emphasizes\\nimproving the user experience for the platform by efficiently managing data.\\nKeeping in mind all the new age requirements, especially after COVID-19 the\\nsudden surge in subscription has led the stakeholders to try new things to lead\\nthe OTT market. Collection of shows being the root of the tree here, this paper\\nimprovises the currently existing branches via various tables and suggests some\\nnew features on how the data collected can be utilized for introducing new and\\nmuch-required query results for the consumer.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 2, 4, 42, 1, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins',\n",
       "  'authors': ['Sahaana Suri',\n",
       "   'Ihab F. Ilyas',\n",
       "   'Christopher Ré',\n",
       "   'Theodoros Rekatsinas'],\n",
       "  'summary': 'Structured data, or data that adheres to a pre-defined schema, can suffer\\nfrom fragmented context: information describing a single entity can be\\nscattered across multiple datasets or tables tailored for specific business\\nneeds, with no explicit linking keys (e.g., primary key-foreign key\\nrelationships or heuristic functions). Context enrichment, or rebuilding\\nfragmented context, using keyless joins is an implicit or explicit step in\\nmachine learning (ML) pipelines over structured data sources. This process is\\ntedious, domain-specific, and lacks support in now-prevalent no-code ML systems\\nthat let users create ML pipelines using just input data and high-level\\nconfiguration files. In response, we propose Ember, a system that abstracts and\\nautomates keyless joins to generalize context enrichment. Our key insight is\\nthat Ember can enable a general keyless join operator by constructing an index\\npopulated with task-specific embeddings. Ember learns these embeddings by\\nleveraging Transformer-based representation learning techniques. We describe\\nour core architectural principles and operators when developing Ember, and\\nempirically demonstrate that Ember allows users to develop no-code pipelines\\nfor five domains, including search, recommendation and question answering, and\\ncan exceed alternatives by up to 39% recall, with as little as a single line\\nconfiguration change.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 2, 23, 2, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Ver: View Discovery in the Wild',\n",
       "  'authors': ['Yue Gong',\n",
       "   'Zhiru Zhu',\n",
       "   'Sainyam Galhotra',\n",
       "   'Raul Castro Fernandez'],\n",
       "  'summary': 'We present Ver, a data discovery system that identifies project-join views\\nover large repositories of tables that do not contain join path information,\\nand even when input queries are inaccurate. Ver implements a reference\\narchitecture to solve both the technical (scale and search) and human (semantic\\nambiguity, navigating a large number of results) problems of view discovery. We\\ndemonstrate users find the view they want when using Ver with a user study and\\nwe demonstrate its performance with large-scale end-to-end experiments on\\nreal-world datasets containing tens of millions of join paths.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 3, 1, 58, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Facade-X: an opinionated approach to SPARQL anything',\n",
       "  'authors': ['Enrico Daga',\n",
       "   'Luigi Asprino',\n",
       "   'Paul Mulholland',\n",
       "   'Aldo Gangemi'],\n",
       "  'summary': 'The Semantic Web research community understood since its beginning how\\ncrucial it is to equip practitioners with methods to transform non-RDF\\nresources into RDF. Proposals focus on either engineering content\\ntransformations or accessing non-RDF resources with SPARQL. Existing solutions\\nrequire users to learn specific mapping languages (e.g. RML), to know how to\\nquery and manipulate a variety of source formats (e.g. XPATH, JSON-Path), or to\\ncombine multiple languages (e.g. SPARQL Generate). In this paper, we explore an\\nalternative solution and contribute a general-purpose meta-model for converting\\nnon-RDF resources into RDF: Facade-X. Our approach can be implemented by\\noverriding the SERVICE operator and does not require to extend the SPARQL\\nsyntax. We compare our approach with the state of art methods RML and SPARQL\\nGenerate and show how our solution has lower learning demands and cognitive\\ncomplexity, and it is cheaper to implement and maintain, while having\\ncomparable extensibility and efficiency.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 4, 9, 19, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An Ontology Model for Climatic Data Analysis',\n",
       "  'authors': ['Jiantao Wu',\n",
       "   'Fabrizio Orlandi',\n",
       "   \"Declan O'Sullivan\",\n",
       "   'Soumyabrata Dev'],\n",
       "  'summary': 'Recently ontologies have been exploited in a wide range of research areas for\\ndata modeling and data management. They greatly assists in defining the\\nsemantic model of the underlying data combined with domain knowledge. In this\\npaper, we propose the Climate Analysis (CA) Ontology to model climate datasets\\nused by remote sensing analysts. We use the data published by National Oceanic\\nand Atmospheric Administration (NOAA) to further explore how ontology modeling\\ncan be used to facilitate the field of climatic data processing. The idea of\\nthis work is to convert relational climate data to the Resource Description\\nFramework (RDF) data model, so that it can be stored in a graph database and\\neasily accessed through the Web as Linked Data. Typically, this provides\\nclimate researchers, who are interested in datasets such as NOAA, with the\\npotential of enriching and interlinking with other databases. As a result, our\\napproach facilitates data integration and analysis of diverse climatic data\\nsources and allows researchers to interrogate these sources directly on the Web\\nusing the standard SPARQL query language.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 6, 10, 33, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Sub-trajectory Similarity Join with Obfuscation',\n",
       "  'authors': ['Yanchuan Chang',\n",
       "   'Jianzhong Qi',\n",
       "   'Egemen Tanin',\n",
       "   'Xingjun Ma',\n",
       "   'Hanan Samet'],\n",
       "  'summary': \"User trajectory data is becoming increasingly accessible due to the\\nprevalence of GPS-equipped devices such as smartphones. Many existing studies\\nfocus on querying trajectories that are similar to each other in their\\nentirety. We observe that trajectories partially similar to each other contain\\nuseful information about users' travel patterns which should not be ignored.\\nSuch partially similar trajectories are critical in applications such as\\nepidemic contact tracing. We thus propose to query trajectories that are within\\na given distance range from each other for a given period of time. We formulate\\nthis problem as a sub-trajectory similarity join query named as the STS-Join.\\nWe further propose a distributed index structure and a query algorithm for\\nSTS-Join, where users retain their raw location data and only send obfuscated\\ntrajectories to a server for query processing. This helps preserve user\\nlocation privacy which is vital when dealing with such data. Theoretical\\nanalysis and experiments on real data confirm the effectiveness and the\\nefficiency of our proposed index structure and query algorithm.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 7, 6, 8, 6, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Efficient Exact k-Flexible Aggregate Nearest Neighbor Search in Road Networks Using the M-tree',\n",
       "  'authors': ['Moonyoung Chung', 'Soon J. Hyun', 'Woong-Kee Loh'],\n",
       "  'summary': 'This study proposes an efficient exact k-flexible aggregate nearest neighbor\\n(k-FANN) search algorithm in road networks using the M-tree. The\\nstate-of-the-art IER-kNN algorithm used the R-tree and pruned off unnecessary\\nnodes based on the Euclidean coordinates of objects in road networks. However,\\nIER-kNN made many unnecessary accesses to index nodes since the Euclidean\\ndistances between objects are significantly different from the actual\\nshortest-path distances between them. In contrast, our algorithm proposed in\\nthis study can greatly reduce unnecessary accesses to index nodes compared with\\nIER-kNN since the M-tree is constructed based on the actual shortest-path\\ndistances between objects. To the best of our knowledge, our algorithm is the\\nfirst exact FANN algorithm that uses the M-tree. We prove that our algorithm\\ndoes not cause any false drop. In conducting a series of experiments using\\nvarious real road network datasets, our algorithm consistently outperformed\\nIER-kNN by up to 6.92 times.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 10, 9, 56, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Machamp: A Generalized Entity Matching Benchmark',\n",
       "  'authors': ['Jin Wang', 'Yuliang Li', 'Wataru Hirota'],\n",
       "  'summary': 'Entity Matching (EM) refers to the problem of determining whether two\\ndifferent data representations refer to the same real-world entity. It has been\\na long-standing interest of the data management community and many efforts have\\nbeen paid in creating benchmark tasks as well as in developing advanced\\nmatching techniques. However, existing benchmark tasks for EM are limited to\\nthe case where the two data collections of entities are structured tables with\\nthe same schema. Meanwhile, the data collections for matching could be\\nstructured, semi-structured, or unstructured in real-world scenarios of data\\nscience. In this paper, we come up with a new research problem -- Generalized\\nEntity Matching to satisfy this requirement and create a benchmark Machamp for\\nit. Machamp consists of seven tasks having diverse characteristics and thus\\nprovides good coverage of use cases in real applications. We summarize existing\\nEM benchmark tasks for structured tables and conduct a series of processing and\\ncleaning efforts to transform them into matching tasks between tables with\\ndifferent structures. Based on that, we further conduct comprehensive profiling\\nof the proposed benchmark tasks and evaluate popular entity matching approaches\\non them. With the help of Machamp, it is the first time that researchers can\\nevaluate EM techniques between data collections with different structures.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 15, 22, 2, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data Lakes: A Survey of Functions and Systems',\n",
       "  'authors': ['Rihan Hai',\n",
       "   'Christos Koutras',\n",
       "   'Christoph Quix',\n",
       "   'Matthias Jarke'],\n",
       "  'summary': \"Data lakes are becoming increasingly prevalent for big data management and\\ndata analytics. In contrast to traditional 'schema-on-write' approaches such as\\ndata warehouses, data lakes are repositories storing raw data in its original\\nformats and providing a common access interface. Despite the strong interest\\nraised from both academia and industry, there is a large body of ambiguity\\nregarding the definition, functions and available technologies for data lakes.\\nA complete, coherent picture of data lake challenges and solutions is still\\nmissing. This survey reviews the development, architectures, and systems of\\ndata lakes. We provide a comprehensive overview of research questions for\\ndesigning and building data lakes. We classify the existing approaches and\\nsystems based on their provided functions for data lakes, which makes this\\nsurvey a useful technical reference for designing, implementing and deploying\\ndata lakes. We hope that the thorough comparison of existing solutions and the\\ndiscussion of open research challenges in this survey will motivate the future\\ndevelopment of data lake research and practice.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 17, 15, 18, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A probabilistic database approach to autoencoder-based data cleaning',\n",
       "  'authors': ['R. R. Mauritz',\n",
       "   'F. P. J. Nijweide',\n",
       "   'J. Goseling',\n",
       "   'M. van Keulen'],\n",
       "  'summary': 'Data quality problems are a large threat in data science. In this paper, we\\npropose a data-cleaning autoencoder capable of near-automatic data quality\\nimprovement. It learns the structure and dependencies in the data and uses it\\nas evidence to identify and correct doubtful values. We apply a probabilistic\\ndatabase approach to represent weak and strong evidence for attribute value\\nrepairs. A theoretical framework is provided, and experiments show that it can\\nremove significant amounts of noise (i.e., data quality problems) from\\ncategorical and numeric probabilistic data. Our method does not require clean\\ndata. We do, however, show that manually cleaning a small fraction of the data\\nsignificantly improves performance.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 17, 18, 46, 56, tzinfo=datetime.timezone.utc)},\n",
       " {'title': \"Introducing PathQuery, Google's Graph Query Language\",\n",
       "  'authors': ['Jesse Weaver',\n",
       "   'Eric Paniagua',\n",
       "   'Tushar Agarwal',\n",
       "   'Nicholas Guy',\n",
       "   'Alexandre Mattos'],\n",
       "  'summary': 'We introduce PathQuery, a graph query language developed to scale with\\nGoogle\\'s query and data volumes as well as its internal developer community.\\nPathQuery supports flexible and declarative semantics. We have found that this\\nenables query developers to think in a naturally \"graphy\" design space and to\\navoid the additional cognitive effort of coordinating numerous joins and\\nsubqueries often required to express an equivalent query in a relational space.\\nDespite its traversal-oriented syntactic style, PathQuery has a foundation on a\\ncustom variant of relational algebra -- the exposition of which we presently\\ndefer -- allowing for the application of both common and novel optimizations.\\nWe believe that PathQuery has withstood a \"test of time\" at Google, under both\\nlarge scale and low latency requirements. We thus share herein a language\\ndesign that admits a rigorous declarative semantics, has scaled well in\\npractice, and provides a natural syntax for graph traversals while also\\nadmitting complex graph patterns.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 17, 20, 27, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Generic Distributed Clustering Framework for Massive Data',\n",
       "  'authors': ['Pingyi Luo', 'Qiang Huang', 'Anthony K. H. Tung'],\n",
       "  'summary': 'In this paper, we introduce a novel Generic distributEd clustEring frameworK\\n(GEEK) beyond $k$-means clustering to process massive amounts of data. To deal\\nwith different data types, GEEK first converts data in the original feature\\nspace into a unified format of buckets; then, we design a new Seeding method\\nbased on simILar bucKets (SILK) to determine initial seeds. Compared with\\nstate-of-the-art seeding methods such as $k$-means++ and its variants, SILK can\\nautomatically identify the number of initial seeds based on the closeness of\\nshared data objects in similar buckets instead of pre-specifying $k$. Thus, its\\ntime complexity is independent of $k$. With these well-selected initial seeds,\\nGEEK only needs a one-pass data assignment to get the final clusters. We\\nimplement GEEK on a distributed CPU-GPU platform for large-scale clustering. We\\nevaluate the performance of GEEK over five large-scale real-life datasets and\\nshow that GEEK can deal with massive data of different types and is comparable\\nto (or even better than) many state-of-the-art customized GPU-based methods,\\nespecially in large $k$ values.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 19, 15, 20, 21, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'HDMM: Optimizing error of high-dimensional statistical queries under differential privacy',\n",
       "  'authors': ['Ryan McKenna',\n",
       "   'Gerome Miklau',\n",
       "   'Michael Hay',\n",
       "   'Ashwin Machanavajjhala'],\n",
       "  'summary': 'In this work we describe the High-Dimensional Matrix Mechanism (HDMM), a\\ndifferentially private algorithm for answering a workload of predicate counting\\nqueries. HDMM represents query workloads using a compact implicit matrix\\nrepresentation and exploits this representation to efficiently optimize over (a\\nsubset of) the space of differentially private algorithms for one that is\\nunbiased and answers the input query workload with low expected error. HDMM can\\nbe deployed for both $\\\\epsilon$-differential privacy (with Laplace noise) and\\n$(\\\\epsilon, \\\\delta)$-differential privacy (with Gaussian noise), although the\\ncore techniques are slightly different for each. We demonstrate empirically\\nthat HDMM can efficiently answer queries with lower expected error than\\nstate-of-the-art techniques, and in some cases, it nearly matches existing\\nlower bounds for the particular class of mechanisms we consider.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 23, 1, 19, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Mr. Plotter: Unifying Data Reduction Techniques in Storage and Visualization Systems',\n",
       "  'authors': ['Sam Kumar', 'Michael P Andersen', 'David E. Culler'],\n",
       "  'summary': 'As the rate of data collection continues to grow rapidly, developing\\nvisualization tools that scale to immense data sets is a serious and\\never-increasing challenge. Existing approaches generally seek to decouple\\nstorage and visualization systems, performing just-in-time data reduction to\\ntransparently avoid overloading the visualizer. We present a new architecture\\nin which the visualizer and data store are tightly coupled. Unlike systems that\\nread raw data from storage, the performance of our system scales linearly with\\nthe size of the final visualization, essentially independent of the size of the\\ndata. Thus, it scales to massive data sets while supporting interactive\\nperformance (sub-100 ms query latency). This enables a new class of\\nvisualization clients that automatically manage data, quickly and transparently\\nrequesting data from the underlying database without requiring the user to\\nexplicitly initiate queries. It lays a groundwork for supporting truly\\ninteractive exploration of big data and opens new directions for research on\\nscalable information visualization systems.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 23, 16, 23, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Novel Approach to Discover Switch Behaviours in Process Mining',\n",
       "  'authors': ['Yang Lu', 'Qifan Chen', 'Simon Poon'],\n",
       "  'summary': 'Process mining is a relatively new subject which builds a bridge between\\nprocess modelling and data mining. An exclusive choice in a process model\\nusually splits the process into different branches. However, in some processes,\\nit is possible to switch from one branch to another. The inductive miner\\nguarantees to return sound process models, but fails to return a precise model\\nwhen there are switch behaviours between different exclusive choice branches\\ndue to the limitation of process trees. In this paper, we present a novel\\nextension to the process tree model to support switch behaviours between\\ndifferent branches of the exclusive choice operator and propose a novel\\nextension to the inductive miner to discover sound process models with switch\\nbehaviours. The proposed discovery technique utilizes the theory of a previous\\nstudy to detect possible switch behaviours. We apply both artificial and\\npublicly-available datasets to evaluate our approach. Our results show that our\\napproach can improve the precision of discovered models by 36% while\\nmaintaining high fitness values compared to the original inductive miner.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 24, 4, 25, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The Complexity of Boolean Conjunctive Queries with Intersection Joins',\n",
       "  'authors': ['Mahmoud Abo Khamis',\n",
       "   'George Chichirim',\n",
       "   'Antonia Kormpa',\n",
       "   'Dan Olteanu'],\n",
       "  'summary': 'Intersection joins over interval data are relevant in spatial and temporal\\ndata settings. A set of intervals join if their intersection is non-empty. In\\ncase of point intervals, the intersection join becomes the standard equality\\njoin.\\n  We establish the complexity of Boolean conjunctive queries with intersection\\njoins by a many-one equivalence to disjunctions of Boolean conjunctive queries\\nwith equality joins. The complexity of any query with intersection joins is\\nthat of the hardest query with equality joins in the disjunction exhibited by\\nour equivalence. This is captured by a new width measure called the IJ-width.\\n  We also introduce a new syntactic notion of acyclicity called iota-acyclicity\\nto characterise the class of Boolean queries with intersection joins that admit\\nlinear time computation modulo a poly-logarithmic factor in the data size.\\nIota-acyclicity is for intersection joins what alpha-acyclicity is for equality\\njoins. It strictly sits between gamma-acyclicity and Berge-acyclicity. The\\nintersection join queries that are not iota-acyclic are at least as hard as the\\nBoolean triangle query with equality joins, which is widely considered not\\ncomputable in linear time.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 24, 22, 44, 30, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'TOPIC: Top-k High-Utility Itemset Discovering',\n",
       "  'authors': ['Jiahui Chen',\n",
       "   'Shicheng Wan',\n",
       "   'Wensheng Gan',\n",
       "   'Guoting Chen',\n",
       "   'Hamido Fujita'],\n",
       "  'summary': 'Utility-driven itemset mining is widely applied in many real-world scenarios.\\nHowever, most algorithms do not work for itemsets with negative utilities.\\nSeveral efficient algorithms for high-utility itemset (HUI) mining with\\nnegative utilities have been proposed. These algorithms can find complete HUIs\\nwith or without negative utilities. However, the major problem with these\\nalgorithms is how to select an appropriate minimum utility (minUtil) threshold.\\nTo address this issue, some efficient algorithms for extracting top-k HUIs have\\nbeen proposed, where parameter k is the quantity of HUIs to be discovered.\\nHowever, all of these algorithms can solve only one part of the above problem.\\nIn this paper, we present a method for TOP-k high-utility Itemset disCovering\\n(TOPIC) with positive and negative utility values, which utilizes the\\nadvantages of the above algorithms. TOPIC adopts transaction merging and\\ndatabase projection techniques to reduce the database scanning cost, and\\nutilizes minUtil threshold raising strategies. It also uses an array-based\\nutility technique, which calculates the utility of itemsets and upper bounds in\\nlinear time. We conducted extensive experiments on several real and synthetic\\ndatasets, and the results showed that TOPIC outperforms state-of-the-art\\nalgorithm in terms of runtime, memory costs, and scalability.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 28, 15, 34, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'THUE: Discovering Top-K High Utility Episodes',\n",
       "  'authors': ['Shicheng Wan',\n",
       "   'Jiahui Chen',\n",
       "   'Wensheng Gan',\n",
       "   'Guoting Chen',\n",
       "   'Vikram Goyal'],\n",
       "  'summary': 'Episode discovery from an event is a popular framework for data mining tasks\\nand has many real-world applications. An episode is a partially ordered set of\\nobjects (e.g., item, node), and each object is associated with an event type.\\nThis episode can also be considered as a complex event sub-sequence.\\nHigh-utility episode mining is an interesting utility-driven mining task in the\\nreal world. Traditional episode mining algorithms, by setting a threshold,\\nusually return a huge episode that is neither intuitive nor saves time. In\\ngeneral, finding a suitable threshold in a pattern-mining algorithm is a\\ntrivial and time-consuming task. In this paper, we propose a novel algorithm,\\ncalled Top-K High Utility Episode (THUE) mining within the complex event\\nsequence, which redefines the previous mining task by obtaining the K highest\\nepisodes. We introduce several threshold-raising strategies and optimize the\\nepisode-weighted utilization upper bounds to speed up the mining process and\\neffectively reduce the memory cost. Finally, the experimental results on both\\nreal-life and synthetic datasets reveal that the THUE algorithm can offer six\\nto eight orders of magnitude running time performance improvement over the\\nstate-of-the-art algorithm and has low memory consumption.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 28, 16, 11, 54, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Threshold Queries in Theory and in the Wild',\n",
       "  'authors': ['Angela Bonifati',\n",
       "   'Stefania Dumbrava',\n",
       "   'George Fletcher',\n",
       "   'Jan Hidders',\n",
       "   'Matthias Hofer',\n",
       "   'Wim Martens',\n",
       "   'Filip Murlak',\n",
       "   'Joshua Shinavier',\n",
       "   'Sławek Staworko',\n",
       "   'Dominik Tomaszuk'],\n",
       "  'summary': 'Threshold queries are an important class of queries that only require\\ncomputing or counting answers up to a specified threshold value. To the best of\\nour knowledge, threshold queries have been largely disregarded in the research\\nliterature, which is surprising considering how common they are in practice. In\\nthis paper, we present a deep theoretical analysis of threshold query\\nevaluation and show that thresholds can be used to significantly improve the\\nasymptotic bounds of state-of-the-art query evaluation algorithms. We also\\nempirically show that threshold queries are significant in practice. In\\nsurprising contrast to conventional wisdom, we found important scenarios in\\nreal-world data sets in which users are interested in computing the results of\\nqueries up to a certain threshold, independent of a ranking function that\\norders the query results by importance.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 29, 20, 14, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Critical Analysis of Recursive Model Indexes',\n",
       "  'authors': ['Marcel Maltry', 'Jens Dittrich'],\n",
       "  'summary': \"The recursive model index (RMI) has recently been introduced as a\\nmachine-learned replacement for traditional indexes over sorted data, achieving\\nremarkably fast lookups. Follow-up work focused on explaining RMI's performance\\nand automatically configuring RMIs through enumeration. Unfortunately,\\nconfiguring RMIs involves setting several hyperparameters, the enumeration of\\nwhich is often too time-consuming in practice. Therefore, in this work, we\\nconduct the first inventor-independent broad analysis of RMIs with the goal of\\nunderstanding the impact of each hyperparameter on performance. In particular,\\nwe show that in addition to model types and layer size, error bounds and search\\nalgorithms must be considered to achieve the best possible performance. Based\\non our findings, we develop a simple-to-follow guideline for configuring RMIs.\\nWe evaluate our guideline by comparing the resulting RMIs with a number of\\nstate-of-the-art indexes, both learned and traditional. We show that our simple\\nguideline is sufficient to achieve competitive performance with other learned\\nindexes and RMIs whose configuration was determined using an expensive\\nenumeration procedure. In addition, while carefully reimplementing RMIs, we are\\nable to improve the build time by 2.5x to 6.3x.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 6, 30, 16, 0, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A comparative analysis of state-of-the-art SQL-on-Hadoop systems for interactive analytics',\n",
       "  'authors': ['Ashish Tapdiya', 'Daniel Fabbri'],\n",
       "  'summary': 'Hadoop is emerging as the primary data hub in enterprises, and SQL represents\\nthe de facto language for data analysis. This combination has led to the\\ndevelopment of a variety of SQL-on-Hadoop systems in use today. While the\\nvarious SQL-on-Hadoop systems target the same class of analytical workloads,\\ntheir different architectures, design decisions and implementations impact\\nquery performance. In this work, we perform a comparative analysis of four\\nstate-of-the-art SQL-on-Hadoop systems (Impala, Drill, Spark SQL and Phoenix)\\nusing the Web Data Analytics micro benchmark and the TPC-H benchmark on the\\nAmazon EC2 cloud platform. The TPC-H experiment results show that, although\\nImpala outperforms other systems (4.41x - 6.65x) in the text format, trade-offs\\nexists in the parquet format, with each system performing best on subsets of\\nqueries. A comprehensive analysis of execution profiles expands upon the\\nperformance results to provide insights into performance variations,\\nperformance bottlenecks and query execution characteristics.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 3, 31, 23, 16, 1, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Differentially Private Hierarchical Count-of-Counts Histograms',\n",
       "  'authors': ['Yu-Hsuan Kuo',\n",
       "   'Cho-Chun Chiu',\n",
       "   'Daniel Kifer',\n",
       "   'Michael Hay',\n",
       "   'Ashwin Machanavajjhala'],\n",
       "  'summary': 'We consider the problem of privately releasing a class of queries that we\\ncall hierarchical count-of-counts histograms. Count-of-counts histograms\\npartition the rows of an input table into groups (e.g., group of people in the\\nsame household), and for every integer j report the number of groups of size j.\\nHierarchical count-of-counts queries report count-of-counts histograms at\\ndifferent granularities as per hierarchy defined on an attribute in the input\\ndata (e.g., geographical location of a household at the national, state and\\ncounty levels). In this paper, we introduce this problem, along with\\nappropriate error metrics and propose a differentially private solution that\\ngenerates count-of-counts histograms that are consistent across all levels of\\nthe hierarchy.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 4, 2, 1, 51, 10, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Note on the Hardness of the Critical Tuple Problem',\n",
       "  'authors': ['Egor V. Kostylev', 'Dan Suciu'],\n",
       "  'summary': 'The notion of critical tuple was introduced by Miklau and Suciu (Gerome\\nMiklau and Dan Suciu. A formal analysis of information disclosure in data\\nexchange. J. Comput. Syst. Sci., 73(3):507-534, 2007), who also claimed that\\nthe problem of checking whether a tuple is non-critical is complete for the\\nsecond level of the polynomial hierarchy. Kostylev identified an error in the\\n12-page-long hardness proof. It turns out that the issue is rather fundamental:\\nthe proof can be adapted to show hardness of a relative variant of\\ntuple-non-criticality, but we have neither been able to prove the original\\nclaim nor found an algorithm for it of lower complexity. In this note we state\\nformally the relative variant and present an alternative, simplified proof of\\nits hardness; we also give an NP-hardness proof for the original problem, the\\nbest lower bound we have been able to show. Hence, the precise complexity of\\nthe original critical tuple problem remains open.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 4, 2, 10, 22, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Mining User Behavioral Rules from Smartphone Data through Association Analysis',\n",
       "  'authors': ['Iqbal H. Sarker', 'Flora D. Salim'],\n",
       "  'summary': 'The increasing popularity of smart mobile phones and their powerful sensing\\ncapabilities have enabled the collection of rich contextual information and\\nmobile phone usage records through the device logs. This paper formulates the\\nproblem of mining behavioral association rules of individual mobile phone users\\nutilizing their smartphone data. Association rule learning is the most popular\\ntechnique to discover rules utilizing large datasets. However, it is well-known\\nthat a large proportion of association rules generated are redundant. This\\nredundant production makes not only the rule-set unnecessarily large but also\\nmakes the decision making process more complex and ineffective. In this paper,\\nwe propose an approach that effectively identifies the redundancy in\\nassociations and extracts a concise set of behavioral association rules that\\nare non-redundant. The effectiveness of the proposed approach is examined by\\nconsidering the real mobile phone datasets of individual users.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 3, 19, 4, 37, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'IDEBench: A Benchmark for Interactive Data Exploration',\n",
       "  'authors': ['Philipp Eichmann',\n",
       "   'Carsten Binnig',\n",
       "   'Tim Kraska',\n",
       "   'Emanuel Zgraggen'],\n",
       "  'summary': 'Existing benchmarks for analytical database systems such as TPC-DS and TPC-H\\nare designed for static reporting scenarios. The main metric of these\\nbenchmarks is the performance of running individual SQL queries over a\\nsynthetic database. In this paper, we argue that such benchmarks are not\\nsuitable for evaluating database workloads originating from interactive data\\nexploration (IDE) systems where most queries are ad-hoc, not based on\\npredefined reports, and built incrementally. As a main contribution, we present\\na novel benchmark called IDEBench that can be used to evaluate the performance\\nof database systems for IDE workloads. As opposed to traditional benchmarks for\\nanalytical database systems, our goal is to provide more meaningful workloads\\nand datasets that can be used to benchmark IDE query engines, with a particular\\nfocus on metrics that capture the trade-off between query performance and\\nquality of the result. As a second contribution, this paper evaluates and\\ndiscusses the performance results of selected IDE query engines using our\\nbenchmark. The study includes two commercial systems, as well as two research\\nprototypes (IDEA, approXimateDB/XDB), and one traditional analytical database\\nsystem (MonetDB).',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 4, 7, 21, 23, 16, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Counting Triangles under Updates in Worst-Case Optimal Time',\n",
       "  'authors': ['Ahmet Kara',\n",
       "   'Hung Q. Ngo',\n",
       "   'Milos Nikolic',\n",
       "   'Dan Olteanu',\n",
       "   'Haozhe Zhang'],\n",
       "  'summary': 'We consider the problem of incrementally maintaining the triangle count query\\nunder single-tuple updates to the input relations. We introduce an approach\\nthat exhibits a space-time tradeoff such that the space-time product is\\nquadratic in the size of the input database and the update time can be as low\\nas the square root of this size. This lowest update time is worst-case optimal\\nconditioned on the Online Matrix-Vector Multiplication conjecture. The\\nclassical and factorized incremental view maintenance approaches are recovered\\nas special cases of our approach within the space-time tradeoff. In particular,\\nthey require linear-time update maintenance, which is suboptimal. Our approach\\nalso recovers the worst-case optimal time complexity for computing the triangle\\ncount in the non-incremental setting.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 4, 9, 0, 51, 11, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Graph Pattern Matching Preserving Label-Repetition Constraints',\n",
       "  'authors': ['Houari Mahfoud'],\n",
       "  'summary': 'Graph pattern matching is a routine process for a wide variety of\\napplications such as social network analysis. It is typically defined in terms\\nof subgraph isomorphism which is NP-Complete. To lower its complexity, many\\nextensions of graph simulation have been proposed which focus on some\\ntopological constraints of pattern graphs that can be preserved in\\npolynomial-time over data graphs. We discuss in this paper the satisfaction of\\na new topological constraint, called Label-Repetition constraint. To the best\\nof our knowledge, existing polynomial approaches fail to preserve this\\nconstraint, and moreover, one can adopt only subgraph isomorphism for this end\\nwhich is cost-prohibitive. We present first a necessary and sufficient\\ncondition that a data subgraph must satisfy to preserve the Label-Repetition\\nconstraints of the pattern graph. Furthermore, we define matching based on a\\nnotion of triple simulation, an extension of graph simulation by considering\\nthe new topological constraint. We show that with this extension, graph pattern\\nmatching can be performed in polynomial-time, by providing such an algorithm.\\nOur algorithm is sub-quadratic in the size of data graphs only, and quartic in\\ngeneral. We show that our results can be combined with orthogonal approaches\\nfor more expressive graph pattern matching.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 4, 12, 0, 4, 5, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'NELL2RDF: Reading the Web, and Publishing it as Linked Data',\n",
       "  'authors': ['José M. Giménez-García',\n",
       "   'Maísa Duarte',\n",
       "   'Antoine Zimmermann',\n",
       "   'Christophe Gravier',\n",
       "   'Estevam R. Hruschke Jr.',\n",
       "   'Pierre Maret'],\n",
       "  'summary': \"NELL is a system that continuously reads the Web to extract knowledge in form\\nof entities and relations between them. It has been running since January 2010\\nand extracted over 50,000,000 candidate statements. NELL's generated data\\ncomprises all the candidate statements together with detailed information about\\nhow it was generated. This information includes how each component of the\\nsystem contributed to the extraction of the statement, as well as when that\\nhappened and how confident the system is in the veracity of the statement.\\nHowever, the data is only available in an ad hoc CSV format that makes it\\ndifficult to exploit out of the context of NELL. In order to make it more\\nusable for other communities, we adopt Linked Data principles to publish a more\\nstandardized, self-describing dataset with rich provenance metadata.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 4, 16, 12, 37, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Consensus Community Detection in Multilayer Networks using Parameter-free Graph Pruning',\n",
       "  'authors': ['Domenico Mandaglio', 'Alessia Amelio', 'Andrea Tagarelli'],\n",
       "  'summary': 'The clustering ensemble paradigm has emerged as an effective tool for\\ncommunity detection in multilayer networks, which allows for producing\\nconsensus solutions that are designed to be more robust to the algorithmic\\nselection and configuration bias. However, one limitation is related to the\\ndependency on a co-association threshold that controls the degree of consensus\\nin the community structure solution. The goal of this work is to overcome this\\nlimitation with a new framework of ensemble-based multilayer community\\ndetection, which features parameter-free identification of consensus\\ncommunities based on generative models of graph pruning that are able to filter\\nout noisy co-associations. We also present an enhanced version of the\\nmodularity-driven ensemble-based multilayer community detection method, in\\nwhich community memberships of nodes are reconsidered to optimize the\\nmultilayer modularity of the consensus solution. Experimental evidence on\\nreal-world networks confirms the beneficial effect of using model-based\\nfiltering methods and also shows the superiority of the proposed method on\\nstate-of-the-art multilayer community detection.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 4, 18, 11, 19, 1, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'HD-Index: Pushing the Scalability-Accuracy Boundary for Approximate kNN Search in High-Dimensional Spaces',\n",
       "  'authors': ['Akhil Arora',\n",
       "   'Sakshi Sinha',\n",
       "   'Piyush Kumar',\n",
       "   'Arnab Bhattacharya'],\n",
       "  'summary': 'Nearest neighbor searching of large databases in high-dimensional spaces is\\ninherently difficult due to the curse of dimensionality. A flavor of\\napproximation is, therefore, necessary to practically solve the problem of\\nnearest neighbor search. In this paper, we propose a novel yet simple indexing\\nscheme, HD-Index, to solve the problem of approximate k-nearest neighbor\\nqueries in massive high-dimensional databases. HD-Index consists of a set of\\nnovel hierarchical structures called RDB-trees built on Hilbert keys of\\ndatabase objects. The leaves of the RDB-trees store distances of database\\nobjects to reference objects, thereby allowing efficient pruning using distance\\nfilters. In addition to triangular inequality, we also use Ptolemaic inequality\\nto produce better lower bounds. Experiments on massive (up to billion scale)\\nhigh-dimensional (up to 1000+) datasets show that HD-Index is effective,\\nefficient, and scalable.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 4, 18, 17, 28, 48, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Dichotomies in Ontology-Mediated Querying with the Guarded Fragment',\n",
       "  'authors': ['Andre Hernich',\n",
       "   'Carsten Lutz',\n",
       "   'Fabio Papacchini',\n",
       "   'Frank Wolter'],\n",
       "  'summary': \"We study the complexity of ontology-mediated querying when ontologies are\\nformulated in the guarded fragment of first-order logic (GF). Our general aim\\nis to classify the data complexity on the level of ontologies where query\\nevaluation w.r.t. an ontology O is considered to be in PTime if all (unions of\\nconjunctive) queries can be evaluated in PTime w.r.t. O and coNP-hard if at\\nleast one query is coNP-hard w.r.t. O. We identify several large and relevant\\nfragments of GF that enjoy a dichotomy between PTime and coNP, some of them\\nadditionally admitting a form of counting. In fact, almost all ontologies in\\nthe BioPortal repository fall into these fragments or can easily be rewritten\\nto do so. We then establish a variation of Ladner's Theorem on the existence of\\nNP-intermediate problems and use this result to show that for other fragments,\\nthere is provably no such dichotomy. Again for other fragments (such as full\\nGF), establishing a dichotomy implies the Feder-Vardi conjecture on the\\ncomplexity of constraint satisfaction problems. We also link these results to\\nDatalog-rewritability and study the decidability of whether a given ontology\\nenjoys PTime query evaluation, presenting both positive and negative results.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 4, 18, 19, 49, 15, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Heuristic and Cost-based Optimization for Diverse Provenance Tasks',\n",
       "  'authors': ['Xing Niu',\n",
       "   'Raghav Kapoor',\n",
       "   'Boris Glavic',\n",
       "   'Dieter Gawlick',\n",
       "   'Zhen Hua Liu',\n",
       "   'Vasudha Krishnaswamy',\n",
       "   'Venkatesh Radhakrishnan'],\n",
       "  'summary': 'A well-established technique for capturing database provenance as annotations\\non data is to instrument queries to propagate such annotations. However, even\\nsophisticated query optimizers often fail to produce efficient execution plans\\nfor instrumented queries. We develop provenance-aware optimization techniques\\nto address this problem. Specifically, we study algebraic equivalences targeted\\nat instrumented queries and alternative ways of instrumenting queries for\\nprovenance capture. Furthermore, we present an extensible heuristic and\\ncost-based optimization framework utilizing these optimizations. Our\\nexperiments confirm that these optimizations are highly effective, improving\\nperformance by several orders of magnitude for diverse provenance tasks.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 4, 17, 22, 35, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Benchmarking Top-K Keyword and Top-K Document Processing with T${}^2$K${}^2$ and T${}^2$K${}^2$D${}^2$',\n",
       "  'authors': ['Ciprian-Octavian Truica',\n",
       "   'Jérôme Darmont',\n",
       "   'Alexandru Boicea',\n",
       "   'Florin Radulescu'],\n",
       "  'summary': \"Top-k keyword and top-k document extraction are very popular text analysis\\ntechniques. Top-k keywords and documents are often computed on-the-fly, but\\nthey exploit weighted vocabularies that are costly to build. To compare\\ncompeting weighting schemes and database implementations, benchmarking is\\ncustomary. To the best of our knowledge, no benchmark currently addresses these\\nproblems. Hence, in this paper, we present T${}^2$K${}^2$, a top-k keywords and\\ndocuments benchmark, and its decision support-oriented evolution\\nT${}^2$K${}^2$D${}^2$. Both benchmarks feature a real tweet dataset and queries\\nwith various complexities and selectivities. They help evaluate weighting\\nschemes and database implementations in terms of computing performance. To\\nillustrate our bench-marks' relevance and genericity, we successfully ran\\nperformance tests on the TF-IDF and Okapi BM25 weighting schemes, on one hand,\\nand on different relational (Oracle, PostgreSQL) and document-oriented\\n(MongoDB) database implementations, on the other hand.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 4, 20, 10, 1, 43, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Specialty-Aware Task Assignment in Spatial Crowdsourcing',\n",
       "  'authors': ['Tianshu Song', 'Feng Zhu', 'Ke Xu'],\n",
       "  'summary': \"With the rapid development of Mobile Internet, spatial crowdsourcing is\\ngaining more and more attention from both academia and industry.\\n  In spatial crowdsourcing, spatial tasks are sent to workers based on their\\nlocations.\\n  A wide kind of tasks in spatial crowdsourcing are specialty-aware, which are\\ncomplex and need to be completed by workers with different skills\\ncollaboratively.\\n  Existing studies on specialty-aware spatial crowdsourcing assume that each\\nworker has a united charge when performing different tasks, no matter how many\\nskills of her/him are used to complete the task, which is not fair and\\npractical.\\n  In this paper, we study the problem of specialty-aware task assignment in\\nspatial crowdsourcing, where each worker has fine-grained charge for each of\\ntheir skills, and the goal is to maximize the total number of completed tasks\\nbased on tasks' budget and requirements on particular skills.\\n  The problem is proven to be NP-hard. Thus, we propose two efficient\\nheuristics to solve the problem.\\n  Experiments on both synthetic and real datasets demonstrate the effectiveness\\nand efficiency of our solutions.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 4, 20, 11, 11, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'In-Browser Split-Execution Support for Interactive Analytics in the Cloud',\n",
       "  'authors': ['Kareem El Gebaly', 'Jimmy Lin'],\n",
       "  'summary': 'The canonical analytics architecture today consists of a browser connected to\\na backend in the cloud. In all deployments that we are aware of, the browser is\\nsimply a dumb rendering endpoint. As an alternative, this paper explores\\nsplit-execution architectures that push analytics capabilities into the\\nbrowser. We show that, by taking advantage of typed arrays and asm.js, it is\\npossible to build an analytical RDBMS in JavaScript that runs in a browser,\\nachieving performance rivaling native databases. To support interactive data\\nexploration, our Afterburner prototype automatically generates local\\nmaterialized views from a backend database that are then shipped to the browser\\nto facilitate subsequent interactions seamlessly and efficiently. We compare\\nthis architecture to several alternative deployments, experimentally\\ndemonstrating performance parity, while at the same time providing additional\\nadvantages in terms of administrative and operational simplicity.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 4, 24, 3, 2, 54, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Processing Database Joins over a Shared-Nothing System of Multicore Machines',\n",
       "  'authors': ['Abhirup Chakraborty'],\n",
       "  'summary': 'To process a large volume of data, modern data management systems use a\\ncollection of machines connected through a network. This paper looks into the\\nfeasibility of scaling up such a shared-nothing system while processing a\\ncompute- and communication-intensive workload---processing distributed joins.\\nBy exploiting multiple processing cores within the individual machines, we\\nimplement a system to process database joins that parallelizes computation\\nwithin each node, pipelines the computation with communication, parallelizes\\nthe communication by allowing multiple simultaneous data transfers\\n(send/receive), and removes synchronization barriers (a scalability bottleneck\\nin a distributed data processing system). Our experimental results show that\\nusing only four threads per node the framework achieves a 3.5x gains in\\nintra-node performance while compared with a single-threaded counterpart.\\nMoreover, with the join processing workload the cluster-wide performance (and\\nspeedup) is observed to be dictated by the intra-node computational loads; this\\nproperty brings a near-linear speedup with increasing nodes in the system, a\\nfeature much desired in modern large-scale data processing system.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 4, 25, 2, 30, 43, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Event Forecasting with Pattern Markov Chains',\n",
       "  'authors': ['Elias Alevizos', 'Alexander Artikis', 'Georgios Paliouras'],\n",
       "  'summary': 'We present a system for online probabilistic event forecasting. We assume\\nthat a user is interested in detecting and forecasting event patterns, given in\\nthe form of regular expressions. Our system can consume streams of events and\\nforecast when the pattern is expected to be fully matched. As more events are\\nconsumed, the system revises its forecasts to reflect possible changes in the\\nstate of the pattern. The framework of Pattern Markov Chains is used in order\\nto learn a probabilistic model for the pattern, with which forecasts with\\nguaranteed precision may be produced, in the form of intervals within which a\\nfull match is expected. Experimental results from real-world datasets are shown\\nand the quality of the produced forecasts is explored, using both precision\\nscores and two other metrics: spread, which refers to the \"focusing resolution\"\\nof a forecast (interval length), and distance, which captures how early a\\nforecast is reported.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 4, 27, 8, 33, 12, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Relational to RDF Data Exchange in Presence of a Shape Expression Schema',\n",
       "  'authors': ['Iovka Boneva', 'Jose Lozano', 'Sławek Staworko'],\n",
       "  'summary': 'We study the relational to RDF data exchange problem, where the tar- get\\nconstraints are specified using Shape Expression schema (ShEx). We investi-\\ngate two fundamental problems: 1) consistency which is checking for a given\\ndata exchange setting whether there always exists a solution for any source\\ninstance, and 2) constructing a universal solution which is a solution that\\nrepresents the space of all solutions. We propose to use typed IRI constructors\\nin source-to- target tuple generating dependencies to create the IRIs of the\\nRDF graph from the values in the relational instance, and we translate ShEx\\ninto a set of target dependencies. We also identify data exchange settings that\\nare key covered, a property that is decidable and guarantees consistency.\\nFurthermore, we show that this property is a sufficient and necessary condition\\nfor the existence of universal solutions for a practical subclass of\\nweakly-recursive ShEx.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 4, 30, 5, 45, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'BlazeIt: Optimizing Declarative Aggregation and Limit Queries for Neural Network-Based Video Analytics',\n",
       "  'authors': ['Daniel Kang', 'Peter Bailis', 'Matei Zaharia'],\n",
       "  'summary': \"Recent advances in neural networks (NNs) have enabled automatic querying of\\nlarge volumes of video data with high accuracy. While these deep NNs can\\nproduce accurate annotations of an object's position and type in video, they\\nare computationally expensive and require complex, imperative deployment code\\nto answer queries. Prior work uses approximate filtering to reduce the cost of\\nvideo analytics, but does not handle two important classes of queries,\\naggregation and limit queries; moreover, these approaches still require complex\\ncode to deploy. To address the computational and usability challenges of\\nquerying video at scale, we introduce BlazeIt, a system that optimizes queries\\nof spatiotemporal information of objects in video. BlazeIt accepts queries via\\nFrameQL, a declarative extension of SQL for video analytics that enables\\nvideo-specific query optimization. We introduce two new query optimization\\ntechniques in BlazeIt that are not supported by prior work. First, we develop\\nmethods of using NNs as control variates to quickly answer approximate\\naggregation queries with error bounds. Second, we present a novel search\\nalgorithm for cardinality-limited video queries. Through these these\\noptimizations, BlazeIt can deliver up to 83x speedups over the recent\\nliterature on video processing.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 2, 22, 30, 22, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Scalable Semantic Querying of Text',\n",
       "  'authors': ['Xiaolan Wang',\n",
       "   'Aaron Feng',\n",
       "   'Behzad Golshan',\n",
       "   'Alon Halevy',\n",
       "   'George Mihaila',\n",
       "   'Hidekazu Oiwa',\n",
       "   'Wang-Chiew Tan'],\n",
       "  'summary': \"We present the KOKO system that takes declarative information extraction to a\\nnew level by incorporating advances in natural language processing techniques\\nin its extraction language. KOKO is novel in that its extraction language\\nsimultaneously supports conditions on the surface of the text and on the\\nstructure of the dependency parse tree of sentences, thereby allowing for more\\nrefined extractions. KOKO also supports conditions that are forgiving to\\nlinguistic variation of expressing concepts and allows to aggregate evidence\\nfrom the entire document in order to filter extractions.\\n  To scale up, KOKO exploits a multi-indexing scheme and heuristics for\\nefficient extractions. We extensively evaluate KOKO over publicly available\\ntext corpora. We show that KOKO indices take up the smallest amount of space,\\nare notably faster and more effective than a number of prior indexing schemes.\\nFinally, we demonstrate KOKO's scale up on a corpus of 5 million Wikipedia\\narticles.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 3, 1, 57, 31, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Simplified SPARQL REST API - CRUD on JSON Object Graphs via URI Paths',\n",
       "  'authors': ['Markus Schröder',\n",
       "   'Jörn Hees',\n",
       "   'Ansgar Bernardi',\n",
       "   'Daniel Ewert',\n",
       "   'Peter Klotz',\n",
       "   'Steffen Stadtmüller'],\n",
       "  'summary': 'Within the Semantic Web community, SPARQL is one of the predominant languages\\nto query and update RDF knowledge. However, the complexity of SPARQL, the\\nunderlying graph structure and various encodings are common sources of\\nconfusion for Semantic Web novices.\\n  In this paper we present a general purpose approach to convert any given\\nSPARQL endpoint into a simple to use REST API. To lower the initial hurdle, we\\nrepresent the underlying graph as an interlinked view of nested JSON objects\\nthat can be traversed by the API path.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 3, 9, 57, 13, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Efficient Top K Temporal Spatial Keyword Search',\n",
       "  'authors': ['Chengyuan Zhang',\n",
       "   'Lei Zhu',\n",
       "   'Weiren Yu',\n",
       "   'Jun Long',\n",
       "   'Fang Huang',\n",
       "   'Hongbo Zhao'],\n",
       "  'summary': 'Massive amount of data that are geo-tagged and associated with text\\ninformation are being generated at an unprecedented scale in many emerging\\napplications such as location based services and social networks. Due to their\\nimportance, a large body of work has focused on efficiently computing various\\nspatial keyword queries. In this paper,we study the top-$k$ temporal spatial\\nkeyword query which considers three important constraints during the search\\nincluding time, spatial proximity and textual relevance. A novel index\\nstructure, namely SSG-tree, to efficiently insert/delete spatio-temporal web\\nobjects with high rates. Base on SSG-tree an efficient algorithm is developed\\nto support top-k temporal spatial keyword query. We show via extensive\\nexperimentation with real spatial databases that our method has increased\\nperformance over alternate techniques',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 5, 5, 43, 49, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Computational Social Choice Meets Databases',\n",
       "  'authors': ['Benny Kimelfeld', 'Phokion G. Kolaitis', 'Julia Stoyanovich'],\n",
       "  'summary': 'We develop a novel framework that aims to create bridges between the\\ncomputational social choice and the database management communities. This\\nframework enriches the tasks currently supported in computational social choice\\nwith relational database context, thus making it possible to formulate\\nsophisticated queries about voting rules, candidates, voters, issues, and\\npositions. At the conceptual level, we give rigorous semantics to queries in\\nthis framework by introducing the notions of necessary answers and possible\\nanswers to queries. At the technical level, we embark on an investigation of\\nthe computational complexity of the necessary answers. We establish a number of\\nresults about the complexity of the necessary answers of conjunctive queries\\ninvolving positional scoring rules that contrast sharply with earlier results\\nabout the complexity of the necessary winners.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 10, 20, 5, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'HOC-Tree: A Novel Index for efficient Spatio-temporal Range Search',\n",
       "  'authors': ['Jun Long',\n",
       "   'Lei Zhu',\n",
       "   'Chengyuan Zhang',\n",
       "   'Shuangqiao Lin',\n",
       "   'Zhan Yang',\n",
       "   'Xinpan Yuan'],\n",
       "  'summary': 'With the rapid development of mobile computing and Web services, a huge\\namount of data with spatial and temporal information have been collected\\neveryday by smart mobile terminals, in which an object is described by its\\nspatial information and temporal information. Motivated by the significance of\\nspatio-temporal range search and the lack of efficient search algorithm, in\\nthis paper, we study the problem of spatio-temporal range search (STRS), a\\nnovel index structure is proposed, called HOC-Tree, which is based on Hilbert\\ncurve and OC-Tree, and takes both spatial and temporal information into\\nconsideration. Based on HOC-Tree, we develop an efficient algorithm to solve\\nthe problem of spatio-temporal range search. Comprehensive experiments on real\\nand synthetic data demonstrate that our method is more efficient than the\\nstate-of-the-art technique.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 12, 3, 8, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Incremental Database Design using UML-B and Event-B',\n",
       "  'authors': ['Ahmed Al-Brashdi', 'Michael Butler', 'Abdolbaghi Rezazadeh'],\n",
       "  'summary': 'Correct operation of many critical systems is dependent on the data\\nconsistency and integrity properties of underlying databases. Therefore, a\\nverifiable and rigorous database design process is highly desirable. This\\nresearch aims to investigate and deliver a comprehensive and practical approach\\nfor modelling databases in formal methods through layered refinements. The\\nmethodology is being guided by a number of case studies, using abstraction and\\nrefinement in UML-B and verification with the Rodin tool. UML-B is a graphical\\nrepresentation of the Event-B formalism and the Rodin tool supports\\nverification for Event-B and UML-B. Our method guides developers to model\\nrelational databases in UML-B through layered refinement and to specify the\\nnecessary constraints and operations on the database.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 15, 1, 18, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Matching Consecutive Subpatterns Over Streaming Time Series',\n",
       "  'authors': ['Rong Kang',\n",
       "   'Chen Wang',\n",
       "   'Peng Wang',\n",
       "   'Yuting Ding',\n",
       "   'Jianmin Wang'],\n",
       "  'summary': 'Pattern matching of streaming time series with lower latency under limited\\ncomputing resource comes to a critical problem, especially as the growth of\\nIndustry 4.0 and Industry Internet of Things. However, against traditional\\nsingle pattern matching model, a pattern may contain multiple subpatterns\\nrepresenting different physical meanings in the real world. Hence, we formulate\\na new problem, called \"consecutive subpatterns matching\", which allows users to\\nspecify a pattern containing several consecutive subpatterns with various\\nspecified thresholds. We propose a novel representation Equal-Length Block\\n(ELB) together with two efficient implementations, which work very well under\\nall Lp-Norms without false dismissals. Extensive experiments are performed on\\nsynthetic and real-world datasets to illustrate that our approach outperforms\\nthe brute-force method and MSM, a multi-step filter mechanism over the\\nmulti-scaled representation by orders of magnitude.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 17, 13, 34, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A hybrid index model for efficient spatio-temporal search in HBase',\n",
       "  'authors': ['Chengyuan Zhangy',\n",
       "   'Lei Zhuy',\n",
       "   'Jun Longy',\n",
       "   'Shuangqiao Liny',\n",
       "   'Zhan Yangy',\n",
       "   'Wenti Huang'],\n",
       "  'summary': 'With advances in geo-positioning technologies and geo-location services,\\nthere are a rapidly growing massive amount of spatio-temporal data collected in\\nmany applications such as location-aware devices and wireless communication, in\\nwhich an object is described by its spatial location and its timestamp.\\nConsequently, the study of spatio-temporal search which explores both\\ngeo-location information and temporal information of the data has attracted\\nsignificant concern from research organizations and commercial communities.\\nThis work study the problem of spatio-temporal \\\\emph{k}-nearest neighbors\\nsearch (ST$k$NNS), which is fundamental in the spatial temporal queries. Based\\non HBase, a novel index structure is proposed, called \\\\textbf{H}ybrid\\n\\\\textbf{S}patio-\\\\textbf{T}emporal HBase \\\\textbf{I}ndex (\\\\textbf{HSTI} for\\nshort), which is carefully designed and takes both spatial and temporal\\ninformation into consideration to effectively reduce the search space. Based on\\nHSTI, an efficient algorithm is developed to deal with spatio-temporal\\n\\\\emph{k}-nearest neighbors search. Comprehensive experiments on real and\\nsynthetic data clearly show that HSTI is three to five times faster than the\\nstate-of-the-art technique.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 19, 14, 26, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Cancer Research UK Drug Discovery Process Mining',\n",
       "  'authors': ['Haochao Huang'],\n",
       "  'summary': 'Background. The Drug Discovery Unit (DDU) of Cancer Research UK (CRUK) is\\nusing the software Dotmatics for storage and analysis of scientific data during\\ndrug discovery process. Whilst the data include event logs, time stamps,\\nactivities, and user information are mostly sitting in the database without\\nfully utilising their potential value. Aims. This dissertation aims at\\nextracting knowledge from event logs data which recorded during drug discovery\\nprocess, to capture the operational business process of the DDU of Cancer\\nResearch UK (CRUK) as it was being executed. It provides the evaluations and\\nmethodologies of drawing the process mining panoramic models for the drug\\ndiscovery process. Thus by enabling the DDU to maximise its efficiency in\\nreviewing its resources and works allocations, patients will benefit from more\\nnew treatments faster. Conclusion. Management of organisations can be benefit\\nfrom the process mining methodologies. Disco is excellent for non-experts on\\nmanagement purposes. ProM is great for expert on research purposes. However,\\nthe process mining is not once and for all but is a regular operation\\nmanagement process. Indeed, event logs needs to be understand more on the\\ntarget organisational behaviours and organisational business process. The\\nresearchers have to be aware that event logs data are the most important and\\npriority elements in process mining.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 18, 17, 53, 17, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'MonetDBLite: An Embedded Analytical Database',\n",
       "  'authors': ['Mark Raasveldt', 'Hannes Mühleisen'],\n",
       "  'summary': 'While traditional RDBMSes offer a lot of advantages, they require significant\\neffort to setup and to use. Because of these challenges, many data scientists\\nand analysts have switched to using alternative data management solutions.\\nThese alternatives, however, lack features that are standard for RDBMSes, e.g.\\nout-of-core query execution. In this paper, we introduce the embedded\\nanalytical database MonetDBLite. MonetDBLite is designed to be both highly\\nefficient and easy to use in conjunction with standard analytical tools. It can\\nbe installed using standard package managers, and requires no configuration or\\nserver management. It is designed for OLAP scenarios, and offers\\nnear-instantaneous data transfer between the database and analytical tools, all\\nthe while maintaining the transactional guarantees and ACID properties of a\\nstandard relational system. These properties make MonetDBLite highly suitable\\nas a storage engine for data used in analytics, machine learning and\\nclassification tasks.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 22, 11, 50, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Cache-based Multi-query Optimization for Data-intensive Scalable Computing Frameworks',\n",
       "  'authors': ['Pietro Michiardi', 'Damiano Carra', 'Sara Migliorini'],\n",
       "  'summary': 'In modern large-scale distributed systems, analytics jobs submitted by\\nvarious users often share similar work, for example scanning and processing the\\nsame subset of data. Instead of optimizing jobs independently, which may result\\nin redundant and wasteful processing, multi-query optimization techniques can\\nbe employed to save a considerable amount of cluster resources. In this work,\\nwe introduce a novel method combining in-memory cache primitives and\\nmulti-query optimization, to improve the efficiency of data-intensive, scalable\\ncomputing frameworks. By careful selection and exploitation of common\\n(sub)expressions, while satisfying memory constraints, our method transforms a\\nbatch of queries into a new, more efficient one which avoids unnecessary\\nrecomputations. To find feasible and efficient execution plans, our method uses\\na cost-based optimization formulation akin to the multiple-choice knapsack\\nproblem. Extensive experiments on a prototype implementation of our system show\\nsignificant benefits of worksharing for both TPC-DS workloads and detailed\\nmicro-benchmarks.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 22, 14, 59, 2, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Model-based Pricing for Machine Learning in a Data Marketplace',\n",
       "  'authors': ['Lingjiao Chen', 'Paraschos Koutris', 'Arun Kumar'],\n",
       "  'summary': \"Data analytics using machine learning (ML) has become ubiquitous in science,\\nbusiness intelligence, journalism and many other domains. While a lot of work\\nfocuses on reducing the training cost, inference runtime and storage cost of ML\\nmodels, little work studies how to reduce the cost of data acquisition, which\\npotentially leads to a loss of sellers' revenue and buyers' affordability and\\nefficiency.\\n  In this paper, we propose a model-based pricing (MBP) framework, which\\ninstead of pricing the data, directly prices ML model instances. We first\\nformally describe the desired properties of the MBP framework, with a focus on\\navoiding arbitrage. Next, we show a concrete realization of the MBP framework\\nvia a noise injection approach, which provably satisfies the desired formal\\nproperties. Based on the proposed framework, we then provide algorithmic\\nsolutions on how the seller can assign prices to models under different market\\nscenarios (such as to maximize revenue). Finally, we conduct extensive\\nexperiments, which validate that the MBP framework can provide high revenue to\\nthe seller, high affordability to the buyer, and also operate on low runtime\\ncost.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 26, 6, 2, 40, tzinfo=datetime.timezone.utc)},\n",
       " {'title': \"You Say 'What', I Hear 'Where' and 'Why': (Mis-)Interpreting SQL to Derive Fine-Grained Provenance\",\n",
       "  'authors': ['Tobias Müller', 'Benjamin Dietrich', 'Torsten Grust'],\n",
       "  'summary': 'SQL declaratively specifies what the desired output of a query is. This work\\nshows that a non-standard interpretation of the SQL semantics can, instead,\\ndisclose where a piece of the output originated in the input and why that piece\\nfound its way into the result. We derive such data provenance for very rich SQL\\ndialects (including recursion, windowed aggregates, and user-defined functions)\\nat the fine-grained level of individual table cells. The approach is\\nnon-invasive and implemented as a compositional source-level SQL rewrite: an\\ninput SQL query is transformed into its own interpreter that wields data\\ndependencies instead of regular values. We deliberately design this\\ntransformation to preserve the shape of both data and query, which allows\\nprovenance derivation to scale to complex queries without overwhelming the\\nunderlying database system.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 29, 14, 50, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Building your Cross-Platform Application with RHEEM',\n",
       "  'authors': ['Sanjay Chawla',\n",
       "   'Bertty Contreras-Rojas',\n",
       "   'Zoi Kaoudi',\n",
       "   'Sebastian Kruse',\n",
       "   'Jorge-Arnulfo Quiané-Ruiz'],\n",
       "  'summary': 'Today, organizations typically perform tedious and costly tasks to juggle\\ntheir code and data across different data processing platforms. Addressing this\\npain and achieving automatic cross-platform data processing is quite\\nchallenging because it requires quite good expertise for all the available data\\nprocessing platforms. In this report, we present Rheem, a general-purpose\\ncross-platform data processing system that alleviates users from the pain of\\nfinding the most efficient data processing platform for a given task. It also\\nsplits a task into subtasks and assigns each subtask to a specific platform to\\nminimize the overall cost (e.g., runtime or monetary cost). To offer\\ncross-platform functionality, it features (i) a robust interface to easily\\ncompose data analytic tasks; (ii) a novel cost-based optimizer able to find the\\nmost efficient platform in almost all cases; and (iii) an executor to\\nefficiently orchestrate tasks over different platforms. As a result, it allows\\nusers to focus on the business logic of their applications rather than on the\\nmechanics of how to compose and execute them. Rheem is released under an open\\nsource license.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 29, 21, 46, 50, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Sapphire: Querying RDF Data Made Simple',\n",
       "  'authors': ['Ahmed El-Roby',\n",
       "   'Khaled Ammar',\n",
       "   'Ashraf Aboulnaga',\n",
       "   'Jimmy Lin'],\n",
       "  'summary': 'RDF data in the linked open data (LOD) cloud is very valuable for many\\ndifferent applications. In order to unlock the full value of this data, users\\nshould be able to issue complex queries on the RDF datasets in the LOD cloud.\\nSPARQL can express such complex queries, but constructing SPARQL queries can be\\na challenge to users since it requires knowing the structure and vocabulary of\\nthe datasets being queried. In this paper, we introduce Sapphire, a tool that\\nhelps users write syntactically and semantically correct SPARQL queries without\\nprior knowledge of the queried datasets. Sapphire interactively helps the user\\nwhile typing the query by providing auto-complete suggestions based on the\\nqueried data. After a query is issued, Sapphire provides suggestions on ways to\\nchange the query to better match the needs of the user. We evaluated Sapphire\\nbased on performance experiments and a user study and showed it to be superior\\nto competing approaches.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 29, 22, 22, 21, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Q-Graph: Preserving Query Locality in Multi-Query Graph Processing',\n",
       "  'authors': ['Christian Mayer',\n",
       "   'Ruben Mayer',\n",
       "   'Jonas Grunert',\n",
       "   'Kurt Rothermel',\n",
       "   'Muhammad Adnan Tariq'],\n",
       "  'summary': 'Arising user-centric graph applications such as route planning and\\npersonalized social network analysis have initiated a shift of paradigms in\\nmodern graph processing systems towards multi-query analysis, i.e., processing\\nmultiple graph queries in parallel on a shared graph. These applications\\ngenerate a dynamic number of localized queries around query hotspots such as\\npopular urban areas. However, existing graph processing systems are not yet\\ntailored towards these properties: The employed methods for graph partitioning\\nand synchronization management disregard query locality and dynamism which\\nleads to high query latency. To this end, we propose the system Q-Graph for\\nmulti-query graph analysis that considers query locality on three levels. (i)\\nThe query-aware graph partitioning algorithm Q-cut maximizes query locality to\\nreduce communication overhead. (ii) The method for synchronization management,\\ncalled hybrid barrier synchronization, allows for full exploitation of local\\nqueries spanning only a subset of partitions. (iii) Both methods adapt at\\nruntime to changing query workloads in order to maintain and exploit locality.\\nOur experiments show that Q-cut reduces average query latency by up to 57\\npercent compared to static query-agnostic partitioning algorithms.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 30, 11, 14, 30, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'PIQUE: Progressive Integrated QUery Operator with Pay-As-You-Go Enrichment',\n",
       "  'authors': ['Dhrubajyoti Ghosh',\n",
       "   'Roberto Yus',\n",
       "   'Yasser Altowim',\n",
       "   'Sharad Mehrotra'],\n",
       "  'summary': 'Big data today in the form of text, images, video, and sensor data needs to\\nbe enriched (i.e., annotated with tags) prior to be effectively queried or\\nanalyzed. Data enrichment (that, depending upon the application could be\\ncompiled code, declarative queries, or expensive machine learning and/or signal\\nprocessing techniques) often cannot be performed in its entirety as a\\npre-processing step at the time of data ingestion. Enriching data as a separate\\noffline step after ingestion makes it unavailable for analysis during the\\nperiod between the ingestion and enrichment. To bridge such a gap, this paper\\nexplores a novel approach that supports progressive data enrichment during\\nquery processing in order to support interactive exploratory analysis. Our\\napproach is based on integrating an operator, entitled PIQUE, to support a\\nprioritized execution of the enrichment functions during query processing.\\nQuery processing with the PIQUE operator significantly outperforms the\\nbaselines in terms of rate at which answer quality improves during query\\nprocessing.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 30, 15, 22, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Improving Machine-based Entity Resolution with Limited Human Effort: A Risk Perspective',\n",
       "  'authors': ['Zhaoqiang Chen',\n",
       "   'Qun Chen',\n",
       "   'Boyi Hou',\n",
       "   'Murtadha Ahmed',\n",
       "   'Zhanhuai Li'],\n",
       "  'summary': 'Pure machine-based solutions usually struggle in the challenging\\nclassification tasks such as entity resolution (ER). To alleviate this problem,\\na recent trend is to involve the human in the resolution process, most notably\\nthe crowdsourcing approach. However, it remains very challenging to effectively\\nimprove machine-based entity resolution with limited human effort. In this\\npaper, we investigate the problem of human and machine cooperation for ER from\\na risk perspective. We propose to select the machine-labeled instances at high\\nrisk of being mislabeled for manual verification. For this task, we present a\\nrisk model that takes into consideration the human-labeled instances as well as\\nthe output of machine resolution. Finally, we evaluate the performance of the\\nproposed risk model on real data. Our experiments demonstrate that it can pick\\nup the mislabeled instances with considerably higher accuracy than the existing\\nalternatives. Provided with the same amount of human cost budget, it can also\\nachieve better resolution quality than the state-of-the-art approach based on\\nactive learning.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 5, 31, 14, 54, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Eliminating Boundaries in Cloud Storage with Anna',\n",
       "  'authors': ['Chenggang Wu', 'Vikram Sreekanti', 'Joseph M. Hellerstein'],\n",
       "  'summary': \"In this paper, we describe how we extended a distributed key-value store\\ncalled Anna into an elastic, multi-tier service for the cloud. In its extended\\nform, Anna is designed to overcome the narrow cost-performance limitations\\ntypical of current cloud storage systems. We describe three key aspects of\\nAnna's new design: multi-master selective replication of hot keys, a vertical\\ntiering of storage layers with different cost-performance tradeoffs, and\\nhorizontal elasticity of each tier to add and remove nodes in response to load\\ndynamics. Anna's policy engine uses these mechanisms to balance service-level\\nobjectives around cost, latency and fault tolerance. Experimental results\\nexplore the behavior of Anna's mechanisms and policy, exhibiting orders of\\nmagnitude efficiency improvements over both commodity cloud KVS services and\\nresearch systems.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 9, 1, 0, 20, 5, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Query Log Compression for Workload Analytics',\n",
       "  'authors': ['Ting Xie', 'Oliver Kennedy', 'Varun Chandola'],\n",
       "  'summary': 'Analyzing database access logs is a key part of performance tuning, intrusion\\ndetection, benchmark development, and many other database administration tasks.\\nUnfortunately, it is common for production databases to deal with millions or\\neven more queries each day, so these logs must be summarized before they can be\\nused. Designing an appropriate summary encoding requires trading off between\\nconciseness and information content. For example: simple workload sampling may\\nmiss rare, but high impact queries. In this paper, we present LogR, a lossy log\\ncompression scheme suitable use for many automated log analytics tools, as well\\nas for human inspection. We formalize and analyze the space/fidelity trade-off\\nin the context of a broader family of \"pattern\" and \"pattern mixture\" log\\nencodings to which LogR belongs. We show through a series of experiments that\\nLogR compressed encodings can be created efficiently, come with provable\\ninformation-theoretic bounds on their accuracy, and outperform state-of-art log\\nsummarization strategies.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 9, 2, 22, 41, 32, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Typed Linear Algebra for Efficient Analytical Querying',\n",
       "  'authors': ['João M. Afonso',\n",
       "   'Gabriel D. Fernandes',\n",
       "   'João P. Fernandes',\n",
       "   'Filipe Oliveira',\n",
       "   'Bruno M. Ribeiro',\n",
       "   'Rogério Pontes',\n",
       "   'José N. Oliveira',\n",
       "   'Alberto J. Proença'],\n",
       "  'summary': 'This paper uses typed linear algebra (LA) to represent data and perform\\nanalytical querying in a single, unified framework. The typed approach offers\\nstrong type checking (as in modern programming languages) and a diagrammatic\\nway of expressing queries (paths in LA diagrams). A kernel of LA operators has\\nbeen implemented so that paths extracted from LA diagrams can be executed. The\\napproach is validated and evaluated taking TPC-H benchmark queries as\\nreference. The performance of the LA-based approach is compared with popular\\ndatabase competitors (PostgreSQL and MySQL).',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 9, 3, 16, 19, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Ranking RDF Instances in Degree-decoupled RDF Graphs',\n",
       "  'authors': ['Elisa S. Menendez',\n",
       "   'Marco A. Casanova',\n",
       "   'Mohand Boughanem',\n",
       "   'Luiz André P. Paes Leme'],\n",
       "  'summary': 'In the last decade, RDF emerged as a new kind of standardized data model, and\\na sizable body of knowledge from fields such as Information Retrieval was\\nadapted to RDF graphs. One common task in graph databases is to define an\\nimportance score for nodes based on centrality measures, such as PageRank and\\nHITS. The majority of the strategies highly depend on the degree of the node.\\nHowever, in some RDF graphs, called degree-decoupled RDF graphs, the notion of\\nimportance is not directly related to the node degree. Therefore, this work\\nfirst proposes three novel node importance measures, named InfoRank I, II and\\nIII, for degree-decoupled RDF graphs. It then compares the proposed measures\\nwith traditional PageRank and other familiar centrality measures, using with an\\nIMDb dataset.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 9, 5, 17, 2, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Hierarchical Characteristic Set Merging for Optimizing SPARQL Queries in Heterogeneous RDF',\n",
       "  'authors': ['Marios Meimaris', 'George Papastefanatos'],\n",
       "  'summary': 'Characteristic sets (CS) organize RDF triples based on the set of properties\\ncharacterizing their subject nodes. This concept is recently used in indexing\\ntechniques, as it can capture the implicit schema of RDF data. While most\\nCS-based approaches yield significant improvements in space and query\\nperformance, they fail to perform well in the presence of schema heterogeneity,\\ni.e., when the number of CSs becomes very large, resulting in a highly\\npartitioned data organization. In this paper, we address this problem by\\nintroducing a novel technique, for merging CSs based on their hierarchical\\nstructure. Our technique employs a lattice to capture the hierarchical\\nrelationships between CSs, identifies dense CSs and merges dense CSs with their\\nancestors, thus reducing the size of the CSs as well as the links between them.\\nWe implemented our algorithm on top of a relational backbone, where each merged\\nCS is stored in a relational table, and we performed an extensive experimental\\nstudy to evaluate the performance and impact of merging to the storage and\\nquerying of RDF datasets, indicating significant improvements.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 9, 7, 8, 21, 1, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Pushing the Limits of Encrypted Databases with Secure Hardware',\n",
       "  'authors': ['Panagiotis Antonopoulos',\n",
       "   'Arvind Arasu',\n",
       "   'Ken Eguro',\n",
       "   'Joachim Hammer',\n",
       "   'Raghav Kaushik',\n",
       "   'Donald Kossmann',\n",
       "   'Ravi Ramamurthy',\n",
       "   'Jakub Szymaszek'],\n",
       "  'summary': 'Encrypted databases have been studied for more than 10 years and are quickly\\nemerging as a critical technology for the cloud. The current state of the art\\nis to use property-preserving encrypting techniques (e.g., deterministic\\nencryption) to protect the confidentiality of the data and support query\\nprocessing at the same time. Unfortunately, these techniques have many\\nlimitations. Recently, trusted computing platforms (e.g., Intel SGX) have\\nemerged as an alternative to implement encrypted databases. This paper\\ndemonstrates some vulnerabilities and the limitations of this technology, but\\nit also shows how to make best use of it in order to improve on\\nconfidentiality, functionality, and performance.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 9, 7, 18, 31, 52, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Reducing Uncertainty of Schema Matching via Crowdsourcing with Accuracy Rates',\n",
       "  'authors': ['Chen Jason Zhang',\n",
       "   'Lei Chen',\n",
       "   'H. V. Jagadish',\n",
       "   'Mengchen Zhang',\n",
       "   'Yongxin Tong'],\n",
       "  'summary': \"Schema matching is a central challenge for data integration systems. Inspired\\nby the popularity and the success of crowdsourcing platforms, we explore the\\nuse of crowdsourcing to reduce the uncertainty of schema matching. Since\\ncrowdsourcing platforms are most effective for simple questions, we assume that\\neach Correspondence Correctness Question (CCQ) asks the crowd to decide whether\\na given correspondence should exist in the correct matching. Furthermore,\\nmembers of a crowd may sometimes return incorrect answers with different\\nprobabilities. Accuracy rates of individual crowd workers are probabilities of\\nreturning correct answers which can be attributes of CCQs as well as\\nevaluations of individual workers. We prove that uncertainty reduction equals\\nto entropy of answers minus entropy of crowds and show how to obtain lower and\\nupper bounds for it. We propose frameworks and efficient algorithms to\\ndynamically manage the CCQs to maximize the uncertainty reduction within a\\nlimited budget of questions. We develop two novel approaches, namely `Single\\nCCQ' and `Multiple CCQ', which adaptively select, publish and manage questions.\\nWe verify the value of our solutions with simulation and real implementation.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 9, 11, 16, 42, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An Approach to Handle Big Data Warehouse Evolution',\n",
       "  'authors': ['Darja Solodovnikova', 'Laila Niedrite'],\n",
       "  'summary': 'One of the purposes of Big Data systems is to support analysis of data\\ngathered from heterogeneous data sources. Since data warehouses have been used\\nfor several decades to achieve the same goal, they could be leveraged also to\\nprovide analysis of data stored in Big Data systems. The problem of adapting\\ndata warehouse data and schemata to changes in these requirements as well as\\ndata sources has been studied by many researchers worldwide. However,\\ninnovative methods must be developed also to support evolution of data\\nwarehouses that are used to analyze data stored in Big Data systems. In this\\npaper, we propose a data warehouse architecture that allows to perform\\ndifferent kinds of analytical tasks, including OLAP-like analysis, on big data\\nloaded from multiple heterogeneous data sources with different latency and is\\ncapable of processing changes in data sources as well as evolving analysis\\nrequirements. The operation of the architecture is highly based on the metadata\\nthat are outlined in the paper.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 9, 12, 7, 32, 29, tzinfo=datetime.timezone.utc)},\n",
       " {'title': \"HDTCat: let's make HDT scale\",\n",
       "  'authors': ['Dennis Diefenbach', 'Josée M. Giménez-García'],\n",
       "  'summary': 'HDT (Header, Dictionary, Triples) is a serialization for RDF. HDT has become\\nvery popular in the last years because it allows to store RDF data with a small\\ndisk footprint, while remaining at the same time queriable. For this reason HDT\\nis often used when scalability becomes an issue. Once RDF data is serialized\\ninto HDT, the disk footprint to store it and the memory footprint to query it\\nare very low. However, generating HDT files from raw text RDF serializations\\n(like N-Triples) is a time-consuming and (especially) memory-consuming task. In\\nthis publication we present HDTCat, an algorithm and command line tool to join\\ntwo HDT files with low memory footprint. HDTCat can be used in a\\ndivide-and-conquer strategy to generate HDT files from huge datasets using a\\nlow-memory footprint.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 9, 18, 15, 10, 52, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards a Hands-Free Query Optimizer through Deep Learning',\n",
       "  'authors': ['Ryan Marcus', 'Olga Papaemmanouil'],\n",
       "  'summary': 'Query optimization remains one of the most important and well-studied\\nproblems in database systems. However, traditional query optimizers are complex\\nheuristically-driven systems, requiring large amounts of time to tune for a\\nparticular database and requiring even more time to develop and maintain in the\\nfirst place. In this vision paper, we argue that a new type of query optimizer,\\nbased on deep reinforcement learning, can drastically improve on the\\nstate-of-the-art. We identify potential complications for future research that\\nintegrates deep learning with query optimization, and we describe three novel\\ndeep learning based approaches that can lead the way to end-to-end\\nlearning-based query optimizers.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 9, 26, 19, 51, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Reuse and Adaptation for Entity Resolution through Transfer Learning',\n",
       "  'authors': ['Saravanan Thirumuruganathan',\n",
       "   'Shameem A Puthiya Parambath',\n",
       "   'Mourad Ouzzani',\n",
       "   'Nan Tang',\n",
       "   'Shafiq Joty'],\n",
       "  'summary': 'Entity resolution (ER) is one of the fundamental problems in data\\nintegration, where machine learning (ML) based classifiers often provide the\\nstate-of-the-art results. Considerable human effort goes into feature\\nengineering and training data creation. In this paper, we investigate a new\\nproblem: Given a dataset D_T for ER with limited or no training data, is it\\npossible to train a good ML classifier on D_T by reusing and adapting the\\ntraining data of dataset D_S from same or related domain? Our major\\ncontributions include (1) a distributed representation based approach to encode\\neach tuple from diverse datasets into a standard feature space; (2)\\nidentification of common scenarios where the reuse of training data can be\\nbeneficial; and (3) five algorithms for handling each of the aforementioned\\nscenarios. We have performed comprehensive experiments on 12 datasets from 5\\ndifferent domains (publications, movies, songs, restaurants, and books). Our\\nexperiments show that our algorithms provide significant benefits such as\\nproviding superior performance for a fixed training data size.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2018, 9, 28, 15, 26, 17, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Uncertainty Annotated Databases - A Lightweight Approach for Approximating Certain Answers (extended version)',\n",
       "  'authors': ['Su Feng', 'Aaron Huber', 'Boris Glavic', 'Oliver Kennedy'],\n",
       "  'summary': 'Certain answers are a principled method for coping with uncertainty that\\narises in many practical data management tasks. Unfortunately, this method is\\nexpensive and may exclude useful (if uncertain) answers. Thus, users frequently\\nresort to less principled approaches to resolve the uncertainty. In this paper,\\nwe propose Uncertainty Annotated Databases (UA-DBs), which combine an under-\\nand over-approximation of certain answers to achieve the reliability of certain\\nanswers, with the performance of a classical database system. Furthermore, in\\ncontrast to prior work on certain answers, UA-DBs achieve a higher utility by\\nincluding some (explicitly marked) answers that are not certain. UA-DBs are\\nbased on incomplete K-relations, which we introduce to generalize the classical\\nset-based notions of incomplete databases and certain answers to a much larger\\nclass of data models. Using an implementation of our approach, we demonstrate\\nexperimentally that it efficiently produces tight approximations of certain\\nanswers that are of high utility.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 3, 30, 15, 23, 5, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Boundedness of Conjunctive Regular Path Queries',\n",
       "  'authors': ['Pablo Barceló', 'Diego Figueira', 'Miguel Romero'],\n",
       "  'summary': 'We study the boundedness problem for unions of conjunctive regular path\\nqueries with inverses (UC2RPQs). This is the problem of, given a UC2RPQ,\\nchecking whether it is equivalent to a union of conjunctive queries (UCQ). We\\nshow the problem to be ExpSpace-complete, thus coinciding with the complexity\\nof containment for UC2RPQs. As a corollary, when a UC2RPQ is bounded, it is\\nequivalent to a UCQ of at most triple-exponential size, and in fact we show\\nthat this bound is optimal. We also study better behaved classes of UC2RPQs,\\nnamely acyclic UC2RPQs of bounded thickness, and strongly connected UCRPQs,\\nwhose boundedness problem are, respectively, PSpace-complete and\\n$\\\\Pi^p_2$-complete. Most upper bounds exploit results on limitedness for\\ndistance automata, in particular extending the model with alternation and\\ntwo-wayness, which may be of independent interest.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 1, 13, 47, 58, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Learning a Partitioning Advisor with Deep Reinforcement Learning',\n",
       "  'authors': ['Benjamin Hilprecht', 'Carsten Binnig', 'Uwe Roehm'],\n",
       "  'summary': 'Commercial data analytics products such as Microsoft Azure SQL Data Warehouse\\nor Amazon Redshift provide ready-to-use scale-out database solutions for\\nOLAP-style workloads in the cloud. While the provisioning of a database cluster\\nis usually fully automated by cloud providers, customers typically still have\\nto make important design decisions which were traditionally made by the\\ndatabase administrator such as selecting the partitioning schemes.\\n  In this paper we introduce a learned partitioning advisor for analytical\\nOLAP-style workloads based on Deep Reinforcement Learning (DRL). The main idea\\nis that a DRL agent learns its decisions based on experience by monitoring the\\nrewards for different workloads and partitioning schemes. We evaluate our\\nlearned partitioning advisor in an experimental evaluation with different\\ndatabases schemata and workloads of varying complexity. In the evaluation, we\\nshow that our advisor is not only able to find partitionings that outperform\\nexisting approaches for automated partitioning design but that it also can\\neasily adjust to different deployments. This is especially important in cloud\\nsetups where customers can easily migrate their cluster to a new set of\\n(virtual) machines.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 2, 8, 29, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Identifying Patient Groups based on Frequent Patterns of Patient Samples',\n",
       "  'authors': ['Seyed Amin Tabatabaei',\n",
       "   'Xixi Lu',\n",
       "   'Mark Hoogendoorn',\n",
       "   'Hajo A. Reijers'],\n",
       "  'summary': 'Grouping patients meaningfully can give insights about the different types of\\npatients, their needs, and the priorities. Finding groups that are meaningful\\nis however very challenging as background knowledge is often required to\\ndetermine what a useful grouping is. In this paper we propose an approach that\\nis able to find groups of patients based on a small sample of positive examples\\ngiven by a domain expert. Because of that, the approach relies on very limited\\nefforts by the domain experts. The approach groups based on the activities and\\ndiagnostic/billing codes within health pathways of patients. To define such a\\ngrouping based on the sample of patients efficiently, frequent patterns of\\nactivities are discovered and used to measure the similarity between the care\\npathways of other patients to the patients in the sample group. This approach\\nresults in an insightful definition of the group. The proposed approach is\\nevaluated using several datasets obtained from a large university medical\\ncenter. The evaluation shows F1-scores of around 0.7 for grouping kidney injury\\nand around 0.6 for diabetes.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 3, 9, 10, 29, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'HoloDetect: Few-Shot Learning for Error Detection',\n",
       "  'authors': ['Alireza Heidari',\n",
       "   'Joshua McGrath',\n",
       "   'Ihab F. Ilyas',\n",
       "   'Theodoros Rekatsinas'],\n",
       "  'summary': 'We introduce a few-shot learning framework for error detection. We show that\\ndata augmentation (a form of weak supervision) is key to training high-quality,\\nML-based error detection models that require minimal human involvement. Our\\nframework consists of two parts: (1) an expressive model to learn rich\\nrepresentations that capture the inherent syntactic and semantic heterogeneity\\nof errors; and (2) a data augmentation model that, given a small seed of clean\\nrecords, uses dataset-specific transformations to automatically generate\\nadditional training data. Our key insight is to learn data augmentation\\npolicies from the noisy input dataset in a weakly supervised manner. We show\\nthat our framework detects errors with an average precision of ~94% and an\\naverage recall of ~93% across a diverse array of datasets that exhibit\\ndifferent types and amounts of errors. We compare our approach to a\\ncomprehensive collection of error detection methods, ranging from traditional\\nrule-based methods to ensemble-based and active learning approaches. We show\\nthat data augmentation yields an average improvement of 20 F1 points while it\\nrequires access to 3x fewer labeled examples compared to other ML approaches.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 4, 0, 38, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Mining Precision Interfaces From Query Logs',\n",
       "  'authors': ['Qianrui Zhang', 'Haoci Zhang', 'Thibault Sellam', 'Eugene Wu'],\n",
       "  'summary': \"Interactive tools make data analysis more efficient and more accessible to\\nend-users by hiding the underlying query complexity and exposing interactive\\nwidgets for the parts of the query that matter to the analysis. However,\\ncreating custom tailored (i.e., precise) interfaces is very costly, and\\nautomated approaches are desirable. We propose a syntactic approach that uses\\nqueries from an analysis to generate a tailored interface. We model interface\\nwidgets as functions I(q) -> q' that modify the current analysis query $q$, and\\ninterfaces as the set of queries that its widgets can express. Our system,\\nPrecision Interfaces, analyzes structural changes between input queries from an\\nanalysis, and generates an output interface with widgets to express those\\nchanges. Our experiments on the Sloan Digital Sky Survey query log suggest that\\nPrecision Interfaces can generate useful interfaces for simple unanticipated\\ntasks, and our optimizations can generate interfaces from logs of up to 10,000\\nqueries in <10s.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 4, 4, 40, 38, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Safe Disassociation of Set-Valued Datasets',\n",
       "  'authors': ['Nancy Awad',\n",
       "   'Bechara Al Bouna',\n",
       "   'Jean-Francois Couchot',\n",
       "   'Laurent Philippe'],\n",
       "  'summary': 'Disassociation introduced by Terrovitis et al. is a bucketization based\\nanonimyzation technique that divides a set-valued dataset into several clusters\\nto hide the link between individuals and their complete set of items. It\\nincreases the utility of the anonymized dataset, but on the other side, it\\nraises many privacy concerns, one in particular, is when the items are tightly\\ncoupled to form what is called, a cover problem. In this paper, we present safe\\ndisassociation, a technique that relies on partial-suppression, to overcome the\\naforementioned privacy breach encountered when disassociating set-valued\\ndatasets. Safe disassociation allows the $k^m$-anonymity privacy constraint to\\nbe extended to a bucketized dataset and copes with the cover problem. We\\ndescribe our algorithm that achieves the safe disassociation and we provide a\\nset of experiments to demonstrate its efficiency.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 5, 15, 11, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Neo: A Learned Query Optimizer',\n",
       "  'authors': ['Ryan Marcus',\n",
       "   'Parimarjan Negi',\n",
       "   'Hongzi Mao',\n",
       "   'Chi Zhang',\n",
       "   'Mohammad Alizadeh',\n",
       "   'Tim Kraska',\n",
       "   'Olga Papaemmanouil',\n",
       "   'Nesime Tatbul'],\n",
       "  'summary': 'Query optimization is one of the most challenging problems in database\\nsystems. Despite the progress made over the past decades, query optimizers\\nremain extremely complex components that require a great deal of hand-tuning\\nfor specific workloads and datasets. Motivated by this shortcoming and inspired\\nby recent advances in applying machine learning to data management challenges,\\nwe introduce Neo (Neural Optimizer), a novel learning-based query optimizer\\nthat relies on deep neural networks to generate query executions plans. Neo\\nbootstraps its query optimization model from existing optimizers and continues\\nto learn from incoming queries, building upon its successes and learning from\\nits failures. Furthermore, Neo naturally adapts to underlying data patterns and\\nis robust to estimation errors. Experimental results demonstrate that Neo, even\\nwhen bootstrapped from a simple optimizer like PostgreSQL, can learn a model\\nthat offers similar performance to state-of-the-art commercial optimizers, and\\nin some cases even surpass them.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 7, 19, 0, 45, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Modeling Corruption in Eventually-Consistent Graph Databases',\n",
       "  'authors': ['Jim Webber', 'Paul Ezhilchelvan', 'Isi Mitrani'],\n",
       "  'summary': 'We present a model and analysis of an eventually consistent graph database\\nwhere loosely cooperating servers accept concurrent updates to a partitioned,\\ndistributed graph. The model is high-fidelity and preserves design choices from\\ncontemporary graph database management systems. To explore the problem space,\\nwe use two common graph topologies as data models for realistic\\nexperimentation. The analysis reveals, even assuming completely fault-free\\nhardware and bug-free software, that if it is possible for updates to interfere\\nwith one-another, corruption will occur and spread significantly through the\\ngraph within the production database lifetime. Using our model, database\\ndesigners and operators can compute the rate of corruption for their systems\\nand determine whether they are sufficiently dependable for their intended use.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 9, 14, 35, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Cold Storage Data Archives: More Than Just a Bunch of Tapes',\n",
       "  'authors': ['Bunjamin Memishi', 'Raja Appuswamy', 'Marcus Paradies'],\n",
       "  'summary': 'The abundance of available sensor and derived data from large scientific\\nexperiments, such as earth observation programs, radio astronomy sky surveys,\\nand high-energy physics already exceeds the storage hardware globally\\nfabricated per year. To that end, cold storage data archives are the---often\\noverlooked---spearheads of modern big data analytics in scientific,\\ndata-intensive application domains. While high-performance data analytics has\\nreceived much attention from the research community, the growing number of\\nproblems in designing and deploying cold storage archives has only received\\nvery little attention.\\n  In this paper, we take the first step towards bridging this gap in knowledge\\nby presenting an analysis of four real-world cold storage archives from three\\ndifferent application domains. In doing so, we highlight (i) workload\\ncharacteristics that differentiate these archives from traditional,\\nperformance-sensitive data analytics, (ii) design trade-offs involved in\\nbuilding cold storage systems for these archives, and (iii) deployment\\ntrade-offs with respect to migration to the public cloud. Based on our\\nanalysis, we discuss several other important research challenges that need to\\nbe addressed by the data management community.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 9, 15, 36, 6, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Big Data Quality: A systematic literature review and future research directions',\n",
       "  'authors': ['Mostafa Mirzaie', 'Behshid Behkamal', 'Samad Paydar'],\n",
       "  'summary': 'One of the most significant problems of Big Data is to extract knowledge\\nthrough the huge amount of data. The usefulness of the extracted information\\ndepends strongly on data quality. In addition to the importance, data quality\\nhas recently been taken into consideration by the big data community and there\\nis not any comprehensive review conducted in this area. Therefore, the purpose\\nof this study is to review and present the state of the art on the quality of\\nbig data research through a hierarchical framework. The dimensions of the\\nproposed framework cover various aspects in the quality assessment of Big Data\\nincluding 1) the processing types of big data, i.e. stream, batch, and hybrid,\\n2) the main task, and 3) the method used to conduct the task. We compare and\\ncritically review all of the studies reported during the last ten years through\\nour proposed framework to identify which of the available data quality\\nassessment methods have been successfully adopted by the big data community.\\nFinally, we provide a critical discussion on the limitations of existing\\nmethods and offer suggestions on potential valuable research directions that\\ncan be taken in future research in this domain.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 10, 13, 27, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The Shapley Value of Tuples in Query Answering',\n",
       "  'authors': ['Ester Livshits',\n",
       "   'Leopoldo Bertossi',\n",
       "   'Benny Kimelfeld',\n",
       "   'Moshe Sebag'],\n",
       "  'summary': 'We investigate the application of the Shapley value to quantifying the\\ncontribution of a tuple to a query answer. The Shapley value is a widely known\\nnumerical measure in cooperative game theory and in many applications of game\\ntheory for assessing the contribution of a player to a coalition game. It has\\nbeen established already in the 1950s, and is theoretically justified by being\\nthe very single wealth-distribution measure that satisfies some natural axioms.\\nWhile this value has been investigated in several areas, it received little\\nattention in data management. We study this measure in the context of\\nconjunctive and aggregate queries by defining corresponding coalition games. We\\nprovide algorithmic and complexity-theoretic results on the computation of\\nShapley-based contributions to query answers; and for the hard cases we present\\napproximation algorithms.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 18, 10, 43, 12, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Mining Closed Episodes with Simultaneous Events',\n",
       "  'authors': ['Nikolaj Tatti', 'Boris Cule'],\n",
       "  'summary': 'Sequential pattern discovery is a well-studied field in data mining. Episodes\\nare sequential patterns describing events that often occur in the vicinity of\\neach other. Episodes can impose restrictions to the order of the events, which\\nmakes them a versatile technique for describing complex patterns in the\\nsequence. Most of the research on episodes deals with special cases such as\\nserial, parallel, and injective episodes, while discovering general episodes is\\nunderstudied.\\n  In this paper we extend the definition of an episode in order to be able to\\nrepresent cases where events often occur simultaneously. We present an\\nefficient and novel miner for discovering frequent and closed general episodes.\\nSuch a task presents unique challenges. Firstly, we cannot define closure based\\non frequency. We solve this by computing a more conservative closure that we\\nuse to reduce the search space and discover the closed episodes as a\\npostprocessing step. Secondly, episodes are traditionally presented as directed\\nacyclic graphs. We argue that this representation has drawbacks leading to\\nredundancy in the output. We solve these drawbacks by defining a subset\\nrelationship in such a way that allows us to remove the redundant episodes. We\\ndemonstrate the efficiency of our algorithm and the need for using closed\\nepisodes empirically on synthetic and real-world datasets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 16, 22, 53, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Approximate Queries and Representations for Large Data Sequences',\n",
       "  'authors': ['Hagit Shatkay', 'Stanley B. Zdonik'],\n",
       "  'summary': 'Many new database application domains such as experimental sciences and\\nmedicine are characterized by large sequences as their main form of data. Using\\napproximate representation can significantly reduce the required storage and\\nsearch space. A good choice of representation, can support a broad new class of\\napproximate queries, needed in these domains. These queries are concerned with\\napplication dependent features of the data as opposed to the actual sampled\\npoints. We introduce a new notion of generalized approximate queries and a\\ngeneral divide and conquer approach that supports them. This approach uses\\nfamilies of real-valued functions as an approximate representation. We present\\nan algorithm for realizing our technique, and the results of applying it to\\nmedical cardiology data.\\n  (Extended version is available in Tech Report CS-95-03, Dept of Computer\\nScience, Brown University.\\nhttp://cs.brown.edu/research/pubs/techreports/reports/CS-95-03.html)',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 19, 16, 45, 20, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Mining Rules Incrementally over Large Knowledge Bases',\n",
       "  'authors': ['Xiaofeng Zhou', 'Ali Sadeghian', 'Daisy Zhe Wang'],\n",
       "  'summary': 'Multiple web-scale Knowledge Bases, e.g., Freebase, YAGO, NELL, have been\\nconstructed using semi-supervised or unsupervised information extraction\\ntechniques and many of them, despite their large sizes, are continuously\\ngrowing. Much research effort has been put into mining inference rules from\\nknowledge bases. To address the task of rule mining over evolving web-scale\\nknowledge bases, we propose a parallel incremental rule mining framework. Our\\napproach is able to efficiently mine rules based on the relational model and\\napply updates to large knowledge bases; we propose an alternative metric that\\nreduces computation complexity without compromising quality; we apply multiple\\noptimization techniques that reduce runtime by more than 2 orders of magnitude.\\nExperiments show that our approach efficiently scales to web-scale knowledge\\nbases and saves over 90% time compared to the state-of-the-art batch rule\\nmining system. We also apply our optimization techniques to the batch rule\\nmining algorithm, reducing runtime by more than half compared to the\\nstate-of-the-art. To the best of our knowledge, our incremental rule mining\\nsystem is the first that handles updates to web-scale knowledge bases.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 20, 4, 9, 1, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Crowdsourced Truth Discovery in the Presence of Hierarchies for Knowledge Fusion',\n",
       "  'authors': ['Woohwan Jung', 'Younghoon Kim', 'Kyuseok Shim'],\n",
       "  'summary': 'Existing works for truth discovery in categorical data usually assume that\\nclaimed values are mutually exclusive and only one among them is correct.\\nHowever, many claimed values are not mutually exclusive even for functional\\npredicates due to their hierarchical structures. Thus, we need to consider the\\nhierarchical structure to effectively estimate the trustworthiness of the\\nsources and infer the truths. We propose a probabilistic model to utilize the\\nhierarchical structures and an inference algorithm to find the truths. In\\naddition, in the knowledge fusion, the step of automatically extracting\\ninformation from unstructured data (e.g., text) generates a lot of false\\nclaims. To take advantages of the human cognitive abilities in understanding\\nunstructured data, we utilize crowdsourcing to refine the result of the truth\\ndiscovery. We propose a task assignment algorithm to maximize the accuracy of\\nthe inferred truths. The performance study with real-life datasets confirms the\\neffectiveness of our truth inference and task assignment algorithms.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 23, 9, 23, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Declarative Recursive Computation on an RDBMS, or, Why You Should Use a Database For Distributed Machine Learning',\n",
       "  'authors': ['Dimitrije Jankov',\n",
       "   'Shangyu Luo',\n",
       "   'Binhang Yuan',\n",
       "   'Zhuhua Cai',\n",
       "   'Jia Zou',\n",
       "   'Chris Jermaine',\n",
       "   'Zekai J. Gao'],\n",
       "  'summary': \"A number of popular systems, most notably Google's TensorFlow, have been\\nimplemented from the ground up to support machine learning tasks. We consider\\nhow to make a very small set of changes to a modern relational database\\nmanagement system (RDBMS) to make it suitable for distributed learning\\ncomputations. Changes include adding better support for recursion, and\\noptimization and execution of very large compute plans. We also show that there\\nare key advantages to using an RDBMS as a machine learning platform. In\\nparticular, learning based on a database management system allows for trivial\\nscaling to large data sets and especially large models, where different\\ncomputational units operate on different parts of a model that may be too large\\nto fit into RAM.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 25, 1, 50, 52, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'GPU-based Efficient Join Algorithms on Hadoop',\n",
       "  'authors': ['Hongzhi Wang', 'Ning Li', 'Zheng Wang', 'Jianing Li'],\n",
       "  'summary': 'The growing data has brought tremendous pressure for query processing and\\nstorage, so there are many studies that focus on using GPU to accelerate join\\noperation, which is one of the most important operations in modern database\\nsystems. However, existing GPU acceleration join operation researches are not\\nvery suitable for the join operation on big data. Based on this, this paper\\nspeeds up nested loop join, hash join and theta join, combining Hadoop with\\nGPU, which is also the first to use GPU to accelerate theta join. At the same\\ntime, after the data pre-filtering and pre-processing, using Map-Reduce and\\nHDFS in Hadoop proposed in this paper, the larger data table can be handled,\\ncompared to existing GPU acceleration methods. Also with Map-Reduce in Hadoop,\\nthe algorithm proposed in this paper can estimate the number of results more\\naccurately and allocate the appropriate storage space without unnecessary\\ncosts, making it more efficient. The rigorous experiments show that the\\nproposed method can obtain 1.5 to 2 times the speedup, compared to the\\ntraditional GPU acceleration equi join algorithm. And in the synthetic data\\nset, the GPU version of the proposed method can get 1.3 to 2 times the speedup,\\ncompared to CPU version.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 25, 8, 20, 5, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Regular Expression Matching on billion-nodes Graphs',\n",
       "  'authors': ['Hongzhi Wang', 'Jiabao Han', 'Bin Shao', 'Jianzhong Li'],\n",
       "  'summary': 'In many applications, it is necessary to retrieve pairs of vertices with the\\npath between them satisfying certain constraints, since regular expression is a\\npowerful tool to describe patterns of a sequence. To meet such requirements, in\\nthis paper, we define regular expression (RE) query on graphs to use regular\\nexpression to represent the constraints between vertices. To process RE queries\\non large graphs such as social networks, we propose the RE query processing\\nmethod with the index size sublinear to the graph size. Considering that large\\ngraphs may be randomly distributed in multiple machines, the parallel RE\\nprocessing algorithms are presented without the assumption of graph\\ndistribution. To achieve high efficiency for complex RE query processing, we\\ndevelop cost-based query optimization strategies with only a small size\\nstatistical information which is suitable for querying large graphs.\\nComprehensive experimental results show that this approach works scale well for\\nlarge graphs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 26, 2, 31, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'AlphaClean: Automatic Generation of Data Cleaning Pipelines',\n",
       "  'authors': ['Sanjay Krishnan', 'Eugene Wu'],\n",
       "  'summary': 'The analyst effort in data cleaning is gradually shifting away from the\\ndesign of hand-written scripts to building and tuning complex pipelines of\\nautomated data cleaning libraries. Hyper-parameter tuning for data cleaning is\\nvery different than hyper-parameter tuning for machine learning since the\\npipeline components and objective functions have structure that tuning\\nalgorithms can exploit. This paper proposes a framework, called AlphaClean,\\nthat rethinks parameter tuning for data cleaning pipelines. AlphaClean provides\\nusers with a rich library to define data quality measures with weighted sums of\\nSQL aggregate queries. AlphaClean applies generate-then-search framework where\\neach pipelined cleaning operator contributes candidate transformations to a\\nshared pool. Asynchronously, in separate threads, a search algorithm sequences\\nthem into cleaning pipelines that maximize the user-defined quality measures.\\nThis architecture allows AlphaClean to apply a number of optimizations\\nincluding incremental evaluation of the quality measures and learning dynamic\\npruning rules to reduce the search space. Our experiments on real and synthetic\\nbenchmarks suggest that AlphaClean finds solutions of up-to 9x higher quality\\nthan naively applying state-of-the-art parameter tuning methods, is\\nsignificantly more robust to straggling data cleaning methods and redundancy in\\nthe data cleaning library, and can incorporate state-of-the-art cleaning\\nsystems such as HoloClean as cleaning operators.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 26, 13, 7, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards a New Extracting and Querying Approach of Fuzzy Summaries',\n",
       "  'authors': ['Ines Benali-Sougui', 'Minyar Sassi Hidri', 'Amel Grissa-Touzi'],\n",
       "  'summary': 'Diversification of DB applications highlighted the limitations of relational\\ndatabase management system (RDBMS) particularly on the modeling plan. In fact,\\nin the real world, we are increasingly faced with the situation where\\napplications need to handle imprecise data and to offer a flexible querying to\\ntheir users. Several theoretical solutions have been proposed. However, the\\nimpact of this work in practice remained negligible with the exception of a few\\nresearch prototypes based on the formal model GEFRED. In this chapter, the\\nauthors propose a new approach for exploitation of fuzzy relational databases\\n(FRDB) described by the model GEFRED. This approach consists of 1) a new\\ntechnique for extracting summary fuzzy data, Fuzzy SAINTETIQ, based on the\\nclassification of fuzzy data and formal concepts analysis; 2) an approach of\\nassessing flexible queries in the context of FDB based on the set of fuzzy\\nsummaries generated by our fuzzy SAINTETIQ system; 3) an approach of repairing\\nand substituting unanswered query.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 28, 16, 49, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'From Digitalization to Data-Driven Decision Making in Container Terminals',\n",
       "  'authors': ['Leonard Heilig', 'Robert Stahlbock', 'Stefan Voß'],\n",
       "  'summary': 'With the new opportunities emerging from the current wave of digitalization,\\nterminal planning and management need to be revisited by taking a data-driven\\nperspective. Business analytics, as a practice of extracting insights from\\noperational data, assists in reducing uncertainties using predictions and helps\\nto identify and understand causes of inefficiencies, disruptions, and anomalies\\nin intra- and inter-organizational terminal operations. Despite the growing\\ncomplexity of data within and around container terminals, a lack of data-driven\\napproaches in the context of container terminals can be identified. In this\\nchapter, the concept of business analytics for supporting terminal planning and\\nmanagement is introduced. The chapter specifically focuses on data mining\\napproaches and provides a comprehensive overview on applications in container\\nterminals and related research. As such, we aim to establish a data-driven\\nperspective on terminal planning and management, complementing the traditional\\noptimization perspective.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 4, 29, 14, 37, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards a Novel Cooperative Logistics Information System Framework',\n",
       "  'authors': ['Fares Zaidi', 'Laurent Amanton', 'Eric Sanlaville'],\n",
       "  'summary': 'Supply Chains and Logistics have a growing importance in global economy.\\nSupply Chain Information Systems over the world are heterogeneous and each one\\ncan both produce and receive massive amounts of structured and unstructured\\ndata in real-time, which are usually generated by information systems,\\nconnected objects or manually by humans. This heterogeneity is due to Logistics\\nInformation Systems components and processes that are developed by different\\nmodelling methods and running on many platforms; hence, decision making process\\nis difficult in such multi-actor environment. In this paper we identify some\\ncurrent challenges and integration issues between separately designed Logistics\\nInformation Systems (LIS), and we propose a Distributed Cooperative Logistics\\nPlatform (DCLP) framework based on NoSQL, which facilitates real-time\\ncooperation between stakeholders and improves decision making process in a\\nmulti-actor environment. We included also a case study of Hospital Supply Chain\\n(HSC), and a brief discussion on perspectives and future scope of work.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 2, 12, 3, 52, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Can the Optimizer Cost be Used to Predict Query Execution Times?',\n",
       "  'authors': ['Anthony Kleerekoper', 'Javier Navaridas', 'Mikel Lujan'],\n",
       "  'summary': 'Predicting the execution time of queries is an important problem with\\napplications in scheduling, service level agreements and error detection.\\nDuring query planning, a cost is associated with the chosen execution plan and\\nused to rank competing plans. It would be convenient to use that cost to\\npredict execution time, but it has been claimed in the literature that this is\\nnot possible. In this paper, we thoroughly investigate this claim considering\\nboth linear and non-linear models. We find that the accuracy using more complex\\nmodels with only the optimizer cost is comparable to the reported accuracy in\\nthe literature. The most accurate method in the literature is nearest-neighbour\\nregression which does not produce a model. The published results used a large\\nfeature set to identify nearest neighbours. We show that it is possible to\\nachieve the same level of accuracy using only the cost to identify nearest\\nneighbours. Using a smaller feature set brings the advantages of reduced\\noverhead in terms of both storage space for the training data and the time to\\nproduce a prediction.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 2, 14, 35, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'SUMMARIZED: Efficient Framework for Analyzing Multidimensional Process Traces under Edit-distance Constraint',\n",
       "  'authors': ['Phuong Nguyen',\n",
       "   'Vatche Ishakian',\n",
       "   'Vinod Muthusamy',\n",
       "   'Aleksander Slominski'],\n",
       "  'summary': 'Domains such as scientific workflows and business processes exhibit data\\nmodels with complex relationships between objects. This relationship is\\ntypically represented as sequences, where each data item is annotated with\\nmulti-dimensional attributes. There is a need to analyze this data for\\noperational insights. For example, in business processes, users are interested\\nin clustering process traces into smaller subsets to discover less complex\\nprocess models. This requires expensive computation of similarity metrics\\nbetween sequence-based data. Related work on dimension reduction and embedding\\nmethods do not take into account the multi-dimensional attributes of data, and\\ndo not address the interpretability of data in the embedding space (i.e., by\\nfavoring vector-based representation). In this work, we introduce Summarized, a\\nframework for efficient analysis on sequence-based multi-dimensional data using\\nintuitive and user-controlled summarizations. We introduce summarization\\nschemes that provide tunable trade-offs between the quality and efficiency of\\nanalysis tasks and derive an error model for summary-based similarity under an\\nedit-distance constraint. Evaluations using real-world datasets show the\\neffectives of our framework.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 2, 22, 4, 10, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'In Defense of Synthetic Data',\n",
       "  'authors': ['Luke Rodriguez', 'Bill Howe'],\n",
       "  'summary': 'Synthetic datasets have long been thought of as second-rate, to be used only\\nwhen \"real\" data collected directly from the real world is unavailable. But\\nthis perspective assumes that raw data is clean, unbiased, and trustworthy,\\nwhich it rarely is. Moreover, the benefits of synthetic data for privacy and\\nfor bias correction are becoming increasingly important in any domain that\\nworks with people. Curated synthetic datasets - synthetic data derived from\\nminimal perturbations of real data - enable early stage product development and\\ncollaboration, protect privacy, afford reproducibility, increase dataset\\ndiversity in research, and protect disadvantaged groups from problematic\\ninferences on the original data that reflects systematic discrimination. Rather\\nthan representing a departure from the true state of the world, in this paper\\nwe argue that properly generated synthetic data is a step towards responsible\\nand equitable research and development of machine learning systems.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 3, 19, 53, 11, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Learning Functional Dependencies with Sparse Regression',\n",
       "  'authors': ['Zhihan Guo', 'Theodoros Rekatsinas'],\n",
       "  'summary': 'We study the problem of discovering functional dependencies (FD) from a noisy\\ndataset. We focus on FDs that correspond to statistical dependencies in a\\ndataset and draw connections between FD discovery and structure learning in\\nprobabilistic graphical models. We show that discovering FDs from a noisy\\ndataset is equivalent to learning the structure of a graphical model over\\nbinary random variables, where each random variable corresponds to a functional\\nof the dataset attributes. We build upon this observation to introduce AutoFD a\\nconceptually simple framework in which learning functional dependencies\\ncorresponds to solving a sparse regression problem. We show that our methods\\ncan recover true functional dependencies across a diverse array of real-world\\nand synthetic datasets, even in the presence of noisy or missing data. We find\\nthat AutoFD scales to large data instances with millions of tuples and hundreds\\nof attributes while it yields an average F1 improvement of 2 times against\\nstate-of-the-art FD discovery methods.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 4, 3, 59, 5, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Errata Note: Discovering Order Dependencies through Order Compatibility',\n",
       "  'authors': ['Parke Godfrey',\n",
       "   'Lukasz Golab',\n",
       "   'Mehdi Kargar',\n",
       "   'Divesh Srivastava',\n",
       "   'Jaroslaw Szlichta'],\n",
       "  'summary': 'A number of extensions to the classical notion of functional dependencies\\nhave been proposed to express and enforce application semantics. One of these\\nextensions is that of order dependencies (ODs), which express rules involving\\norder. The article entitled \"Discovering Order Dependencies through Order\\nCompatibility\" by Consonni et al., published in the EDBT conference proceedings\\nin March 2019, investigates the OD discovery problem. They claim to prove that\\ntheir OD discovery algorithm, OCDDISCOVER, is complete, as well as being\\nsignificantly more efficient in practice than the state-of-the-art. They\\nfurther claim that the implementation of the existing FASTOD algorithm\\n(ours)-we shared our code base with the authors-which they benchmark against is\\nflawed, as OCDDISCOVER and FASTOD report different sets of ODs over the same\\ndata sets.\\n  In this rebuttal, we show that their claim of completeness is, in fact, not\\ntrue. Built upon their incorrect claim, OCDDISCOVER\\'s pruning rules are overly\\naggressive, and prune parts of the search space that contain legitimate ODs.\\nThis is the reason their approach appears to be \"faster\" in practice. Finally,\\nwe show that Consonni et al. misinterpret our set-based canonical form for ODs,\\nleading to an incorrect claim that our FASTOD implementation has an error.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 6, 13, 2, 54, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Mixing set and bag semantics',\n",
       "  'authors': ['Wilmer Ricciotti', 'James Cheney'],\n",
       "  'summary': \"The conservativity theorem for nested relational calculus implies that query\\nexpressions can freely use nesting and unnesting, yet as long as the query\\nresult type is a flat relation, these capabilities do not lead to an increase\\nin expressiveness over flat relational queries. Moreover, Wong showed how such\\nqueries can be translated to SQL via a constructive rewriting algorithm. While\\nthis result holds for queries over either set or multiset semantics, to the\\nbest of our knowledge, the questions of conservativity and normalization have\\nnot been studied for queries that mix set and bag collections, or provide\\nduplicate-elimination operations such as SQL's\\n$\\\\mathtt{SELECT}~\\\\mathtt{DISTINCT}$. In this paper we formalize the problem,\\nand present partial progress: specifically, we introduce a calculus with both\\nset and multiset collection types, along with natural mappings from sets to\\nbags and vice versa, present a set of valid rewrite rules for normalizing such\\nqueries, and give an inductive characterization of a set of queries whose\\nnormal forms can be translated to SQL. We also consider examples that do not\\nappear straightforward to translate to SQL, illustrating that the relative\\nexpressiveness of flat and nested queries with mixed set and multiset semantics\\nremains an open question.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 6, 14, 47, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'BlockLite: A Lightweight Emulator for Public Blockchains',\n",
       "  'authors': ['Xinying Wang',\n",
       "   'Abdullah Al-Mamun',\n",
       "   'Feng Yan',\n",
       "   'Mohammad Sadoghi',\n",
       "   'Dongfang Zhao'],\n",
       "  'summary': 'Blockchain is an enabler of many emerging decentralized applications in areas\\nof cryptocurrency, Internet of Things, smart healthcare, among many others.\\nAlthough various open-source blockchain frameworks are available, the\\ninfrastructure is complex enough and difficult for many users to modify or test\\nout new research ideas. To make it worse, many advantages of blockchain systems\\ncan be demonstrated only at large scales, e.g., thousands of nodes, which are\\nnot always available to researchers. This demo paper presents a lightweight\\nsingle-node emulator of blockchain systems, namely \\\\mbox{BlockLite}, designed\\nto be executing real proof-of-work workload along with peer-to-peer network\\ncommunications and hash-based immutability. BlockLite employs a preprocessing\\napproach to avoid the per-node computation overhead at runtime and thus scales\\nto thousands of nodes. Moreover, BlockLite offers an easy-to-use programming\\ninterface allowing for a Lego-like customization to the system, e.g. new ad-hoc\\nconsensus protocols.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 6, 17, 17, 44, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Generalized formal model of big data',\n",
       "  'authors': ['Shakhovska Nataliya', 'Veres Oleh', 'Hirnyak Mariia'],\n",
       "  'summary': 'This article dwells on the basic characteristic features of the Big Data\\ntechnologies. It is analyzed the existing definition of the \"big data\" term.\\nThe article proposes and describes the elements of the generalized formal model\\nof big data. It is analyzed the peculiarities of the application of the\\nproposed model components. It described the fundamental differences between Big\\nData technology and business analytics. Big Data is supported by the\\ndistributed file system Google File System technology, Cassandra, HBase, Lustre\\nand ZFS, by the MapReduce and Hadoop programming constructs and many other\\nsolutions. According to the experts, such as McKinsey Institute, the\\nmanufacturing, healthcare, trade, administration and control of individual\\nmovements undergo the transformations under the influence of the Big Data.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 4, 3, 1, 20, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Metadata Management for Textual Documents in Data Lakes',\n",
       "  'authors': ['Pegdwendé Sawadogo', 'Tokio Kibata', 'Jérôme Darmont'],\n",
       "  'summary': 'Data lakes have emerged as an alternative to data warehouses for the storage,\\nexploration and analysis of big data. In a data lake, data are stored in a raw\\nstate and bear no explicit schema. Thence, an efficient metadata system is\\nessential to avoid the data lake turning to a so-called data swamp. Existing\\nworks about managing data lake metadata mostly focus on structured and\\nsemi-structured data, with little research on unstructured data. Thus, we\\npropose in this paper a methodological approach to build and manage a metadata\\nsystem that is specific to textual documents in data lakes. First, we make an\\ninventory of usual and meaningful metadata to extract. Then, we apply some\\nspecific techniques from the text mining and information retrieval domains to\\nextract, store and reuse these metadata within the COREL research project, in\\norder to validate our proposals.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 10, 9, 46, 1, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'NFTracer: A Non-Fungible Token Tracking Proof-of-Concept Using Hyperledger Fabric',\n",
       "  'authors': ['Mustafa Bal', 'Caitlin Ner'],\n",
       "  'summary': 'Various start-up developers and academic researchers have investigated the\\nusage of blockchain as a data storage medium due to the advantages offered by\\nits tamper-proof and decentralized nature. However, there have not been many\\nattempts to provide a standard platform for virtually storing the states of\\nunique tangible entities and their subsequent modifications. In this paper, we\\npropose NFTracer, a non-fungible token tracking proof-of-concept based on\\nHyperledger Composer and Hyperledger Fabric Blockchain. To achieve the\\ncapabilities of our platform, we use NFTracer to build an artwork auction and a\\nreal estate auction, which vary in technical complexity and demonstrate the\\nadvantages of being able to track entities and their resulting modifications in\\na decentralized manner. We also present its accompanying modular architecture\\nand system components, and discuss possible future works on NFTracer.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 12, 21, 39, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Survey of Blocking and Filtering Techniques for Entity Resolution',\n",
       "  'authors': ['George Papadakis',\n",
       "   'Dimitrios Skoutas',\n",
       "   'Emmanouil Thanos',\n",
       "   'Themis Palpanas'],\n",
       "  'summary': 'Efficiency techniques are an integral part of Entity Resolution, since its\\ninfancy. In this survey, we organized the bulk of works in the field into\\nBlocking, Filtering and hybrid techniques, facilitating their understanding and\\nuse. We also provided an in-dept coverage of each category, further classifying\\nthe corresponding works into novel sub-categories. Lately, the efficiency\\ntechniques have received more attention, due to the rise of Big Data. This\\nincludes large volumes of semi-structured data, which pose challenges not only\\nto the scalability of efficiency techniques, but also to their core\\nassumptions: the requirement of Blocking for schema knowledge and of Filtering\\nfor high similarity thresholds. The former led to the introduction of\\nschema-agnostic Blocking in conjunction with Block Processing techniques, while\\nthe latter led to more relaxed criteria of similarity. Our survey covers these\\nnew fields in detail, putting in context all relevant works.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 15, 13, 28, 56, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'MinoanER: Schema-Agnostic, Non-Iterative, Massively Parallel Resolution of Web Entities',\n",
       "  'authors': ['Vasilis Efthymiou',\n",
       "   'George Papadakis',\n",
       "   'Kostas Stefanidis',\n",
       "   'Vassilis Christophides'],\n",
       "  'summary': 'Entity Resolution (ER) aims to identify different descriptions in various\\nKnowledge Bases (KBs) that refer to the same entity. ER is challenged by the\\nVariety, Volume and Veracity of entity descriptions published in the Web of\\nData. To address them, we propose the MinoanER framework that simultaneously\\nfulfills full automation, support of highly heterogeneous entities, and massive\\nparallelization of the ER process. MinoanER leverages a token-based similarity\\nof entities to define a new metric that derives the similarity of neighboring\\nentities from the most important relations, as they are indicated only by\\nstatistics. A composite blocking method is employed to capture different\\nsources of matching evidence from the content, neighbors, or names of entities.\\nThe search space of candidate pairs for comparison is compactly abstracted by a\\nnovel disjunctive blocking graph and processed by a non-iterative, massively\\nparallel matching algorithm that consists of four generic, schema-agnostic\\nmatching rules that are quite robust with respect to their internal\\nconfiguration. We demonstrate that the effectiveness of MinoanER is comparable\\nto existing ER tools over real KBs exhibiting low Variety, but it outperforms\\nthem significantly when matching KBs with high Variety.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 15, 13, 31, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Scalable Learned Index Scheme in Storage Systems',\n",
       "  'authors': ['Pengfei Li', 'Yu Hua', 'Pengfei Zuo', 'Jingnan Jia'],\n",
       "  'summary': 'Index structures are important for efficient data access, which have been\\nwidely used to improve the performance in many in-memory systems. Due to high\\nin-memory overheads, traditional index structures become difficult to process\\nthe explosive growth of data, let alone providing low latency and high\\nthroughput performance with limited system resources. The promising learned\\nindexes leverage deep-learning models to complement existing index structures\\nand obtain significant memory savings. However, the learned indexes fail to\\nbecome scalable due to the heavy inter-model dependency and expensive\\nretraining. To address these problems, we propose a scalable learned index\\nscheme to construct different linear regression models according to the data\\ndistribution. Moreover, the used models are independent so as to reduce the\\ncomplexity of retraining and become easy to partition and store the data into\\ndifferent pages, blocks or distributed systems. Our experimental results show\\nthat compared with state-of-the-art schemes, AIDEL improves the insertion\\nperformance by about 2$\\\\times$ and provides comparable lookup performance,\\nwhile efficiently supporting scalability.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 8, 8, 14, 19, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'End-to-End Entity Resolution for Big Data: A Survey',\n",
       "  'authors': ['Vassilis Christophides',\n",
       "   'Vasilis Efthymiou',\n",
       "   'Themis Palpanas',\n",
       "   'George Papadakis',\n",
       "   'Kostas Stefanidis'],\n",
       "  'summary': 'One of the most important tasks for improving data quality and the\\nreliability of data analytics results is Entity Resolution (ER). ER aims to\\nidentify different descriptions that refer to the same real-world entity, and\\nremains a challenging problem. While previous works have studied specific\\naspects of ER (and mostly in traditional settings), in this survey, we provide\\nfor the first time an end-to-end view of modern ER workflows, and of the novel\\naspects of entity indexing and matching methods in order to cope with more than\\none of the Big Data characteristics simultaneously. We present the basic\\nconcepts, processing steps and execution strategies that have been proposed by\\ndifferent communities, i.e., database, semantic Web and machine learning, in\\norder to cope with the loose structuredness, extreme diversity, high speed and\\nlarge scale of entity descriptions used by real-world applications. Finally, we\\nprovide a synthetic discussion of the existing approaches, and conclude with a\\ndetailed presentation of open research directions.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 15, 19, 15, 11, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An Empirical Analysis of Deep Learning for Cardinality Estimation',\n",
       "  'authors': ['Jennifer Ortiz',\n",
       "   'Magdalena Balazinska',\n",
       "   'Johannes Gehrke',\n",
       "   'S. Sathiya Keerthi'],\n",
       "  'summary': 'We implement and evaluate deep learning for cardinality estimation by\\nstudying the accuracy, space and time trade-offs across several architectures.\\nWe find that simple deep learning models can learn cardinality estimations\\nacross a variety of datasets (reducing the error by 72% - 98% on average\\ncompared to PostgreSQL). In addition, we empirically evaluate the impact of\\ninjecting cardinality estimates produced by deep learning models into the\\nPostgreSQL optimizer. In many cases, the estimates from these models lead to\\nbetter query plans across all datasets, reducing the runtimes by up to 49% on\\nselect-project-join workloads. As promising as these models are, we also\\ndiscuss and address some of the challenges of using them in practice.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 15, 20, 30, 44, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The CEDAR Workbench: An Ontology-Assisted Environment for Authoring Metadata that Describe Scientific Experiments',\n",
       "  'authors': ['Rafael S. Gonçalves',\n",
       "   \"Martin J. O'Connor\",\n",
       "   'Marcos Martínez-Romero',\n",
       "   'Attila L. Egyedi',\n",
       "   'Debra Willrett',\n",
       "   'John Graybeal',\n",
       "   'Mark A. Musen'],\n",
       "  'summary': 'The Center for Expanded Data Annotation and Retrieval (CEDAR) aims to\\nrevolutionize the way that metadata describing scientific experiments are\\nauthored. The software we have developed--the CEDAR Workbench--is a suite of\\nWeb-based tools and REST APIs that allows users to construct metadata\\ntemplates, to fill in templates to generate high-quality metadata, and to share\\nand manage these resources. The CEDAR Workbench provides a versatile,\\nREST-based environment for authoring metadata that are enriched with terms from\\nontologies. The metadata are available as JSON, JSON-LD, or RDF for easy\\nintegration in scientific applications and reusability on the Web. Users can\\nleverage our APIs for validating and submitting metadata to external\\nrepositories. The CEDAR Workbench is freely available and open-source.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 16, 0, 19, 49, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Persistent Buffer Management with Optimistic Consistency',\n",
       "  'authors': ['Lucas Lersch', 'Wolfgang Lehner', 'Ismail Oukid'],\n",
       "  'summary': 'Finding the best way to leverage non-volatile memory (NVM) on modern database\\nsystems is still an open problem. The answer is far from trivial since the\\nclear boundary between memory and storage present in most systems seems to be\\nincompatible with the intrinsic memory-storage duality of NVM. Rather than\\ntreating NVM either solely as memory or solely as storage, in this work we\\npropose how NVM can be simultaneously used as both in the context of modern\\ndatabase systems. We design a persistent buffer pool on NVM, enabling pages to\\nbe directly read/written by the CPU (like memory) while recovering corrupted\\npages after a failure (like storage). The main benefits of our approach are an\\neasy integration in the existing database architectures, reduced costs (by\\nreplacing DRAM with NVM), and faster peak-performance recovery.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 16, 14, 2, 29, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'High Throughput Push Based Storage Manager',\n",
       "  'authors': ['Ye Zhu'],\n",
       "  'summary': 'The storage manager, as a key component of the database system, is\\nresponsible for organizing, reading, and delivering data to the execution\\nengine for processing. According to the data serving mechanism, existing\\nstorage managers are either pull-based, incurring high latency, or push-based,\\nleading to a high number of I/O requests when the CPU is busy. To improve these\\nshortcomings, this thesis proposes a push-based prefetching strategy in a\\ncolumn-wise storage manager. The proposed strategy implements an efficient\\ncache layer to store shared data among queries to reduce the number of I/O\\nrequests. The capacity of the cache is maintained by a time access-aware\\neviction mechanism. Our strategy enables the storage manager to coordinate\\nmultiple queries by merging their requests and dynamically generate an optimal\\nread order that maximizes the overall I/O throughput. We evaluated our storage\\nmanager both over a disk-based redundant array of independent disks (RAID) and\\nan NVM Express (NVMe) solid-state drive (SSD). With the high read performance\\nof the SSD, we successfully minimized the total read time and number of I/O\\naccesses.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 17, 4, 50, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Regions In a Linked Dataset For Change Detection',\n",
       "  'authors': ['Anuj Singh'],\n",
       "  'summary': 'Linked Datasets (LDs) are constantly evolving and the applications using a\\nLinked Dataset (LD) may face several issues such as outdated data or broken\\ninterlinks due to evolution of the dataset. To overcome these issues, the\\ndetection of changes in LDs during their evolution has proven crucial. As LDs\\nevolve frequently, the change detection during the evolution should also be\\ndone at frequent intervals. However, due to limitation of available\\ncomputational resources such as capacity to fetch data from LD and time to\\ndetect changes, the frequent change detection may not be possible with existing\\nchange detection techniques. This research proposes to explore the notion of\\nprioritization of regions (subsets) in LDs for change detection with the aim of\\nachieving optimal accuracy and efficient use of available computational\\nresources. This will facilitate the detection of changes in an evolving LD at\\nfrequent intervals and will allow the applications to update their data closest\\nto real-time data.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 19, 0, 1, 5, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Ingesting High-Velocity Streaming Graphs from Social Media Sources',\n",
       "  'authors': ['Subhasis Dasgupta', 'Aditya Bagchi', 'Amarnath Gupta'],\n",
       "  'summary': 'Many data science applications like social network analysis use graphs as\\ntheir primary form of data. However, acquiring graph-structured data from\\nsocial media presents some interesting challenges. The first challenge is the\\nhigh data velocity and bursty nature of the social media data. The second\\nchallenge is that the complex nature of the data makes the ingestion process\\nexpensive. If we want to store the streaming graph data in a graph database, we\\nface a third challenge -- the database is very often unable to sustain the\\ningestion of high-velocity, high-burst data. We have developed an adaptive\\nbuffering mechanism and a graph compression technique that effectively\\nmitigates the problem. A novel aspect of our method is that the adaptive\\nbuffering algorithm uses the data rate, the data content as well as the CPU\\nresources of the database machine to determine an optimal data ingestion\\nmechanism. We further show that an ingestion-time graph-compression strategy\\nimproves the efficiency of the data ingestion into the database. We have\\nverified the efficacy of our ingestion optimization strategy through extensive\\nexperiments.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 20, 20, 29, 44, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'IPAW 2020 Preprint: Efficient Computation of Provenance for Query Result Exploration',\n",
       "  'authors': ['Murali Mani', 'Naveenkumar Singaraj', 'Zhenyan Liu'],\n",
       "  'summary': 'Users typically interact with a database by asking queries and examining the\\nresults. We refer to the user examining the query results and asking follow-up\\nquestions as query result exploration. Our work builds on two decades of\\nprovenance research useful for query result exploration. Three approaches for\\ncomputing provenance have been described in the literature: lazy, eager, and\\nhybrid. We investigate lazy and eager approaches that utilize constraints that\\nwe have identified in the context of query result exploration, as well as novel\\nhybrid approaches. For the TPC-H benchmark, these constraints are applicable to\\n19 out of the 22 queries, and result in a better performance for all queries\\nthat have a join. Furthermore, the performance benefits from our approaches are\\nsignificant, sometimes several orders of magnitude.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 22, 17, 14, 20, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards Global Asset Management in Blockchain Systems',\n",
       "  'authors': ['Victor Zakhary',\n",
       "   'Mohammad Javad Amiri',\n",
       "   'Sujaya Maiyya',\n",
       "   'Divyakant Agrawal',\n",
       "   'Amr El Abbadi'],\n",
       "  'summary': \"Permissionless blockchains (e.g., Bitcoin, Ethereum, etc) have shown a wide\\nsuccess in implementing global scale peer-to-peer cryptocurrency systems. In\\nsuch blockchains, new currency units are generated through the mining process\\nand are used in addition to transaction fees to incentivize miners to maintain\\nthe blockchain. Although it is clear how currency units are generated and\\ntransacted on, it is unclear how to use the infrastructure of permissionless\\nblockchains to manage other assets than the blockchain's currency units (e.g.,\\ncars, houses, etc). In this paper, we propose a global asset management system\\nby unifying permissioned and permissionless blockchains. A governmental\\npermissioned blockchain authenticates the registration of end-user assets\\nthrough smart contract deployments on a permissionless blockchain. Afterwards,\\nend-users can transact on their assets through smart contract function calls\\n(e.g., sell a car, rent a room in a house, etc). In return, end-users get paid\\nin currency units of the same blockchain or other blockchains through atomic\\ncross-chain transactions and governmental offices receive taxes on these\\ntransactions in cryptocurrency units.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 22, 20, 44, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'COBS: a Compact Bit-Sliced Signature Index',\n",
       "  'authors': ['Timo Bingmann',\n",
       "   'Phelim Bradley',\n",
       "   'Florian Gauger',\n",
       "   'Zamin Iqbal'],\n",
       "  'summary': \"We present COBS, a COmpact Bit-sliced Signature index, which is a cross-over\\nbetween an inverted index and Bloom filters. Our target application is to index\\n$k$-mers of DNA samples or $q$-grams from text documents and process\\napproximate pattern matching queries on the corpus with a user-chosen coverage\\nthreshold. Query results may contain a number of false positives which\\ndecreases exponentially with the query length. We compare COBS to seven other\\nindex software packages on 100000 microbial DNA samples. COBS' compact but\\nsimple data structure outperforms the other indexes in construction time and\\nquery performance with Mantis by Pandey et al. in second place. However, unlike\\nMantis and other previous work, COBS does not need the complete index in RAM\\nand is thus designed to scale to larger document sets.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 23, 12, 49, 58, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Multi-Model Investigative Exploration of Social Media Data with boutique: A Case Study in Public Health',\n",
       "  'authors': ['Junan Guo', 'Subhasis Dasgupta', 'Amarnath Gupta'],\n",
       "  'summary': 'We present our experience with a data science problem in Public Health, where\\nresearchers use social media (Twitter) to determine whether the public shows\\nawareness of HIV prevention measures offered by Public Health campaigns. To\\nhelp the researcher, we develop an investigative exploration system called\\nboutique that allows a user to perform a multi-step visualization and\\nexploration of data through a dashboard interface. Unique features of boutique\\nincludes its ability to handle heterogeneous types of data provided by a\\npolystore, and its ability to use computation as part of the investigative\\nexploration process. In this paper, we present the design of the boutique\\nmiddleware and walk through an investigation process for a real-life problem.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 24, 23, 34, 48, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'One SQL to Rule Them All',\n",
       "  'authors': ['Edmon Begoli',\n",
       "   'Tyler Akidau',\n",
       "   'Fabian Hueske',\n",
       "   'Julian Hyde',\n",
       "   'Kathryn Knight',\n",
       "   'Kenneth Knowles'],\n",
       "  'summary': 'Real-time data analysis and management are increasingly critical for today`s\\nbusinesses. SQL is the de facto lingua franca for these endeavors, yet support\\nfor robust streaming analysis and management with SQL remains limited. Many\\napproaches restrict semantics to a reduced subset of features and/or require a\\nsuite of non-standard constructs. Additionally, use of event timestamps to\\nprovide native support for analyzing events according to when they actually\\noccurred is not pervasive, and often comes with important limitations. We\\npresent a three-part proposal for integrating robust streaming into the SQL\\nstandard, namely: (1) time-varying relations as a foundation for classical\\ntables as well as streaming data, (2) event time semantics, (3) a limited set\\nof optional keyword extensions to control the materialization of time-varying\\nquery results. Motivated and illustrated using examples and lessons learned\\nfrom implementations in Apache Calcite, Apache Flink, and Apache Beam, we show\\nhow with these minimal additions it is possible to utilize the complete suite\\nof standard SQL semantics to perform robust stream processing.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 28, 23, 26, 8, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Designing and Implementing Data Warehouse for Agricultural Big Data',\n",
       "  'authors': ['Vuong M. Ngo', 'Nhien-An Le-Khac', 'M-Tahar Kechadi'],\n",
       "  'summary': 'In recent years, precision agriculture that uses modern information and\\ncommunication technologies is becoming very popular. Raw and semi-processed\\nagricultural data are usually collected through various sources, such as:\\nInternet of Thing (IoT), sensors, satellites, weather stations, robots, farm\\nequipment, farmers and agribusinesses, etc. Besides, agricultural datasets are\\nvery large, complex, unstructured, heterogeneous, non-standardized, and\\ninconsistent. Hence, the agricultural data mining is considered as Big Data\\napplication in terms of volume, variety, velocity and veracity. It is a key\\nfoundation to establishing a crop intelligence platform, which will enable\\nresource efficient agronomy decision making and recommendations. In this paper,\\nwe designed and implemented a continental level agricultural data warehouse by\\ncombining Hive, MongoDB and Cassandra. Our data warehouse capabilities: (1)\\nflexible schema; (2) data integration from real agricultural multi datasets;\\n(3) data science and business intelligent support; (4) high performance; (5)\\nhigh storage; (6) security; (7) governance and monitoring; (8) replication and\\nrecovery; (9) consistency, availability and partition tolerant; (10)\\ndistributed and cloud deployment. We also evaluate the performance of our data\\nwarehouse.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 29, 13, 18, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Fair Decision Making using Privacy-Protected Data',\n",
       "  'authors': ['Satya Kuppam',\n",
       "   'Ryan Mckenna',\n",
       "   'David Pujol',\n",
       "   'Michael Hay',\n",
       "   'Ashwin Machanavajjhala',\n",
       "   'Gerome Miklau'],\n",
       "  'summary': 'Data collected about individuals is regularly used to make decisions that\\nimpact those same individuals. We consider settings where sensitive personal\\ndata is used to decide who will receive resources or benefits. While it is well\\nknown that there is a tradeoff between protecting privacy and the accuracy of\\ndecisions, we initiate a first-of-its-kind study into the impact of formally\\nprivate mechanisms (based on differential privacy) on fair and equitable\\ndecision-making. We empirically investigate novel tradeoffs on two real-world\\ndecisions made using U.S. Census data (allocation of federal funds and\\nassignment of voting rights benefits) as well as a classic apportionment\\nproblem. Our results show that if decisions are made using an\\n$\\\\epsilon$-differentially private version of the data, under strict privacy\\nconstraints (smaller $\\\\epsilon$), the noise added to achieve privacy may\\ndisproportionately impact some groups over others. We propose novel measures of\\nfairness in the context of randomized differentially private algorithms and\\nidentify a range of causes of outcome disparities.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 29, 21, 32, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': \"Don't Persist All : Efficient Persistent Data Structures\",\n",
       "  'authors': ['Pratyush Mahapatra', 'Mark D. Hill', 'Michael M. Swift'],\n",
       "  'summary': 'Data structures used in software development have inbuilt redundancy to\\nimprove software reliability and to speed up performance. Examples include a\\nDoubly Linked List which allows a faster deletion due to the presence of the\\nprevious pointer. With the introduction of Persistent Memory, storing the\\nredundant data fields into persistent memory adds a significant write overhead,\\nand reduces performance. In this work, we focus on three data structures -\\nDoubly Linked List, B+Tree and Hashmap, and showcase alternate partly\\npersistent implementations where we only store a limited set of data fields to\\npersistent memory. After a crash/restart, we use the persistent data fields to\\nrecreate the data structures along with the redundant data fields. We compare\\nour implementation with the base implementation and show that we achieve\\nspeedups around 5-20% for some data structures, and up to 165% for a\\nflush-dominated data structure.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 29, 17, 56, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Efficient Multiway Hash Join on Reconfigurable Hardware',\n",
       "  'authors': ['Kunle Olukotun',\n",
       "   'Raghu Prabhakar',\n",
       "   'Rekha Singhal',\n",
       "   'Jeffrey D. Ullman',\n",
       "   'Yaqi Zhang'],\n",
       "  'summary': \"We propose the algorithms for performing multiway joins using a new type of\\ncoarse grain reconfigurable hardware accelerator~-- ``Plasticine''~-- that,\\ncompared with other accelerators, emphasizes high compute capability and high\\non-chip communication bandwidth. Joining three or more relations in a single\\nstep, i.e. multiway join, is efficient when the join of any two relations\\nyields too large an intermediate relation. We show at least 200X speedup for a\\nsequence of binary hash joins execution on Plasticine over CPU. We further show\\nthat in some realistic cases, a Plasticine-like accelerator can make 3-way\\njoins more efficient than a cascade of binary hash joins on the same hardware,\\nby a factor of up to 45X.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 5, 31, 1, 48, 58, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Rule Applicability on RDF Triplestore Schemas',\n",
       "  'authors': ['Paolo Pareti',\n",
       "   'George Konstantinidis',\n",
       "   'Timothy J. Norman',\n",
       "   'Murat Şensoy'],\n",
       "  'summary': 'Rule-based systems play a critical role in health and safety, where policies\\ncreated by experts are usually formalised as rules. When dealing with\\nincreasingly large and dynamic sources of data, as in the case of Internet of\\nThings (IoT) applications, it becomes important not only to efficiently apply\\nrules, but also to reason about their applicability on datasets confined by a\\ncertain schema. In this paper we define the notion of a triplestore schema\\nwhich models a set of RDF graphs. Given a set of rules and such a schema as\\ninput we propose a method to determine rule applicability and produce output\\nschemas. Output schemas model the graphs that would be obtained by running the\\nrules on the graph models of the input schema. We present two approaches: one\\nbased on computing a canonical (critical) instance of the schema, and a novel\\napproach based on query rewriting. We provide theoretical, complexity and\\nevaluation results that show the superior efficiency of our rewriting approach.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 2, 20, 50, 1, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'GeoPrune: Efficiently Finding Shareable Vehicles Based on Geometric Properties',\n",
       "  'authors': ['Yixin Xu',\n",
       "   'Jianzhong Qi',\n",
       "   'Renata Borovica-Gajic',\n",
       "   'Lars Kulik'],\n",
       "  'summary': 'On-demand ride-sharing is rapidly growing.Matching trip requests to vehicles\\nefficiently is critical for the service quality of ride-sharing. To match trip\\nrequests with vehicles, a prune-and-select scheme is commonly used. The pruning\\nstage identifies feasible vehicles that can satisfy the trip constraints (e.g.,\\ntrip time). The selection stage selects the optimal one(s) from the feasible\\nvehicles. The pruning stage is crucial to reduce the complexity of the\\nselection stage and to achieve efficient matching. We propose an effective and\\nefficient pruning algorithm called GeoPrune. GeoPrune represents the time\\nconstraints of trip requests using circles and ellipses, which can be computed\\nand updated efficiently. Experiments on real-world datasets show that GeoPrune\\nreduces the number of vehicle candidates in nearly all cases by an order of\\nmagnitude and the update cost by two to three orders of magnitude compared to\\nthe state-of-the-art.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 3, 10, 15, 7, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Software Framework and Datasets for the Analysis of Graph Measures on RDF Graphs',\n",
       "  'authors': ['Matthäus Zloch',\n",
       "   'Maribel Acosta',\n",
       "   'Daniel Hienert',\n",
       "   'Stefan Dietze',\n",
       "   'Stefan Conrad'],\n",
       "  'summary': 'As the availability and the inter-connectivity of RDF datasets grow, so does\\nthe necessity to understand the structure of the data. Understanding the\\ntopology of RDF graphs can guide and inform the development of, e.g. synthetic\\ndataset generators, sampling methods, index structures, or query optimizers. In\\nthis work, we propose two resources: (i) a software framework able to acquire,\\nprepare, and perform a graph-based analysis on the topology of large RDF\\ngraphs, and (ii) results on a graph-based analysis of 280 datasets from the LOD\\nCloud with values for 28 graph measures computed with the framework. We present\\na preliminary analysis based on the proposed resources and point out\\nimplications for synthetic dataset generators. Finally, we identify a set of\\nmeasures, that can be used to characterize graphs in the Semantic Web.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 3, 12, 32, 50, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Interlinking Heterogeneous Data for Smart Energy Systems',\n",
       "  'authors': ['Fabrizio Orlandi',\n",
       "   'Alan Meehan',\n",
       "   'Murhaf Hossari',\n",
       "   'Soumyabrata Dev',\n",
       "   \"Declan O'Sullivan\",\n",
       "   'Tarek AlSkaif'],\n",
       "  'summary': 'Smart energy systems in general, and solar energy analysis in particular,\\nhave recently gained increasing interest. This is mainly due to stronger focus\\non smart energy saving solutions and recent developments in photovoltaic (PV)\\ncells. Various data-driven and machine-learning frameworks are being proposed\\nby the research community. However, these frameworks perform their analysis -\\nand are designed on - specific, heterogeneous and isolated datasets,\\ndistributed across different sites and sources, making it hard to compare\\nresults and reproduce the analysis on similar data. We propose an approach\\nbased on Web (W3C) standards and Linked Data technologies for representing and\\nconverting PV and weather records into an Resource Description Framework (RDF)\\ngraph-based data format. This format, and the presented approach, is ideal in a\\ndata integration scenario where data needs to be converted into homogeneous\\nform and different datasets could be interlinked for distributed analysis.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 5, 12, 16, 30, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Property Graph Exchange Format',\n",
       "  'authors': ['Hirokazu Chiba', 'Ryota Yamanaka', 'Shota Matsumoto'],\n",
       "  'summary': 'Recently, a variety of database implementations adopting the property graph\\nmodel have emerged. However, interoperable management of graph data on these\\nimplementations is challenging due to the differences in data models and\\nformats. Here, we redefine the property graph model incorporating the\\ndifferences in the existing models and propose interoperable serialization\\nformats for property graphs. The model is independent of specific\\nimplementations and provides a basis of interoperable management of property\\ngraph data. The proposed serialization is not only general but also intuitive,\\nthus it is useful for creating and maintaining graph data. To demonstrate the\\npractical use of our model and serialization, we implemented converters from\\nour serialization into existing formats, which can then be loaded into various\\ngraph databases. This work provides a basis of an interoperable platform for\\ncreating, exchanging, and utilizing property graph data.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 9, 1, 43, 54, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Multi-source Relations for Contextual Data Mining in Learning Analytics',\n",
       "  'authors': ['Julie Bu Daher', 'Armelle Brun', 'Anne Boyer'],\n",
       "  'summary': \"The goals of Learning Analytics (LA) are manifold, among which helping\\nstudents to understand their academic progress and improving their learning\\nprocess, which are at the core of our work. To reach this goal, LA relies on\\neducational data: students' traces of activities on VLE, or academic,\\nsocio-demographic information, information about teachers, pedagogical\\nresources, curricula, etc. The data sources that contain such information are\\nmultiple and diverse. Data mining, specifically pattern mining, aims at\\nextracting valuable and understandable information from large datasets. In our\\nwork, we assume that multiple educational data sources form a rich dataset that\\ncan result in valuable patterns. Mining such data is thus a promising way to\\nreach the goal of helping students. However, heterogeneity and interdependency\\nwithin data lead to high computational complexity. We thus aim at designing low\\ncomplex pattern mining algorithms that mine multi-source data, taking into\\nconsideration the dependency and heterogeneity among sources. The patterns\\nformed are meaningful and interpretable, they can thus be directly used for\\nstudents.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 6, 28, 12, 26, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Delivery, consistency, and determinism: rethinking guarantees in distributed stream processing',\n",
       "  'authors': ['Artem Trofimov',\n",
       "   'Igor E. Kuralenok',\n",
       "   'Nikita Marshalkin',\n",
       "   'Boris Novikov'],\n",
       "  'summary': 'Consistency requirements for state-of-the-art stream processing systems are\\ndefined in terms of delivery guarantees. Exactly-once is the strongest one and\\nthe most desirable for end-user. However, there are several issues regarding\\nthis concept. Commonly used techniques that enforce exactly-once produce\\nsignificant performance overhead. Besides, the notion of exactly-once is not\\nformally defined and does not capture all properties that provide stream\\nprocessing systems supporting this guarantee. In this paper, we introduce a\\nformal framework that allows us to define streaming guarantees more regularly.\\nWe demonstrate that the properties of delivery, consistency, and determinism\\nare tightly connected within distributed stream processing. We also show that\\nhaving lightweight determinism, it is possible to provide exactly-once with\\nalmost no performance overhead. Experiments show that the proposed approach can\\nsignificantly outperform alternative industrial solutions.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 14, 17, 24, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An Approach Based on Bayesian Networks for Query Selectivity Estimation',\n",
       "  'authors': ['Max Halford', 'Philippe Saint-Pierre', 'Frank Morvan'],\n",
       "  'summary': 'The efficiency of a query execution plan depends on the accuracy of the\\nselectivity estimates given to the query optimiser by the cost model. The cost\\nmodel makes simplifying assumptions in order to produce said estimates in a\\ntimely manner. These assumptions lead to selectivity estimation errors that\\nhave dramatic effects on the quality of the resulting query execution plans. A\\nconvenient assumption that is ubiquitous among current cost models is to assume\\nthat attributes are independent with each other. However, it ignores potential\\ncorrelations which can have a huge negative impact on the accuracy of the cost\\nmodel. In this paper we attempt to relax the attribute value independence\\nassumption without unreasonably deteriorating the accuracy of the cost model.\\nWe propose a novel approach based on a particular type of Bayesian networks\\ncalled Chow-Liu trees to approximate the distribution of attribute values\\ninside each relation of a database. Our results on the TPC-DS benchmark show\\nthat our method is an order of magnitude more precise than other approaches\\nwhilst remaining reasonably efficient in terms of time and space.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 14, 23, 13, 19, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Subjective Interestingness measure for Business Intelligence explorations',\n",
       "  'authors': ['Alexandre Chanson',\n",
       "   'Ben Crulis',\n",
       "   'Nicolas Labroche',\n",
       "   'Patrick Marcel'],\n",
       "  'summary': \"This paper addresses the problem of defining a subjective interestingness\\nmeasure for BI exploration. Such a measure involves prior modeling of the\\nbelief of the user. The complexity of this problem lies in the impossibility to\\nask the user about the degree of belief in each element composing their\\nknowledge prior to the writing of a query. To this aim, we propose to\\nautomatically infer this user belief based on the user's past interactions over\\na data cube, the cube schema and other users past activities. We express the\\nbelief under the form of a probability distribution over all the query parts\\npotentially accessible to the user, and use a random walk to learn this\\ndistribution. This belief is then used to define a first Subjective\\nInterestingness measure over multidimensional queries. Experiments conducted on\\nsimulated and real explorations show how this new subjective interestingness\\nmeasure relates to prototypical and real user behaviors, and that query parts\\noffer a reasonable proxy to infer user belief.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 16, 11, 43, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'In-Depth Benchmarking of Graph Database Systems with the Linked Data Benchmark Council (LDBC) Social Network Benchmark (SNB)',\n",
       "  'authors': ['Florin Rusu', 'Zhiyi Huang'],\n",
       "  'summary': 'In this study, we present the first results of a complete implementation of\\nthe LDBC SNB benchmark -- interactive short, interactive complex, and business\\nintelligence -- in two native graph database systems---Neo4j and TigerGraph. In\\naddition to thoroughly evaluating the performance of all of the 46 queries in\\nthe benchmark on four scale factors -- SF-1, SF-10, SF-100, and SF-1000 -- and\\nthree computing architectures -- on premise and in the cloud -- we also measure\\nthe bulk loading time and storage size. Our results show that TigerGraph is\\nconsistently outperforming Neo4j on the majority of the queries---by two or\\nmore orders of magnitude (100X factor) on certain interactive complex and\\nbusiness intelligence queries. The gap increases with the size of the data\\nsince only TigerGraph is able to scale to SF-1000---Neo4j finishes only 12 of\\nthe 25 business intelligence queries in reasonable time. Nonetheless, Neo4j is\\ngenerally faster at bulk loading graph data up to SF-100. A key to our study is\\nthe active involvement of the vendors in the tuning of their platforms. In\\norder to encourage reproducibility, we make all the code, scripts, and\\nconfiguration parameters publicly available online.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 17, 9, 17, 27, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Linked Crunchbase: A Linked Data API and RDF Data Set About Innovative Companies',\n",
       "  'authors': ['Michael Färber'],\n",
       "  'summary': 'Crunchbase is an online platform collecting information about startups and\\ntechnology companies, including attributes and relations of companies, people,\\nand investments. Data contained in Crunchbase is, to a large extent, not\\navailable elsewhere, making Crunchbase to a unique data source. In this paper,\\nwe present how to bring Crunchbase to the Web of Data so that its data can be\\nused in the machine-readable RDF format by anyone on the Web. First, we give\\ninsights into how we developed and hosted a Linked Data API for Crunchbase and\\nhow sameAs links to other data sources are integrated. Then, we present our\\nmethod for crawling RDF data based on this API to build a custom Crunchbase RDF\\nknowledge graph. We created an RDF data set with over 347 million triples,\\nincluding 781k people, 659k organizations, and 343k investments. Our Crunchbase\\nLinked Data API is available online at http://linked-crunchbase.org.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 19, 20, 8, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Application of Data Stream Processing Technologies in Industry 4.0 -- What is Missing?',\n",
       "  'authors': ['Guenter Hesse',\n",
       "   'Werner Sinzig',\n",
       "   'Christoph Matthies',\n",
       "   'Matthias Uflacker'],\n",
       "  'summary': 'Industry 4.0 is becoming more and more important for manufacturers as the\\ndevelopments in the area of Internet of Things advance. Another technology\\ngaining more attention is data stream processing systems. Although such\\nstreaming frameworks seem to be a natural fit for Industry 4.0 scenarios, their\\napplication in this context is still low. The contributions in this paper are\\nthreefold. Firstly, we present industry findings that we derived from site\\ninspections with a focus on Industry 4.0. Moreover, our view on Industry 4.0\\nand important related aspects is elaborated. As a third contribution, we\\nillustrate our opinion on why data stream processing technologies could act as\\nan enabler for Industry 4.0 and point out possible obstacles on this way.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 22, 16, 1, 21, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Generalized Deletion Propagation on Counting Conjunctive Query Answers',\n",
       "  'authors': ['Debmalya Panigrahi', 'Shweta Patwa', 'Sudeepa Roy'],\n",
       "  'summary': 'We investigate the computational complexity of minimizing the source\\nside-effect in order to remove a given number of tuples from the output of a\\nconjunctive query. In particular, given a multi-relational database $D$, a\\nconjunctive query $Q$, and a positive integer $k$ as input, the goal is to find\\na minimum subset of input tuples to remove from D that would eliminate at least\\n$k$ output tuples from $Q(D)$. This problem generalizes the well-studied\\ndeletion propagation problem in databases. In addition, it encapsulates the\\nnotion of intervention for aggregate queries used in data analysis with\\napplications to explaining interesting observations on the output. We show a\\ndichotomy in the complexity of this problem for the class of full conjunctive\\nqueries without self-joins by giving a characterization on the structure of $Q$\\nthat makes the problem either polynomial-time solvable or NP-hard. Our proof of\\nthis dichotomy result already gives an exact algorithm in the easy cases; we\\ncomplement this by giving an approximation algorithm for the hard cases of the\\nproblem.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 23, 20, 56, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The sameAs Problem: A Survey on Identity Management in the Web of Data',\n",
       "  'authors': ['Joe Raad',\n",
       "   'Nathalie Pernelle',\n",
       "   'Fatiha Saïs',\n",
       "   'Wouter Beek',\n",
       "   'Frank van Harmelen'],\n",
       "  'summary': 'In a decentralised knowledge representation system such as the Web of Data,\\nit is common and indeed desirable for different knowledge graphs to overlap.\\nWhenever multiple names are used to denote the same thing, owl:sameAs\\nstatements are needed in order to link the data and foster reuse. Whilst the\\ndeductive value of such identity statements can be extremely useful in\\nenhancing various knowledge-based systems, incorrect use of identity can have\\nwide-ranging effects in a global knowledge space like the Web of Data. With\\nseveral works already proven that identity in the Web is broken, this survey\\ninvestigates the current state of this \"sameAs problem\". An open discussion\\nhighlights the main weaknesses suffered by solutions in the literature, and\\ndraws open challenges to be faced in the future.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 24, 15, 42, 22, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Semi Automatic Construction of ShEx and SHACL Schemas',\n",
       "  'authors': ['Iovka Boneva',\n",
       "   'Jérémie Dusart',\n",
       "   'Daniel Fernández Álvarez',\n",
       "   'Jose Emilio Labra Gayo'],\n",
       "  'summary': 'We present a method for the construction of SHACL or ShEx constraints for an\\nexisting RDF dataset. It has two components that are used conjointly: an\\nalgorithm for automatic schema construction, and an interactive workflow for\\nediting the schema. The schema construction algorithm takes as input sets of\\nsample nodes and constructs a shape constraint for every sample set. It can be\\nparametrized by a schema pattern that defines structural requirements for the\\nschema to be constructed. Schema patterns are also used to feed the algorithm\\nwith relevant information about the dataset coming from a domain expert or from\\nsome ontology. The interactive workflow provides useful information about the\\ndataset, shows validation results w.r.t. the schema under construction, and\\noffers schema editing operations that combined with the schema construction\\nalgorithm allow to build a complex ShEx or SHACL schema.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 24, 10, 47, 21, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Applying Constraint Logic Programming to SQL Semantic Analysis',\n",
       "  'authors': ['Fernando Sáenz-Pérez'],\n",
       "  'summary': 'This paper proposes the use of Constraint Logic Programming (CLP) to model\\nSQL queries in a data-independent abstract layer by focusing on some semantic\\nproperties for signalling possible errors in such queries. First, we define a\\ntranslation from SQL to Datalog, and from Datalog to CLP, so that solving this\\nCLP program will give information about inconsistency, tautology, and possible\\nsimplifications. We use different constraint domains which are mapped to SQL\\ntypes, and propose them to cooperate for improving accuracy. Our approach\\nleverages a deductive system that includes SQL and Datalog, and we present an\\nimplementation in this system which is currently being tested in classroom,\\nshowing its advantages and differences with respect to other approaches, as\\nwell as some performance data. This paper is under consideration for acceptance\\nin TPLP.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 25, 9, 19, 30, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'qwLSH: Cache-conscious Indexing for Processing Similarity Search Query Workloads in High-Dimensional Spaces',\n",
       "  'authors': ['Omid Jafari', 'John Ossorgin', 'Parth Nagarkar'],\n",
       "  'summary': 'Similarity search queries in high-dimensional spaces are an important type of\\nqueries in many domains such as image processing, machine learning, etc. Since\\nexact similarity search indexing techniques suffer from the well-known curse of\\ndimensionality in high-dimensional spaces, approximate search techniques are\\noften utilized instead. Locality Sensitive Hashing (LSH) has been shown to be\\nan effective approximate search method for solving similarity search queries in\\nhigh-dimensional spaces. Often times, queries in real-world settings arrive as\\npart of a query workload. LSH and its variants are particularly designed to\\nsolve single queries effectively. They suffer from one major drawback while\\nexecuting query workloads: they do not take into consideration important data\\ncharacteristics for effective cache utilization while designing the index\\nstructures. In this paper, we present qwLSH, an index structure for efficiently\\nprocessing similarity search query workloads in high-dimensional spaces. We\\nintelligently divide a given cache during processing of a query workload by\\nusing novel cost models. Experimental results show that, given a query\\nworkload, qwLSH is able to perform faster than existing techniques due to its\\nunique cost models and strategies.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 26, 22, 17, 7, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'sql4ml A declarative end-to-end workflow for machine learning',\n",
       "  'authors': ['Nantia Makrynioti', 'Ruy Ley-Wild', 'Vasilis Vassalos'],\n",
       "  'summary': 'We present sql4ml, a system for expressing supervised machine learning (ML)\\nmodels in SQL and automatically training them in TensorFlow. The primary\\nmotivation for this work stems from the observation that in many data science\\ntasks there is a back-and-forth between a relational database that stores the\\ndata and a machine learning framework. Data preprocessing and feature\\nengineering typically happen in a database, whereas learning is usually\\nexecuted in separate ML libraries. This fragmented workflow requires from the\\nusers to juggle between different programming paradigms and software systems.\\nWith sql4ml the user can express both feature engineering and ML algorithms in\\nSQL, while the system translates this code to an appropriate representation for\\ntraining inside a machine learning framework. We describe our translation\\nmethod, present experimental results from applying it on three well-known ML\\nalgorithms and discuss the usability benefits from concentrating the entire\\nworkflow on the database side.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 29, 13, 28, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Increasing Scalability of Process Mining using Event Dataframes: How Data Structure Matters',\n",
       "  'authors': ['Alessandro Berti'],\n",
       "  'summary': 'Process Mining is a branch of Data Science that aims to extract\\nprocess-related information from event data contained in information systems,\\nthat is steadily increasing in amount. Many algorithms, and a general-purpose\\nopen source framework (ProM 6), have been developed in the last years for\\nprocess discovery, conformance checking, machine learning on event data.\\nHowever, in very few cases scalability has been a target, prioritizing the\\nquality of the output over the execution speed and the optimization of\\nresources. This is making progressively more difficult to apply process mining\\nwith mainstream workstations on real-life event data with any open source\\nprocess mining framework. Hence, exploring more scalable storage techniques,\\nin-memory data structures, more performant algorithms is a strictly incumbent\\nneed. In this paper, we propose the usage of mainstream columnar storages and\\ndataframes to increase the scalability of process mining. These can replace the\\nclassic event log structures in most tasks, but require completely different\\nimplementations with regards to mainstream process mining algorithms.\\nDataframes will be defined, some algorithms on such structures will be\\npresented and their complexity will be calculated.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 30, 10, 0, 52, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Multi-Media Exchange Format for Time-Series Dataset Curation',\n",
       "  'authors': ['Philipp M. Scholl',\n",
       "   'Benjamin Völker',\n",
       "   'Bernd Becker',\n",
       "   'Kristof Van Laerhoven'],\n",
       "  'summary': 'Exchanging data as character-separated values (CSV) is slow, cumbersome and\\nerror-prone. Especially for time-series data, which is common in Activity\\nRecognition, synchronizing several independently recorded sensors is\\nchallenging. Adding second level evidence, like video recordings from multiple\\nangles and time-coded annotations, further complicates the matter of curating\\nsuch data. A possible alternative is to make use of standardized multi-media\\nformats. Sensor data can be encoded in audio format, and time-coded\\ninformation, like annotations, as subtitles. Video data can be added easily.\\nAll this media can be merged into a single container file, which makes the\\nissue of synchronization explicit. The incurred performance overhead by this\\nencoding is shown to be negligible and compression can be applied to optimize\\nstorage and transmission overhead.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 7, 12, 13, 54, 15, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Managing the Complexity of Processing Financial Data at Scale -- an Experience Report',\n",
       "  'authors': ['Sebastian Frischbier',\n",
       "   'Mario Paic',\n",
       "   'Alexander Echler',\n",
       "   'Christian Roth'],\n",
       "  'summary': \"Financial markets are extremely data-driven and regulated. Participants rely\\non notifications about significant events and background information that meet\\ntheir requirements regarding timeliness, accuracy, and completeness. As one of\\nEurope's leading providers of financial data and regulatory solutions vwd\\nprocesses a daily average of 18 billion notifications from 500+ data sources\\nfor 30 million symbols. Our large-scale geo-distributed systems handle daily\\npeak rates of 1+ million notifications/sec. In this paper we give practical\\ninsights about the different types of complexity we face regarding the data we\\nprocess, the systems we operate, and the regulatory constraints we must comply\\nwith. We describe the volume, variety, velocity, and veracity of the data we\\nprocess, the infrastructure we operate, and the architecture we apply. We\\nillustrate the load patterns created by trading and how the markets' attention\\nto the Brexit vote and similar events stressed our systems.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 8, 21, 27, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An Efficient Skyline Computation Framework',\n",
       "  'authors': ['Rui Liu', 'Dominique Li'],\n",
       "  'summary': 'Skyline computation aims at looking for the set of tuples that are not worse\\nthan any other tuples in all dimensions from a multidimensional database. In\\nthis paper, we present SDI (Skyline on Dimension Index), a dimension indexing\\nconducted general framework to skyline computation. We prove that to determine\\nwhether a tuple belongs to the skyline, it is enough to compare this tuple with\\na bounded subset of skyline tuples in an arbitrary dimensional index, but not\\nwith all existing skyline tuples. Base on SDI, we also show that any skyline\\ntuple can be used to stop the whole skyline computation process with outputting\\nthe complete set of all skyline tuples. We develop an efficient algorithm\\nSDI-RS that significantly reduces the skyline computation time, of which the\\nspace and time complexity can be guaranteed. Our experimental evaluation shows\\nthat SDI-RS outperforms the baseline algorithms in general and is especially\\nvery efficient on high-dimensional data.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 12, 10, 57, 1, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Linking Graph Entities with Multiplicity and Provenance',\n",
       "  'authors': ['Jixue Liu',\n",
       "   'Selasi Kwashie',\n",
       "   'Jiuyong Li',\n",
       "   'Lin Liu',\n",
       "   'Michael Bewong'],\n",
       "  'summary': 'Entity linking and resolution is a fundamental database problem with\\napplications in data integration, data cleansing, information retrieval,\\nknowledge fusion, and knowledge-base population. It is the task of accurately\\nidentifying multiple, differing, and possibly contradicting representations of\\nthe same real-world entity in data. In this work, we propose an entity linking\\nand resolution system capable of linking entities across different databases\\nand mentioned-entities extracted from text data. Our entity linking/resolution\\nsolution, called Certus, uses a graph model to represent the profiles of\\nentities. The graph model is versatile, thus, it is capable of handling\\nmultiple values for an attribute or a relationship, as well as the provenance\\ndescriptions of the values. Provenance descriptions of a value provide the\\nsettings of the value, such as validity periods, sources, security\\nrequirements, etc. This paper presents the architecture for the entity linking\\nsystem, the logical, physical, and indexing models used in the system, and the\\ngeneral linking process. Furthermore, we demonstrate the performance of update\\noperations of the physical storage models when the system is implemented in two\\nstate-of-the-art database management systems, HBase and Postgres.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 13, 2, 28, 37, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'On the Complexity of Checking Transactional Consistency',\n",
       "  'authors': ['Ranadeep Biswas', 'Constantin Enea'],\n",
       "  'summary': 'Transactions simplify concurrent programming by enabling computations on\\nshared data that are isolated from other concurrent computations and are\\nresilient to failures. Modern databases provide different consistency models\\nfor transactions corresponding to different tradeoffs between consistency and\\navailability. In this work, we investigate the problem of checking whether a\\ngiven execution of a transactional database adheres to some consistency model.\\nWe show that consistency models like read committed, read atomic, and causal\\nconsistency are polynomial time checkable while prefix consistency and snapshot\\nisolation are NP-complete in general. These results complement a previous\\nNP-completeness result concerning serializability. Moreover, in the context of\\nNP-complete consistency models, we devise algorithms that are polynomial time\\nassuming that certain parameters in the input executions, e.g., the number of\\nsessions, are fixed. We evaluate the scalability of these algorithms in the\\ncontext of several production databases.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 13, 6, 19, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Adaptive Learning of Aggregate Analytics under Dynamic Workloads',\n",
       "  'authors': ['Fotis Savva',\n",
       "   'Christos Anagnostopoulos',\n",
       "   'Peter Triantafillou'],\n",
       "  'summary': \"Large organizations have seamlessly incorporated data-driven decision making\\nin their operations. However, as data volumes increase, expensive big data\\ninfrastructures are called to rescue. In this setting, analytics tasks become\\nvery costly in terms of query response time, resource consumption, and money in\\ncloud deployments, especially when base data are stored across geographically\\ndistributed data centers. Therefore, we introduce an adaptive Machine Learning\\nmechanism which is light-weight, stored client-side, can estimate the answers\\nof a variety of aggregate queries and can avoid the big data backend. The\\nestimations are performed in milliseconds are inexpensive and accurate as the\\nmechanism learns from past analytical-query patterns. However, as analytic\\nqueries are ad-hoc and analysts' interests change over time we develop\\nsolutions that can swiftly and accurately detect such changes and adapt to new\\nquery patterns. The capabilities of our approach are demonstrated using\\nextensive evaluation with real and synthetic datasets.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 13, 17, 32, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards an Integrated Graph Algebra for Graph Pattern Matching with Gremlin (Extended Version)',\n",
       "  'authors': ['Harsh Thakkar',\n",
       "   'Dharmen Punjani',\n",
       "   'Soeren Auer',\n",
       "   'Maria-Esther Vidal'],\n",
       "  'summary': 'Graph data management (also called NoSQL) has revealed beneficial\\ncharacteristics in terms of flexibility and scalability by differently\\nbalancing between query expressivity and schema flexibility. This peculiar\\nadvantage has resulted into an unforeseen race of developing new task-specific\\ngraph systems, query languages and data models, such as property graphs,\\nkey-value, wide column, resource description framework (RDF), etc. Present-day\\ngraph query languages are focused towards flexible graph pattern matching (aka\\nsub-graph matching), whereas graph computing frameworks aim towards providing\\nfast parallel (distributed) execution of instructions. The consequence of this\\nrapid growth in the variety of graph-based data management systems has resulted\\nin a lack of standardization. Gremlin, a graph traversal language, and machine\\nprovides a common platform for supporting any graph computing system (such as\\nan OLTP graph database or OLAP graph processors). We present a formalization of\\ngraph pattern matching for Gremlin queries. We also study, discuss and\\nconsolidate various existing graph algebra operators into an integrated graph\\nalgebra.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 17, 9, 4, 2, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'LabelECG: A Web-based Tool for Distributed Electrocardiogram Annotation',\n",
       "  'authors': ['Zijian Ding',\n",
       "   'Shan Qiu',\n",
       "   'Yutong Guo',\n",
       "   'Jianping Lin',\n",
       "   'Li Sun',\n",
       "   'Dapeng Fu',\n",
       "   'Zhen Yang',\n",
       "   'Chengquan Li',\n",
       "   'Yang Yu',\n",
       "   'Long Meng',\n",
       "   'Tingting Lv',\n",
       "   'Dan Li',\n",
       "   'Ping Zhang'],\n",
       "  'summary': 'Electrocardiography plays an essential role in diagnosing and screening\\ncardiovascular diseases in daily healthcare. Deep neural networks have shown\\nthe potentials to improve the accuracies of arrhythmia detection based on\\nelectrocardiograms (ECGs). However, more ECG records with ground truth are\\nneeded to promote the development and progression of deep learning techniques\\nin automatic ECG analysis. Here we propose a web-based tool for ECG viewing and\\nannotating, LabelECG. With the facilitation of unified data management,\\nLabelECG is able to distribute large cohorts of ECGs to dozens of technicians\\nand physicians, who can simultaneously make annotations through web-browsers on\\nPCs, tablets and cell phones. Along with the doctors from four hospitals in\\nChina, we applied LabelECG to support the annotations of about 15,000 12-lead\\nresting ECG records in three months. These annotated ECGs have successfully\\nsupported the First China ECG intelligent Competition. La-belECG will be freely\\naccessible on the Internet to support similar researches, and will also be\\nupgraded through future works.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 19, 1, 36, 52, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'AFrame: Extending DataFrames for Large-Scale Modern Data Analysis (Extended Version)',\n",
       "  'authors': ['Phanwadee Sinthong', 'Michael J. Carey'],\n",
       "  'summary': 'Analyzing the increasingly large volumes of data that are available today,\\npossibly including the application of custom machine learning models, requires\\nthe utilization of distributed frameworks. This can result in serious\\nproductivity issues for \"normal\" data scientists. This paper introduces AFrame,\\na new scalable data analysis package powered by a Big Data management system\\nthat extends the data scientists\\' familiar DataFrame operations to efficiently\\noperate on managed data at scale. AFrame is implemented as a layer on top of\\nApache AsterixDB, transparently scaling out the execution of DataFrame\\noperations and machine learning model invocation through a parallel,\\nshared-nothing big data management system. AFrame incrementally constructs\\nSQL++ queries and leverages AsterixDB\\'s semistructured data management\\nfacilities, user-defined function support, and live data ingestion support. In\\norder to evaluate the proposed approach, this paper also introduces an\\nextensible micro-benchmark for use in evaluating DataFrame performance in both\\nsingle-node and distributed settings via a collection of representative\\nanalytic operations. This paper presents the architecture of AFrame, describes\\nthe underlying capabilities of AsterixDB that efficiently support modern data\\nanalytic operations, and utilizes the proposed benchmark to evaluate and\\ncompare the performance and support for large-scale data analyses provided by\\nalternative DataFrame libraries.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 19, 12, 0, 57, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards Efficient Discriminative Pattern Mining in Hybrid Domains',\n",
       "  'authors': ['Yoshitaka Kameya'],\n",
       "  'summary': 'Discriminative pattern mining is a data mining task in which we find patterns\\nthat distinguish transactions in the class of interest from those in other\\nclasses, and is also called emerging pattern mining or subgroup discovery. One\\npractical problem in discriminative pattern mining is how to handle numeric\\nvalues in the input dataset. In this paper, we propose an algorithm for\\ndiscriminative pattern mining that can deal with a transactional dataset in a\\nhybrid domain, i.e. the one that includes both symbolic and numeric values. We\\nalso show the execution results of a prototype implementation of the proposed\\nalgorithm for two standard benchmark datasets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 15, 13, 28, 50, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'On the Diversity of Memory and Storage Technologies',\n",
       "  'authors': ['Ismail Oukid', 'Lucas Lersch'],\n",
       "  'summary': 'The last decade has seen tremendous developments in memory and storage\\ntechnologies, starting with Flash Memory and continuing with the upcoming\\nStorage-Class Memories. Combined with an explosion of data processing, data\\nanalytics, and machine learning, this led to a segmentation of the memory and\\nstorage market. Consequently, the traditional storage hierarchy, as we know it\\ntoday, might be replaced by a multitude of storage hierarchies, with\\npotentially different depths, each tailored for specific workloads. In this\\ncontext, we explore in this \"Kurz Erkl\\\\\"art\" the state of memory technologies\\nand reflect on their future use with a focus on data management systems.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 20, 15, 28, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Improved Cardinality Estimation by Learning Queries Containment Rates',\n",
       "  'authors': ['Rojeh Hayek', 'Oded Shmueli'],\n",
       "  'summary': \"The containment rate of query Q1 in query Q2 over database D is the\\npercentage of Q1's result tuples over D that are also in Q2's result over D. We\\ndirectly estimate containment rates between pairs of queries over a specific\\ndatabase. For this, we use a specialized deep learning scheme, CRN, which is\\ntailored to representing pairs of SQL queries. Result-cardinality estimation is\\na core component of query optimization. We describe a novel approach for\\nestimating queries result-cardinalities using estimated containment rates among\\nqueries. This containment rate estimation may rely on CRN or embed, unchanged,\\nknown cardinality estimation methods. Experimentally, our novel approach for\\nestimating cardinalities, using containment rates between queries, on a\\nchallenging real-world database, realizes significant improvements to state of\\nthe art cardinality estimation methods.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 21, 7, 7, 27, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Efficient Join Processing Over Incomplete Data Streams (Technical Report)',\n",
       "  'authors': ['Weilong Ren', 'Xiang Lian', 'Kambiz Ghazinour'],\n",
       "  'summary': 'For decades, the join operator over fast data streams has always drawn much\\nattention from the database community, due to its wide spectrum of real-world\\napplications, such as online clustering, intrusion detection, sensor data\\nmonitoring, and so on. Existing works usually assume that the underlying\\nstreams to be joined are complete (without any missing values). However, this\\nassumption may not always hold, since objects from streams may contain some\\nmissing attributes, due to various reasons such as packet losses, network\\ncongestion/failure, and so on. In this paper, we formalize an important\\nproblem, namely join over incomplete data streams (Join-iDS), which retrieves\\njoining object pairs from incomplete data streams with high confidences. We\\ntackle the Join-iDS problem in the style of \"data imputation and query\\nprocessing at the same time\". To enable this style, we design an effective and\\nefficient cost-model-based imputation method via deferential dependency (DD),\\ndevise effective pruning strategies to reduce the Join-iDS search space, and\\npropose efficient algorithms via our proposed cost-model-based data\\nsynopsis/indexes. Extensive experiments have been conducted to verify the\\nefficiency and effectiveness of our proposed Join-iDS approach on both real and\\nsynthetic data sets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 23, 3, 39, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Revisiting Wedge Sampling for Budgeted Maximum Inner Product Search',\n",
       "  'authors': ['Stephan S. Lorenzen', 'Ninh Pham'],\n",
       "  'summary': 'Top-k maximum inner product search (MIPS) is a central task in many machine\\nlearning applications. This paper extends top-k MIPS with a budgeted setting,\\nthat asks for the best approximate top-k MIPS given a limit of B computational\\noperations. We investigate recent advanced sampling algorithms, including wedge\\nand diamond sampling to solve it. Though the design of these sampling schemes\\nnaturally supports budgeted top-k MIPS, they suffer from the linear cost from\\nscanning all data points to retrieve top-k results and the performance\\ndegradation for handling negative inputs.\\n  This paper makes two main contributions. First, we show that diamond sampling\\nis essentially a combination between wedge sampling and basic sampling for\\ntop-k MIPS. Our theoretical analysis and empirical evaluation show that wedge\\nis competitive (often superior) to diamond on approximating top-k MIPS\\nregarding both efficiency and accuracy. Second, we propose a series of\\nalgorithmic engineering techniques to deploy wedge sampling on budgeted top-k\\nMIPS. Our novel deterministic wedge-based algorithm runs significantly faster\\nthan the state-of-the-art methods for budgeted and exact top-k MIPS while\\nmaintaining the top-5 precision at least 80% on standard recommender system\\ndata sets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 23, 4, 5, 25, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Datalog Reasoning over Compressed RDF Knowledge Bases',\n",
       "  'authors': ['Pan Hu', 'Jacopo Urbani', 'Boris Motik', 'Ian Horrocks'],\n",
       "  'summary': 'Materialisation is often used in RDF systems as a preprocessing step to\\nderive all facts implied by given RDF triples and rules. Although widely used,\\nmaterialisation considers all possible rule applications and can use a lot of\\nmemory for storing the derived facts, which can hinder performance. We present\\na novel materialisation technique that compresses the RDF triples so that the\\nrules can sometimes be applied to multiple facts at once, and the derived facts\\ncan be represented using structure sharing. Our technique can thus require less\\nspace, as well as skip certain rule applications. Our experiments show that our\\ntechnique can be very effective: when the rules are relatively simple, our\\nsystem is both faster and requires less memory than prominent state-of-the-art\\nRDF systems.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 27, 13, 12, 21, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'DDSketch: A fast and fully-mergeable quantile sketch with relative-error guarantees',\n",
       "  'authors': ['Charles Masson', 'Jee E. Rim', 'Homin K. Lee'],\n",
       "  'summary': 'Summary statistics such as the mean and variance are easily maintained for\\nlarge, distributed data streams, but order statistics (i.e., sample quantiles)\\ncan only be approximately summarized. There is extensive literature on\\nmaintaining quantile sketches where the emphasis has been on bounding the rank\\nerror of the sketch while using little memory. Unfortunately, rank error\\nguarantees do not preclude arbitrarily large relative errors, and this often\\noccurs in practice when the data is heavily skewed. Given the distributed\\nnature of contemporary large-scale systems, another crucial property for\\nquantile sketches is mergeablility, i.e., several combined sketches must be as\\naccurate as a single sketch of the same data. We present the first\\nfully-mergeable, relative-error quantile sketching algorithm with formal\\nguarantees. The sketch is extremely fast and accurate, and is currently being\\nused by Datadog at a wide-scale.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 28, 12, 48, 44, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Parameter-free Structural Diversity Search',\n",
       "  'authors': ['Jinbin Huang', 'Xin Huang', 'Yuanyuan Zhu', 'Jianliang Xu'],\n",
       "  'summary': 'The problem of structural diversity search is to find the top-k vertices with\\nthe largest structural diversity in a graph. However, when identifying distinct\\nsocial contexts, existing structural diversity models (e.g., t-sized component,\\nt-core, and t-brace) are sensitive to an input parameter of t. To address this\\ndrawback, we propose a parameter-free structural diversity model. Specifically,\\nwe propose a novel notation of discriminative core, which automatically models\\nvarious kinds of social contexts without parameter t. Leveraging on\\ndiscriminative cores and h-index, the structural diversity score for a vertex\\nis calculated. We study the problem of parameter-free structural diversity\\nsearch in this paper. An efficient top-k search algorithm with a well-designed\\nupper bound for pruning is proposed. Extensive experiment results demonstrate\\nthe parameter sensitivity of existing t-core based model and verify the\\nsuperiority of our methods.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 30, 9, 36, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Parallel In-Memory Evaluation of Spatial Joins',\n",
       "  'authors': ['Dimitrios Tsitsigkos',\n",
       "   'Panagiotis Bouros',\n",
       "   'Nikos Mamoulis',\n",
       "   'Manolis Terrovitis'],\n",
       "  'summary': 'The spatial join is a popular operation in spatial database systems and its\\nevaluation is a well-studied problem. As main memories become bigger and faster\\nand commodity hardware supports parallel processing, there is a need to revamp\\nclassic join algorithms which have been designed for I/O-bound processing. In\\nview of this, we study the in-memory and parallel evaluation of spatial joins,\\nby re-designing a classic partitioning-based algorithm to consider alternative\\napproaches for space partitioning. Our study shows that, compared to a\\nstraightforward implementation of the algorithm, our tuning can improve\\nperformance significantly. We also show how to select appropriate partitioning\\nparameters based on data statistics, in order to tune the algorithm for the\\ngiven join inputs. Our parallel implementation scales gracefully with the\\nnumber of threads reducing the cost of the join to at most one second even for\\njoin inputs with tens of millions of rectangles.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 8, 30, 13, 48, 31, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'RCC: Resilient Concurrent Consensus for High-Throughput Secure Transaction Processing',\n",
       "  'authors': ['Suyash Gupta', 'Jelle Hellings', 'Mohammad Sadoghi'],\n",
       "  'summary': 'Recently, we saw the emergence of consensus-based database systems that\\npromise resilience against failures, strong data provenance, and federated data\\nmanagement. Typically, these fully-replicated systems are operated on top of a\\nprimary-backup consensus protocol, which limits the throughput of these systems\\nto the capabilities of a single replica (the primary). To push throughput\\nbeyond this single-replica limit, we propose concurrent consensus. In\\nconcurrent consensus, replicas independently propose transactions, thereby\\nreducing the influence of any single replica on performance. To put this idea\\nin practice, we propose our RCC paradigm that can turn any primary-backup\\nconsensus protocol into a concurrent consensus protocol by running many\\nconsensus instances concurrently. RCC is designed with performance in mind and\\nrequires minimal coordination between instances. Furthermore, RCC also promises\\nincreased resilience against failures. We put the design of RCC to the test by\\nimplementing it in ResilientDB, our high-performance resilient blockchain\\nfabric, and comparing it with state-of-the-art primary-backup consensus\\nprotocols. Our experiments show that RCC achieves up to 2.75x higher throughput\\nthan other consensus protocols and can be scaled to 91 replicas.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 3, 6, 3, 16, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Spatial-Temporal Cluster Relations -- A Foundation for Trajectory Cluster Lifetime Analysis',\n",
       "  'authors': ['Ivens Portugal', 'Paulo Alencar', 'Donald Cowan'],\n",
       "  'summary': 'Spatial-temporal data, that is information about objects that exist at a\\nparticular location and time period, are rich in value and, as a consequence,\\nthe target of so many initiative efforts. Clustering approaches aim at grouping\\ndatapoints based on similar properties for classification tasks. These\\napproaches have been widely used in domains such as human mobility, ecology,\\nhealth and astronomy. However, clustering approaches typically address only the\\nstatic nature of a cluster, and do not take into consideration its dynamic\\naspects. A desirable approach needs to investigate relations between dynamic\\nclusters and their elements that can be used to derive new insights about what\\nhappened to the clusters during their lifetimes. A fundamental step towards\\nthis goal is to provide a formal definition of spatial-temporal cluster\\nrelations. This report introduces, describes, and formalizes 14 novel\\nspatial-temporal cluster relations that may occur during the existence of a\\ncluster and involve both trajectory-cluster membership conditions and\\ncluster-cluster comparisons. We evaluate the proposed relations with a\\ndiscussion on how they are able to interpret complex cases that are difficult\\nto be distinguished without a formal relation specification. We conclude the\\nreport by summarizing our results and describing avenues for further research.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 5, 22, 0, 15, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Hybrid Approach To Hierarchical Density-based Cluster Selection',\n",
       "  'authors': ['Claudia Malzer', 'Marcus Baum'],\n",
       "  'summary': \"HDBSCAN is a density-based clustering algorithm that constructs a cluster\\nhierarchy tree and then uses a specific stability measure to extract flat\\nclusters from the tree. We show how the application of an additional threshold\\nvalue can result in a combination of DBSCAN* and HDBSCAN clusters, and\\ndemonstrate potential benefits of this hybrid approach when clustering data of\\nvariable densities. In particular, our approach is useful in scenarios where we\\nrequire a low minimum cluster size but want to avoid an abundance of\\nmicro-clusters in high-density regions. The method can directly be applied to\\nHDBSCAN's tree of cluster candidates and does not require any modifications to\\nthe hierarchy itself. It can easily be integrated as an addition to existing\\nHDBSCAN implementations.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 6, 9, 59, 56, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Optimizing Semi-Stream CACHEJOIN for Near-Real-Time Data Warehousing',\n",
       "  'authors': ['M. Asif Naeem', 'Erum Mehmood', 'M G Abbas', 'Noreen Jamil'],\n",
       "  'summary': 'Streaming data join is a critical process in the field of near-real-time data\\nwarehousing. For this purpose, an adaptive semi-stream join algorithm called\\nCACHEJOIN (Cache Join) focusing non-uniform stream data is provided in the\\nliterature. However, this algorithm cannot exploit the memory and CPU resources\\noptimally and consequently it leaves its service rate suboptimal due to\\nsequential execution of both of its phases, called stream-probing (SP) phase\\nand disk-probing (DP) phase. By integrating the advantages of CACHEJOIN, in\\nthis paper we present two modifications in it. First is called P-CACHEJOIN\\n(Parallel Cache Join) that enables the parallel processing of two phases in\\nCACHEJOIN. This increases number of joined stream records and therefore\\nimproves throughput considerably. Second is called OP-CACHEJOIN (Optimized\\nParallel Cache Join) that implements a parallel loading of stored data into\\nmemory while the DP phase is executing. We present the performance analysis of\\nboth of our approaches with existing CACHEJOIN empirically using synthetic\\nskewed dataset.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 10, 15, 11, 37, 56, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'PnyxDB: a Lightweight Leaderless Democratic Byzantine Fault Tolerant Replicated Datastore',\n",
       "  'authors': ['Loïck Bonniot', 'Christoph Neumann', 'François Taïani'],\n",
       "  'summary': 'Byzantine-Fault-Tolerant (BFT) systems are rapidly emerging as a viable\\ntechnology for production-grade systems, notably in closed consortia\\ndeployments for nancial and supply-chain applications. Unfortunately, most\\nalgorithms proposed so far to coordinate these systems suffer from substantial\\nscalability issues, and lack important features to implement Internet-scale\\ngovernance mechanisms. In this paper, we observe that many application\\nworkloads offer little concurrency, and propose PnyxDB, an\\neventually-consistent Byzantine Fault Tolerant replicated datastore that\\nexhibits both high scalability and low latency. Our approach is based on\\nconditional endorsements, that allow nodes to specify the set of transactions\\nthat must not be committed for the endorsement to be valid. In addition to its\\nhigh scalability, PnyxDB supports application-level voting, i.e. individual\\nnodes are able to endorse or reject a transaction according to\\napplication-defined policies without compromising consistency. We provide a\\ncomparison against BFTSMaRt and Tendermint, two competitors with different\\ndesign aims, and show that our implementation speeds up commit latencies by a\\nfactor of 11, remaining below 5 seconds in a worldwide geodistributed\\ndeployment of 180 nodes.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 8, 14, 43, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'EntropyDB: A Probabilistic Approach to Approximate Query Processing',\n",
       "  'authors': ['Laurel Orr', 'Magdalena Balazinska', 'Dan Suciu'],\n",
       "  'summary': 'We present EntropyDB, an interactive data exploration system that uses a\\nprobabilistic approach to generate a small, query-able summary of a dataset.\\nDeparting from traditional summarization techniques, we use the Principle of\\nMaximum Entropy to generate a probabilistic representation of the data that can\\nbe used to give approximate query answers. We develop the theoretical framework\\nand formulation of our probabilistic representation and show how to use it to\\nanswer queries. We then present solving techniques, give two critical\\noptimizations to improve preprocessing time and query execution time, and\\nexplore methods to reduce query error. Lastly, we experimentally evaluate our\\nwork using a 5 GB dataset of flights within the United States and a 210 GB\\ndataset from an astronomy particle simulation. While our current work only\\nsupports linear queries, we show that our technique can successfully answer\\nqueries faster than sampling while introducing, on average, no more error than\\nsampling and can better distinguish between rare and nonexistent values. We\\nalso discuss extensions that can allow for data updates and linear queries over\\njoins.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 9, 23, 15, 13, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Proceedings of the Third Workshop on Software Foundations for Data Interoperability (SFDI2019+), October 28, 2019, Fukuoka, Japan',\n",
       "  'authors': ['Soichiro Hidaka', 'Yasunori Ishihara', 'Zachary G. Ives'],\n",
       "  'summary': 'This volume contains the papers presented at the Third Workshop on Software\\nFoundations for Data Interoperability (SFDI2019+) held on October 28, 2019, in\\nFukuoka, co-located with the 11th Asia-Pasific Symposium on Internetware\\n(Internetware 2019). One regular paper and six short papers have been accepted\\nfor presentation, each of which has its unique ideas and/or interesting results\\non interoperability of autonomic distributed data. Moreover, SFDI2019+ featured\\ntwo keynote talks by Hiroyuki Seki (Nagoya University) and Zinovy Diskin\\n(McMaster University), which introduce concepts and directions novel to the\\nseries of SFDI workshops so far.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 14, 2, 32, 16, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Programmable View Update Strategies on Relations',\n",
       "  'authors': ['Van-Dang Tran', 'Hiroyuki Kato', 'Zhenjiang Hu'],\n",
       "  'summary': 'View update is an important mechanism that allows updates on a view by\\ntranslating them into the corresponding updates on the base relations. The\\nexisting literature has shown the ambiguity of translating view updates. To\\naddress this ambiguity, we propose a robust language-based approach for making\\nview update strategies programmable and validatable. Specifically, we introduce\\na novel approach to use Datalog to describe these update strategies. We propose\\na validation algorithm to check the well-behavedness of the written Datalog\\nprograms. We present a fragment of the Datalog language for which our\\nvalidation is both sound and complete. This fragment not only has good\\nproperties in theory but is also useful for solving practical view updates.\\nFurthermore, we develop an algorithm for optimizing user-written programs to\\nefficiently implement updatable views in relational database management\\nsystems. We have implemented our proposed approach. The experimental results\\nshow that our framework is feasible and efficient in practice.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 14, 3, 40, 32, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Sato: Contextual Semantic Type Detection in Tables',\n",
       "  'authors': ['Dan Zhang',\n",
       "   'Yoshihiko Suhara',\n",
       "   'Jinfeng Li',\n",
       "   'Madelon Hulsebos',\n",
       "   'Çağatay Demiralp',\n",
       "   'Wang-Chiew Tan'],\n",
       "  'summary': 'Detecting the semantic types of data columns in relational tables is\\nimportant for various data preparation and information retrieval tasks such as\\ndata cleaning, schema matching, data discovery, and semantic search. However,\\nexisting detection approaches either perform poorly with dirty data, support\\nonly a limited number of semantic types, fail to incorporate the table context\\nof columns or rely on large sample sizes for training data. We introduce Sato,\\na hybrid machine learning model to automatically detect the semantic types of\\ncolumns in tables, exploiting the signals from the context as well as the\\ncolumn values. Sato combines a deep learning model trained on a large-scale\\ntable corpus with topic modeling and structured prediction to achieve\\nsupport-weighted and macro average F1 scores of 0.925 and 0.735, respectively,\\nexceeding the state-of-the-art performance by a significant margin. We\\nextensively analyze the overall and per-type performance of Sato, discussing\\nhow individual modeling components, as well as feature categories, contribute\\nto its performance.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 14, 18, 51, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Learning Models over Relational Data: A Brief Tutorial',\n",
       "  'authors': ['Maximilian Schleich',\n",
       "   'Dan Olteanu',\n",
       "   'Mahmoud Abo-Khamis',\n",
       "   'Hung Q. Ngo',\n",
       "   'XuanLong Nguyen'],\n",
       "  'summary': 'This tutorial overviews the state of the art in learning models over\\nrelational databases and makes the case for a first-principles approach that\\nexploits recent developments in database research.\\n  The input to learning classification and regression models is a training\\ndataset defined by feature extraction queries over relational databases. The\\nmainstream approach to learning over relational data is to materialize the\\ntraining dataset, export it out of the database, and then learn over it using a\\nstatistical package. This approach can be expensive as it requires the\\nmaterialization of the training dataset. An alternative approach is to cast the\\nmachine learning problem as a database problem by transforming the\\ndata-intensive component of the learning task into a batch of aggregates over\\nthe feature extraction query and by computing this batch directly over the\\ninput database.\\n  The tutorial highlights a variety of techniques developed by the database\\ntheory and systems communities to improve the performance of the learning task.\\nThey rely on structural properties of the relational data and of the feature\\nextraction query, including algebraic (semi-ring), combinatorial (hypertree\\nwidth), statistical (sampling), or geometric (distance) structure. They also\\nrely on factorized computation, code specialization, query compilation, and\\nparallelization.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 15, 11, 50, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Concept-oriented model: Modeling and processing data using functions',\n",
       "  'authors': ['Alexandr Savinov'],\n",
       "  'summary': 'We describe a new logical data model, called the concept-oriented model\\n(COM). It uses mathematical functions as first-class constructs for data\\nrepresentation and data processing as opposed to using exclusively sets in\\nconventional set-oriented models. Functions and function composition are used\\nas primary semantic units for describing data connectivity instead of relations\\nand relation composition (join), respectively. Grouping and aggregation are\\nalso performed by using (accumulate) functions providing an alternative to\\ngroup-by and reduce operations. This model was implemented in an open source\\ndata processing toolkit examples of which are used to illustrate the model and\\nits operations. The main benefit of this model is that typical data processing\\ntasks become simpler and more natural when using functions in comparison to\\nadopting sets and set operations.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 17, 12, 38, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Using Mapping Languages for Building Legal Knowledge Graphs from XML Files',\n",
       "  'authors': ['Ademar Crotti Junior',\n",
       "   'Fabrizio Orlandi',\n",
       "   \"Declan O'Sullivan\",\n",
       "   'Christian Dirschl',\n",
       "   'Quentin Reul'],\n",
       "  'summary': 'This paper presents our experience on building RDF knowledge graphs for an\\nindustrial use case in the legal domain. The information contained in legal\\ninformation systems are often accessed through simple keyword interfaces and\\npresented as a simple list of hits. In order to improve search accuracy one may\\navail of knowledge graphs, where the semantics of the data can be made\\nexplicit. Significant research effort has been invested in the area of building\\nknowledge graphs from semi-structured text documents, such as XML, with the\\nprevailing approach being the use of mapping languages. In this paper, we\\npresent a semantic model for representing legal documents together with an\\nindustrial use case. We also present a set of use case requirements based on\\nthe proposed semantic model, which are used to compare and discuss the use of\\nstate-of-the-art mapping languages for building knowledge graphs for legal\\ndata.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 18, 14, 50, 31, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'PDBMine: A Reformulation of the Protein Data Bank to Facilitate Structural Data Mining',\n",
       "  'authors': ['Casey A Cole',\n",
       "   'Christopher Ott',\n",
       "   'Diego Valdes',\n",
       "   'Homayoun Valafar'],\n",
       "  'summary': 'Large scale initiatives such as the Human Genome Project, Structural\\nGenomics, and individual research teams have provided large deposits of genomic\\nand proteomic data. The transfer of data to knowledge has become one of the\\nexisting challenges, which is a consequence of capturing data in databases that\\nare optimally designed for archiving and not mining. In this research, we have\\ntargeted the Protein Databank (PDB) and demonstrated a transformation of its\\ncontent, named PDBMine, that reduces storage space by an order of magnitude,\\nand allows for powerful mining in relation to the topic of protein structure\\ndetermination. We have demonstrated the utility of PDBMine in exploring the\\nprevalence of dimeric and trimeric amino acid sequences and provided a\\nmechanism of predicting protein structure.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 19, 22, 25, 25, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Schemaless Queries over Document Tables with Dependencies',\n",
       "  'authors': ['Mustafa Canim',\n",
       "   'Cristina Cornelio',\n",
       "   'Arun Iyengar',\n",
       "   'Ryan Musa',\n",
       "   'Mariano Rodrigez Muro'],\n",
       "  'summary': 'Unstructured enterprise data such as reports, manuals and guidelines often\\ncontain tables. The traditional way of integrating data from these tables is\\nthrough a two-step process of table detection/extraction and mapping the table\\nlayouts to an appropriate schema. This can be an expensive process. In this\\npaper we show that by using semantic technologies (RDF/SPARQL and database\\ndependencies) paired with a simple but powerful way to transform tables with\\nnon-relational layouts, it is possible to offer query answering services over\\nthese tables with minimal manual work or domain-specific mappings. Our method\\nenables users to exploit data in tables embedded in documents with little\\neffort, not only for simple retrieval queries, but also for structured queries\\nthat require joining multiple interrelated tables.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 21, 9, 20, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Managing Variability in Relational Databases by VDBMS',\n",
       "  'authors': ['Parisa Ataei',\n",
       "   'Qiaoran Li',\n",
       "   'Eric Walkingshaw',\n",
       "   'Arash Termehchy'],\n",
       "  'summary': 'Variability inherently exists in databases in various contexts which creates\\ndatabase variants. For example, variants of a database could have different\\nschemas/content (database evolution problem), variants of a database could root\\nfrom different sources (data integration problem), variants of a database could\\nbe deployed differently for specific application domain (deploying a database\\nfor different configurations of a software system), etc. Unfortunately, while\\nthere are specific solutions to each of the problems arising in these contexts,\\nthere is no general solution that accounts for variability in databases and\\naddresses managing variability within a database. In this paper, we formally\\ndefine variational databases (VDBs) and statically-typed variational relational\\nalgebra (VRA) to query VDBs---both database and queries explicitly account for\\nvariation. We also design and implement variational database management system\\n(VDBMS) to run variational queries over a VDB effectively and efficiently. To\\nassess this, we generate two VDBs from real-world databases in the context of\\nsoftware development and database evolution with a set of experimental queries\\nfor each.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 25, 19, 33, 25, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Cracking In-Memory Database Index A Case Study for Adaptive Radix Tree Index',\n",
       "  'authors': ['Gang Wu',\n",
       "   'Yidong Song',\n",
       "   'Guodong Zhao',\n",
       "   'Wei Sun',\n",
       "   'Donghong Han',\n",
       "   'Baiyou Qiao',\n",
       "   'Guoren Wang',\n",
       "   'Ye Yuan'],\n",
       "  'summary': 'Indexes provide a method to access data in databases quickly. It can improve\\nthe response speed of subsequent queries by building a complete index in\\nadvance. However, it also leads to a huge overhead of the continuous updating\\nduring creating the index. An in-memory database usually has a higher query\\nprocessing performance than disk databases and is more suitable for real-time\\nquery processing. Therefore, there is an urgent need to reduce the index\\ncreation and update cost for in-memory databases. Database cracking technology\\nis currently recognized as an effective method to reduce the index\\ninitialization time. However, conventional cracking algorithms are focused on\\nsimple column data structure rather than those complex index structure for\\nin-memory databases. In order to show the feasibility of in-memory database\\nindex cracking and promote to future more extensive research, this paper\\nconducted a case study on the Adaptive Radix Tree (ART), a popular tree index\\nstructure of in-memory databases. On the basis of carefully examining the ART\\nindex construction overhead, an algorithm using auxiliary data structures to\\ncrack the ART index is proposed.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 26, 8, 0, 29, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Join Query Optimization with Deep Reinforcement Learning Algorithms',\n",
       "  'authors': ['Jonas Heitz', 'Kurt Stockinger'],\n",
       "  'summary': 'Join query optimization is a complex task and is central to the performance\\nof query processing. In fact it belongs to the class of NP-hard problems.\\nTraditional query optimizers use dynamic programming (DP) methods combined with\\na set of rules and restrictions to avoid exhaustive enumeration of all possible\\njoin orders. However, DP methods are very resource intensive. Moreover, given\\nsimplifying assumptions of attribute independence, traditional query optimizers\\nrely on erroneous cost estimations, which can lead to suboptimal query plans.\\nRecent success of deep reinforcement learning (DRL) creates new opportunities\\nfor the field of query optimization to tackle the above-mentioned problems. In\\nthis paper, we present our DRL-based Fully Observed Optimizer (FOOP) which is a\\ngeneric query optimization framework that enables plugging in different machine\\nlearning algorithms. The main idea of FOOP is to use a data-adaptive learning\\nquery optimizer that avoids exhaustive enumerations of join orders and is thus\\nsignificantly faster than traditional approaches based on dynamic programming.\\nIn particular, we evaluate various DRL-algorithms and show that Proximal Policy\\nOptimization significantly outperforms Q-learning based algorithms. Finally we\\ndemonstrate how ensemble learning techniques combined with DRL can further\\nimprove the query optimizer.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 26, 16, 48, 25, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Starling: A Scalable Query Engine on Cloud Function Services',\n",
       "  'authors': ['Matthew Perron',\n",
       "   'Raul Castro Fernandez',\n",
       "   'David DeWitt',\n",
       "   'Samuel Madden'],\n",
       "  'summary': 'Much like on-premises systems, the natural choice for running database\\nanalytics workloads in the cloud is to provision a cluster of nodes to run a\\ndatabase instance. However, analytics workloads are often bursty or low volume,\\nleaving clusters idle much of the time, meaning customers pay for compute\\nresources even when unused. The ability of cloud function services, such as AWS\\nLambda or Azure Functions, to run small, fine granularity tasks make them\\nappear to be a natural choice for query processing in such settings. But\\nimplementing an analytics system on cloud functions comes with its own set of\\nchallenges. These include managing hundreds of tiny stateless\\nresource-constrained workers, handling stragglers, and shuffling data through\\nopaque cloud services. In this paper we present Starling, a query execution\\nengine built on cloud function services that employs number of techniques to\\nmitigate these challenges, providing interactive query latency at a lower total\\ncost than provisioned systems with low-to-moderate utilization. In particular,\\non a 1TB TPC-H dataset in cloud storage, Starling is less expensive than the\\nbest provisioned systems for workloads when queries arrive 1 minute apart or\\nmore. Starling also has lower latency than competing systems reading from cloud\\nobject stores and can scale to larger datasets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 26, 18, 3, 16, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Mining Approximate Acyclic Schemes from Relations',\n",
       "  'authors': ['Batya Kenig',\n",
       "   'Pranay Mundra',\n",
       "   'Guna Prasad',\n",
       "   'Babak Salimi',\n",
       "   'Dan Suciu'],\n",
       "  'summary': 'Acyclic schemes have numerous applications in databases and in machine\\nlearning, such as improved design, more efficient storage, and increased\\nperformance for queries and machine learning algorithms. Multivalued\\ndependencies (MVDs) are the building blocks of acyclic schemes. The discovery\\nfrom data of both MVDs and acyclic schemes is more challenging than other forms\\nof data dependencies, such as Functional Dependencies, because these\\ndependencies do not hold on subsets of data, and because they are very\\nsensitive to noise in the data; for example a single wrong or missing tuple may\\ninvalidate the schema. In this paper we present Maimon, a system for\\ndiscovering approximate acyclic schemes and MVDs from data. We give a\\nprincipled definition of approximation, by using notions from information\\ntheory, then describe the two components of Maimon: mining for approximate\\nMVDs, then reconstructing acyclic schemes from approximate MVDs. We conduct an\\nexperimental evaluation of Maimon on 20 real-world datasets, and show that it\\ncan scale up to 1M rows, and up to 30 columns.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 29, 3, 9, 41, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'SOSD: A Benchmark for Learned Indexes',\n",
       "  'authors': ['Andreas Kipf',\n",
       "   'Ryan Marcus',\n",
       "   'Alexander van Renen',\n",
       "   'Mihail Stoian',\n",
       "   'Alfons Kemper',\n",
       "   'Tim Kraska',\n",
       "   'Thomas Neumann'],\n",
       "  'summary': 'A groundswell of recent work has focused on improving data management systems\\nwith learned components. Specifically, work on learned index structures has\\nproposed replacing traditional index structures, such as B-trees, with learned\\nmodels. Given the decades of research committed to improving index structures,\\nthere is significant skepticism about whether learned indexes actually\\noutperform state-of-the-art implementations of traditional structures on\\nreal-world data. To answer this question, we propose a new benchmarking\\nframework that comes with a variety of real-world datasets and baseline\\nimplementations to compare against. We also show preliminary results for\\nselected index structures, and find that learned models indeed often outperform\\nstate-of-the-art implementations, and are therefore a promising direction for\\nfuture research.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 29, 9, 35, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'HCA-DBSCAN: HyperCube Accelerated Density Based Spatial Clustering for Applications with Noise',\n",
       "  'authors': ['Vinayak Mathur', 'Jinesh Mehta', 'Sanjay Singh'],\n",
       "  'summary': 'Density-based clustering has found numerous applications across various\\ndomains. The Density-Based Spatial Clustering of Applications with Noise\\n(DBSCAN) algorithm is capable of finding clusters of varied shapes that are not\\nlinearly separable, at the same time it is not sensitive to outliers in the\\ndata. Combined with the fact that the number of clusters in the data are not\\nrequired apriori makes DBSCAN really powerfully. Slower performance (O(n2))\\nlimits its applications. In this work, we present a new clustering algorithm,\\nthe HyperCube Accelerated DBSCAN(HCA-DBSCAN) which uses a combination of\\ndistance-based aggregation by overlaying the data with customized grids. We use\\nrepresentative points to reduce the number of comparisons that need to be\\ncomputed. Experimental results show that the proposed algorithm achieves a\\nsignificant run time speedup of up to 58.27% when compared to other\\nimprovements that try to reduce the time complexity of theDBSCAN algorithm',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 1, 5, 42, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Area Queries Based on Voronoi Diagrams',\n",
       "  'authors': ['Yang Li'],\n",
       "  'summary': 'The area query, to find all elements contained in a specified area from a\\ncertain set of spatial objects, is a very important spatial query widely\\nrequired in various fields. A number of approaches have been proposed to\\nimplement this query, the best known of which is to obtain a rough candidate\\nset through spatial indexes and then refine the candidates through geometric\\nvalidations to get the final result. When the shape of the query area is a\\nrectangle, this method has very high efficiency. However, when the query area\\nis irregular, the candidate set is usually much larger than the final result\\nset, which means a lot of redundant detection needs to be done, thus the\\nefficiency is greatly limited. In view of this issue, we propose a method of\\niteratively generating candidates based on Voronoi diagrams and apply it to\\narea queries. The experimental results indicate that with our approach, the\\nnumber of candidates in the process of area query is greatly reduced and the\\nefficiency of the query is significantly improved.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 1, 15, 16, 46, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Prototype Selection Based on Clustering and Conformance Metrics for Model Discovery',\n",
       "  'authors': ['Mohammadreza Fani Sani',\n",
       "   'Mathilde Boltenhagen',\n",
       "   'Wil van der Aalst'],\n",
       "  'summary': 'Process discovery aims at automatically creating process models on the basis\\nof event data captured during the execution of business processes. Process\\ndiscovery algorithms tend to use all of the event data to discover a process\\nmodel. This attitude sometimes leads to discover imprecise and/or complex\\nprocess models that may conceal important information of processes. To address\\nthis problem, several techniques, from data filtering to model repair, have\\nbeen elaborated in the literature. In this paper, we introduce a new\\nincremental prototype selection algorithm based on clustering of process\\ninstances. The method aims to iteratively compute a unique process model with a\\ndifferent set of selected prototypes, i.e., representative of whole event data\\nand stops when conformance metrics decrease. The proposed method has been\\nimplemented in both the ProM and the RapidProM platforms. We applied the\\nproposed method on several real event data with state-of-the-art, process\\ndiscovery algorithms. Results show that using the proposed method leads to\\nimprove the general quality of discovered process models.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 11, 29, 14, 29, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Lambada: Interactive Data Analytics on Cold Data using Serverless Cloud Infrastructure',\n",
       "  'authors': ['Ingo Müller', 'Renato Marroquín', 'Gustavo Alonso'],\n",
       "  'summary': 'The promise of ultimate elasticity and operational simplicity of serverless\\ncomputing has recently lead to an explosion of research in this area. In the\\ncontext of data analytics, the concept sounds appealing, but due to the\\nlimitations of current offerings, there is no consensus yet on whether or not\\nthis approach is technically and economically viable. In this paper, we\\nidentify interactive data analytics on cold data as a use case where serverless\\ncomputing excels. We design and implement Lambada, a system following a purely\\nserverless architecture, in order to illustrate when and how serverless\\ncomputing should be employed for data analytics. We propose several system\\ncomponents that overcome the previously known limitations inherent in the\\nserverless paradigm as well as additional ones we identify in this work. We can\\nshow that, thanks to careful design, a serverless query processing system can\\nbe at the same time one order of magnitude faster and two orders of magnitude\\ncheaper compared to commercial Query-as-a-Service systems, the only alternative\\nwith similar operational simplicity.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 2, 17, 7, 43, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Ontologies for the Virtual Materials Marketplace',\n",
       "  'authors': ['Martin Thomas Horsch',\n",
       "   'Silvia Chiacchiera',\n",
       "   'Michael A. Seaton',\n",
       "   'Ilian T. Todorov',\n",
       "   'Karel Šindelka',\n",
       "   'Martin Lísal',\n",
       "   'Barbara Andreon',\n",
       "   'Esteban Bayro Kaiser',\n",
       "   'Gabriele Mogni',\n",
       "   'Gerhard Goldbeck',\n",
       "   'Ralf Kunze',\n",
       "   'Georg Summer',\n",
       "   'Andreas Fiseni',\n",
       "   'Hauke Brüning',\n",
       "   'Peter Schiffels',\n",
       "   'Welchy Leite Cavalcanti'],\n",
       "  'summary': 'The Virtual Materials Marketplace (VIMMP) project, which develops an open\\nplatform for providing and accessing services related to materials modelling,\\nis presented with a focus on its ontology development and data technology\\naspects. Within VIMMP, a system of marketplace-level ontologies is developed to\\ncharacterize services, models, and interactions between users; the European\\nMaterials and Modelling Ontology (EMMO) is employed as a top-level ontology.\\nThe ontologies are used to annotate data that are stored in the ZONTAL Space\\ncomponent of VIMMP and to support the ingest and retrieval of data and metadata\\nat the VIMMP marketplace frontend.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 3, 16, 48, 5, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Learning Multi-dimensional Indexes',\n",
       "  'authors': ['Vikram Nathan',\n",
       "   'Jialin Ding',\n",
       "   'Mohammad Alizadeh',\n",
       "   'Tim Kraska'],\n",
       "  'summary': 'Scanning and filtering over multi-dimensional tables are key operations in\\nmodern analytical database engines. To optimize the performance of these\\noperations, databases often create clustered indexes over a single dimension or\\nmulti-dimensional indexes such as R-trees, or use complex sort orders (e.g.,\\nZ-ordering). However, these schemes are often hard to tune and their\\nperformance is inconsistent across different datasets and queries. In this\\npaper, we introduce Flood, a multi-dimensional in-memory index that\\nautomatically adapts itself to a particular dataset and workload by jointly\\noptimizing the index structure and data storage. Flood achieves up to three\\norders of magnitude faster performance for range scans with predicates than\\nstate-of-the-art multi-dimensional indexes or sort orders on real-world\\ndatasets and workloads. Our work serves as a building block towards an\\nend-to-end learned database system.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 3, 20, 10, 31, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Directly Mapping RDF Databases to Property Graph Databases',\n",
       "  'authors': ['Renzo Angles', 'Harsh Thakkar', 'Dominik Tomaszuk'],\n",
       "  'summary': 'RDF triplestores and property graph databases are two approaches for data\\nmanagement which are based on modeling, storing, and querying graph-like data.\\nIn spite of such common principles, they present special features that\\ncomplicate the task of database interoperability. While there exist some\\nmethods to transform RDF graphs into property graphs, and vice versa, they lack\\ncompatibility and a solid formal foundation. This paper presents three direct\\nmappings (schema-dependent and schema-independent) for transforming an RDF\\ndatabase into a property graph database, including data and schema. We show\\nthat two of the proposed mappings satisfy the properties of semantics\\npreservation and information preservation. The existence of both mappings\\nallows us to conclude that the property graph data model subsumes the\\ninformation capacity of the RDF data model.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 4, 17, 20, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards Interpretable and Learnable Risk Analysis for Entity Resolution',\n",
       "  'authors': ['Zhaoqiang Chen',\n",
       "   'Qun Chen',\n",
       "   'Boyi Hou',\n",
       "   'Tianyi Duan',\n",
       "   'Zhanhuai Li',\n",
       "   'Guoliang Li'],\n",
       "  'summary': 'Machine-learning-based entity resolution has been widely studied. However,\\nsome entity pairs may be mislabeled by machine learning models and existing\\nstudies do not study the risk analysis problem -- predicting and interpreting\\nwhich entity pairs are mislabeled. In this paper, we propose an interpretable\\nand learnable framework for risk analysis, which aims to rank the labeled pairs\\nbased on their risks of being mislabeled. We first describe how to\\nautomatically generate interpretable risk features, and then present a\\nlearnable risk model and its training technique. Finally, we empirically\\nevaluate the performance of the proposed approach on real data. Our extensive\\nexperiments have shown that the learning risk model can identify the mislabeled\\npairs with considerably higher accuracy than the existing alternatives.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 6, 1, 59, 43, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'AutoBlock: A Hands-off Blocking Framework for Entity Matching',\n",
       "  'authors': ['Wei Zhang',\n",
       "   'Hao Wei',\n",
       "   'Bunyamin Sisman',\n",
       "   'Xin Luna Dong',\n",
       "   'Christos Faloutsos',\n",
       "   'David Page'],\n",
       "  'summary': 'Entity matching seeks to identify data records over one or multiple data\\nsources that refer to the same real-world entity. Virtually every entity\\nmatching task on large datasets requires blocking, a step that reduces the\\nnumber of record pairs to be matched. However, most of the traditional blocking\\nmethods are learning-free and key-based, and their successes are largely built\\non laborious human effort in cleaning data and designing blocking keys.\\n  In this paper, we propose AutoBlock, a novel hands-off blocking framework for\\nentity matching, based on similarity-preserving representation learning and\\nnearest neighbor search. Our contributions include: (a) Automation: AutoBlock\\nfrees users from laborious data cleaning and blocking key tuning. (b)\\nScalability: AutoBlock has a sub-quadratic total time complexity and can be\\neasily deployed for millions of records. (c) Effectiveness: AutoBlock\\noutperforms a wide range of competitive baselines on multiple large-scale,\\nreal-world datasets, especially when datasets are dirty and/or unstructured.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 7, 2, 42, 48, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Optimal Two-Sided Market Mechanism Design for Large-Scale Data Sharing and Trading in Massive IoT Networks',\n",
       "  'authors': ['Tao Zhang', 'Quanyan Zhu'],\n",
       "  'summary': 'The development of the Internet of Things (IoT) generates a significant\\namount of data that contains valuable knowledge for system operations and\\nbusiness opportunities. Since the data is the property of the IoT data owners,\\nthe access to the data requires permission from the data owners, which gives\\nrise to a potential market opportunity for the IoT data sharing and trading to\\ncreate economic values and market opportunities for both data owners and\\nbuyers. In this work, we leverage optimal mechanism design theory to develop a\\nmonopolist matching platform for data trading over massive IoT networks. The\\nproposed mechanism is composed of a pair of matching and payment rules for each\\nside of the market. We analyze the incentive compatibility of the market and\\ncharacterize the optimal mechanism with a class of cut-off matching rules for\\nboth welfare-maximization and revenue-maximization mechanisms and study three\\nmatching behaviors including complete-matched, bottom-eliminated, and\\ntop-reserved.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 12, 21, 35, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Drawbacks and Proposed Solutions for Real-time Processing on Existing State-of-the-art Locality Sensitive Hashing Techniques',\n",
       "  'authors': ['Omid Jafari', 'Khandker Mushfiqul Islam', 'Parth Nagarkar'],\n",
       "  'summary': 'Nearest-neighbor query processing is a fundamental operation for many image\\nretrieval applications. Often, images are stored and represented by\\nhigh-dimensional vectors that are generated by feature-extraction algorithms.\\nSince tree-based index structures are shown to be ineffective for high\\ndimensional processing due to the well-known \"Curse of Dimensionality\",\\napproximate nearest neighbor techniques are used for faster query processing.\\nLocality Sensitive Hashing (LSH) is a very popular and efficient approximate\\nnearest neighbor technique that is known for its sublinear query processing\\ncomplexity and theoretical guarantees. Nowadays, with the emergence of\\ntechnology, several diverse application domains require real-time\\nhigh-dimensional data storing and processing capacity. Existing LSH techniques\\nare not suitable to handle real-time data and queries. In this paper, we\\ndiscuss the challenges and drawbacks of existing LSH techniques for processing\\nreal-time high-dimensional image data. Additionally, through experimental\\nanalysis, we propose improvements for existing state-of-the-art LSH techniques\\nfor efficient processing of high-dimensional image data.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 15, 19, 4, 19, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Lauca: Generating Application-Oriented Synthetic Workloads',\n",
       "  'authors': ['Yuming Li',\n",
       "   'Rong Zhang',\n",
       "   'Yuchen Li',\n",
       "   'Ke Shu',\n",
       "   'Shuyan Zhang',\n",
       "   'Aoying Zhou'],\n",
       "  'summary': 'The synthetic workload is essential and critical to the performance\\nevaluation of database systems. When evaluating the database performance for a\\nspecific application, the similarity between synthetic workload and real\\napplication workload determines the credibility of evaluation results. However,\\nthe workload currently used for performance evaluation is difficult to have the\\nsame workload characteristics as the target application, which leads to\\ninaccurate evaluation results. To address this problem, we propose a workload\\nduplicator (Lauca) that can generate synthetic workloads with highly similar\\nperformance metrics for specific applications. To the best of our knowledge,\\nLauca is the first application-oriented transactional workload generator. By\\ncarefully studying the application-oriented synthetic workload generation\\nproblem, we present the key workload characteristics (transaction logic and\\ndata access distribution) of online transaction processing (OLTP) applications,\\nand propose novel workload characterization and generation algorithms, which\\nguarantee the high fidelity of synthetic workloads. We conduct extensive\\nexperiments using workloads from TPC-C, SmallBank and micro benchmarks on both\\nMySQL and PostgreSQL databases, and experimental results show that Lauca\\nconsistently generates high-quality synthetic workloads.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 16, 3, 13, 17, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Mosaic: A Sample-Based Database System for Open World Query Processing',\n",
       "  'authors': ['Laurel Orr',\n",
       "   'Samuel Ainsworth',\n",
       "   'Walter Cai',\n",
       "   'Kevin Jamieson',\n",
       "   'Magda Balazinska',\n",
       "   'Dan Suciu'],\n",
       "  'summary': 'Data scientists have relied on samples to analyze populations of interest for\\ndecades. Recently, with the increase in the number of public data repositories,\\nsample data has become easier to access. It has not, however, become easier to\\nanalyze. This sample data is arbitrarily biased with an unknown sampling\\nprobability, meaning data scientists must manually debias the sample with\\ncustom techniques to avoid inaccurate results. In this vision paper, we propose\\nMosaic, a database system that treats samples as first-class citizens and\\nallows users to ask questions over populations represented by these samples.\\nAnswering queries over biased samples is non-trivial as there is no existing,\\nstandard technique to answer population queries when the sampling probability\\nis unknown. In this paper, we show how our envisioned system solves this\\nproblem by having a unique sample-based data model with extensions to the SQL\\nlanguage. We propose how to perform population query answering using biased\\nsamples and give preliminary results for one of our novel query answering\\ntechniques.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 17, 1, 34, 5, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Querying Linked Data: An Experimental Evaluation of State-of-the-Art Interfaces',\n",
       "  'authors': ['Gabriela Montoya', 'Ilkcan Keles', 'Katja Hose'],\n",
       "  'summary': 'The adoption of Semantic Web technologies, and in particular the Open Data\\ninitiative, has contributed to the steady growth of the number of datasets and\\ntriples accessible on the Web. Most commonly, queries over RDF data are\\nevaluated over SPARQL endpoints. Recently, however, alternatives such as TPF\\nhave been proposed with the goal of shifting query processing load from the\\nserver running the SPARQL endpoint towards the client that issued the query.\\nAlthough these interfaces have been evaluated against standard benchmarks and\\ntestbeds that showed their benefits over previous work in general, a\\nfine-granular evaluation of what types of queries exploit the strengths of the\\ndifferent available interfaces has never been done. In this paper, we present\\nthe results of our in-depth evaluation of existing RDF interfaces. In addition,\\nwe also examine the influence of the backend on the performance of these\\ninterfaces. Using representative and diverse query loads based on the query log\\nof a public SPARQL endpoint, we stress test the different interfaces and\\nbackends and identify their strengths and weaknesses.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 17, 13, 45, 10, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'ORCA: a Benchmark for Data Web Crawlers',\n",
       "  'authors': ['Michael Röder',\n",
       "   'Geraldo de Souza',\n",
       "   'Denis Kuchelev',\n",
       "   'Abdelmoneim Amer Desouki',\n",
       "   'Axel-Cyrille Ngonga Ngomo'],\n",
       "  'summary': 'The number of RDF knowledge graphs available on the Web grows constantly.\\nGathering these graphs at large scale for downstream applications hence\\nrequires the use of crawlers. Although Data Web crawlers exist, and general Web\\ncrawlers could be adapted to focus on the Data Web, there is currently no\\nbenchmark to fairly evaluate their performance. Our work closes this gap by\\npresenting the Orca benchmark. Orca generates a synthetic Data Web, which is\\ndecoupled from the original Web and enables a fair and repeatable comparison of\\nData Web crawlers. Our evaluations show that Orca can be used to reveal the\\ndifferent advantages and disadvantages of existing crawlers. The benchmark is\\nopen-source and available at https://github.com/dice-group/orca.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 17, 14, 8, 1, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Detecting Incorrect Behavior of Cloud Databases as an Outsider',\n",
       "  'authors': ['Cheng Tan', 'Changgeng Zhao', 'Shuai Mu', 'Michael Walfish'],\n",
       "  'summary': 'Cloud DBs offer strong properties, including serializability, sometimes\\ncalled the gold standard database correctness property. But cloud DBs are\\ncomplicated black boxes, running in a different administrative domain from\\ntheir clients; thus, clients might like to know whether the DBs are meeting\\ntheir contract. A core difficulty is that the underlying problem here, namely\\nverifying serializability, is NP-complete. Nevertheless, we hypothesize that on\\nreal-world workloads, verifying serializability is tractable, and we treat the\\nquestion as a systems problem, for the first time. We build Cobra, which tames\\nthe underlying search problem by blending a new encoding of the problem,\\nhardware acceleration, and a careful choice of a suitable SMT solver. cobra\\nalso introduces a technique to address the challenge of garbage collection in\\nthis context. cobra improves over natural baselines by at least 10x in the\\nproblem size it can handle, while imposing modest overhead on clients.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 19, 5, 5, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Fast Mining of Spatial Frequent Wordset from Social Database',\n",
       "  'authors': ['Yongmi Lee', 'Kwang Woo Nam', 'Keun Ho Ryu'],\n",
       "  'summary': 'In this paper, we propose an algorithm that extracts spatial frequent\\npatterns to explain the relative characteristics of a specific location from\\nthe available social data. This paper proposes a spatial social data model\\nwhich includes spatial social data, spatial support, spatial frequent patterns,\\nspatial partition, and spatial clustering; these concepts are used for\\ndescribing the exploration algorithm of spatial frequent patterns. With these\\ndefined concepts as the foundation, an SFP-tree structure that maintains not\\nonly the frequent words but also the frequent cells was proposed, and an\\nSFP-growth algorithm that explores the frequent patterns on the basis of this\\nSFP-tree was proposed.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 19, 11, 8, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards Deterministic Decomposable Circuits for Safe Queries',\n",
       "  'authors': ['Mikaël Monet', 'Dan Olteanu'],\n",
       "  'summary': 'There exist two approaches for exact probabilistic inference of UCQs on\\ntuple-independent databases. In the extensional approach, query evaluation is\\nperformed within a DBMS by exploiting the structure of the query. In the\\nintensional approach, one first builds a representation of the lineage of the\\nquery on the database, then computes the probability of the lineage. In this\\npaper we propose a new technique to construct lineage representations as\\ndeterministic decomposable circuits in PTIME. The technique can apply to a\\nclass of UCQs that has been conjectured to separate the complexity of the two\\napproaches. We test our technique experimentally, and show that it succeeds on\\nall the queries of this class up to a certain size parameter, i.e., over $20$\\nmillion queries.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 23, 20, 37, 40, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Real Time Pattern Matching with Dynamic Normalization',\n",
       "  'authors': ['Renzhi Wu', 'Sergey Sukhanov', 'Christian Debes'],\n",
       "  'summary': 'Pattern matching in time series data streams is considered to be an essential\\ndata mining problem that still stays challenging for many practical scenarios.\\nDifferent factors such as noise, varying amplitude scale or shift, signal\\nstretches or shrinks in time are all leading to performance degradation of many\\nexisting pattern matching algorithms. In this paper, we introduce a dynamic\\nz-normalization mechanism allowing for proper signal scaling even under\\nsignificant time and amplitude distortions. Based on that, we further propose a\\nDynamic Time Warping-based real-time pattern matching method to recover hidden\\npatterns that can be distorted in both time and amplitude. We evaluate our\\nproposed method on synthetic and real-world scenarios under realistic\\nconditions demonstrating its high operational characteristics comparing to\\nother state-of-the-art pattern matching methods.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 27, 4, 2, 6, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The Impact of Negation on the Complexity of the Shapley Value in Conjunctive Queries',\n",
       "  'authors': ['Alon Reshef', 'Benny Kimelfeld', 'Ester Livshits'],\n",
       "  'summary': 'The Shapley value is a conventional and well-studied function for determining\\nthe contribution of a player to the coalition in a cooperative game. Among its\\napplications in a plethora of domains, it has recently been proposed to use the\\nShapley value for quantifying the contribution of a tuple to the result of a\\ndatabase query. In particular, we have a thorough understanding of the\\ntractability frontier for the class of Conjunctive Queries (CQs) and aggregate\\nfunctions over CQs. It has also been established that a tractable (randomized)\\nmultiplicative approximation exists for every union of CQs. Nevertheless, all\\nof these results are based on the monotonicity of CQs. In this work, we\\ninvestigate the implication of negation on the complexity of Shapley\\ncomputation, in both the exact and approximate senses. We generalize a known\\ndichotomy to account for negated atoms. We also show that negation\\nfundamentally changes the complexity of approximation. We do so by drawing a\\nconnection to the problem of deciding whether a tuple is \"relevant\" to a query,\\nand by analyzing its complexity.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2019, 12, 29, 8, 55, 27, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'ResilientDB: Global Scale Resilient Blockchain Fabric',\n",
       "  'authors': ['Suyash Gupta',\n",
       "   'Sajjad Rahnama',\n",
       "   'Jelle Hellings',\n",
       "   'Mohammad Sadoghi'],\n",
       "  'summary': 'Recent developments in blockchain technology have inspired innovative new\\ndesigns in resilient distributed and database systems. At their core, these\\nblockchain applications typically use Byzantine fault-tolerant consensus\\nprotocols to maintain a common state across all replicas, even if some replicas\\nare faulty or malicious. Unfortunately, existing consensus protocols are not\\ndesigned to deal with geo-scale deployments in which many replicas spread\\nacross a geographically large area participate in consensus. To address this,\\nwe present the Geo-Scale Byzantine FaultTolerant consensus protocol (GeoBFT).\\nGeoBFT is designed for excellent scalability by using a topological-aware\\ngrouping of replicas in local clusters, by introducing parallelization of\\nconsensus at the local level, and by minimizing communication between clusters.\\nTo validate our vision of high-performance geo-scale resilient distributed\\nsystems, we implement GeoBFT in our efficient ResilientDB permissioned\\nblockchain fabric. We show that GeoBFT is not only sound and provides great\\nscalability, but also outperforms state-of-the-art consensus protocols by a\\nfactor of six in geo-scale deployments.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 1, 7, 20, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Providing Insights for Queries affected by Failures and Stragglers',\n",
       "  'authors': ['Bruhathi Sundarmurthy',\n",
       "   'Harshad Deshmukh',\n",
       "   'Paris Koutris',\n",
       "   'Jeffrey Naughton'],\n",
       "  'summary': 'Interactive time responses are a crucial requirement for users analyzing\\nlarge amounts of data. Such analytical queries are typically run in a\\ndistributed setting, with data being sharded across thousands of nodes for high\\nthroughput. However, providing real-time analytics is still a very big\\nchallenge; with data distributed across thousands of nodes, the probability\\nthat some of the required nodes are unavailable or very slow during query\\nexecution is very high and unavailability may result in slow execution or even\\nfailures. The sheer magnitude of data and users increase resource contention\\nand this exacerbates the phenomenon of stragglers and node failures during\\nexecution. In this paper, we propose a novel solution to alleviate the\\nstraggler/failure problem that exploits existing efficient partitioning\\nproperties of the data, particularly, co-hash partitioned data, and provides\\napproximate answers along with confidence bounds to queries affected by\\nfailed/straggler nodes. We consider aggregate queries that involve joins, group\\nbys, having clauses and a subclass of nested subqueries. Finally, we validate\\nour approach through extensive experiments on the TPC-H dataset.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 4, 20, 52, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A workload-adaptive mechanism for linear queries under local differential privacy',\n",
       "  'authors': ['Ryan McKenna',\n",
       "   'Raj Kumar Maity',\n",
       "   'Arya Mazumdar',\n",
       "   'Gerome Miklau'],\n",
       "  'summary': 'We propose a new mechanism to accurately answer a user-provided set of linear\\ncounting queries under local differential privacy (LDP). Given a set of linear\\ncounting queries (the workload) our mechanism automatically adapts to provide\\naccuracy on the workload queries. We define a parametric class of mechanisms\\nthat produce unbiased estimates of the workload, and formulate a constrained\\noptimization problem to select a mechanism from this class that minimizes\\nexpected total squared error. We solve this optimization problem numerically\\nusing projected gradient descent and provide an efficient implementation that\\nscales to large workloads. We demonstrate the effectiveness of our\\noptimization-based approach in a wide variety of settings, showing that it\\noutperforms many competitors, even outperforming existing mechanisms on the\\nworkloads for which they were intended.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 5, 0, 10, 54, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Observations on Porting In-memory KV stores to Persistent Memory',\n",
       "  'authors': ['Brian Choi', 'Parv Saxena', 'Ryan Huang', 'Randal Burns'],\n",
       "  'summary': 'Systems that require high-throughput and fault tolerance, such as key-value\\nstores and databases, are looking to persistent memory to combine the\\nperformance of in-memory systems with the data-consistent fault-tolerance of\\nnonvolatile stores. Persistent memory devices provide fast bytea-ddressable\\naccess to non-volatile memory. We analyze the design space when integrating\\npersistent memory into in-memory key value stores and quantify performance\\ntradeoffs between throughput, latency, and and recovery time. Previous works\\nhave explored many design choices, but did not quantify the tradeoffs. We\\nimplement persistent memory support in Redis and Memcached, adapting the data\\nstructures of each to work in two modes: (1) with all data in persistent memory\\nand (2) a hybrid mode that uses persistent memory for key/value data and\\nnon-volatile memory for indexing and metadata. Our experience reveals three\\nactionable design principles that hold in Redis and Memcached, despite their\\nvery different implementations. We conclude that the hybrid design increases\\nthroughput and decreases latency at a minor cost in recovery time and code\\ncomplexity',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 5, 22, 11, 45, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Storyboard: Optimizing Precomputed Summaries for Aggregation',\n",
       "  'authors': ['Edward Gan', 'Peter Bailis', 'Moses Charikar'],\n",
       "  'summary': 'An emerging class of data systems partition their data and precompute\\napproximate summaries (i.e., sketches and samples) for each segment to reduce\\nquery costs. They can then aggregate and combine the segment summaries to\\nestimate results without scanning the raw data. However, given limited storage\\nspace each summary introduces approximation errors that affect query accuracy.\\nFor instance, systems that use existing mergeable summaries cannot reduce query\\nerror below the error of an individual precomputed summary. We introduce\\nStoryboard, a query system that optimizes item frequency and quantile summaries\\nfor accuracy when aggregating over multiple segments. Compared to conventional\\nmergeable summaries, Storyboard leverages additional memory available for\\nsummary construction and aggregation to derive a more precise combined result.\\nThis reduces error by up to 25x over interval aggregations and 4.4x over data\\ncube aggregations on industrial datasets compared to standard summarization\\nmethods, with provable worst-case error guarantees.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 8, 1, 35, 50, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Optimization of Retrieval Algorithms on Large Scale Knowledge Graphs',\n",
       "  'authors': ['Jens Dörpinghaus', 'Andreas Stefan'],\n",
       "  'summary': 'Knowledge graphs have been shown to play an important role in recent\\nknowledge mining and discovery, for example in the field of life sciences or\\nbioinformatics. Although a lot of research has been done on the field of query\\noptimization, query transformation and of course in storing and retrieving\\nlarge scale knowledge graphs the field of algorithmic optimization is still a\\nmajor challenge and a vital factor in using graph databases. Few researchers\\nhave addressed the problem of optimizing algorithms on large scale labeled\\nproperty graphs. Here, we present two optimization approaches and compare them\\nwith a naive approach of directly querying the graph database. The aim of our\\nwork is to determine limiting factors of graph databases like Neo4j and we\\ndescribe a novel solution to tackle these challenges. For this, we suggest a\\nclassification schema to differ between the complexity of a problem on a graph\\ndatabase. We evaluate our optimization approaches on a test system containing a\\nknowledge graph derived biomedical publication data enriched with text mining\\ndata. This dense graph has more than 71M nodes and 850M relationships. The\\nresults are very encouraging and - depending on the problem - we were able to\\nshow a speedup of a factor between 44 and 3839.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 10, 12, 37, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'PushdownDB: Accelerating a DBMS using S3 Computation',\n",
       "  'authors': ['Xiangyao Yu',\n",
       "   'Matt Youill',\n",
       "   'Matthew Woicik',\n",
       "   'Abdurrahman Ghanem',\n",
       "   'Marco Serafini',\n",
       "   'Ashraf Aboulnaga',\n",
       "   'Michael Stonebraker'],\n",
       "  'summary': 'This paper studies the effectiveness of pushing parts of DBMS analytics\\nqueries into the Simple Storage Service (S3) engine of Amazon Web Services\\n(AWS), using a recently released capability called S3 Select. We show that some\\nDBMS primitives (filter, projection, aggregation) can always be\\ncost-effectively moved into S3. Other more complex operations (join, top-K,\\ngroup-by) require reimplementation to take advantage of S3 Select and are often\\ncandidates for pushdown. We demonstrate these capabilities through\\nexperimentation using a new DBMS that we developed, PushdownDB. Experimentation\\nwith a collection of queries including TPC-H queries shows that PushdownDB is\\non average 30% cheaper and 6.7X faster than a baseline that does not use S3\\nSelect.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 14, 1, 23, 54, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Online Process Monitoring Using Incremental State-Space Expansion: An Exact Algorithm',\n",
       "  'authors': ['Daniel Schuster', 'Sebastiaan J. van Zelst'],\n",
       "  'summary': \"The execution of (business) processes generates valuable traces of event data\\nin the information systems employed within companies. Recently, approaches for\\nmonitoring the correctness of the execution of running processes have been\\ndeveloped in the area of process mining, i.e., online conformance checking. The\\nadvantages of monitoring a process' conformity during its execution are clear,\\ni.e., deviations are detected as soon as they occur and countermeasures can\\nimmediately be initiated to reduce the possible negative effects caused by\\nprocess deviations. Existing work in online conformance checking only allows\\nfor obtaining approximations of non-conformity, e.g., overestimating the actual\\nseverity of the deviation. In this paper, we present an exact, parameter-free,\\nonline conformance checking algorithm that computes conformance checking\\nresults on the fly. Our algorithm exploits the fact that the conformance\\nchecking problem can be reduced to a shortest path problem, by incrementally\\nexpanding the search space and reusing previously computed intermediate\\nresults. Our experiments show that our algorithm outperforms comparable\\nstate-of-the-art approximation algorithms.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 14, 10, 10, 43, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Cleaning Denial Constraint Violations through Relaxation',\n",
       "  'authors': ['Stella Giannakopoulou',\n",
       "   'Manos Karpathiotakis',\n",
       "   'Anastasia Ailamaki'],\n",
       "  'summary': 'Data cleaning is a time-consuming process that depends on the data analysis\\nthat users perform. Existing solutions treat data cleaning as a separate\\noffline process that takes place before analysis begins. Applying data cleaning\\nbefore analysis assumes a priori knowledge of the inconsistencies and the query\\nworkload, thereby requiring effort on understanding and cleaning the data that\\nis unnecessary for the analysis. We propose an approach that performs\\nprobabilistic repair of denial constraint violations on-demand, driven by the\\nexploratory analysis that users perform. We introduce Daisy, a system that\\nseamlessly integrates data cleaning into the analysis by relaxing query\\nresults. Daisy executes analytical query-workloads over dirty data by weaving\\ncleaning operators into the query plan. Our evaluation shows that Daisy adapts\\nto the workload and outperforms traditional offline cleaning on both synthetic\\nand real-world workloads.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 14, 18, 27, 37, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The Complexity of Aggregates over Extractions by Regular Expressions',\n",
       "  'authors': ['Johannes Doleschal', 'Benny Kimelfeld', 'Wim Martens'],\n",
       "  'summary': 'Regular expressions with capture variables, also known as regex-formulas,\\nextract relations of spans (intervals identified by their start and end\\nindices) from text. In turn, the class of regular document spanners is the\\nclosure of the regex formulas under the Relational Algebra. We investigate the\\ncomputational complexity of querying text by aggregate functions, such as sum,\\naverage, and quantile, on top of regular document spanners. To this end, we\\nformally define aggregate functions over regular document spanners and analyze\\nthe computational complexity of exact and approximate computation. More\\nprecisely, we show that in a restricted case, all studied aggregate functions\\ncan be computed in polynomial time. In general, however, even though exact\\ncomputation is intractable, some aggregates can still be approximated with\\nfully polynomial-time randomized approximation schemes (FPRAS).',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 20, 16, 7, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Facilitating SQL Query Composition and Analysis',\n",
       "  'authors': ['Zainab Zolaktaf', 'Mostafa Milani', 'Rachel Pottinger'],\n",
       "  'summary': 'Formulating efficient SQL queries requires several cycles of tuning and\\nexecution, particularly for inexperienced users. We examine methods that can\\naccelerate and improve this interaction by providing insights about SQL queries\\nprior to execution. We achieve this by predicting properties such as the query\\nanswer size, its run-time, and error class. Unlike existing approaches, our\\napproach does not rely on any statistics from the database instance or query\\nexecution plans. This is particularly important in settings with limited access\\nto the database instance. Our approach is based on using data-driven machine\\nlearning techniques that rely on large query workloads to model SQL queries and\\ntheir properties. We evaluate the utility of neural network models and\\ntraditional machine learning models. We use two real-world query workloads: the\\nSloan Digital Sky Survey (SDSS) and the SQLShare query workload. Empirical\\nresults show that the neural network models are more accurate in predicting the\\nquery error class, achieving a higher F-measure on classes with fewer samples\\nas well as performing better on other problems such as run-time and answer size\\nprediction. These results are encouraging and confirm that SQL query workloads\\nand data-driven machine learning methods can be leveraged to facilitate query\\ncomposition and analysis.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 21, 2, 10, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Star Pattern Fragments: Accessing Knowledge Graphs through Star Patterns',\n",
       "  'authors': ['Christian Aebeloe',\n",
       "   'Ilkcan Keles',\n",
       "   'Gabriela Montoya',\n",
       "   'Katja Hose'],\n",
       "  'summary': 'The Semantic Web offers access to a vast Web of interlinked information\\naccessible via SPARQL endpoints. Such endpoints offer a well-defined interface\\nto retrieve results for complex SPARQL queries. The computational load for\\nprocessing such SPARQL endpoints offer access to a vast amount of interlinked\\ninformation. While they offer a well-defined interface for efficiently\\nretrieving results for complex SPARQL queries, complex query loads can easily\\noverload or crash endpoints as all the computational load of answering the\\nqueries resides entirely with the server hosting the endpoint. Recently\\nproposed interfaces, such as Triple Pattern Fragments, have therefore shifted\\nsome of the query processing load from the server to the client at the expense\\nof increased network traffic in the case of non-selective triple patterns. This\\npaper therefore proposes Star Pattern Fragments (SPF), an RDF interface\\nenabling a better load balancing between server and client by decomposing\\nSPARQL queries into star-shaped subqueries, evaluating them on the server side.\\nExperiments using synthetic data (WatDiv), as well as real data (DBpedia), show\\nthat SPF does not only significantly reduce network traffic, it is also up to\\ntwo orders of magnitude faster than the state-of-the-art interfaces under high\\nquery load.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 21, 7, 59, 22, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Crowdsourced Collective Entity Resolution with Relational Match Propagation',\n",
       "  'authors': ['Jiacheng Huang', 'Wei Hu', 'Zhifeng Bao', 'Yuzhong Qu'],\n",
       "  'summary': 'Knowledge bases (KBs) store rich yet heterogeneous entities and facts. Entity\\nresolution (ER) aims to identify entities in KBs which refer to the same\\nreal-world object. Recent studies have shown significant benefits of involving\\nhumans in the loop of ER. They often resolve entities with pairwise similarity\\nmeasures over attribute values and resort to the crowds to label uncertain\\nones. However, existing methods still suffer from high labor costs and\\ninsufficient labeling to some extent. In this paper, we propose a novel\\napproach called crowdsourced collective ER, which leverages the relationships\\nbetween entities to infer matches jointly rather than independently.\\nSpecifically, it iteratively asks human workers to label picked entity pairs\\nand propagates the labeling information to their neighbors in distance. During\\nthis process, we address the problems of candidate entity pruning,\\nprobabilistic propagation, optimal question selection and error-tolerant truth\\ninference. Our experiments on real-world datasets demonstrate that, compared\\nwith state-of-the-art methods, our approach achieves superior accuracy with\\nmuch less labeling.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 21, 15, 33, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Toolkit for Generating Code Knowledge Graphs',\n",
       "  'authors': ['Ibrahim Abdelaziz',\n",
       "   'Julian Dolby',\n",
       "   'Jamie McCusker',\n",
       "   'Kavitha Srinivas'],\n",
       "  'summary': 'Knowledge graphs have been proven extremely useful in powering diverse\\napplications in semantic search and natural language understanding. In this\\npaper, we present GraphGen4Code, a toolkit to build code knowledge graphs that\\ncan similarly power various applications such as program search, code\\nunderstanding, bug detection, and code automation. GraphGen4Code uses generic\\ntechniques to capture code semantics with the key nodes in the graph\\nrepresenting classes, functions, and methods. Edges indicate function usage\\n(e.g., how data flows through function calls, as derived from program analysis\\nof real code), and documentation about functions (e.g., code documentation,\\nusage documentation, or forum discussions such as StackOverflow). Our toolkit\\nuses named graphs in RDF to model graphs per program, or can output graphs as\\nJSON. We show the scalability of the toolkit by applying it to 1.3 million\\nPython files drawn from GitHub, 2,300 Python modules, and 47 million forum\\nposts. This results in an integrated code graph with over 2 billion triples. We\\nmake the toolkit to build such graphs as well as the sample extraction of the 2\\nbillion triples graph publicly available to the community for use.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 21, 17, 40, 20, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Sample Debiasing in the Themis Open World Database System (Extended Version)',\n",
       "  'authors': ['Laurel Orr', 'Magda Balazinska', 'Dan Suciu'],\n",
       "  'summary': 'Open world database management systems assume tuples not in the database\\nstill exist and are becoming an increasingly important area of research. We\\npresent Themis, the first open world database that automatically rebalances\\narbitrarily biased samples to approximately answer queries as if they were\\nissued over the entire population. We leverage apriori population aggregate\\ninformation to develop and combine two different approaches for automatic\\ndebiasing: sample reweighting and Bayesian network probabilistic modeling. We\\nbuild a prototype of Themis and demonstrate that Themis achieves higher query\\naccuracy than the default AQP approach, an alternative sample reweighting\\ntechnique, and a variety of Bayesian network models while maintaining\\ninteractive query response times. We also show that \\\\name is robust to\\ndifferences in the support between the sample and population, a key use case\\nwhen using social media samples.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 23, 0, 53, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A metric Suite for Systematic Quality Assessment of Linked Open Data',\n",
       "  'authors': ['Behshid Behkamal',\n",
       "   'Moshen Kahani',\n",
       "   'Ebrahim Bagheri',\n",
       "   'Majid Sazvar'],\n",
       "  'summary': 'Abstract- The vision of the Linked Open Data (LOD) initiative is to provide a\\ndistributed model for publishing and meaningfully interlinking open data. The\\nrealization of this goal depends strongly on the quality of the data that is\\npublished as a part of the LOD. This paper focuses on the systematic quality\\nassessment of datasets prior to publication on the LOD cloud. To this end, we\\nidentify important quality deficiencies that need to be avoided and/or resolved\\nprior to the publication of a dataset. We then propose a set of metrics to\\nmeasure these quality deficiencies in a dataset. This way, we enable the\\nassessment and identification of undesirable quality characteristics of a\\ndataset through our proposed metrics. This will help publishers to filter out\\nlow-quality data based on the quality assessment results, which in turn enables\\ndata consumers to make better and more informed decisions when using the open\\ndatasets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 25, 6, 2, 7, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Revisiting compact RDF stores based on k2-trees',\n",
       "  'authors': ['Nieves R. Brisaboa',\n",
       "   'Ana Cerdeira-Pena',\n",
       "   'Guillermo de Bernardo',\n",
       "   'Antonio Fariña'],\n",
       "  'summary': 'We present a new compact representation to efficiently store and query large\\nRDF datasets in main memory. Our proposal, called BMatrix, is based on the\\nk2-tree, a data structure devised to represent binary matrices in a compressed\\nway, and aims at improving the results of previous state-of-the-art\\nalternatives, especially in datasets with a relatively large number of\\npredicates. We introduce our technique, together with some improvements on the\\nbasic k2-tree that can be applied to our solution in order to boost\\ncompression. Experimental results in the flagship RDF dataset DBPedia show that\\nour proposal achieves better compression than existing alternatives, while\\nyielding competitive query times, particularly in the most frequent triple\\npatterns and in queries with unbound predicate, in which we outperform existing\\nsolutions.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 26, 17, 3, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Fast Join Project Query Evaluation using Matrix Multiplication',\n",
       "  'authors': ['Shaleen Deep', 'Xiao Hu', 'Paraschos Koutris'],\n",
       "  'summary': 'In the last few years, much effort has been devoted to developing join\\nalgorithms in order to achieve worst-case optimality for join queries over\\nrelational databases. Towards this end, the database community has had\\nconsiderable success in developing succinct algorithms that achieve worst-case\\noptimal runtime for full join queries, i.e the join is over all variables\\npresent in the input database. However, not much is known about join evaluation\\nwith {\\\\em projections} beyond some simple techniques of pushing down the\\nprojection operator in the query execution plan. Such queries have a large\\nnumber of applications in entity matching, graph analytics and searching over\\ncompressed graphs. In this paper, we study how a class of join queries with\\nprojections can be evaluated faster using worst-case optimal algorithms\\ntogether with matrix multiplication. Crucially, our algorithms are\\nparameterized by the output size of the final result, allowing for choice of\\nthe best execution strategy. We implement our algorithms as a subroutine and\\ncompare the performance with state-of-the-art techniques to show they can be\\nimproved upon by as much as 50x. More importantly, our experiments indicate\\nthat matrix multiplication is a useful operation that can help speed up join\\nprocessing owing to highly optimized open source libraries that are also highly\\nparallelizable.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 2, 27, 21, 50, 40, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Top-K Deep Video Analytics: A Probabilistic Approach',\n",
       "  'authors': ['Ziliang Lai',\n",
       "   'Chenxia Han',\n",
       "   'Chris Liu',\n",
       "   'Pengfei Zhang',\n",
       "   'Eric Lo',\n",
       "   'Ben Kao'],\n",
       "  'summary': 'The impressive accuracy of deep neural networks (DNNs) has created great\\ndemands on practical analytics over video data. Although efficient and\\naccurate, the latest video analytic systems have not supported analytics beyond\\nselection and aggregation queries. In data analytics, Top-K is a very important\\nanalytical operation that enables analysts to focus on the most important\\nentities. In this paper, we present Everest, the first system that supports\\nefficient and accurate Top-K video analytics. Everest ranks and identifies the\\nmost interesting frames/moments from videos with probabilistic guarantees.\\nEverest is a system built with a careful synthesis of deep computer vision\\nmodels, uncertain data management, and Top-K query processing. Evaluations on\\nreal-world videos and the latest Visual Road benchmark show that Everest\\nachieves between 14.3x to 20.6x higher efficiency than baseline approaches with\\nhigh result accuracy',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 2, 11, 30, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Bridging the Gap Between Theory and Practice on Insertion-Intensive Database',\n",
       "  'authors': ['Sepanta Zeighami', 'Raymond Chi-Wing Wong'],\n",
       "  'summary': 'With the prevalence of online platforms, today, data is being generated and\\naccessed by users at a very high rate. Besides, applications such as stock\\ntrading or high frequency trading require guaranteed low delays for performing\\nan operation on a database. It is consequential to design databases that\\nguarantee data insertion and query at a consistently high rate without\\nintroducing any long delay during insertion. In this paper, we propose Nested\\nB-trees (NB-trees), an index that can achieve a consistently high insertion\\nrate on large volumes of data, while providing asymptotically optimal query\\nperformance that is very efficient in practice. Nested B-trees support\\ninsertions at rates higher than LSM-trees, the state-of-the-art index for\\ninsertion-intensive workloads, while avoiding their long insertion delays and\\nimproving on their query performance. They approach the query performance of\\nB-trees when complemented with Bloom filters. In our experiments, NB-trees had\\nworst-case delays up to 1000 smaller than LevelDB, RocksDB and bLSM, commonly\\nused LSM-tree data-stores, could perform queries more than 4 times faster than\\nLevelDB and 1.5 times faster than bLSM and RocksDB, while also outperforming\\nthem in terms of average insertion rate.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 2, 17, 50, 7, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Constant delay enumeration with FPT-preprocessing for conjunctive queries of bounded submodular width',\n",
       "  'authors': ['Christoph Berkholz', 'Nicole Schweikardt'],\n",
       "  'summary': \"Marx (STOC~2010, J.~ACM 2013) introduced the notion of submodular width of a\\nconjunctive query (CQ) and showed that for any class $\\\\Phi$ of Boolean CQs of\\nbounded submodular width, the model-checking problem for $\\\\Phi$ on the class of\\nall finite structures is fixed-parameter tractable (FPT). Note that for\\nnon-Boolean queries, the size of the query result may be far too large to be\\ncomputed entirely within FPT time. We investigate the free-connex variant of\\nsubmodular width and generalise Marx's result to non-Boolean queries as\\nfollows: For every class $\\\\Phi$ of CQs of bounded free-connex submodular width,\\nwithin FPT-preprocessing time we can build a data structure that allows to\\nenumerate, without repetition and with constant delay, all tuples of the query\\nresult. Our proof builds upon Marx's splitting routine to decompose the query\\nresult into a union of results; but we have to tackle the additional technical\\ndifficulty to ensure that these can be enumerated efficiently.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 2, 18, 9, 43, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Flow Computation in Temporal Interaction Networks',\n",
       "  'authors': ['Chrysanthi Kosyfaki',\n",
       "   'Nikos Mamoulis',\n",
       "   'Evaggelia Pitoura',\n",
       "   'Panayiotis Tsaparas'],\n",
       "  'summary': 'Temporal interaction networks capture the history of activities between\\nentities along a timeline. At each interaction, some quantity of data (money,\\ninformation, kbytes, etc.) flows from one vertex of the network to another.\\nFlow-based analysis can reveal important information. For instance, financial\\nintelligent units (FIUs) are interested in finding subgraphs in transactions\\nnetworks with significant flow of money transfers. In this paper, we introduce\\nthe flow computation problem in an interaction network or a subgraph thereof.\\nWe propose and study two models of flow computation, one based on a greedy flow\\ntransfer assumption and one that finds the maximum possible flow. We show that\\nthe greedy flow computation problem can be easily solved by a single scan of\\nthe interactions in time order. For the harder maximum flow problem, we propose\\ngraph precomputation and simplification approaches that can greatly reduce its\\ncomplexity in practice. As an application of flow computation, we formulate and\\nsolve the problem of flow pattern search, where, given a graph pattern, the\\nobjective is to find its instances and their flows in a large interaction\\nnetwork. We evaluate our algorithms using real datasets. The results show that\\nthe techniques proposed in this paper can greatly reduce the cost of flow\\ncomputation and pattern enumeration.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 4, 9, 52, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Order-Preserving Key Compression for In-Memory Search Trees',\n",
       "  'authors': ['Huanchen Zhang',\n",
       "   'Xiaoxuan Liu',\n",
       "   'David G. Andersen',\n",
       "   'Michael Kaminsky',\n",
       "   'Kimberly Keeton',\n",
       "   'Andrew Pavlo'],\n",
       "  'summary': \"We present the High-speed Order-Preserving Encoder (HOPE) for in-memory\\nsearch trees. HOPE is a fast dictionary-based compressor that encodes arbitrary\\nkeys while preserving their order. HOPE's approach is to identify common key\\npatterns at a fine granularity and exploit the entropy to achieve high\\ncompression rates with a small dictionary. We first develop a theoretical model\\nto reason about order-preserving dictionary designs. We then select six\\nrepresentative compression schemes using this model and implement them in HOPE.\\nThese schemes make different trade-offs between compression rate and encoding\\nspeed. We evaluate HOPE on five data structures used in databases: SuRF, ART,\\nHOT, B+tree, and Prefix B+tree. Our experiments show that using HOPE allows the\\nsearch trees to achieve lower query latency (up to 40\\\\% lower) and better\\nmemory efficiency (up to 30\\\\% smaller) simultaneously for most string key\\nworkloads.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 5, 1, 2, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Efficient and Effective Similar Subtrajectory Search with Deep Reinforcement Learning',\n",
       "  'authors': ['Zheng Wang', 'Cheng Long', 'Gao Cong', 'Yiding Liu'],\n",
       "  'summary': 'Similar trajectory search is a fundamental problem and has been well studied\\nover the past two decades. However, the similar subtrajectory search (SimSub)\\nproblem, aiming to return a portion of a trajectory (i.e., a subtrajectory)\\nwhich is the most similar to a query trajectory, has been mostly disregarded\\ndespite that it could capture trajectory similarity in a finer-grained way and\\nmany applications take subtrajectories as basic units for analysis. In this\\npaper, we study the SimSub problem and develop a suite of algorithms including\\nboth exact and approximate ones. Among those approximate algorithms, two that\\nare based on deep reinforcement learning stand out and outperform those\\nnon-learning based algorithms in terms of effectiveness and efficiency. We\\nconduct experiments on real-world trajectory datasets, which verify the\\neffectiveness and efficiency of the proposed algorithms.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 5, 11, 38, 21, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'NoSQL Databases: Yearning for Disambiguation',\n",
       "  'authors': ['Chaimae Asaad', 'Karim Baïna', 'Mounir Ghogho'],\n",
       "  'summary': 'The demanding requirements of the new Big Data intensive era raised the need\\nfor flexible storage systems capable of handling huge volumes of unstructured\\ndata and of tackling the challenges that traditional databases were facing.\\nNoSQL Databases, in their heterogeneity, are a powerful and diverse set of\\ndatabases tailored to specific industrial and business needs. However, the lack\\nof theoretical background creates a lack of consensus even among experts about\\nmany NoSQL concepts, leading to ambiguity and confusion. In this paper, we\\npresent a survey of NoSQL databases and their classification by data model\\ntype. We also conduct a benchmark in order to compare different NoSQL databases\\nand distinguish their characteristics. Additionally, we present the major areas\\nof ambiguity and confusion around NoSQL databases and their related concepts,\\nand attempt to disambiguate them.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 9, 12, 35, 46, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Note On Operator-Level Query Execution Cost Modeling',\n",
       "  'authors': ['Wentao Wu'],\n",
       "  'summary': 'External query execution cost modeling using query execution feedback has\\nfound its way in various database applications such as admission control and\\nquery scheduling. Existing techniques in general fall into two categories,\\nplan-level cost modeling and operator-level cost modeling. It has been shown in\\nthe literature that operator-level cost modeling can often significantly\\noutperform plan-level cost modeling. In this paper, we study operator-level\\ncost modeling from a robustness perspective. We address two main challenges in\\npractice regarding limited execution feedback (for certain operators) and mixed\\ncost estimates due to the use of multiple cost modeling techniques. We propose\\na framework that deals with these issues and present a comprehensive analysis\\nof this framework. We further provide a case study to demonstrate the efficacy\\nof our framework in the context of index tuning, which is itself a new\\napplication of external cost modeling techniques.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 9, 21, 3, 52, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Managing Data Lineage of O&G Machine Learning Models: The Sweet Spot for Shale Use Case',\n",
       "  'authors': ['Raphael Thiago',\n",
       "   'Renan Souza',\n",
       "   'L. Azevedo',\n",
       "   'E. Soares',\n",
       "   'Rodrigo Santos',\n",
       "   'Wallas Santos',\n",
       "   'Max De Bayser',\n",
       "   'M. Cardoso',\n",
       "   'M. Moreno',\n",
       "   'Renato Cerqueira'],\n",
       "  'summary': 'Machine Learning (ML) has increased its role, becoming essential in several\\nindustries. However, questions around training data lineage, such as \"where has\\nthe dataset used to train this model come from?\"; the introduction of several\\nnew data protection legislation; and, the need for data governance\\nrequirements, have hindered the adoption of ML models in the real world. In\\nthis paper, we discuss how data lineage can be leveraged to benefit the ML\\nlifecycle to build ML models to discover sweet-spots for shale oil and gas\\nproduction, a major application in the Oil and Gas O&G Industry.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 10, 18, 10, 16, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Crop Knowledge Discovery Based on Agricultural Big Data Integration',\n",
       "  'authors': ['Vuong M. Ngo', 'M-Tahar Kechadi'],\n",
       "  'summary': 'Nowadays, the agricultural data can be generated through various sources,\\nsuch as: Internet of Thing (IoT), sensors, satellites, weather stations,\\nrobots, farm equipment, agricultural laboratories, farmers, government agencies\\nand agribusinesses. The analysis of this big data enables farmers, companies\\nand agronomists to extract high business and scientific knowledge, improving\\ntheir operational processes and product quality. However, before analysing this\\ndata, different data sources need to be normalised, homogenised and integrated\\ninto a unified data representation. In this paper, we propose an agricultural\\ndata integration method using a constellation schema which is designed to be\\nflexible enough to incorporate other datasets and big data models. We also\\napply some methods to extract knowledge with the view to improve crop yield;\\nthese include finding suitable quantities of soil properties, herbicides and\\ninsecticides for both increasing crop yield and protecting the environment.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 11, 0, 13, 17, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'mmLSH: A Practical and Efficient Technique for Processing Approximate Nearest Neighbor Queries on Multimedia Data',\n",
       "  'authors': ['Omid Jafari', 'Parth Nagarkar', 'Jonathan Montaño'],\n",
       "  'summary': 'Many large multimedia applications require efficient processing of nearest\\nneighbor queries. Often, multimedia data are represented as a collection of\\nimportant high-dimensional feature vectors. Existing Locality Sensitive Hashing\\n(LSH) techniques require users to find top-k similar feature vectors for each\\nof the feature vectors that represent the query object. This leads to wasted\\nand redundant work due to two main reasons: 1) not all feature vectors may\\ncontribute equally in finding the top-k similar multimedia objects, and 2)\\nfeature vectors are treated independently during query processing.\\nAdditionally, there is no theoretical guarantee on the returned multimedia\\nresults. In this work, we propose a practical and efficient indexing approach\\nfor finding top-k approximate nearest neighbors for multimedia data using LSH\\ncalled mmLSH, which can provide theoretical guarantees on the returned\\nmultimedia results. Additionally, we present a buffer-conscious strategy to\\nspeed up the query processing. Experimental evaluation shows significant gains\\nin performance time and accuracy for different real multimedia datasets when\\ncompared against state-of-the-art LSH techniques.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 13, 17, 57, 8, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'ML-AQP: Query-Driven Approximate Query Processing based on Machine Learning',\n",
       "  'authors': ['Fotis Savva',\n",
       "   'Christos Anagnostopoulos',\n",
       "   'Peter Triantafillou'],\n",
       "  'summary': \"As more and more organizations rely on data-driven decision making,\\nlarge-scale analytics become increasingly important. However, an analyst is\\noften stuck waiting for an exact result. As such, organizations turn to Cloud\\nproviders that have infrastructure for efficiently analyzing large quantities\\nof data. But, with increasing costs, organizations have to optimize their\\nusage. Having a cheap alternative that provides speed and efficiency will go a\\nlong way. Concretely, we offer a solution that can provide approximate answers\\nto aggregate queries, relying on Machine Learning (ML), which is able to work\\nalongside Cloud systems. Our developed lightweight ML-led system can be stored\\non an analyst's local machine or deployed as a service to instantly answer\\nanalytic queries, having low response times and monetary/computational costs\\nand energy footprint. To accomplish this we leverage the knowledge obtained by\\npreviously answered queries and build ML models that can estimate the result of\\nnew queries in an efficient and inexpensive manner. The capabilities of our\\nsystem are demonstrated using extensive evaluation with both real and synthetic\\ndatasets/workloads and well known benchmarks.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 14, 12, 8, 6, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Scrutinizer: A Mixed-Initiative Approach to Large-Scale, Data-Driven Claim Verification',\n",
       "  'authors': ['Georgios Karagiannis',\n",
       "   'Mohammed Saeed',\n",
       "   'Paolo Papotti',\n",
       "   'Immanuel Trummer'],\n",
       "  'summary': 'Organizations such as the International Energy Agency (IEA) spend significant\\namounts of time and money to manually fact check text documents summarizing\\ndata. The goal of the Scrutinizer system is to reduce verification overheads by\\nsupporting human fact checkers in translating text claims into SQL queries on\\nan associated database.\\n  Scrutinizer coordinates teams of human fact checkers. It reduces verification\\ntime by proposing queries or query fragments to the users. Those proposals are\\nbased on claim text classifiers, that gradually improve during the verification\\nof a large document. In addition, Scrutinizer uses tentative execution of query\\ncandidates to narrow down the set of alternatives. The verification process is\\ncontrolled by a cost-based optimizer. It optimizes the interaction with users\\nand prioritizes claim verifications. For the latter, it considers expected\\nverification overheads as well as the expected claim utility as training\\nsamples for the classifiers. We evaluate the Scrutinizer system using\\nsimulations and a user study, based on actual claims and data and using\\nprofessional fact checkers employed by IEA. Our experiments consistently\\ndemonstrate significant savings in verification time, without reducing result\\naccuracy.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 14, 21, 28, 43, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Recommending Deployment Strategies for Collaborative Tasks',\n",
       "  'authors': ['Dong Wei', 'Senjuti Basu Roy', 'Sihem Amer-Yahia'],\n",
       "  'summary': 'Our work contributes to aiding requesters in deploying collaborative tasks in\\ncrowdsourcing. We initiate the study of recommending deployment strategies for\\ncollaborative tasks to requesters that are consistent with deployment\\nparameters they desire: a lower-bound on the quality of the crowd contribution,\\nan upper-bound on the latency of task completion, and an upper-bound on the\\ncost incurred by paying workers. A deployment strategy is a choice of value for\\nthree dimensions: Structure (whether to solicit the workforce sequentially or\\nsimultaneously), Organization (to organize it collaboratively or\\nindependently), and Style (to rely solely on the crowd or to combine it with\\nmachine algorithms). We propose StratRec, an optimization-driven middle layer\\nthat recommends deployment strategies and alternative deployment parameters to\\nrequesters by accounting for worker availability. Our solutions are grounded in\\ndiscrete optimization and computational geometry techniques that produce\\nresults with theoretical guarantees. We present extensive experiments on Amazon\\nMechanical Turk and conduct synthetic experiments to validate the qualitative\\nand scalability aspects of StratRec.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 15, 17, 36, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Grammars for Document Spanners',\n",
       "  'authors': ['Liat Peterfreund'],\n",
       "  'summary': 'We propose a new grammar-based language for defining information-extractors\\nfrom documents (text) that is built upon the well-studied framework of document\\nspanners for extracting structured data from text. While previously studied\\nformalisms for document spanners are mainly based on regular expressions, we\\nuse an extension of context-free grammars, called {extraction grammars}, to\\ndefine the new class of context-free spanners. Extraction grammars are simply\\ncontext-free grammars extended with variables that capture interval positions\\nof the document, namely spans. While regular expressions are efficient for\\ntokenizing and tagging, context-free grammars are also efficient for capturing\\nstructural properties. Indeed, we show that context-free spanners are strictly\\nmore expressive than their regular counterparts. We reason about the expressive\\npower of our new class and present a pushdown-automata model that captures it.\\nWe show that extraction grammars can be evaluated with polynomial data\\ncomplexity. Nevertheless, as the degree of the polynomial depends on the query,\\nwe present an enumeration algorithm for unambiguous extraction grammars that,\\nafter quintic preprocessing, outputs the results sequentially, without\\nrepetitions, with a constant delay between every two consecutive ones.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 15, 17, 50, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Dash: Scalable Hashing on Persistent Memory',\n",
       "  'authors': ['Baotong Lu', 'Xiangpeng Hao', 'Tianzheng Wang', 'Eric Lo'],\n",
       "  'summary': 'Byte-addressable persistent memory (PM) brings hash tables the potential of\\nlow latency, cheap persistence and instant recovery. The recent advent of Intel\\nOptane DC Persistent Memory Modules (DCPMM) further accelerates this trend.\\nMany new hash table designs have been proposed, but most of them were based on\\nemulation and perform sub-optimally on real PM. They were also piece-wise and\\npartial solutions that side-step many important properties, in particular good\\nscalability, high load factor and instant recovery. We present Dash, a holistic\\napproach to building dynamic and scalable hash tables on real PM hardware with\\nall the aforementioned properties. Based on Dash, we adapted two popular\\ndynamic hashing schemes (extendible hashing and linear hashing). On a 24-core\\nmachine with Intel Optane DCPMM, we show that compared to state-of-the-art,\\nDash-enabled hash tables can achieve up to ~3.9X higher performance with up to\\nover 90% load factor and an instant recovery time of 57ms regardless of data\\nsize.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 16, 16, 15, 46, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Equivalent Rewritings on Path Views with Binding Patterns',\n",
       "  'authors': ['Julien Romero',\n",
       "   'Nicoleta Preda',\n",
       "   'Antoine Amarilli',\n",
       "   'Fabian Suchanek'],\n",
       "  'summary': 'A view with a binding pattern is a parameterized query on a database. Such\\nviews are used, e.g., to model Web services. To answer a query on such views,\\nthe views have to be orchestrated together in execution plans. We show how\\nqueries can be rewritten into equivalent execution plans, which are guaranteed\\nto deliver the same results as the query on all databases. We provide a correct\\nand complete algorithm to find these plans for path views and atomic queries.\\nFinally, we show that our method can be used to answer queries on real-world\\nWeb services.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 16, 16, 38, 56, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Hihooi: A Database Replication Middleware for Scaling Transactional Databases Consistently',\n",
       "  'authors': ['Michael A. Georgiou',\n",
       "   'Aristodemos Paphitis',\n",
       "   'Michael Sirivianos',\n",
       "   'Herodotos Herodotou'],\n",
       "  'summary': 'With the advent of the Internet and Internet-connected devices, modern\\nbusiness applications can experience rapid increases as well as variability in\\ntransactional workloads. Database replication has been employed to scale\\nperformance and improve availability of relational databases but past\\napproaches have suffered from various issues including limited scalability,\\nperformance versus consistency tradeoffs, and requirements for database or\\napplication modifications. This paper presents Hihooi, a replication-based\\nmiddleware system that is able to achieve workload scalability, strong\\nconsistency guarantees, and elasticity for existing transactional databases at\\na low cost. A novel replication algorithm enables Hihooi to propagate database\\nmodifications asynchronously to all replicas at high speeds, while ensuring\\nthat all replicas are consistent. At the same time, a fine-grained routing\\nalgorithm is used to load balance incoming transactions to available replicas\\nin a consistent way. Our thorough experimental evaluation with several\\nwell-established benchmarks shows how Hihooi is able to achieve almost linear\\nworkload scalability for transactional databases.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 16, 20, 32, 20, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'PolyFit: Polynomial-based Indexing Approach for Fast Approximate Range Aggregate Queries',\n",
       "  'authors': ['Zhe Li', 'Tsz Nam Chan', 'Man Lung Yiu', 'Christian S. Jensen'],\n",
       "  'summary': 'Range aggregate queries find frequent application in data analytics. In some\\nuse cases, approximate results are preferred over accurate results if they can\\nbe computed rapidly and satisfy approximation guarantees. Inspired by a recent\\nindexing approach, we provide means of representing a discrete point data set\\nby continuous functions that can then serve as compact index structures. More\\nspecifically, we develop a polynomial-based indexing approach, called PolyFit,\\nfor processing approximate range aggregate queries. PolyFit is capable of\\nsupporting multiple types of range aggregate queries, including COUNT, SUM, MIN\\nand MAX aggregates, with guaranteed absolute and relative error bounds.\\nExperiment results show that PolyFit is faster and more accurate and compact\\nthan existing learned index structures.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 18, 3, 54, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Discovering Business Area Effects to Process Mining Analysis Using Clustering and Influence Analysis',\n",
       "  'authors': ['Teemu Lehto', 'Markku Hinkka'],\n",
       "  'summary': 'A common challenge for improving business processes in large organizations is\\nthat business people in charge of the operations are lacking a fact-based\\nunderstanding of the execution details, process variants, and exceptions taking\\nplace in business operations. While existing process mining methodologies can\\ndiscover these details based on event logs, it is challenging to communicate\\nthe process mining findings to business people. In this paper, we present a\\nnovel methodology for discovering business areas that have a significant effect\\non the process execution details. Our method uses clustering to group similar\\ncases based on process flow characteristics and then influence analysis for\\ndetecting those business areas that correlate most with the discovered\\nclusters. Our analysis serves as a bridge between BPM people and business,\\npeople facilitating the knowledge sharing between these groups. We also present\\nan example analysis based on publicly available real-life purchase order\\nprocess data.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 18, 11, 58, 1, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Migratory Near Memory Processing Architecture Applied to Big Data Problems',\n",
       "  'authors': ['Ed T. Upchurch'],\n",
       "  'summary': 'Servers produced by mainstream vendors are inefficient in processing Big Data\\nqueries due to bottlenecks inherent in the fundamental architecture of these\\nsystems. Current server blades contain multicore processors connected to DRAM\\nmemory and disks by an interconnection chipset. The multicore processor chips\\nperform all the computations while the DRAM and disks store the data but have\\nno processing capability. To perform a database query, data must be moved back\\nand forth between DRAM and a small cache as well as between DRAM and disks. For\\nBig Data applications this data movement in onerous. Migratory Near Memory\\nServers address this bottleneck by placing large numbers of lightweight\\nprocessors directly into the memory system. These processors operate directly\\non the relations, vertices and edges of Big Data applications in place without\\nhaving to shuttle large quantities of data back and forth between DRAM, cache\\nand heavyweight multicore processors. This paper addresses the application of\\nsuch an architecture to relational database SELECT and JOIN queries.\\nPreliminary results indicate end-to-end orders of magnitude speedup.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 20, 2, 37, 40, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Synopses Data Engine for Interactive Extreme-Scale Analytics',\n",
       "  'authors': ['Antonis Kontaxakis',\n",
       "   'Nikos Giatrakos',\n",
       "   'Antonios Deligiannakis'],\n",
       "  'summary': 'In this work, we detail the design and structure of a Synopses Data Engine\\n(SDE) which combines the virtues of parallel processing and stream\\nsummarization towards delivering interactive analytics at extreme scale. Our\\nSDE is built on top of Apache Flink and implements a synopsis-as-a-service\\nparadigm. In that it achieves (a) concurrently maintaining thousands of\\nsynopses of various types for thousands of streams on demand, (b) reusing\\nmaintained synopses among various concurrent workflows, (c) providing data\\nsummarization facilities even for cross-(Big Data) platform workflows, (d)\\npluggability of new synopses on-the-fly, (e) increased potential for workflow\\nexecution optimization. The proposed SDE is useful for interactive analytics at\\nextreme scales because it enables (i) enhanced horizontal scalability, i.e.,\\nnot only scaling out the computation to a number of processing units available\\nin a computer cluster, but also harnessing the processing load assigned to each\\nby operating on carefully-crafted data summaries, (ii) vertical scalability,\\ni.e., scaling the computation to very high numbers of processed streams and\\n(iii) federated scalability i.e., scaling the computation beyond single\\nclusters and clouds by controlling the communication required to answer global\\nqueries posed over a number of potentially geo-dispersed clusters.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 21, 0, 55, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Translation of Array-Based Loops to Distributed Data-Parallel Programs',\n",
       "  'authors': ['Leonidas Fegaras', 'Md Hasanuzzaman Noor'],\n",
       "  'summary': 'Large volumes of data generated by scientific experiments and simulations\\ncome in the form of arrays, while programs that analyze these data are\\nfrequently expressed in terms of array operations in an imperative, loop-based\\nlanguage. But, as datasets grow larger, new frameworks in distributed Big Data\\nanalytics have become essential tools to large-scale scientific computing.\\nScientists, who are typically comfortable with numerical analysis tools but are\\nnot familiar with the intricacies of Big Data analytics, must now learn to\\nconvert their loop-based programs to distributed data-parallel programs. We\\npresent a novel framework for translating programs expressed as array-based\\nloops to distributed data parallel programs that is more general and efficient\\nthan related work. Although our translations are over sparse arrays, we extend\\nour framework to handle packed arrays, such as tiled matrices, without\\nsacrificing performance. We report on a prototype implementation on top of\\nSpark and evaluate the performance of our system relative to hand-written\\nprograms.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 21, 23, 40, 44, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'EQL -- an extremely easy to learn knowledge graph query language, achieving highspeed and precise search',\n",
       "  'authors': ['Han Liu', 'Shantao Liu'],\n",
       "  'summary': \"EQL, also named as Extremely Simple Query Language, can be widely used in the\\nfield of knowledge graph, precise search, strong artificial intelligence,\\ndatabase, smart speaker ,patent search and other fields. EQL adopt the\\nprinciple of minimalism in design and pursues simplicity and easy to learn so\\nthat everyone can master it quickly. EQL language and lambda calculus are\\ninterconvertible, that reveals the mathematical nature of EQL language, and\\nlays a solid foundation for rigor and logical integrity of EQL language. The\\nEQL language and a comprehensive knowledge graph system with the world's\\ncommonsense can together form the foundation of strong AI in the future, and\\nmake up for the current lack of understanding of world's commonsense by current\\nAI system. EQL language can be used not only by humans, but also as a basic\\nlanguage for data query and data exchange between robots.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 19, 3, 32, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'MultiRI: Fast Subgraph Matching in Labeled Multigraphs',\n",
       "  'authors': ['Giovanni Micale',\n",
       "   'Vincenzo Bonnici',\n",
       "   'Alfredo Ferro',\n",
       "   'Dennis Shasha',\n",
       "   'Rosalba Giugno',\n",
       "   'Alfredo Pulvirenti'],\n",
       "  'summary': 'The Subgraph Matching (SM) problem consists of finding all the embeddings of\\na given small graph, called the query, into a large graph, called the target.\\nThe SM problem has been widely studied for simple graphs, i.e. graphs where\\nthere is exactly one edge between two nodes and nodes have single labels, but\\nfew approaches have been devised for labeled multigraphs, i.e. graphs having\\npossibly multiple labels on nodes in which pair of nodes may have multiple\\nlabeled edges between them. Here we present MultiRI, a novel algorithm for the\\nSub-Multigraph Matching (SMM) problem, i.e. subgraph matching in labeled\\nmultigraphs. MultiRI improves on the state-of-the-art by computing\\ncompatibility domains and symmetry breaking conditions on query nodes to filter\\nthe search space of possible solutions. Empirically, we show that MultiRI\\noutperforms the state-of-the-art method for the SMM problem in both synthetic\\nand real graphs, with a multiplicative speedup between five and ten for large\\ngraphs, by using a limited amount of memory.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 25, 15, 4, 5, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Survey on Trajectory Data Management, Analytics, and Learning',\n",
       "  'authors': ['Sheng Wang', 'Zhifeng Bao', 'J. Shane Culpepper', 'Gao Cong'],\n",
       "  'summary': 'Recent advances in sensor and mobile devices have enabled an unprecedented\\nincrease in the availability and collection of urban trajectory data, thus\\nincreasing the demand for more efficient ways to manage and analyze the data\\nbeing produced. In this survey, we comprehensively review recent research\\ntrends in trajectory data management, ranging from trajectory pre-processing,\\nstorage, common trajectory analytic tools, such as querying spatial-only and\\nspatial-textual trajectory data, and trajectory clustering. We also explore\\nfour closely related analytical tasks commonly used with trajectory data in\\ninteractive or real-time processing. Deep trajectory learning is also reviewed\\nfor the first time. Finally, we outline the essential qualities that a\\ntrajectory data management system should possess in order to maximize\\nflexibility.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 25, 16, 19, 41, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Time Series Data Cleaning: From Anomaly Detection to Anomaly Repairing (Technical Report)',\n",
       "  'authors': ['Aoqian Zhang', 'Shaoxu Song', 'Jianmin Wang', 'Philip S. Yu'],\n",
       "  'summary': 'Errors are prevalent in time series data, such as GPS trajectories or sensor\\nreadings. Existing methods focus more on anomaly detection but not on repairing\\nthe detected anomalies. By simply filtering out the dirty data via anomaly\\ndetection, applications could still be unreliable over the incomplete time\\nseries. Instead of simply discarding anomalies, we propose to (iteratively)\\nrepair them in time series data, by creatively bonding the beauty of temporal\\nnature in anomaly detection with the widely considered minimum change principle\\nin data repairing. Our major contributions include: (1) a novel framework of\\niterative minimum repairing (IMR) over time series data, (2) explicit analysis\\non convergence of the proposed iterative minimum repairing, and (3) efficient\\nestimation of parameters in each iteration. Remarkably, with incremental\\ncomputation, we reduce the complexity of parameter estimation from O(n) to\\nO(1). Experiments on real datasets demonstrate the superiority of our proposal\\ncompared to the state-of-the-art approaches. In particular, we show that (the\\nproposed) repairing indeed improves the time series classification application.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 27, 13, 5, 11, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Consistency and Certain Answers in Relational to RDF Data Exchange with Shape Constraints',\n",
       "  'authors': ['Iovka Boneva', 'Jose Lozano', 'Sławek Staworko'],\n",
       "  'summary': 'We investigate the data exchange from relational databases to RDF graphs\\ninspired by R2RML with the addition of target shape schemas. We study the\\nproblems of consistency i.e., checking that every source instance admits a\\nsolution, and certain query answering i.e., finding answers present in every\\nsolution. We identify the class of constructive relational to RDF data exchange\\nthat uses IRI constructors and full tgds (with no existential variables) in its\\nsource to target dependencies. We show that the consistency problem is\\ncoNP-complete. We introduce the notion of universal simulation solution that\\nallows to compute certain query answers to any class of queries that is robust\\nunder simulation. One such class are nested regular expressions (NREs) that are\\nforward i.e., do not use the inverse operation. Using universal simulation\\nsolution renders tractable the computation of certain answers to forward NREs\\n(data-complexity). Finally, we present a number of results that show that\\nrelaxing the restrictions of the proposed framework leads to an increase in\\ncomplexity.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 3, 30, 21, 36, 49, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Technical Report: Developing a Working Data Hub',\n",
       "  'authors': ['Vijay Gadepally', 'Jeremy Kepner'],\n",
       "  'summary': 'Data forms a key component of any enterprise. The need for high quality and\\neasy access to data is further amplified by organizations wishing to leverage\\nmachine learning or artificial intelligence for their operations. To this end,\\nmany organizations are building resources for managing heterogenous data,\\nproviding end-users with an organization wide view of available data, and\\nacting as a centralized repository for data owned/collected by an organization.\\nVery broadly, we refer to these class of techniques as a \"data hub.\" While\\nthere is no clear definition of what constitutes a data hub, some of the key\\ncharacteristics include: data catalog; links to data sets or owners of data\\nsets or centralized data repository; basic ability to serve / visualize data\\nsets; access control policies that ensure secure data access and respects\\npolicies of data owners; and computing capabilities tied with data hub\\ninfrastructure. Of course, developing such a data hub entails numerous\\nchallenges. This document provides background in databases, data management and\\noutlines best practices and recommendations for developing and deploying a\\nworking data hub.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 1, 1, 52, 11, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Leveraging Data Preparation, HBase NoSQL Storage, and HiveQL Querying for COVID-19 Big Data Analytics Projects',\n",
       "  'authors': ['Karim Baïna'],\n",
       "  'summary': 'Epidemiologist, Scientists, Statisticians, Historians, Data engineers and\\nData scientists are working on finding descriptive models and theories to\\nexplain COVID-19 expansion phenomena or on building analytics predictive models\\nfor learning the apex of COVID-19 confimed cases, recovered cases, and deaths\\nevolution curves. In CRISP-DM life cycle, 75% of time is consumed only by data\\npreparation phase causing lot of pressions and stress on scientists and data\\nscientists building machine learning models. This paper aims to help reducing\\ndata preparation efforts by presenting detailed schemas design and data\\npreparation technical scripts for formatting and storing Johns Hopkins\\nUniversity COVID-19 daily data in HBase NoSQL data store, and enabling HiveQL\\nCOVID-19 data querying in a relational Hive SQL-like style.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 1, 6, 45, 50, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Nass: A New Approach to Graph Similarity Search',\n",
       "  'authors': ['Jongik Kim'],\n",
       "  'summary': 'In this paper, we study the problem of graph similarity search with graph\\nedit distance (GED) constraints. Due to the NP-hardness of GED computation,\\nexisting solutions to this problem adopt the filtering-and-verification\\nframework with a main focus on the filtering phase to generate a small number\\nof candidate graphs. However, they have a limitation that the number of\\ncandidates grows extremely rapidly as a GED threshold increases. To address the\\nlimitation, we propose a new approach that utilizes GED computation results in\\ngenerating candidate graphs. The main idea is that whenever we identify a\\nresult graph of the query, we immediately regenerate candidate graphs using a\\nsubset of pre-computed graphs similar to the identified result graph. To speed\\nup GED computation, we also develop a novel GED computation algorithm. The\\nproposed algorithm reduces the search space for GED computation by utilizing a\\nseries of filtering techniques, which have been used to generate candidates in\\nexisting solutions. Experimental results on real datasets demonstrate the\\nproposed approach significantly outperforms the state-of-the art techniques.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 2, 16, 53, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Regular Path Query Evaluation on Streaming Graphs',\n",
       "  'authors': ['Anil Pacaci', 'Angela Bonifati', 'M. Tamer Özsu'],\n",
       "  'summary': 'We study persistent query evaluation over streaming graphs, which is becoming\\nincreasingly important. We focus on navigational queries that determine if\\nthere exists a path between two entities that satisfies a user-specified\\nconstraint. We adopt the Regular Path Query (RPQ) model that specifies\\nnavigational patterns with labeled constraints. We propose deterministic\\nalgorithms to efficiently evaluate persistent RPQs under both arbitrary and\\nsimple path semantics in a uniform manner. Experimental analysis on real and\\nsynthetic streaming graphs shows that the proposed algorithms can process up to\\ntens of thousands of edges per second and efficiently answer RPQs that are\\ncommonly used in real-world workloads.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 4, 20, 35, 30, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Learning Over Dirty Data Without Cleaning',\n",
       "  'authors': ['Jose Picado', 'John Davis', 'Arash Termehchy', 'Ga Young Lee'],\n",
       "  'summary': 'Real-world datasets are dirty and contain many errors. Examples of these\\nissues are violations of integrity constraints, duplicates, and inconsistencies\\nin representing data values and entities. Learning over dirty databases may\\nresult in inaccurate models. Users have to spend a great deal of time and\\neffort to repair data errors and create a clean database for learning.\\nMoreover, as the information required to repair these errors is not often\\navailable, there may be numerous possible clean versions for a dirty database.\\nWe propose DLearn, a novel relational learning system that learns directly over\\ndirty databases effectively and efficiently without any preprocessing. DLearn\\nleverages database constraints to learn accurate relational models over\\ninconsistent and heterogeneous data. Its learned models represent patterns over\\nall possible clean instances of the data in a usable form. Our empirical study\\nindicates that DLearn learns accurate models over large real-world databases\\nefficiently.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 5, 20, 21, 13, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'DySky: Dynamic Skyline Queries on Uncertain Graphs',\n",
       "  'authors': ['Suman Banerjee', 'Bithika Pal'],\n",
       "  'summary': 'Given a graph, and a set of query vertices (subset of the vertices), the\\ndynamic skyline query problem returns a subset of data vertices (other than\\nquery vertices) which are not dominated by other data vertices based on certain\\ndistance measure. In this paper, we study the dynamic skyline query problem on\\nuncertain graphs (DySky). The input to this problem is an uncertain graph, a\\nsubset of its nodes as query vertices, and the goal here is to return all the\\ndata vertices which are not dominated by others. We employ two distance\\nmeasures in uncertain graphs, namely, \\\\emph{Majority Distance}, and\\n\\\\emph{Expected Distance}. Our approach is broadly divided into three steps:\\n\\\\emph{Pruning}, \\\\emph{Distance Computation}, and \\\\emph{Skyline Vertex Set\\nGeneration}. We implement the proposed methodology with three publicly\\navailable datasets and observe that it can find out skyline vertex set without\\ntaking much time even for million sized graphs if expected distance is\\nconcerned. Particularly, the pruning strategy reduces the computational time\\nsignificantly.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 6, 11, 22, 43, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Dynamic Ridesharing in Peak Travel Periods',\n",
       "  'authors': ['Hui Luo',\n",
       "   'Zhifeng Bao',\n",
       "   'Farhana M. Choudhury',\n",
       "   'J. Shane Culpepper'],\n",
       "  'summary': \"In this paper, we study a variant of the dynamic ridesharing problem with a\\nspecific focus on peak hours: Given a set of drivers and rider requests, we aim\\nto match drivers to each rider request by achieving two objectives: maximizing\\nthe served rate and minimizing the total additional distance, subject to a\\nseries of spatio-temporal constraints. Our problem can be distinguished from\\nexisting work in three aspects: (1) Previous work did not fully explore the\\nimpact of peak travel periods where the number of rider requests is much\\ngreater than the number of available drivers. (2) Existing solutions usually\\nrely on single objective optimization techniques, such as minimizing the total\\ntravel cost. (3) When evaluating the overall system performance, the runtime\\nspent on updating drivers' trip schedules as per incoming rider requests should\\nbe incorporated, while it is excluded by most existing solutions. We propose an\\nindex structure together with a set of pruning rules and an efficient algorithm\\nto include new riders into drivers' existing trip schedule. To answer new rider\\nrequests effectively, we propose two algorithms that match drivers with rider\\nrequests. Finally, we perform extensive experiments on a large-scale test\\ncollection to validate the proposed methods.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 6, 11, 34, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'GeoFlink: A Distributed and Scalable Framework for the Real-time Processing of Spatial Streams',\n",
       "  'authors': ['Salman Ahmed Shaikh',\n",
       "   'Komal Mariam',\n",
       "   'Hiroyuki Kitagawa',\n",
       "   'Kyoung-Sook Kim'],\n",
       "  'summary': 'Apache Flink is an open-source system for scalable processing of batch and\\nstreaming data. Flink does not natively support efficient processing of spatial\\ndata streams, which is a requirement of many applications dealing with spatial\\ndata. Besides Flink, other scalable spatial data processing platforms including\\nGeoSpark, Spatial Hadoop, etc. do not support streaming workloads and can only\\nhandle static/batch workloads. To fill this gap, we present GeoFlink, which\\nextends Apache Flink to support spatial data types, indexes and continuous\\nqueries over spatial data streams. To enable the efficient processing of\\nspatial continuous queries and for the effective data distribution across Flink\\ncluster nodes, a gird-based index is introduced. GeoFlink currently supports\\nspatial range, spatial $k$NN and spatial join queries on point data type. An\\nextensive experimental study on real spatial data streams shows that GeoFlink\\nachieves significantly higher query throughput than ordinary Flink processing.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 7, 13, 27, 2, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An Algorithm for Context-Free Path Queries over Graph Databases',\n",
       "  'authors': ['Ciro M. Medeiros', 'Martin A. Musicante', 'Umberto S. Costa'],\n",
       "  'summary': 'RDF (Resource Description Framework) is a standard language to represent\\ngraph databases. Query languages for RDF databases usually include primitives\\nto support path queries, linking pairs of vertices of the graph that are\\nconnected by a path of labels belonging to a given language. Languages such as\\nSPARQL include support for paths defined by regular languages (by means of\\nRegular Expressions). A context-free path query is a path query whose language\\ncan be defined by a context-free grammar. Context-free path queries can be used\\nto implement queries such as the \"same generation queries\", that are not\\nexpressible by Regular Expressions. In this paper, we present a novel algorithm\\nfor context-free path query processing. We prove the correctness of our\\napproach and show its run-time and memory complexity. We show the viability of\\nour approach by means of a prototype implemented in Go. We run our prototype\\nusing the same cases of study as proposed in recent works, comparing our\\nresults with another, recently published algorithm. The experiments include\\nboth synthetic and real RDF databases. Our algorithm can be seen as a step\\nforward, towards the implementation of more expressive query languages.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 7, 15, 26, 50, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'DataFed: Towards Reproducible Research via Federated Data Management',\n",
       "  'authors': ['Dale Stansberry',\n",
       "   'Suhas Somnath',\n",
       "   'Jessica Breet',\n",
       "   'Gregory Shutt',\n",
       "   'Mallikarjun Shankar'],\n",
       "  'summary': 'The increasingly collaborative, globalized nature of scientific research\\ncombined with the need to share data and the explosion in data volumes present\\nan urgent need for a scientific data management system (SDMS). An SDMS presents\\na logical and holistic view of data that greatly simplifies and empowers data\\norganization, curation, searching, sharing, dissemination, etc. We present\\nDataFed -- a lightweight, distributed SDMS that spans a federation of storage\\nsystems within a loosely-coupled network of scientific facilities. Unlike\\nexisting SDMS offerings, DataFed uses high-performance and scalable user\\nmanagement and data transfer technologies that simplify deployment,\\nmaintenance, and expansion of DataFed. DataFed provides web-based and\\ncommand-line interfaces to manage data and integrate with complex scientific\\nworkflows. DataFed represents a step towards reproducible scientific research\\nby enabling reliable staging of the correct data at the desired environment.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 7, 21, 5, 22, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Maintaining Triangle Queries under Updates',\n",
       "  'authors': ['Ahmet Kara',\n",
       "   'Milos Nikolic',\n",
       "   'Hung Q. Ngo',\n",
       "   'Dan Olteanu',\n",
       "   'Haozhe Zhang'],\n",
       "  'summary': 'We consider the problem of incrementally maintaining the triangle queries\\nwith arbitrary free variables under single-tuple updates to the input\\nrelations. We introduce an approach called IVM$^\\\\epsilon$ that exhibits a\\ntrade-off between the update time, the space, and the delay for the enumeration\\nof the query result, such that the update time ranges from the square root to\\nlinear in the database size while the delay ranges from constant to linear\\ntime. IVM$^\\\\epsilon$ achieves Pareto worst-case optimality in the update-delay\\nspace conditioned on the Online Matrix-Vector Multiplication conjecture. It is\\nstrongly Pareto optimal for the triangle queries with zero or three free\\nvariables and weakly Pareto optimal for the triangle queries with one or two\\nfree variables.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 7, 21, 9, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Bao: Learning to Steer Query Optimizers',\n",
       "  'authors': ['Ryan Marcus',\n",
       "   'Parimarjan Negi',\n",
       "   'Hongzi Mao',\n",
       "   'Nesime Tatbul',\n",
       "   'Mohammad Alizadeh',\n",
       "   'Tim Kraska'],\n",
       "  'summary': 'Query optimization remains one of the most challenging problems in data\\nmanagement systems. Recent efforts to apply machine learning techniques to\\nquery optimization challenges have been promising, but have shown few practical\\ngains due to substantive training overhead, inability to adapt to changes, and\\npoor tail performance. Motivated by these difficulties and drawing upon a long\\nhistory of research in multi-armed bandits, we introduce Bao (the BAndit\\nOptimizer). Bao takes advantage of the wisdom built into existing query\\noptimizers by providing per-query optimization hints. Bao combines modern tree\\nconvolutional neural networks with Thompson sampling, a decades-old and\\nwell-studied reinforcement learning algorithm. As a result, Bao automatically\\nlearns from its mistakes and adapts to changes in query workloads, data, and\\nschema. Experimentally, we demonstrate that Bao can quickly (an order of\\nmagnitude faster than previous approaches) learn strategies that improve\\nend-to-end query execution performance, including tail latency. In cloud\\nenvironments, we show that Bao can offer both reduced costs and better\\nperformance compared with a sophisticated commercial system.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 8, 5, 15, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Fast and Reliable Missing Data Contingency Analysis with Predicate-Constraints',\n",
       "  'authors': ['Xi Liang',\n",
       "   'Zechao Shang',\n",
       "   'Aaron J. Elmore',\n",
       "   'Sanjay Krishnan',\n",
       "   'Michael J. Franklin'],\n",
       "  'summary': 'Today, data analysts largely rely on intuition to determine whether missing\\nor withheld rows of a dataset significantly affect their analyses. We propose a\\nframework that can produce automatic contingency analysis, i.e., the range of\\nvalues an aggregate SQL query could take, under formal constraints describing\\nthe variation and frequency of missing data tuples. We describe how to process\\nSUM, COUNT, AVG, MIN, and MAX queries in these conditions resulting in hard\\nerror bounds with testable constraints. We propose an optimization algorithm\\nbased on an integer program that reconciles a set of such constraints, even if\\nthey are overlapping, conflicting, or unsatisfiable, into such bounds. Our\\nexperiments on real-world datasets against several statistical imputation and\\ninference baselines show that statistical techniques can have a deceptively\\nhigh error rate that is often unpredictable. In contrast, our framework offers\\nhard bounds that are guaranteed to hold if the constraints are not violated. In\\nspite of these hard bounds, we show competitive accuracy to statistical\\nbaselines.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 8, 17, 50, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The Effects of Different JSON Representations on Querying Knowledge Graphs',\n",
       "  'authors': ['Masoud Salehpour', 'Joseph G. Davis'],\n",
       "  'summary': 'Knowledge Graphs (KGs) have emerged as the de-facto standard for modeling and\\nquerying datasets with a graph-like structure in the Semantic Web domain. Our\\nfocus is on the performance challenges associated with querying KGs. We\\ndeveloped three informationally equivalent JSON-based representations for KGs,\\nnamely, Subject-based Name/Value (JSON-SNV), Documents of Triples (JSON-DT),\\nand Chain-based Name/Value (JSON-CNV). We analyzed the effects of these\\nrepresentations on query performance by storing them on two prominent\\ndocument-based Data Management Systems (DMSs), namely, MongoDB and Couchbase\\nand executing a set of benchmark queries over them. We also compared the\\nexecution times with row-store Virtuoso, column-store Virtuoso, and\\n\\\\mbox{Blazegraph} as three major DMSs with different architectures (aka,\\nRDF-stores). Our results indicate that the representation type has a\\nsignificant performance impact on query execution. For instance, the JSON-SNV\\noutperforms others by nearly one order of magnitude to execute subject-subject\\njoin queries. This and the other results presented in this paper can assist in\\nmore accurate benchmarking of the emerging DMSs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 8, 22, 37, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'On Multiple Semantics for Declarative Database Repairs',\n",
       "  'authors': ['Amir Gilad', 'Daniel Deutch', 'Sudeepa Roy'],\n",
       "  'summary': 'We study the problem of database repairs through a rule-based framework that\\nwe refer to as Delta Rules. Delta Rules are highly expressive and allow\\nspecifying complex, cross-relations repair logic associated with Denial\\nConstraints, Causal Rules, and allowing to capture Database Triggers of\\ninterest. We show that there are no one-size-fits-all semantics for repairs in\\nthis inclusive setting, and we consequently introduce multiple alternative\\nsemantics, presenting the case for using each of them. We then study the\\nrelationships between the semantics in terms of their output and the complexity\\nof computation. Our results formally establish the tradeoff between the\\npermissiveness of the semantics and its computational complexity. We\\ndemonstrate the usefulness of the framework in capturing multiple data repair\\nscenarios for an Academic Search database and the TPC-H databases, showing how\\nusing different semantics affects the repair in terms of size and runtime, and\\nexamining the relationships between the repairs. We also compare our approach\\nwith SQL triggers and a state-of-the-art data repair system.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 10, 15, 0, 29, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Cheetah: Accelerating Database Queries with Switch Pruning',\n",
       "  'authors': ['Muhammad Tirmazi', 'Ran Ben Basat', 'Jiaqi Gao', 'Minlan Yu'],\n",
       "  'summary': 'Modern database systems are growing increasingly distributed and struggle to\\nreduce query completion time with a large volume of data. In this paper, we\\nleverage programmable switches in the network to partially offload query\\ncomputation to the switch. While switches provide high performance, they have\\nresource and programming constraints that make implementing diverse queries\\ndifficult. To fit in these constraints, we introduce the concept of data\\n\\\\emph{pruning} -- filtering out entries that are guaranteed not to affect\\noutput. The database system then runs the same query but on the pruned data,\\nwhich significantly reduces processing time. We propose pruning algorithms for\\na variety of queries. We implement our system, Cheetah, on a Barefoot Tofino\\nswitch and Spark. Our evaluation on multiple workloads shows $40 - 200\\\\%$\\nimprovement in the query completion time compared to Spark.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 10, 15, 34, 15, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Graphsurge: Graph Analytics on View Collections Using Differential Computation',\n",
       "  'authors': ['Siddhartha Sahu', 'Semih Salihoglu'],\n",
       "  'summary': 'This paper presents the design and implementation of a new open-source\\nview-based graph analytics system called Graphsurge. Graphsurge is designed to\\nsupport applications that analyze multiple snapshots or views of a large-scale\\ngraph. Users program Graphsurge through a declarative graph view definition\\nlanguage (GVDL) to create views over input graphs and a Differential\\nDataflow-based programming API to write analytics computations. A key feature\\nof GVDL is the ability to organize views into view collections, which allows\\nGraphsurge to automatically share computation across views, without users\\nwriting any incrementalization code, by performing computations differentially.\\nWe then introduce two optimization problems that naturally arise in our\\nsetting. First is the collection ordering problem to determine the order of\\nviews that leads to minimum differences across consecutive views. We prove this\\nproblem is NP-hard and show a constant-factor approximation algorithm drawn\\nfrom literature. Second is the collection splitting problem to decide on which\\nviews to run computations differentially vs from scratch, for which we present\\nan adaptive solution that makes decisions at runtime. We present extensive\\nexperiments to demonstrate the benefits of running computations differentially\\nfor view collections and our collection ordering and splitting optimizations.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 11, 3, 47, 27, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Locality-Sensitive Hashing Scheme based on Longest Circular Co-Substring',\n",
       "  'authors': ['Yifan Lei',\n",
       "   'Qiang Huang',\n",
       "   'Mohan Kankanhalli',\n",
       "   'Anthony K. H. Tung'],\n",
       "  'summary': 'Locality-Sensitive Hashing (LSH) is one of the most popular methods for\\n$c$-Approximate Nearest Neighbor Search ($c$-ANNS) in high-dimensional spaces.\\nIn this paper, we propose a novel LSH scheme based on the Longest Circular\\nCo-Substring (LCCS) search framework (LCCS-LSH) with a theoretical guarantee.\\nWe introduce a novel concept of LCCS and a new data structure named Circular\\nShift Array (CSA) for $k$-LCCS search. The insight of LCCS search framework is\\nthat close data objects will have a longer LCCS than the far-apart ones with\\nhigh probability. LCCS-LSH is \\\\emph{LSH-family-independent}, and it supports\\n$c$-ANNS with different kinds of distance metrics. We also introduce a\\nmulti-probe version of LCCS-LSH and conduct extensive experiments over five\\nreal-life datasets. The experimental results demonstrate that LCCS-LSH\\noutperforms state-of-the-art LSH schemes.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 11, 9, 24, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Relational Matrix Algebra and its Implementation in a Column Store',\n",
       "  'authors': ['Oksana Dolmatova', 'Nikolaus Augsten', 'Michael H. Boehlen'],\n",
       "  'summary': 'Analytical queries often require a mixture of relational and linear algebra\\noperations applied to the same data. This poses a challenge to analytic systems\\nthat must bridge the gap between relations and matrices. Previous work has\\nmainly strived to fix the problem at the implementation level. This paper\\nproposes a principled solution at the logical level. We introduce the\\nrelational matrix algebra (RMA), which seamlessly integrates linear algebra\\noperations into the relational model and eliminates the dichotomy between\\nmatrices and relations. RMA is closed: All our relational matrix operations are\\nperformed on relations and result in relations; no additional data structure is\\nrequired. Our implementation in MonetDB shows the feasibility of our approach,\\nand empirical evaluations suggest that in-database analytics performs well for\\nmixed workloads.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 12, 0, 59, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Comparative Analysis of Knowledge Graph Query Performance',\n",
       "  'authors': ['Masoud Salehpour', 'Joseph G. Davis'],\n",
       "  'summary': 'As Knowledge Graphs (KGs) continue to gain widespread momentum for use in\\ndifferent domains, storing the relevant KG content and efficiently executing\\nqueries over them are becoming increasingly important. A range of Data\\nManagement Systems (DMSs) have been employed to process KGs. This paper aims to\\nprovide an in-depth analysis of query performance across diverse DMSs and KG\\nquery types. Our aim is to provide a fine-grained, comparative analysis of four\\nmajor DMS types, namely, row-, column-, graph-, and document-stores, against\\nmajor query types, namely, subject-subject, subject-object, tree-like, and\\noptional joins. In particular, we analyzed the performance of row-store\\nVirtuoso, column-store Virtuoso, Blazegraph (i.e., graph-store), and MongoDB\\n(i.e., document-store) using five well-known benchmarks, namely, BSBM, WatDiv,\\nFishMark, BowlognaBench, and BioBench-Allie. Our results show that no single\\nDMS displays superior query performance across the four query types. In\\nparticular, row- and column-store Virtuoso are a factor of 3-8 faster for\\ntree-like joins, Blazegraph performs around one order of magnitude faster for\\nsubject-object joins, and MongoDB performs over one order of magnitude faster\\nfor high-selective queries.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 8, 23, 4, 16, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A1: A Distributed In-Memory Graph Database',\n",
       "  'authors': ['Chiranjeeb Buragohain',\n",
       "   'Knut Magne Risvik',\n",
       "   'Paul Brett',\n",
       "   'Miguel Castro',\n",
       "   'Wonhee Cho',\n",
       "   'Joshua Cowhig',\n",
       "   'Nikolas Gloy',\n",
       "   'Karthik Kalyanaraman',\n",
       "   'Richendra Khanna',\n",
       "   'John Pao',\n",
       "   'Matthew Renzelmann',\n",
       "   'Alex Shamis',\n",
       "   'Timothy Tan',\n",
       "   'Shuheng Zheng'],\n",
       "  'summary': 'A1 is an in-memory distributed database used by the Bing search engine to\\nsupport complex queries over structured data. The key enablers for A1 are\\navailability of cheap DRAM and high speed RDMA (Remote Direct Memory Access)\\nnetworking in commodity hardware. A1 uses FaRM as its underlying storage layer\\nand builds the graph abstraction and query engine on top. The combination of\\nin-memory storage and RDMA access requires rethinking how data is allocated,\\norganized and queried in a large distributed system. A single A1 cluster can\\nstore tens of billions of vertices and edges and support a throughput of 350+\\nmillion of vertex reads per second with end to end query latency in single\\ndigit milliseconds. In this paper we describe the A1 data model, RDMA optimized\\ndata structures and query execution.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 12, 22, 58, 46, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Complaint-driven Training Data Debugging for Query 2.0',\n",
       "  'authors': ['Weiyuan Wu', 'Lampros Flokas', 'Eugene Wu', 'Jiannan Wang'],\n",
       "  'summary': 'As the need for machine learning (ML) increases rapidly across all industry\\nsectors, there is a significant interest among commercial database providers to\\nsupport \"Query 2.0\", which integrates model inference into SQL queries.\\nDebugging Query 2.0 is very challenging since an unexpected query result may be\\ncaused by the bugs in training data (e.g., wrong labels, corrupted features).\\nIn response, we propose Rain, a complaint-driven training data debugging\\nsystem. Rain allows users to specify complaints over the query\\'s intermediate\\nor final output, and aims to return a minimum set of training examples so that\\nif they were removed, the complaints would be resolved. To the best of our\\nknowledge, we are the first to study this problem. A naive solution requires\\nretraining an exponential number of ML models. We propose two novel heuristic\\napproaches based on influence functions which both require linear retraining\\nsteps. We provide an in-depth analytical and empirical analysis of the two\\napproaches and conduct extensive experiments to evaluate their effectiveness\\nusing four real-world datasets. Results show that Rain achieves the highest\\nrecall@k among all the baselines while still returns results interactively.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 12, 23, 56, 6, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'SLIM: Scalable Linkage of Mobility Data',\n",
       "  'authors': ['Fuat Basık', 'Hakan Ferhatosmanoğlu', 'Buğra Gedik'],\n",
       "  'summary': 'We present a scalable solution to link entities across mobility datasets\\nusing their spatio-temporal information. This is a fundamental problem in many\\napplications such as linking user identities for security, understanding\\nprivacy limitations of location based services, or producing a unified dataset\\nfrom multiple sources for urban planning. Such integrated datasets are also\\nessential for service providers to optimise their services and improve business\\nintelligence. In this paper, we first propose a mobility based representation\\nand similarity computation for entities. An efficient matching process is then\\ndeveloped to identify the final linked pairs, with an automated mechanism to\\ndecide when to stop the linkage. We scale the process with a locality-sensitive\\nhashing (LSH) based approach that significantly reduces candidate pairs for\\nmatching. To realize the effectiveness and efficiency of our techniques in\\npractice, we introduce an algorithm called SLIM. In the experimental\\nevaluation, SLIM outperforms the two existing state-of-the-art approaches in\\nterms of precision and recall. Moreover, the LSH-based approach brings two to\\nfour orders of magnitude speedup.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 13, 14, 7, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Near-Optimal Distributed Band-Joins through Recursive Partitioning',\n",
       "  'authors': ['Rundong Li', 'Wolfgang Gatterbauer', 'Mirek Riedewald'],\n",
       "  'summary': 'We consider running-time optimization for band-joins in a distributed system,\\ne.g., the cloud. To balance load across worker machines, input has to be\\npartitioned, which causes duplication. We explore how to resolve this tension\\nbetween maximum load per worker and input duplication for band-joins between\\ntwo relations. Previous work suffered from high optimization cost or considered\\npartitionings that were too restricted (resulting in suboptimal join\\nperformance). Our main insight is that recursive partitioning of the\\njoin-attribute space with the appropriate split scoring measure can achieve\\nboth low optimization cost and low join cost. It is the first approach that is\\nnot only effective for one-dimensional band-joins but also for joins on\\nmultiple attributes. Experiments indicate that our method is able to find\\npartitionings that are within 10% of the lower bound for both maximum load per\\nworker and input duplication for a broad range of settings, significantly\\nimproving over previous work.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 13, 17, 59, 27, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Knowledge Graphs for Processing Scientific Data: Challenges and Prospects',\n",
       "  'authors': ['Masoud Salehpour', 'Joseph G. Davis'],\n",
       "  'summary': 'There is growing interest in the use of Knowledge Graphs (KGs) for the\\nrepresentation, exchange, and reuse of scientific data. While KGs offer the\\nprospect of improving the infrastructure for working with scalable and reusable\\nscholarly data consistent with the FAIR (Findability, Accessibility,\\nInteroperability, and Reusability) principles, the state-of-the-art Data\\nManagement Systems (DMSs) for processing large KGs leave somewhat to be\\ndesired. In this paper, we studied the performance of some of the major DMSs in\\nthe context of querying KGs with the goal of providing a finely-grained,\\ncomparative analysis of DMSs representing each of the four major DMS types. We\\nexperimented with four well-known scientific KGs, namely, Allie, Cellcycle,\\nDrugBank, and LinkedSPL against Virtuoso, Blazegraph, RDF-3X, and MongoDB as\\nthe representative DMSs. Our results suggest that the DMSs display limitations\\nin processing complex queries on the KG datasets. Depending on the query type,\\nthe performance differentials can be several orders of magnitude. Also, no\\nsingle DMS appears to offer consistently superior performance. We present an\\nanalysis of the underlying issues and outline two integrated approaches and\\nproposals for resolving the problem.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 8, 23, 12, 12, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'BugDoc: Algorithms to Debug Computational Processes',\n",
       "  'authors': ['Raoni Lourenço', 'Juliana Freire', 'Dennis Shasha'],\n",
       "  'summary': 'Data analysis for scientific experiments and enterprises, large-scale\\nsimulations, and machine learning tasks all entail the use of complex\\ncomputational pipelines to reach quantitative and qualitative conclusions. If\\nsome of the activities in a pipeline produce erroneous outputs, the pipeline\\nmay fail to execute or produce incorrect results. Inferring the root cause(s)\\nof such failures is challenging, usually requiring time and much human thought,\\nwhile still being error-prone. We propose a new approach that makes use of\\niteration and provenance to automatically infer the root causes and derive\\nsuccinct explanations of failures. Through a detailed experimental evaluation,\\nwe assess the cost, precision, and recall of our approach compared to the state\\nof the art. Our experimental data and processing software is available for use,\\nreproducibility, and enhancement.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 12, 20, 13, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Efficient Suspected Infected Crowds Detection Based on Spatio-Temporal Trajectories',\n",
       "  'authors': ['Huajun He',\n",
       "   'Ruiyuan Li',\n",
       "   'Rubin Wang',\n",
       "   'Jie Bao',\n",
       "   'Yu Zheng',\n",
       "   'Tianrui Li'],\n",
       "  'summary': 'Virus transmission from person to person is an emergency event facing the\\nglobal public. Early detection and isolation of potentially susceptible crowds\\ncan effectively control the epidemic of its disease. Existing metrics can not\\ncorrectly address the infected rate on trajectories. To solve this problem, we\\npropose a novel spatio-temporal infected rate (IR) measure based on human\\nmoving trajectories that can adequately describe the risk of being infected by\\na given query trajectory of a patient. Then, we manage source data through an\\nefficient spatio-temporal index to make our system more scalable, and can\\nquickly query susceptible crowds from massive trajectories. Besides, we design\\nseveral pruning strategies that can effectively reduce calculations. Further,\\nwe design a spatial first time (SFT) index, which enables us to quickly query\\nmultiple trajectories without much I/O consumption and data redundancy. The\\nperformance of the solutions is demonstrated in experiments based on real and\\nsynthetic trajectory datasets that have shown the effectiveness and efficiency\\nof our solutions.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 11, 8, 35, 54, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Sieve: A Middleware Approach to Scalable Access Control for Database Management Systems',\n",
       "  'authors': ['Primal Pappachan',\n",
       "   'Roberto Yus',\n",
       "   'Sharad Mehrotra',\n",
       "   'Johann-Christoph Freytag'],\n",
       "  'summary': \"Current approaches of enforcing FGAC in Database Management Systems (DBMS) do\\nnot scale in scenarios when the number of policies are in the order of\\nthousands. This paper identifies such a use case in the context of emerging\\nsmart spaces wherein systems may be required by legislation, such as Europe's\\nGDPR and California's CCPA, to empower users to specify who may have access to\\ntheir data and for what purposes. We present Sieve, a layered approach of\\nimplementing FGAC in existing database systems, that exploits a variety of it's\\nfeatures such as UDFs, index usage hints, query explain; to scale to large\\nnumber of policies. Given a query, Sieve exploits it's context to filter the\\npolicies that need to be checked. Sieve also generates guarded expressions that\\nsaves on evaluation cost by grouping the policies and cuts the read cost by\\nexploiting database indices. Our experimental results, on two DBMS and two\\ndifferent datasets, show that Sieve scales to large data sets and to large\\npolicy corpus thus supporting real-time access in applications including\\nemerging smart environments.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 16, 7, 37, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'ForkBase: Immutable, Tamper-evident Storage Substrate for Branchable Applications',\n",
       "  'authors': ['Qian Lin',\n",
       "   'Kaiyuan Yang',\n",
       "   'Tien Tuan Anh Dinh',\n",
       "   'Qingchao Cai',\n",
       "   'Gang Chen',\n",
       "   'Beng Chin Ooi',\n",
       "   'Pingcheng Ruan',\n",
       "   'Sheng Wang',\n",
       "   'Zhongle Xie',\n",
       "   'Meihui Zhang',\n",
       "   'Olafs Vandans'],\n",
       "  'summary': 'Data collaboration activities typically require systematic or protocol-based\\ncoordination to be scalable. Git, an effective enabler for collaborative\\ncoding, has been attested for its success in countless projects around the\\nworld. Hence, applying the Git philosophy to general data collaboration beyond\\ncoding is motivating. We call it Git for data. However, the original Git design\\nhandles data at the file granule, which is considered too coarse-grained for\\nmany database applications. We argue that Git for data should be co-designed\\nwith database systems. To this end, we developed ForkBase to make Git for data\\npractical. ForkBase is a distributed, immutable storage system designed for\\ndata version management and data collaborative operation. In this\\ndemonstration, we show how ForkBase can greatly facilitate collaborative data\\nmanagement and how its novel data deduplication technique can improve storage\\nefficiency for archiving massive data versions.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 16, 10, 52, 25, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Holding a Conference Online and Live due to COVID-19',\n",
       "  'authors': ['Angela Bonifati',\n",
       "   'Giovanna Guerrini',\n",
       "   'Carsten Lutz',\n",
       "   'Wim Martens',\n",
       "   'Lara Mazilu',\n",
       "   'Norman Paton',\n",
       "   'Marcos Antonio Vaz Salles',\n",
       "   'Marc H. Scholl',\n",
       "   'Yongluan Zhou'],\n",
       "  'summary': 'The joint EDBT/ICDT conference (International Conference on Extending\\nDatabase Technology/International Conference on Database Theory) is a well\\nestablished conference series on data management, with annual meetings in the\\nsecond half of March that attract 250 to 300 delegates. Three weeks before\\nEDBT/ICDT 2020 was planned to take place in Copenhagen, the rapidly developing\\nCovid-19 pandemic led to the decision to cancel the face-to-face event. In the\\ninterest of the research community, it was decided to move the conference\\nonline while trying to preserve as much of the real-life experience as\\npossible. As far as we know, we are one of the first conferences that moved to\\na fully synchronous online experience due to the COVID-19 outbreak. With fully\\nsynchronous, we mean that participants jointly listened to presentations, had\\nlive Q&A, and attended other live events associated with the conference. In\\nthis report, we share our decisions, experiences, and lessons learned.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 16, 14, 5, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Knowledge Scientists: Unlocking the data-driven organization',\n",
       "  'authors': ['George Fletcher', 'Paul Groth', 'Juan Sequeda'],\n",
       "  'summary': 'Organizations across all sectors are increasingly undergoing deep\\ntransformation and restructuring towards data-driven operations. The central\\nrole of data highlights the need for reliable and clean data. Unreliable,\\nerroneous, and incomplete data lead to critical bottlenecks in processing\\npipelines and, ultimately, service failures, which are disastrous for the\\ncompetitive performance of the organization. Given its central importance,\\nthose organizations which recognize and react to the need for reliable data\\nwill have the advantage in the coming decade. We argue that the technologies\\nfor reliable data are driven by distinct concerns and expertise which\\ncomplement those of the data scientist and the data engineer. Those\\norganizations which identify the central importance of meaningful, explainable,\\nreproducible, and maintainable data will be at the forefront of the\\ndemocratization of reliable data. We call the new role which must be developed\\nto fill this critical need the Knowledge Scientist. The organizational\\nstructures, tools, methodologies and techniques to support and make possible\\nthe work of knowledge scientists are still in their infancy. As organizations\\nnot only use data but increasingly rely on data, it is time to empower the\\npeople who are central to this transformation.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 16, 20, 14, 20, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Efficient Constrained Pattern Mining Using Dynamic Item Ordering for Explainable Classification',\n",
       "  'authors': ['Hiroaki Iwashita',\n",
       "   'Takuya Takagi',\n",
       "   'Hirofumi Suzuki',\n",
       "   'Keisuke Goto',\n",
       "   'Kotaro Ohori',\n",
       "   'Hiroki Arimura'],\n",
       "  'summary': 'Learning of interpretable classification models has been attracting much\\nattention for the last few years. Discovery of succinct and contrasting\\npatterns that can highlight the differences between the two classes is very\\nimportant. Such patterns are useful for human experts, and can be used to\\nconstruct powerful classifiers. In this paper, we consider mining of minimal\\nemerging patterns from high-dimensional data sets under a variety of\\nconstraints in a supervised setting. We focus on an extension in which patterns\\ncan contain negative items that designate the absence of an item. In such a\\ncase, a database becomes highly dense, and it makes mining more challenging\\nsince popular pattern mining techniques such as fp-tree and occurrence deliver\\ndo not efficiently work. To cope with this difficulty, we present an efficient\\nalgorithm for mining minimal emerging patterns by combining two techniques:\\ndynamic variable-ordering during pattern search for enhancing pruning effect,\\nand the use of a pointer-based dynamic data structure, called dancing links,\\nfor efficiently maintaining occurrence lists. Experiments on benchmark data\\nsets showed that our algorithm achieves significant speed-ups over emerging\\npattern mining approach based on LCM, a very fast depth-first frequent itemset\\nminer using static variable-ordering.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 17, 1, 22, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Duplication Detection in Knowledge Graphs: Literature and Tools',\n",
       "  'authors': ['Elwin Huaman', 'Elias Kärle', 'Dieter Fensel'],\n",
       "  'summary': \"In recent years, an increasing amount of knowledge graphs (KGs) have been\\ncreated as a means to store cross-domain knowledge and billion of facts, which\\nare the basis of costumers' applications like search engines. However, KGs\\ninevitably have inconsistencies such as duplicates that might generate\\nconflicting property values. Duplication detection (DD) aims to identify\\nduplicated entities and resolve their conflicting property values effectively\\nand efficiently. In this paper, we perform a literature review on DD methods\\nand tools, and an evaluation of them. Our main contributions are a performance\\nevaluation of DD tools in KGs, improvement suggestions, and a DD workflow to\\nsupport future development of DD tools, which are based on desirable features\\ndetected through this study.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 17, 14, 12, 40, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Time Series Data Cleaning with Regular and Irregular Time Intervals',\n",
       "  'authors': ['Xi Wang', 'Chen Wang'],\n",
       "  'summary': 'Errors are prevalent in time series data, especially in the industrial field.\\nData with errors could not be stored in the database, which results in the loss\\nof data assets. Handling the dirty data in time series is non-trivial, when\\ngiven irregular time intervals. At present, to deal with these time series\\ncontaining errors, besides keeping original erroneous data, discarding\\nerroneous data and manually checking erroneous data, we can also use the\\ncleaning algorithm widely used in the database to automatically clean the time\\nseries data. This survey provides a classification of time series data cleaning\\ntechniques and comprehensively reviews the state-of-the-art methods of each\\ntype. In particular, we have a special focus on the irregular time intervals.\\nBesides we summarize data cleaning tools, systems and evaluation criteria from\\nresearch and industry. Finally, we highlight possible directions time series\\ndata cleaning.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 17, 14, 53, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Automated System Performance Testing at MongoDB',\n",
       "  'authors': ['Henrik Ingo', 'David Daly'],\n",
       "  'summary': \"Distributed Systems Infrastructure (DSI) is MongoDB's framework for running\\nfully automated system performance tests in our Continuous Integration (CI)\\nenvironment. To run in CI it needs to automate everything end-to-end:\\nprovisioning and deploying multi-node clusters, executing tests, tuning the\\nsystem for repeatable results, and collecting and analyzing the results. Today\\nDSI is MongoDB's most used and most useful performance testing tool. It runs\\nalmost 200 different benchmarks in daily CI, and we also use it for manual\\nperformance investigations. As we can alert the responsible engineer in a\\ntimely fashion, all but one of the major regressions were fixed before the\\n4.2.0 release. We are also able to catch net new improvements, of which DSI\\ncaught 17. We open sourced DSI in March 2020.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 17, 19, 14, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'MorphStore: Analytical Query Engine with a Holistic Compression-Enabled Processing Model',\n",
       "  'authors': ['Patrick Damme',\n",
       "   'Annett Ungethüm',\n",
       "   'Johannes Pietrzyk',\n",
       "   'Alexander Krause',\n",
       "   'Dirk Habich',\n",
       "   'Wolfgang Lehner'],\n",
       "  'summary': 'In this paper, we present MorphStore, an open-source in-memory columnar\\nanalytical query engine with a novel holistic compression-enabled processing\\nmodel. Basically, compression using lightweight integer compression algorithms\\nalready plays an important role in existing in-memory column-store database\\nsystems, but mainly for base data. In particular, during query processing,\\nthese systems only keep the data compressed until an operator cannot process\\nthe compressed data directly, whereupon the data is decompressed, but not\\nrecompressed. Thus, the full potential of compression during query processing\\nis not exploited. To overcome that, we developed a novel compression-enabled\\nprocessing model as presented in this paper. As we are going to show, the\\ncontinuous usage of compression for all base data and all intermediates is very\\nbeneficial to reduce the overall memory footprint as well as to improve the\\nquery performance.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 20, 14, 50, 50, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Breaking Down Memory Walls: Adaptive Memory Management in LSM-based Storage Systems (Extended Version)',\n",
       "  'authors': ['Chen Luo', 'Michael J. Carey'],\n",
       "  'summary': 'Log-Structured Merge-trees (LSM-trees) have been widely used in modern NoSQL\\nsystems. Due to their out-of-place update design, LSM-trees have introduced\\nmemory walls among the memory components of multiple LSM-trees and between the\\nwrite memory and the buffer cache. Optimal memory allocation among these\\nregions is non-trivial because it is highly workload-dependent. Existing\\nLSM-tree implementations instead adopt static memory allocation schemes due to\\ntheir simplicity and robustness, sacrificing performance. In this paper, we\\nattempt to break down these memory walls in LSM-based storage systems. We first\\npresent a memory management architecture that enables adaptive memory\\nmanagement. We then present a partitioned memory component structure with new\\nflush policies to better exploit the write memory to minimize the write cost.\\nTo break down the memory wall between the write memory and the buffer cache, we\\nfurther introduce a memory tuner that tunes the memory allocation between these\\ntwo regions. We have conducted extensive experiments in the context of Apache\\nAsterixDB using the YCSB and TPC-C benchmarks and we present the results here.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 22, 1, 34, 58, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An Efficient Index Method for the Optimal Route Query over Multi-Cost Networks',\n",
       "  'authors': ['Yajun Yang',\n",
       "   'Hang Zhang',\n",
       "   'Hong Gao',\n",
       "   'Qinghua Hu',\n",
       "   'Xin Wang'],\n",
       "  'summary': 'Smart city has been consider the wave of the future and the route\\nrecommendation in networks is a fundamental problem in it. Most existing\\napproaches for the shortest route problem consider that there is only one kind\\nof cost in networks. However, there always are several kinds of cost in\\nnetworks and users prefer to select an optimal route under the global\\nconsideration of these kinds of cost. In this paper, we study the problem of\\nfinding the optimal route in the multi-cost networks. We prove this problem is\\nNP-hard and the existing index techniques cannot be used to this problem. We\\npropose a novel partition-based index with contour skyline techniques to find\\nthe optimal route. We propose a vertex-filtering algorithm to facilitate the\\nquery processing. We conduct extensive experiments on six real-life networks\\nand the experimental results show that our method has an improvement in\\nefficiency by an order of magnitude compared to the previous heuristic\\nalgorithms.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 26, 16, 4, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'SFTM: Fast Comparison of Web Documents using Similarity-based Flexible Tree Matching',\n",
       "  'authors': ['Sacha Brisset',\n",
       "   'Romain Rouvoy',\n",
       "   'Renaud Pawlak',\n",
       "   'Lionel Seinturier'],\n",
       "  'summary': 'Tree matching techniques have been investigated in many fields, including web\\ndata mining and extraction, as a key component to analyze the content of web\\ndocuments, existing tree matching approaches, like Tree-Edit Distance (TED) or\\nFlexible Tree Matching (FTM), fail to scale beyond a few hundreds of nodes,\\nwhich is far below the average complexity of existing web online documents and\\napplications. In this paper, we therefore propose a novel Similarity-based\\nFlexible Tree Matching algorithm (SFTM), which is the first algorithm to enable\\ntree matching on real-life web documents with practical computation times. In\\nparticular, we approach tree matching as an optimisation problem and we\\nleverage node labels and local topology similarity in order to avoid any\\ncombinatorial explosion. Our practical evaluation demonstrates that our\\napproach compares to the reference implementation of TED qualitatively, while\\nimproving the computation times by two orders of magnitude.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 27, 14, 2, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data Engineering for Data Analytics: A Classification of the Issues, and Case Studies',\n",
       "  'authors': ['Alfredo Nazabal',\n",
       "   'Christopher K. I. Williams',\n",
       "   'Giovanni Colavizza',\n",
       "   'Camila Rangel Smith',\n",
       "   'Angus Williams'],\n",
       "  'summary': \"Consider the situation where a data analyst wishes to carry out an analysis\\non a given dataset. It is widely recognized that most of the analyst's time\\nwill be taken up with \\\\emph{data engineering} tasks such as acquiring,\\nunderstanding, cleaning and preparing the data. In this paper we provide a\\ndescription and classification of such tasks into high-levels groups, namely\\ndata organization, data quality and feature engineering. We also make available\\nfour datasets and example analyses that exhibit a wide variety of these\\nproblems, to help encourage the development of tools and techniques to help\\nreduce this burden and push forward research towards the automation or\\nsemi-automation of the data engineering process.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 27, 16, 42, 40, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'KGClean: An Embedding Powered Knowledge Graph Cleaning Framework',\n",
       "  'authors': ['Congcong Ge',\n",
       "   'Yunjun Gao',\n",
       "   'Honghui Weng',\n",
       "   'Chong Zhang',\n",
       "   'Xiaoye Miao',\n",
       "   'Baihua Zheng'],\n",
       "  'summary': 'The quality assurance of the knowledge graph is a prerequisite for various\\nknowledge-driven applications. We propose KGClean, a novel cleaning framework\\npowered by knowledge graph embedding, to detect and repair the heterogeneous\\ndirty data. In contrast to previous approaches that either focus on filling\\nmissing data or clean errors violated limited rules, KGClean enables (i)\\ncleaning both missing data and other erroneous values, and (ii) mining\\npotential rules automatically, which expands the coverage of error detecting.\\nKGClean first learns data representations by TransGAT, an effective knowledge\\ngraph embedding model, which gathers the neighborhood information of each data\\nand incorporates the interactions among data for casting data to continuous\\nvector spaces with rich semantics. KGClean integrates an active learning-based\\nclassification model, which identifies errors with a small seed of labels.\\nKGClean utilizes an efficient PRO-repair strategy to repair errors using a\\nnovel concept of propagation power. Extensive experiments on four typical\\nknowledge graphs demonstrate the effectiveness of KGClean in practice.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 26, 12, 19, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'RadixSpline: A Single-Pass Learned Index',\n",
       "  'authors': ['Andreas Kipf',\n",
       "   'Ryan Marcus',\n",
       "   'Alexander van Renen',\n",
       "   'Mihail Stoian',\n",
       "   'Alfons Kemper',\n",
       "   'Tim Kraska',\n",
       "   'Thomas Neumann'],\n",
       "  'summary': 'Recent research has shown that learned models can outperform state-of-the-art\\nindex structures in size and lookup performance. While this is a very promising\\nresult, existing learned structures are often cumbersome to implement and are\\nslow to build. In fact, most approaches that we are aware of require multiple\\ntraining passes over the data.\\n  We introduce RadixSpline (RS), a learned index that can be built in a single\\npass over the data and is competitive with state-of-the-art learned index\\nmodels, like RMI, in size and lookup performance. We evaluate RS using the SOSD\\nbenchmark and show that it achieves competitive results on all datasets,\\ndespite the fact that it only has two parameters.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 30, 1, 56, 54, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Graph Summarization',\n",
       "  'authors': ['Angela Bonifati', 'Stefania Dumbrava', 'Haridimos Kondylakis'],\n",
       "  'summary': 'The continuous and rapid growth of highly interconnected datasets, which are\\nboth voluminous and complex, calls for the development of adequate processing\\nand analytical techniques. One method for condensing and simplifying such\\ndatasets is graph summarization. It denotes a series of application-specific\\nalgorithms designed to transform graphs into more compact representations while\\npreserving structural patterns, query answers, or specific property\\ndistributions. As this problem is common to several areas studying graph\\ntopologies, different approaches, such as clustering, compression, sampling, or\\ninfluence detection, have been proposed, primarily based on statistical and\\noptimization methods. The focus of our chapter is to pinpoint the main graph\\nsummarization methods, but especially to focus on the most recent approaches\\nand novel research trends on this topic, not yet covered by previous surveys.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 30, 14, 7, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Efficiently Reclaiming Space in a Log Structured Store',\n",
       "  'authors': ['David Lomet', 'Chen Luo'],\n",
       "  'summary': 'A log structured store uses a single write I/O for a number of diverse and\\nnon-contiguous pages within a large buffer instead of using a write I/O for\\neach page separately. This requires that pages be relocated on every write,\\nbecause pages are never updated in place. Instead, pages are dynamically\\nremapped on every write. Log structuring was invented for and used initially in\\nfile systems. Today, a form of log structuring is used in SSD controllers\\nbecause an SSD requires the erasure of a large block of pages before flash\\nstorage can be reused. No update-in-place requires that the storage for\\nout-of-date pages be reclaimed (garbage collected or \"cleaned\"). We analyze\\ncleaning performance and introduce a cleaning strategy that uses a new way to\\nprioritize the order in which stale pages are garbage collected. Our cleaning\\nstrategy approximates an \"optimal cleaning strategy\". Simulation studies\\nconfirm the results of the analysis. This strategy is a significant improvement\\nover previous cleaning strategies.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 4, 30, 18, 29, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The ReProVide Query-Sequence Optimization in a Hardware-Accelerated DBMS',\n",
       "  'authors': ['Lekshmi B. G.', 'Andreas Becher', 'Klaus Meyer-Wegener'],\n",
       "  'summary': 'Hardware acceleration of database query processing can be done with the help\\nof FPGAs. In particular, they are partially reconfigurable at runtime, which\\nallows for the runtime adaption of the hardware to a variety of queries.\\nReconfiguration itself, however, takes some time. As the affected area of the\\nFPGA is not available for computations during the reconfiguration, avoiding\\nsome of the reconfigurations can improve overall performance. This paper\\npresents optimizations based on query sequences, which reduces the impact of\\nthe reconfigurations. Knowledge of upcoming queries is used to (I)\\nspeculatively start reconfiguration already when a query is still running and\\n(II) avoid overwriting of reconfigurable regions that will be used again in\\nsubsequent queries. We evaluate our optimizations with a calibrated model and\\nmeasurements for various parameter values. Improvements in execution time of up\\nto 28% can be obtained even with sequences of only two queries.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 1, 11, 32, 23, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Guided Link-Traversal-Based Query Processing',\n",
       "  'authors': ['Ruben Verborgh', 'Ruben Taelman'],\n",
       "  'summary': 'Link-Traversal-Based Query Processing (LTBQP) is a technique for evaluating\\nqueries over a web of data by starting with a set of seed documents that is\\ndynamically expanded through following hyperlinks. Compared to query evaluation\\nover a static set of sources, LTBQP is significantly slower because of the\\nnumber of needed network requests. Furthermore, there are concerns regarding\\nrelevance and trustworthiness of results, given that sources are selected\\ndynamically. To address both issues, we propose guided LTBQP, a technique in\\nwhich information about document linking structure and content policies is\\npassed to a query processor. Thereby, the processor can prune the search tree\\nof documents by only following relevant links, and restrict the result set to\\ndesired results by limiting which documents are considered for what kinds of\\ncontent. In this exploratory paper, we describe the technique at a high level\\nand sketch some of its applications. We argue that such guidance can make LTBQP\\na valuable query strategy in decentralized environments, where data is spread\\nacross documents with varying levels of user trust.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 3, 22, 35, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': \"Détermination Automatique des Fonctions d'Appartenance et Interrogation Flexible et Coopérative des Bases de Données\",\n",
       "  'authors': ['Narjes Hachani Gharbi'],\n",
       "  'summary': 'Flexible querying of DB allows to extend DBMS in order to support imprecision\\nand flexibility in queries. Flexible queries use vague and imprecise terms\\nwhich have been defined as fuzzy sets. However, there is no consensus on\\nmemberships functions generation. Most of the proposed methods require expert\\nintervention. This thesis is devised in two parts. In the first part, we\\npropose a clustering based approach for automatic and incremental membership\\nfunctions generation. We have proposed the clustering method CLUSTERDB* which\\nevaluates clustering quality underway clusters generation. Moreover, we propose\\nincremental updates of partitions and membership functions after insertion or\\ndeletion of a new object. The second part of this thesis uses these functions\\nand Formal Concepts Analysis in flexible and cooperative querying. In case of\\nempty answers, we formally detect the failure reasons and we generate\\napproximative queries with their answers. These queries help the user to\\nformulate new queries having answers. The different proposed approaches are\\nimplemented and experimented with several databases. The experimentation\\nresults are encouraging.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 7, 22, 33, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Counting Query Answers over a DL-Lite Knowledge Base (extended version)',\n",
       "  'authors': ['Diego Calvanese',\n",
       "   'Julien Corman',\n",
       "   'Davide Lanti',\n",
       "   'Simon Razniewski'],\n",
       "  'summary': 'Counting answers to a query is an operation supported by virtually all\\ndatabase management systems. In this paper we focus on counting answers over a\\nKnowledge Base (KB), which may be viewed as a database enriched with background\\nknowledge about the domain under consideration. In particular, we place our\\nwork in the context of Ontology-Mediated Query Answering/Ontology-based Data\\nAccess (OMQA/OBDA), where the language used for the ontology is a member of the\\nDL-Lite family and the data is a (usually virtual) set of assertions. We study\\nthe data complexity of query answering, for different members of the DL-Lite\\nfamily that include number restrictions, and for variants of conjunctive\\nqueries with counting that differ with respect to their shape (connected,\\nbranching, rooted). We improve upon existing results by providing a PTIME and\\ncoNP lower bounds, and upper bounds in PTIME and LOGSPACE. For the latter case,\\nwe define a novel query rewriting technique into first-order logic with\\ncounting.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 12, 16, 1, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'On Embeddings in Relational Databases',\n",
       "  'authors': ['Siddhant Arora', 'Srikanta Bedathur'],\n",
       "  'summary': 'We address the problem of learning a distributed representation of entities\\nin a relational database using a low-dimensional embedding. Low-dimensional\\nembeddings aim to encapsulate a concise vector representation for an underlying\\ndataset with minimum loss of information. Embeddings across entities in a\\nrelational database have been less explored due to the intricate data relations\\nand representation complexity involved. Relational databases are an\\ninter-weaved collection of relations that not only model relationships between\\nentities but also record complex domain-specific quantitative and temporal\\nattributes of data defining complex relationships among entities. Recent\\nmethods for learning an embedding constitute of a naive approach to consider\\ncomplete denormalization of the database by materializing the full join of all\\ntables and representing as a knowledge graph. This popular approach has certain\\nlimitations as it fails to capture the inter-row relationships and additional\\nsemantics encoded in the relational databases. In this paper we demonstrate; a\\nbetter methodology for learning representations by exploiting the underlying\\nsemantics of columns in a table while using the relation joins and the latent\\ninter-row relationships. Empirical results over a real-world database with\\nevaluations on similarity join and table completion tasks support our\\nproposition.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 13, 17, 21, 27, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Extending Databases to Support Data Manipulation with Functional Dependencies: a Vision Paper',\n",
       "  'authors': ['Nikita Bobrov', 'Kirill Smirnov', 'George Chernishev'],\n",
       "  'summary': 'In the current paper, we propose to fuse together stored data (tables) and\\ntheir functional dependencies (FDs) inside a DBMS. We aim to make FDs\\nfirst-class citizens: objects which can be queried and used to query data. Our\\nidea is to allow analysts to explore both data and functional dependencies\\nusing the database interface. For example, an analyst may be interested in such\\ntasks as: \"find all rows which prevent a given functional dependency from\\nholding\", \"for a given table, find all functional dependencies that involve a\\ngiven attribute\", \"project all attributes that functionally determine a\\nspecified attribute\".\\n  For this purpose, we propose: (1) an SQL-based query language for querying a\\ncollection of functional dependencies (2) an extension of the SQL SELECT clause\\nfor supporting FD-based predicates, including approximate ones (3) a special\\ndata structure intended for containing mined FDs and acting as a mediator\\nbetween user queries and underlying data. We describe the proposed extensions,\\ndemonstrate their use-cases, and finally, discuss implementation details and\\ntheir impact on query processing.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 16, 13, 46, 37, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'kD-STR: A Method for Spatio-Temporal Data Reduction and Modelling',\n",
       "  'authors': ['Liam Steadman',\n",
       "   'Nathan Griffiths',\n",
       "   'Stephen Jarvis',\n",
       "   'Mark Bell',\n",
       "   'Shaun Helman',\n",
       "   'Caroline Wallbank'],\n",
       "  'summary': 'Analysing and learning from spatio-temporal datasets is an important process\\nin many domains, including transportation, healthcare and meteorology. In\\nparticular, data collected by sensors in the environment allows us to\\nunderstand and model the processes acting within the environment. Recently, the\\nvolume of spatio-temporal data collected has increased significantly,\\npresenting several challenges for data scientists. Methods are therefore needed\\nto reduce the quantity of data that needs to be processed in order to analyse\\nand learn from spatio-temporal datasets. In this paper, we present the\\nk-Dimensional Spatio-Temporal Reduction method (kD-STR) for reducing the\\nquantity of data used to store a dataset whilst enabling multiple types of\\nanalysis on the reduced dataset. kD-STR uses hierarchical partitioning to find\\nspatio-temporal regions of similar instances and models the instances within\\neach region to summarise the dataset. We demonstrate the generality of kD-STR\\nwith 3 datasets exhibiting different spatio-temporal characteristics and\\npresent results for a range of data modelling techniques. Finally, we compare\\nkD-STR with other techniques for reducing the volume of spatio-temporal data.\\nOur results demonstrate that kD-STR is effective in reducing spatio-temporal\\ndata and generalises to datasets that exhibit different properties.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 16, 21, 43, 11, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Approximate Denial Constraints',\n",
       "  'authors': ['Ester Livshits',\n",
       "   'Alireza Heidari',\n",
       "   'Ihab F. Ilyas',\n",
       "   'Benny Kimelfeld'],\n",
       "  'summary': 'The problem of mining integrity constraints from data has been extensively\\nstudied over the past two decades for commonly used types of constraints\\nincluding the classic Functional Dependencies (FDs) and the more general Denial\\nConstraints (DCs). In this paper, we investigate the problem of mining\\napproximate DCs (i.e., DCs that are \"almost\" satisfied) from data. Considering\\napproximate constraints allows us to discover more accurate constraints in\\ninconsistent databases, detect rules that are generally correct but may have a\\nfew exceptions, as well as avoid overfitting and obtain more general and less\\ncontrived constraints. We introduce the algorithm ADCMiner for mining\\napproximate DCs. An important feature of this algorithm is that it does not\\nassume any specific definition of an approximate DC, but takes the semantics as\\ninput. Since there is more than one way to define an approximate DC and\\ndifferent definitions may produce very different results, we do not focus on\\none definition, but rather on a general family of approximation functions that\\nsatisfies some natural axioms defined in this paper and captures commonly used\\ndefinitions of approximate constraints. We also show how our algorithm can be\\ncombined with sampling to return results with high accuracy while significantly\\nreducing the running time.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 18, 9, 6, 29, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Machine Learning-based Cardinality Estimation in DBMS on Pre-Aggregated Data',\n",
       "  'authors': ['Lucas Woltmann',\n",
       "   'Claudio Hartmann',\n",
       "   'Dirk Habich',\n",
       "   'Wolfgang Lehner'],\n",
       "  'summary': 'Cardinality estimation is a fundamental task in database query processing and\\noptimization. As shown in recent papers, machine learning (ML)-based approaches\\ncan deliver more accurate cardinality estimations than traditional approaches.\\nHowever, a lot of example queries have to be executed during the model training\\nphase to learn a data-dependent ML model leading to a very time-consuming\\ntraining phase. Many of those example queries use the same base data, have the\\nsame query structure, and only differ in their predicates. Thus, index\\nstructures appear to be an ideal optimization technique at first glance.\\nHowever, their benefit is limited. To speed up this model training phase, our\\ncore idea is to determine a predicate-independent pre-aggregation of the base\\ndata and to execute the example queries over this pre-aggregated data. Based on\\nthis idea, we present a specific aggregate-enabled training phase for ML-based\\ncardinality estimation approaches in this paper. As we are going to show with\\ndifferent workloads in our evaluation, we are able to achieve an average\\nspeedup of 63 with our aggregate-enabled training phase.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 19, 11, 24, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Benchmarking Blocking Algorithms for Web Entities',\n",
       "  'authors': ['Vasilis Efthymiou',\n",
       "   'Kostas Stefanidis',\n",
       "   'Vassilis Christophides'],\n",
       "  'summary': 'An increasing number of entities are described by interlinked data rather\\nthan documents on the Web. Entity Resolution (ER) aims to identify descriptions\\nof the same real-world entity within one or across knowledge bases in the Web\\nof data. To reduce the required number of pairwise comparisons among\\ndescriptions, ER methods typically perform a pre-processing step, called\\n\\\\emph{blocking}, which places similar entity descriptions into blocks and thus\\nonly compare descriptions within the same block. We experimentally evaluate\\nseveral blocking methods proposed for the Web of data using real datasets,\\nwhose characteristics significantly impact their effectiveness and efficiency.\\nThe proposed experimental evaluation framework allows us to better understand\\nthe characteristics of the missed matching entity descriptions and contrast\\nthem with ground truth obtained from different kinds of relatedness links.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 19, 12, 48, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'OBDA for the Web: Creating Virtual RDF Graphs On Top of Web Data Sources',\n",
       "  'authors': ['Konstantina Bereta', 'George Papadakis', 'Manolis Koubarakis'],\n",
       "  'summary': 'Due to Variety, Web data come in many different structures and formats, with\\nHTML tables and REST APIs (e.g., social media APIs) being among the most\\npopular ones. A big subset of Web data is also characterised by Velocity, as\\ndata gets frequently updated so that consumers can obtain the most up-to-date\\nversion of the respective datasets. At the moment, though, these data sources\\nare not effectively supported by Semantic Web tools. To address variety and\\nvelocity, we propose Ontop4theWeb, a system that maps Web data of various\\nformats into virtual RDF triples, thus allowing for querying them on-the-fly\\nwithout materializing them as RDF. We demonstrate how Ontop4theWeb can use\\nSPARQL to uniformly query popular, but heterogeneous Web data sources, like\\nHTML tables and Web APIs. We showcase our approach in a number of use cases,\\nsuch as Twitter, Foursquare, Yelp and HTML tables. We carried out a thorough\\nexperimental evaluation which verifies the high efficiency of our framework,\\nwhich goes beyond the current state-of-the-art in this area, in terms of both\\nfunctionality and performance.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 22, 16, 29, 25, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'QuViS -- The Question of Visual Site Selection',\n",
       "  'authors': ['Sebastian Baumbach',\n",
       "   'Jahanzeb Khan',\n",
       "   'Sheraz Ahmed',\n",
       "   'Andreas Dengel'],\n",
       "  'summary': 'This paper present QuViS, which is an interactive platform for visualization\\nand exploratory data analysis of site selection. The aim of QuViS is to support\\ndecision makers and experts during the process of site selection. In addition\\nto visualization engine for exploratory analysis, QuViS is also integrated with\\nour automatic site selection method (QuIS), which recommend different sites\\nautomatically based on the selected location factors by economists and experts.\\nTo show the potential and highlight the visualization and exploration\\ncapabilities of QuViS, a case study on 1,556 German supermarket site selection\\nis performed. The real publicly available dataset contains 450 location factors\\nfor all 11,162 multiplicities in Germany, covering the last 10-15 years. Case\\nstudy results shows that QuViS provides an easy and intuitive way for\\nexploratory analysis of geospatial multidimensional data.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 21, 8, 58, 4, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'CedrusDB: Persistent Key-Value Store with Memory-Mapped Lazy-Trie',\n",
       "  'authors': ['Maofan Yin',\n",
       "   'Hongbo Zhang',\n",
       "   'Robbert van Renesse',\n",
       "   'Emin Gün Sirer'],\n",
       "  'summary': 'As a result of RAM becoming cheaper, there has been a trend in key-value\\nstore design towards maintaining a fast in-memory index (such as a hash table)\\nwhile logging user operations to disk, allowing high performance under\\nfailure-free conditions while still being able to recover from failures. This\\ndesign, however, comes at the cost of long recovery times or expensive\\ncheckpoint operations. This paper presents a new in-memory index that is also\\nstorage-friendly. A \"lazy-trie\" is a variant of the hash-trie data structure\\nthat achieves near-optimal height, has practical storage overhead, and can be\\nmaintained on-disk with standard write-ahead logging.\\n  We implemented CedrusDB, persistent key-value store based on a lazy-trie. The\\nlazy-trie is kept on disk while made available in memory using standard\\nmemory-mapping. The lazy-trie organization in virtual memory allows CedrusDB to\\nbetter leverage concurrent processing than other on-disk index schemes (LSMs,\\nB+-trees). CedrusDB achieves comparable or superior performance to recent\\nlog-based in-memory key-value stores in mixed workloads while being able to\\nrecover quickly from failures.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 28, 3, 26, 2, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Discovering Domain Orders through Order Dependencies',\n",
       "  'authors': ['Reza Karegar',\n",
       "   'Melicaalsadat Mirsafian',\n",
       "   'Parke Godfrey',\n",
       "   'Lukasz Golab',\n",
       "   'Mehdi Kargar',\n",
       "   'Divesh Srivastava',\n",
       "   'Jaroslaw Szlichta'],\n",
       "  'summary': 'Much real-world data come with explicitly defined domain orders; e.g.,\\nlexicographic order for strings, numeric for integers, and chronological for\\ntime. Our goal is to discover implicit domain orders that we do not already\\nknow; for instance, that the order of months in the Chinese Lunar calendar is\\nCorner < Apricot < Peach. To do so, we enhance data profiling methods by\\ndiscovering implicit domain orders in data through order dependencies. We\\nenumerate tractable special cases and proceed towards the most general case,\\nwhich we prove is NP-complete. We show that the general case nevertheless can\\nbe effectively handled by a SAT solver. We also devise an interestingness\\nmeasure to rank the discovered implicit domain orders, which we validate with a\\nuser study. Based on an extensive suite of experiments with real-world data, we\\nestablish the efficacy of our algorithms, and the utility of the domain orders\\ndiscovered by demonstrating significant added value in three applications (data\\nprofiling, query optimization, and data mining).',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 28, 15, 12, 44, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'From WiscKey to Bourbon: A Learned Index for Log-Structured Merge Trees',\n",
       "  'authors': ['Yifan Dai',\n",
       "   'Yien Xu',\n",
       "   'Aishwarya Ganesan',\n",
       "   'Ramnatthan Alagappan',\n",
       "   'Brian Kroth',\n",
       "   'Andrea C. Arpaci-Dusseau',\n",
       "   'Remzi H. Arpaci-Dusseau'],\n",
       "  'summary': 'We introduce BOURBON, a log-structured merge (LSM) tree that utilizes machine\\nlearning to provide fast lookups. We base the design and implementation of\\nBOURBON on empirically-grounded principles that we derive through careful\\nanalysis of LSM design. BOURBON employs greedy piecewise linear regression to\\nlearn key distributions, enabling fast lookup with minimal computation, and\\napplies a cost-benefit strategy to decide when learning will be worthwhile.\\nThrough a series of experiments on both synthetic and real-world datasets, we\\nshow that BOURBON improves lookup performance by 1.23x-1.78x as compared to\\nstate-of-the-art production LSMs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 5, 28, 18, 5, 46, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Query Based Access Control for Linked Data',\n",
       "  'authors': ['Sabrina Kirrane',\n",
       "   'Alessandra Mileo',\n",
       "   'Axel Polleres',\n",
       "   'Stefan Decker'],\n",
       "  'summary': 'In recent years we have seen significant advances in the technology used to\\nboth publish and consume Linked Data. However, in order to support the next\\ngeneration of ebusiness applications on top of interlinked machine readable\\ndata suitable forms of access control need to be put in place. Although a\\nnumber of access control models and frameworks have been put forward, very\\nlittle research has been conducted into the security implications associated\\nwith granting access to partial data or the correctness of the proposed access\\ncontrol mechanisms. Therefore the contributions of this paper are two fold: we\\npropose a query rewriting algorithm which can be used to partially restrict\\naccess to SPARQL 1.1 queries and updates; and we demonstrate how a set of\\ncriteria, which was originally used to verify that an access control policy\\nholds over different database states, can be adapted to verify the correctness\\nof access control via query rewriting.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 1, 13, 1, 45, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Detecting Opportunities for Differential Maintenance of Extracted Views',\n",
       "  'authors': ['Besat Kassaie', 'Frank Wm. Tompa'],\n",
       "  'summary': 'Semi-structured and unstructured data management is challenging, but many of\\nthe problems encountered are analogous to problems already addressed in the\\nrelational context. In the area of information extraction, for example, the\\nshift from engineering ad hoc, application-specific extraction rules towards\\nusing expressive languages such as CPSL and AQL creates opportunities to\\npropose solutions that can be applied to a wide range of extraction programs.\\nIn this work, we focus on extracted view maintenance, a problem that is\\nwell-motivated and thoroughly addressed in the relational setting. In\\nparticular, we formalize and address the problem of keeping extracted relations\\nconsistent with source documents that can be arbitrarily updated. We formally\\ncharacterize three classes of document updates, namely those that are\\nirrelevant, autonomously computable, and pseudo-irrelevant with respect to a\\ngiven extractor. Finally, we propose algorithms to detect pseudo-irrelevant\\ndocument updates with respect to extractors that are expressed as document\\nspanners, a model of information extraction inspired by SystemT.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 4, 0, 24, 2, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Topic-based Community Search over Spatial-Social Networks (Technical Report)',\n",
       "  'authors': ['Ahmed Al-Baghdadi', 'Xiang Lian'],\n",
       "  'summary': 'Recently, the community search problem has attracted significant attention,\\ndue to its wide spectrum of real-world applications such as event organization,\\nfriend recommendation, advertisement in e-commence, and so on. Given a query\\nvertex, the community search problem finds dense subgraph that contains the\\nquery vertex. In social networks, users have multiple check-in locations,\\ninfluence score, and profile information (keywords). Most previous studies that\\nsolve the CS problem over social networks usually neglect such information in a\\ncommunity. In this paper, we propose a novel problem, named community search\\nover spatial-social networks (TCS-SSN), which retrieves community with high\\nsocial influence, small traveling time, and covering certain keywords. In order\\nto tackle the TCS-SSN problem over the spatial-social networks, we design\\neffective pruning techniques to reduce the problem search space. We also\\npropose an effective indexing mechanism, namely social-spatial index, to\\nfacilitate the community query, and develop an efficient query answering\\nalgorithm via index traversal. We verify the efficiency and effectiveness of\\nour pruning techniques, indexing mechanism, and query processing algorithm\\nthrough extensive experiments on real-world and synthetic data sets under\\nvarious parameter settings.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 6, 19, 0, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'T-REx: Table Repair Explanations',\n",
       "  'authors': ['Daniel Deutch', 'Nave Frost', 'Amir Gilad', 'Oren Sheffer'],\n",
       "  'summary': 'Data repair is a common and crucial step in many frameworks today, as\\napplications may use data from different sources and of different levels of\\ncredibility. Thus, this step has been the focus of many works, proposing\\ndiverse approaches. To assist users in understanding the output of such data\\nrepair algorithms, we propose T-REx, a system for providing data repair\\nexplanations through Shapley values. The system is generic and not specific to\\na given repair algorithm or approach: it treats the algorithm as a black box.\\nGiven a specific table cell selected by the user, T-REx employs Shapley values\\nto explain the significance of each constraint and each table cell in the\\nrepair of the cell of interest. T-REx then ranks the constraints and table\\ncells according to their importance in the repair of this cell. This\\nexplanation allows users to understand the repair process, as well as to act\\nbased on this knowledge, to modify the most influencing constraints or the\\noriginal database.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 8, 21, 39, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Explaining Natural Language Query Results',\n",
       "  'authors': ['Daniel Deutch', 'Nave Frost', 'Amir Gilad'],\n",
       "  'summary': 'Multiple lines of research have developed Natural Language (NL) interfaces\\nfor formulating database queries. We build upon this work, but focus on\\npresenting a highly detailed form of the answers in NL. The answers that we\\npresent are importantly based on the provenance of tuples in the query result,\\ndetailing not only the results but also their explanations. We develop a novel\\nmethod for transforming provenance information to NL, by leveraging the\\noriginal NL query structure. Furthermore, since provenance information is\\ntypically large and complex, we present two solutions for its effective\\npresentation as NL text: one that is based on provenance factorization, with\\nnovel desiderata relevant to the NL case, and one that is based on\\nsummarization. We have implemented our solution in an end-to-end system\\nsupporting questions, answers and provenance, all expressed in NL. Our\\nexperiments, including a user study, indicate the quality of our solution and\\nits scalability.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 8, 22, 0, 1, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Open Data Quality Evaluation: A Comparative Analysis of Open Data in Latvia',\n",
       "  'authors': ['Anastasija Nikiforova'],\n",
       "  'summary': 'Nowadays open data is entering the mainstream - it is free available for\\nevery stakeholder and is often used in business decision-making. It is\\nimportant to be sure data is trustable and error-free as its quality problems\\ncan lead to huge losses. The research discusses how (open) data quality could\\nbe assessed. It also covers main points which should be considered developing a\\ndata quality management solution. One specific approach is applied to several\\nLatvian open data sets. The research provides a step-by-step open data sets\\nanalysis guide and summarizes its results. It is also shown there could exist\\ndifferences in data quality depending on data supplier (centralized and\\ndecentralized data releases) and, unfortunately, trustable data supplier cannot\\nguarantee data quality problems absence. There are also underlined common data\\nquality problems detected not only in Latvian open data but also in open data\\nof 3 European countries.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 9, 10, 43, 28, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'COBRA: Compression via Abstraction of Provenance for Hypothetical Reasoning',\n",
       "  'authors': ['Daniel Deutch', 'Yuval Moskovitch', 'Noam Rinetzky'],\n",
       "  'summary': 'Data analytics often involves hypothetical reasoning: repeatedly modifying\\nthe data and observing the induced effect on the computation result of a\\ndata-centric application. Recent work has proposed to leverage ideas from data\\nprovenance tracking towards supporting efficient hypothetical reasoning:\\ninstead of a costly re-execution of the underlying application, one may assign\\nvalues to a pre-computed provenance expression. A prime challenge in leveraging\\nthis approach for large-scale data and complex applications lies in the size of\\nthe provenance. To this end, we present a framework that allows to reduce\\nprovenance size. Our approach is based on reducing the provenance granularity\\nusing abstraction. We propose a demonstration of COBRA, a system that allows\\nexamine the effect of the provenance compression on the anticipated analysis\\nresults. We will demonstrate the usefulness of COBRA in the context of business\\ndata analysis.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 10, 13, 55, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Hypothetical Reasoning via Provenance Abstraction',\n",
       "  'authors': ['Daniel Deutch', 'Yuval Moskovitch', 'Noam Rinetzky'],\n",
       "  'summary': 'Data analytics often involves hypothetical reasoning: repeatedly modifying\\nthe data and observing the induced effect on the computation result of a\\ndata-centric application. Previous work has shown that fine-grained data\\nprovenance can help make such an analysis more efficient: instead of a costly\\nre-execution of the underlying application, hypothetical scenarios are applied\\nto a pre-computed provenance expression. However, storing provenance for\\ncomplex queries and large-scale data leads to a significant overhead, which is\\noften a barrier to the incorporation of provenance-based solutions.\\n  To this end, we present a framework that allows to reduce provenance size.\\nOur approach is based on reducing the provenance granularity using user defined\\nabstraction trees over the provenance variables; the granularity is based on\\nthe anticipated hypothetical scenarios. We formalize the tradeoff between\\nprovenance size and supported granularity of the hypothetical reasoning, and\\nstudy the complexity of the resulting optimization problem, provide efficient\\nalgorithms for tractable cases and heuristics for others. We experimentally\\nstudy the performance of our solution for various queries and abstraction\\ntrees. Our study shows that the algorithms generally lead to substantial\\nspeedup of hypothetical reasoning, with a reasonable loss of accuracy.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 10, 14, 5, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Open Data Quality',\n",
       "  'authors': ['Anastasija Nikiforova'],\n",
       "  'summary': 'The research discusses how (open) data quality could be described, what\\nshould be considered developing a data quality management solution and how it\\ncould be applied to open data to check its quality. The proposed approach\\nfocuses on development of data quality specification which can be executed to\\nget data quality evaluation results, find errors in data and possible problems\\nwhich must be solved. The proposed approach is applied to several open data\\nsets to evaluate their quality. Open data is very popular, free available for\\nevery stakeholder - it is often used to make business decisions. It is\\nimportant to be sure that this data is trustable and error-free as its quality\\nproblems can lead to huge losses.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 9, 11, 10, 22, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Continuous Prefetch for Interactive Data Applications',\n",
       "  'authors': ['Haneen Mohammed', 'Ziyun Wei', 'Eugene Wu', 'Ravi Netravali'],\n",
       "  'summary': \"Interactive data visualization and exploration (DVE) applications are often\\nnetwork-bottlenecked due to bursty request patterns, large response sizes, and\\nheterogeneous deployments over a range of networks and devices. This makes it\\ndifficult to ensure consistently low response times (< 100ms). Khameleon is a\\nframework for DVE applications that uses a novel combination of prefetching and\\nresponse tuning to dynamically trade-off response quality for low latency.\\nKhameleon exploits DVE's approximation tolerance: immediate lower-quality\\nresponses are preferable to waiting for complete results. To this end,\\nKhameleon progressively encodes responses, and runs a server-side scheduler\\nthat proactively streams portions of responses using available bandwidth to\\nmaximize user's perceived interactivity. The scheduler involves a complex\\noptimization based on available resources, predicted user interactions, and\\nresponse quality levels; yet, decisions must also be real-time. To overcome\\nthis, Khameleon uses a fast greedy approximation which closely mimics the\\noptimal approach. Using image exploration and visualization applications with\\nreal user interaction traces, we show that across a wide range of network and\\nclient resource conditions, Khameleon outperforms classic prefetching\\napproaches that benefit from perfect prediction models: response latencies with\\nKhameleon are never higher, and typically between 2 to 3 orders of magnitude\\nlower while response quality remains within 50%-80%.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 15, 17, 23, 48, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Tackling scalability issues in mining path patterns from knowledge graphs: a preliminary study',\n",
       "  'authors': ['Pierre Monnin',\n",
       "   'Emmanuel Bresso',\n",
       "   'Miguel Couceiro',\n",
       "   'Malika Smaïl-Tabbone',\n",
       "   'Amedeo Napoli',\n",
       "   'Adrien Coulet'],\n",
       "  'summary': 'Features mined from knowledge graphs are widely used within multiple\\nknowledge discovery tasks such as classification or fact-checking. Here, we\\nconsider a given set of vertices, called seed vertices, and focus on mining\\ntheir associated neighboring vertices, paths, and, more generally, path\\npatterns that involve classes of ontologies linked with knowledge graphs. Due\\nto the combinatorial nature and the increasing size of real-world knowledge\\ngraphs, the task of mining these patterns immediately entails scalability\\nissues. In this paper, we address these issues by proposing a pattern mining\\napproach that relies on a set of constraints (e.g., support or degree\\nthresholds) and the monotonicity property. As our motivation comes from the\\nmining of real-world knowledge graphs, we illustrate our approach with PGxLOD,\\na biomedical knowledge graph.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 17, 8, 36, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Diversifying Anonymized Data with Diversity Constraints',\n",
       "  'authors': ['Mostafa Milani', 'Yu Huang', 'Fei Chiang'],\n",
       "  'summary': 'Recently introduced privacy legislation has aimed to restrict and control the\\namount of personal data published by companies and shared to third parties.\\nMuch of this real data is not only sensitive requiring anonymization, but also\\ncontains characteristic details from a variety of individuals. This diversity\\nis desirable in many applications ranging from Web search to drug and product\\ndevelopment. Unfortunately, data anonymization techniques have largely ignored\\ndiversity in its published result. This inadvertently propagates underlying\\nbias in subsequent data analysis. We study the problem of finding a diverse\\nanonymized data instance where diversity is measured via a set of diversity\\nconstraints. We formalize diversity constraints and study their foundations\\nsuch as implication and satisfiability. We show that determining the existence\\nof a diverse, anonymized instance can be done in PTIME, and we present a\\nclustering-based algorithm. We conduct extensive experiments using real and\\nsynthetic data showing the effectiveness of our techniques, and improvement\\nover existing baselines. Our work aligns with recent trends towards responsible\\ndata science by coupling diversity with privacy-preserving data publishing.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 17, 17, 58, 34, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Graph-based process mining',\n",
       "  'authors': ['Amin Jalali'],\n",
       "  'summary': \"Process mining is an area of research that supports discovering information\\nabout business processes from their execution event logs. The increasing amount\\nof event logs in organizations challenges current process mining techniques,\\nwhich tend to load data into the memory of a computer. This issue limits the\\norganizations to apply process mining on a large scale and introduces risks due\\nto the lack of data management capabilities. Therefore, this paper introduces\\nand formalizes a new approach to store and retrieve event logs into/from graph\\ndatabases. It defines an algorithm to compute Directly Follows Graph (DFG)\\ninside the graph database, which shifts the heavy computation parts of process\\nmining into the graph database. Calculating DFG in graph databases enables\\nleveraging the graph databases' horizontal and vertical scaling capabilities in\\nfavor of applying process mining on a large scale. Besides, it removes the\\nrequirement to move data into analysts' computer. Thus, it enables using data\\nmanagement capabilities in graph databases. We implemented this approach in\\nNeo4j and evaluated its performance compared with current techniques using a\\nreal log file. The result shows that our approach enables the calculation of\\nDFG when the data is much bigger than the computational memory. It also shows\\nbetter performance when dicing data into small chunks.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 18, 7, 15, 32, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Support Aggregate Analytic Window Function over Large Data by Spilling',\n",
       "  'authors': ['Xing Shi', 'Chao Wang'],\n",
       "  'summary': 'Analytic function, also called window function, is to query the aggregation\\nof data over a sliding window. For example, a simple query over the online\\nstock platform is to return the average price of a stock of the last three\\ndays. These functions are commonly used features in SQL databases. They are\\nsupported in most of the commercial databases. With the increasing usage of\\ncloud data infra and machine learning technology, the frequency of queries with\\nanalytic window functions rises. Some analytic functions only require const\\nspace in memory to store the state, such as SUM, AVG, while others require\\nlinear space, such as MIN, MAX. When the window is extremely large, the memory\\nspace to store the state may be too large. In this case, we need to spill the\\nstate to disk, which is a heavy operation. In this paper, we proposed an\\nalgorithm to manipulate the state data in the disk to reduce the disk I/O to\\nmake spill available and efficienct. We analyze the complexity of the algorithm\\nwith different data distribution.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 20, 18, 12, 21, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Buffer Pool Aware Query Scheduling via Deep Reinforcement Learning',\n",
       "  'authors': ['Chi Zhang',\n",
       "   'Ryan Marcus',\n",
       "   'Anat Kleiman',\n",
       "   'Olga Papaemmanouil'],\n",
       "  'summary': 'In this extended abstract, we propose a new technique for query scheduling\\nwith the explicit goal of reducing disk reads and thus implicitly increasing\\nquery performance. We introduce SmartQueue, a learned scheduler that leverages\\noverlapping data reads among incoming queries and learns a scheduling strategy\\nthat improves cache hits. SmartQueue relies on deep reinforcement learning to\\nproduce workload-specific scheduling strategies that focus on long-term\\nperformance benefits while being adaptive to previously-unseen data access\\npatterns. We present results from a proof-of-concept prototype, demonstrating\\nthat learned schedulers can offer significant performance improvements over\\nhand-crafted scheduling heuristics. Ultimately, we make the case that this is a\\npromising research direction at the intersection of machine learning and\\ndatabases.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 21, 2, 28, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Reachability Queries with Label and Substructure Constraints on Knowledge Graphs',\n",
       "  'authors': ['Xiaolong Wan', 'Hongzhi Wang'],\n",
       "  'summary': 'Since knowledge graphs (KGs) describe and model the relationships between\\nentities and concepts in the real world, reasoning on KGs often correspond to\\nthe reachability queries with label and substructure constraints (LSCR).\\nSpecially, for a search path p, LSCR queries not only require that the labels\\nof the edges passed by p are in a certain label set, but also claim that a\\nvertex in p could satisfy a certain substructure constraint. LSCR queries is\\nmuch more complex than the label-constraint reachability (LCR) queries, and\\nthere is no efficient solution for LSCR queries on KGs, to the best of our\\nknowledge. Motivated by this, we introduce two solutions for such queries on\\nKGs, UIS and INS. The former can also be utilized for general edge-labeled\\ngraphs, and is relatively handy for practical implementation. The latter is an\\nefficient local-index-based informed search strategy. An extensive experimental\\nevaluation, on both synthetic and real KGs, illustrates that our solutions can\\nefficiently process LSCR queries on KGs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 23, 9, 40, 50, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Recursive Rules with Aggregation: A Simple Unified Semantics',\n",
       "  'authors': ['Yanhong A. Liu', 'Scott D. Stoller'],\n",
       "  'summary': 'Complex reasoning problems are most clearly and easily specified using\\nlogical rules, but require recursive rules with aggregation such as count and\\nsum for practical applications. Unfortunately, the meaning of such rules has\\nbeen a significant challenge, leading to many disagreeing semantics.\\n  This paper describes a unified semantics for recursive rules with\\naggregation, extending the unified founded semantics and constraint semantics\\nfor recursive rules with negation. The key idea is to support simple expression\\nof the different assumptions underlying different semantics, and orthogonally\\ninterpret aggregation operations using their simple usual meaning. We present a\\nformal definition of the semantics, prove important properties of the\\nsemantics, and compare with prior semantics. In particular, we present an\\nefficient inference over aggregation that gives precise answers to all examples\\nwe have studied from the literature. We also apply our semantics to a wide\\nrange of challenging examples, and show that our semantics is simple and\\nmatches the desired results in all cases. Finally, we describe experiments on\\nthe most challenging examples, exhibiting unexpectedly superior performance\\nover well-known systems when they can compute correct answers.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 26, 4, 42, 44, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The Minimum Description Length Principle for Pattern Mining: A Survey',\n",
       "  'authors': ['Esther Galbrun'],\n",
       "  'summary': 'This is about the Minimum Description Length (MDL) principle applied to\\npattern mining. The length of this description is kept to the minimum.\\n  Mining patterns is a core task in data analysis and, beyond issues of\\nefficient enumeration, the selection of patterns constitutes a major challenge.\\nThe MDL principle, a model selection method grounded in information theory, has\\nbeen applied to pattern mining with the aim to obtain compact high-quality sets\\nof patterns. After giving an outline of relevant concepts from information\\ntheory and coding, as well as of work on the theory behind the MDL and similar\\nprinciples, we review MDL-based methods for mining various types of data and\\npatterns. Finally, we open a discussion on some issues regarding these methods,\\nand highlight currently active related data analysis problems.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 28, 6, 24, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Automated Database Indexing using Model-free Reinforcement Learning',\n",
       "  'authors': ['Gabriel Paludo Licks', 'Felipe Meneguzzi'],\n",
       "  'summary': 'Configuring databases for efficient querying is a complex task, often carried\\nout by a database administrator. Solving the problem of building indexes that\\ntruly optimize database access requires a substantial amount of database and\\ndomain knowledge, the lack of which often results in wasted space and memory\\nfor irrelevant indexes, possibly jeopardizing database performance for querying\\nand certainly degrading performance for updating. We develop an architecture to\\nsolve the problem of automatically indexing a database by using reinforcement\\nlearning to optimize queries by indexing data throughout the lifetime of a\\ndatabase. In our experimental evaluation, our architecture shows superior\\nperformance compared to related work on reinforcement learning and genetic\\nalgorithms, maintaining near-optimal index configurations and efficiently\\nscaling to large databases.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 25, 14, 36, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Aggregate Analytic Window Query over Spatial Data',\n",
       "  'authors': ['Xing Shi', 'Chao Wang'],\n",
       "  'summary': 'Analytic window query is a commonly used query in the relational databases.\\nIt answers the aggregations of data over a sliding window. For example, to get\\nthe average prices of a stock for each day. However, it is not supported in the\\nspatial databases. Because the spatial data are not in a one-dimension space,\\nthere is no straightforward way to extend the original analytic window query to\\nspatial databases. But these queries are useful and meaningful. For example, to\\nfind the average number of visits for all the POIs in the circle with a fixed\\nradius for each POI as the centre. In this paper, we define the aggregate\\nanalytic window query over spatial data and propose algorithms for grid index\\nand tree-index. We also analyze the complexity of the algorithms to prove they\\nare efficient and practical.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 7, 29, 9, 47, 54, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The Data Station: Combining Data, Compute, and Market Forces',\n",
       "  'authors': ['Raul Castro Fernandez',\n",
       "   'Kyle Chard',\n",
       "   'Ben Blaiszik',\n",
       "   'Sanjay Krishnan',\n",
       "   'Aaron Elmore',\n",
       "   'Ziad Obermeyer',\n",
       "   'Josh Risley',\n",
       "   'Sendhil Mullainathan',\n",
       "   'Michael Franklin',\n",
       "   'Ian Foster'],\n",
       "  'summary': 'This paper introduces Data Stations, a new data architecture that we are\\ndesigning to tackle some of the most challenging data problems that we face\\ntoday: access to sensitive data; data discovery and integration; and governance\\nand compliance. Data Stations depart from modern data lakes in that both data\\nand derived data products, such as machine learning models, are sealed and\\ncannot be directly seen, accessed, or downloaded by anyone. Data Stations do\\nnot deliver data to users; instead, users bring questions to data. This\\ninversion of the usual relationship between data and compute mitigates many of\\nthe security risks that are otherwise associated with sharing and working with\\nsensitive data.\\n  Data Stations are designed following the principle that many data problems\\nrequire human involvement, and that incentives are the key to obtaining such\\ninvolvement. To that end, Data Stations implement market designs to create,\\nmanage, and coordinate the use of incentives. We explain the motivation for\\nthis new kind of platform and its design.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 31, 18, 4, 20, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'SparkGOR: A unified framework for genomic data analysis',\n",
       "  'authors': ['Sigmar K. Stefánsson', 'Hákon Guðbjartsson'],\n",
       "  'summary': 'Motivation: Our goal was to combine the capabilities of Spark and GOR into a\\nsingle computing framework for use in analysis of large scale genome data.\\n  Results: We have created a relational query engine that unites SparkSQL and\\nGORpipe into a single declarative query framework. This has been achieved by\\nallowing embedding of SQL expressions into the high-level relational statement\\nsyntax in GOR and by supporting virtual relations and nested GORpipe\\nexpressions within SQL. Furthermore, we have built drivers to enable Spark and\\nGOR to use and leverage their preferred file formats, Parquet and GORZ\\nrespectively, and introduced APIs to allow the use of GOR with Spark\\ndataframes.\\n  Availability: The SparkGOR version of the GORpipe software is open-source and\\nfreely available at https://gorpipe-website.now.sh and\\nhttps://github.com/gorpipe.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 31, 18, 59, 58, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Interactive and Explainable Point-of-Interest Recommendation using Look-alike Groups',\n",
       "  'authors': ['Behrooz Omidvar-Tehrani',\n",
       "   'Sruthi Viswanathan',\n",
       "   'Jean-Michel Renders'],\n",
       "  'summary': 'Recommending Points-of-Interest (POIs) is surfacing in many location-based\\napplications. The literature contains personalized and socialized POI\\nrecommendation approaches which employ historical check-ins and social links to\\nmake recommendations. However these systems still lack customizability\\n(incorporating session-based user interactions with the system) and\\ncontextuality (incorporating the situational context of the user), particularly\\nin cold start situations, where nearly no user information is available. In\\nthis paper, we propose LikeMind, a POI recommendation system which tackles the\\nchallenges of cold start, customizability, contextuality, and explainability by\\nexploiting look-alike groups mined in public POI datasets. LikeMind\\nreformulates the problem of POI recommendation, as recommending explainable\\nlook-alike groups (and their POIs) which are in line with user\\'s interests.\\nLikeMind frames the task of POI recommendation as an exploratory process where\\nusers interact with the system by expressing their favorite POIs, and their\\ninteractions impact the way look-alike groups are selected out. Moreover,\\nLikeMind employs \"mindsets\", which capture actual situation and intent of the\\nuser, and enforce the semantics of POI interestingness. In an extensive set of\\nexperiments, we show the quality of our approach in recommending relevant\\nlook-alike groups and their POIs, in terms of efficiency and effectiveness.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 8, 31, 21, 5, 21, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'ParIS+: Data Series Indexing on Multi-Core Architectures',\n",
       "  'authors': ['Botao Peng', 'Panagiota Fatourou', 'Themis Palpanas'],\n",
       "  'summary': 'Data series similarity search is a core operation for several data series\\nanalysis applications across many different domains. Nevertheless, even\\nstate-of-the-art techniques cannot provide the time performance required for\\nlarge data series collections. We propose ParIS and ParIS+, the first\\ndisk-based data series indices carefully designed to inherently take advantage\\nof multi-core architectures, in order to accelerate similarity search\\nprocessing times. Our experiments demonstrate that ParIS+ completely removes\\nthe CPU latency during index construction for disk-resident data, and for exact\\nquery answering is up to 1 order of magnitude faster than the current state of\\nthe art index scan method, and up to 3 orders of magnitude faster than the\\noptimized serial scan method. ParIS+ (which is an evolution of the ADS+ index)\\nowes its efficiency to the effective use of multi-core and multi-socket\\narchitectures, in order to distribute and execute in parallel both index\\nconstruction and query answering, and to the exploitation of the Single\\nInstruction Multiple Data (SIMD) capabilities of modern CPUs, in order to\\nfurther parallelize the execution of instructions inside each core.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 1, 1, 25, 37, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Tensor Relational Algebra for Machine Learning System Design',\n",
       "  'authors': ['Binhang Yuan',\n",
       "   'Dimitrije Jankov',\n",
       "   'Jia Zou',\n",
       "   'Yuxin Tang',\n",
       "   'Daniel Bourgeois',\n",
       "   'Chris Jermaine'],\n",
       "  'summary': 'We consider the question: what is the abstraction that should be implemented\\nby the computational engine of a machine learning system? Current machine\\nlearning systems typically push whole tensors through a series of compute\\nkernels such as matrix multiplications or activation functions, where each\\nkernel runs on an AI accelerator (ASIC) such as a GPU. This implementation\\nabstraction provides little built-in support for ML systems to scale past a\\nsingle machine, or for handling large models with matrices or tensors that do\\nnot easily fit into the RAM of an ASIC. In this paper, we present an\\nalternative implementation abstraction called the tensor relational algebra\\n(TRA). The TRA is a set-based algebra based on the relational algebra.\\nExpressions in the TRA operate over binary tensor relations, where keys are\\nmulti-dimensional arrays and values are tensors. The TRA is easily executed\\nwith high efficiency in a parallel or distributed environment, and amenable to\\nautomatic optimization. Our empirical study shows that the optimized TRA-based\\nback-end can significantly outperform alternatives for running ML workflows in\\ndistributed clusters.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 1, 15, 51, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'MESSI: In-Memory Data Series Indexing',\n",
       "  'authors': ['Botao Peng', 'Panagiota Fatourou', 'Themis Palpanas'],\n",
       "  'summary': 'Data series similarity search is a core operation for several data series\\nanalysis applications across many different domains. However, the\\nstate-of-the-art techniques fail to deliver the time performance required for\\ninteractive exploration, or analysis of large data series collections. In this\\nwork, we propose MESSI, the first data series index designed for in-memory\\noperation on modern hardware. Our index takes advantage of the modern hardware\\nparallelization opportunities (i.e., SIMD instructions, multi-core and\\nmulti-socket architectures), in order to accelerate both index construction and\\nsimilarity search processing times. Moreover, it benefits from a careful design\\nin the setup and coordination of the parallel workers and data structures, so\\nthat it maximizes its performance for in-memory operations. Our experiments\\nwith synthetic and real datasets demonstrate that overall MESSI is up to 4x\\nfaster at index construction, and up to 11x faster at query answering than the\\nstate-of-the-art parallel approach. MESSI is the first to answer exact\\nsimilarity search queries on 100GB datasets in _50msec (30-75msec across\\ndiverse datasets), which enables real-time, interactive data exploration on\\nvery large data series collections.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 2, 2, 10, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data Series Indexing Gone Parallel',\n",
       "  'authors': ['Botao Peng'],\n",
       "  'summary': 'Data series similarity search is a core operation for several data series\\nanalysis applications across many different domains. However, the\\nstate-of-the-art techniques fail to deliver the time performance required for\\ninteractive exploration, or analysis of large data series collections. In this\\nPh.D. work, we present the first data series indexing solutions, for both\\non-disk and in-memory data, that are designed to inherently take advantage of\\nmulti-core architectures, in order to accelerate similarity search processing\\ntimes. Our experiments on a variety of synthetic and real data demonstrate that\\nour approaches are up to orders of magnitude faster than the alternatives. More\\nspecifically, our on-disk solution can answer exact similarity search queries\\non 100GB datasets in a few seconds, and our in-memory solution in a few\\nmilliseconds, which enables real-time, interactive data exploration on very\\nlarge data series collections.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 2, 2, 26, 19, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'HyperBench: A Benchmark and Tool for Hypergraphs and Empirical Findings',\n",
       "  'authors': ['Wolfgang Fischl',\n",
       "   'Georg Gottlob',\n",
       "   'Davide Mario Longo',\n",
       "   'Reinhard Pichler'],\n",
       "  'summary': 'To cope with the intractability of answering Conjunctive Queries (CQs) and\\nsolving Constraint Satisfaction Problems (CSPs), several notions of hypergraph\\ndecompositions have been proposed -- giving rise to different notions of width,\\nnoticeably, plain, generalized, and fractional hypertree width (hw, ghw, and\\nfhw). Given the increasing interest in using such decomposition methods in\\npractice, a publicly accessible repository of decomposition software, as well\\nas a large set of benchmarks, and a web-accessible workbench for inserting,\\nanalyzing, and retrieving hypergraphs are called for.\\n  We address this need by providing (i) concrete implementations of hypergraph\\ndecompositions (including new practical algorithms), (ii) a new, comprehensive\\nbenchmark of hypergraphs stemming from disparate CQ and CSP collections, and\\n(iii) HyperBench, our new web-inter\\\\-face for accessing the benchmark and the\\nresults of our analyses. In addition, we describe a number of actual\\nexperiments we carried out with this new infrastructure.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 2, 13, 8, 55, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'AnyDB: An Architecture-less DBMS for Any Workload',\n",
       "  'authors': ['Tiemo Bang', 'Norman May', 'Ilia Petrov', 'Carsten Binnig'],\n",
       "  'summary': 'In this paper, we propose a radical new approach for scale-out distributed\\nDBMSs. Instead of hard-baking an architectural model, such as a shared-nothing\\narchitecture, into the distributed DBMS design, we aim for a new class of\\nso-called architecture-less DBMSs. The main idea is that an architecture-less\\nDBMS can mimic any architecture on a per-query basis on-the-fly without any\\nadditional overhead for reconfiguration. Our initial results show that our\\narchitecture-less DBMS AnyDB can provide significant speed-ups across varying\\nworkloads compared to a traditional DBMS implementing a static architecture.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 4, 15, 38, 27, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Universal Layout Emulation for Long-Term Database Archival',\n",
       "  'authors': ['Raja Appuswamy', 'Vincent Joguin'],\n",
       "  'summary': \"Research on alternate media technologies, like film, synthetic DNA, and\\nglass, for long-term data archival has received a lot of attention recently due\\nto the media obsolescence issues faced by contemporary storage media like tape,\\nHard Disk Drives (HDD), and Solid State Disks (SSD). While researchers have\\ndeveloped novel layout and encoding techniques for archiving databases on these\\nnew media types, one key question remains unaddressed: How do we ensure that\\nthe decoders developed today will be available and executable by a user who is\\nrestoring an archived database several decades later in the future, on a\\ncomputing platform that potentially does not even exist today?\\n  In this paper, we make the case for Universal Layout Emulation (ULE), a new\\napproach for future-proof, long-term database archival that advocates archiving\\ndecoders together with the data to ensure successful recovery. In order to do\\nso, ULE brings together concepts from Data Management and Digital Preservation\\ncommunities by using emulation for archiving decoders. In order to show that\\nULE can be implemented in practice, we present the design and evaluation of\\nMicr'Olonys, an end-to-end long-term database archival system that can be used\\nto archive databases using visual analog media like film, microform, and\\narchival paper.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 6, 9, 6, 13, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Sequenced Route Query with Semantic Hierarchy',\n",
       "  'authors': ['Yuya Sasaki',\n",
       "   'Yoshiharu Ishikawa',\n",
       "   'Yasuhiro Fujiwara',\n",
       "   'Makoto Onizuka'],\n",
       "  'summary': 'The trip planning query searches for preferred routes starting from a given\\npoint through multiple Point-of-Interests (PoI) that match user requirements.\\nAlthough previous studies have investigated trip planning queries, they lack\\nflexibility for finding routes because all of them output routes that strictly\\nmatch user requirements. We study trip planning queries that output multiple\\nroutes in a flexible manner. We propose a new type of query called skyline\\nsequenced route (SkySR) query, which searches for all preferred sequenced\\nroutes to users by extending the shortest route search with the semantic\\nsimilarity of PoIs in the route. Flexibility is achieved by the {\\\\it semantic\\nhierarchy} of the PoI category. We propose an efficient algorithm for the SkySR\\nquery, bulk SkySR algorithm that simultaneously searches for sequenced routes\\nand prunes unnecessary routes effectively. Experimental evaluations show that\\nthe proposed approach significantly outperforms the existing approaches in\\nterms of response time (up to four orders of magnitude). Moreover, we develop a\\nprototype service that uses the SkySR query, and conduct a user test to\\nevaluate its usefulness.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 8, 14, 8, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Graph-based keyword search in heterogeneous data sources',\n",
       "  'authors': ['Mhd Yamen Haddad',\n",
       "   'Angelos Anadiotis',\n",
       "   'Yamen Mhd',\n",
       "   'Ioana Manolescu'],\n",
       "  'summary': 'Data journalism is the field of investigative journalism which focuses on\\ndigital data by treating them as first-class citizens. Following the trends in\\nhuman activity, which leaves strong digital traces, data journalism becomes\\nincreasingly important. However, as the number and the diversity of data\\nsources increase, heterogeneous data models with different structure, or even\\nno structure at all, need to be considered in query answering. Inspired by our\\ncollaboration with Le Monde, a leading French newspaper, we designed a novel\\nquery algorithm for exploiting such heterogeneous corpora through keyword\\nsearch. We model our underlying data as graphs and, given a set of search\\nterms, our algorithm nds links between them within and across the heterogeneous\\ndatasets included in the graph. We draw inspiration from prior work on keyword\\nsearch in structured and unstructured data, which we extend with the data\\nheterogeneity dimension, which makes the keyword search problem computationally\\nharder. We implement our algorithm and we evaluate its performance using\\nsynthetic and real-world datasets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 9, 13, 5, 32, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Subscribing to Big Data at Scale',\n",
       "  'authors': ['Xikui Wang', 'Michael J. Carey', 'Vassilis J. Tsotras'],\n",
       "  'summary': 'Today, data is being actively generated by a variety of devices, services,\\nand applications. Such data is important not only for the information that it\\ncontains, but also for its relationships to other data and to interested users.\\nMost existing Big Data systems focus on passively answering queries from users,\\nrather than actively collecting data, processing it, and serving it to users.\\nTo satisfy both passive and active requests at scale, users need either to\\nheavily customize an existing passive Big Data system or to glue multiple\\nsystems together. Either choice would require significant effort from users and\\nincur additional overhead. In this paper, we present the BAD (Big Active Data)\\nsystem, which is designed to preserve the merits of passive Big Data systems\\nand introduce new features for actively serving Big Data to users at scale. We\\nshow the design and implementation of the BAD system, demonstrate how BAD\\nfacilitates providing both passive and active data services, investigate the\\nBAD system\\'s performance at scale, and illustrate the complexities that would\\nresult from instead providing BAD-like services with a \"glued\" system.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 10, 0, 4, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'SPARQL with XQuery-based Filtering',\n",
       "  'authors': ['Takahiro Komamizu'],\n",
       "  'summary': 'Linked Open Data (LOD) has been proliferated over various domains, however,\\nthere are still lots of open data in various format other than RDF, a standard\\ndata description framework in LOD. These open data can also be connected to\\nentities in LOD when they are associated with URIs. Document-centric XML data\\nare such open data that are connected with entities in LOD as supplemental\\ndocuments for these entities, and to convert these XML data into RDF requires\\nvarious techniques such as information extraction, ontology design and ontology\\nmapping with human prior knowledge. To utilize document-centric XML data linked\\nfrom entities in LOD, in this paper, a SPARQL-based seamless access method on\\nRDF and XML data is proposed. In particular, an extension to SPARQL,\\nXQueryFILTER, which enables XQuery as a filter in SPARQL is proposed. For\\nefficient query processing of the combination of SPARQL and XQuery, a database\\ntheory-based query optimization is proposed. Real-world scenario-based\\nexperiments in this paper showcase that effectiveness of XQueryFILTER and\\nefficiency of the optimization.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 14, 4, 44, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Revealing Secrets in SPARQL Session Level',\n",
       "  'authors': ['Xinyue Zhang',\n",
       "   'Meng Wang',\n",
       "   'Muhammad Saleem',\n",
       "   'Axel-Cyrille Ngonga Ngomo',\n",
       "   'Guilin Qi',\n",
       "   'Haofen Wang'],\n",
       "  'summary': 'Based on Semantic Web technologies, knowledge graphs help users to discover\\ninformation of interest by using live SPARQL services. Answer-seekers often\\nexamine intermediate results iteratively and modify SPARQL queries repeatedly\\nin a search session. In this context, understanding user behaviors is critical\\nfor effective intention prediction and query optimization. However, these\\nbehaviors have not yet been researched systematically at the SPARQL session\\nlevel. This paper reveals secrets of session-level user search behaviors by\\nconducting a comprehensive investigation over massive real-world SPARQL query\\nlogs. In particular, we thoroughly assess query changes made by users w.r.t.\\nstructural and data-driven features of SPARQL queries. To illustrate the\\npotentiality of our findings, we employ an application example of how to use\\nour findings, which might be valuable to devise efficient SPARQL caching,\\nauto-completion, query suggestion, approximation, and relaxation techniques in\\nthe future.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 13, 16, 0, 21, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Extensible Data Skipping',\n",
       "  'authors': ['Paula Ta-Shma', 'Guy Khazma', 'Gal Lushi', 'Oshrit Feder'],\n",
       "  'summary': 'Data skipping reduces I/O for SQL queries by skipping over irrelevant data\\nobjects (files) based on their metadata. We extend this notion by allowing\\ndevelopers to define their own data skipping metadata types and indexes using a\\nflexible API. Our framework is the first to natively support data skipping for\\narbitrary data types (e.g. geospatial, logs) and queries with User Defined\\nFunctions (UDFs). We integrated our framework with Apache Spark and it is now\\ndeployed across multiple products/services at IBM. We present our extensible\\ndata skipping APIs, discuss index design, and implement various metadata\\nindexes, requiring only around 30 lines of additional code per index. In\\nparticular we implement data skipping for a third party library with geospatial\\nUDFs and demonstrate speedups of two orders of magnitude. Our centralized\\nmetadata approach provides a x3.6 speed up even when compared to queries which\\nare rewritten to exploit Parquet min/max metadata. We demonstrate that\\nextensible data skipping is applicable to broad class of applications, where\\nuser defined indexes achieve significant speedups and cost savings with very\\nlow development cost.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 17, 8, 34, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Multi-source Data Mining for e-Learning',\n",
       "  'authors': ['Julie Bu Daher', 'Armelle Brun', 'Anne Boyer'],\n",
       "  'summary': 'Data mining is the task of discovering interesting, unexpected or valuable\\nstructures in large datasets and transforming them into an understandable\\nstructure for further use . Different approaches in the domain of data mining\\nhave been proposed, among which pattern mining is the most important one.\\nPattern mining mining involves extracting interesting frequent patterns from\\ndata. Pattern mining has grown to be a topic of high interest where it is used\\nfor different purposes, for example, recommendations. Some of the most common\\nchallenges in this domain include reducing the complexity of the process and\\navoiding the redundancy within the patterns. So far, pattern mining has mainly\\nfocused on the mining of a single data source. However, with the increase in\\nthe amount of data, in terms of volume, diversity of sources and nature of\\ndata, mining multi-source and heterogeneous data has become an emerging\\nchallenge in this domain. This challenge is the main focus of our work where we\\npropose to mine multi-source data in order to extract interesting frequent\\npatterns.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 17, 15, 39, 45, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'TODS: An Automated Time Series Outlier Detection System',\n",
       "  'authors': ['Kwei-Herng Lai',\n",
       "   'Daochen Zha',\n",
       "   'Guanchu Wang',\n",
       "   'Junjie Xu',\n",
       "   'Yue Zhao',\n",
       "   'Devesh Kumar',\n",
       "   'Yile Chen',\n",
       "   'Purav Zumkhawaka',\n",
       "   'Minyang Wan',\n",
       "   'Diego Martinez',\n",
       "   'Xia Hu'],\n",
       "  'summary': 'We present TODS, an automated Time Series Outlier Detection System for\\nresearch and industrial applications. TODS is a highly modular system that\\nsupports easy pipeline construction. The basic building block of TODS is\\nprimitive, which is an implementation of a function with hyperparameters. TODS\\ncurrently supports 70 primitives, including data processing, time series\\nprocessing, feature analysis, detection algorithms, and a reinforcement module.\\nUsers can freely construct a pipeline using these primitives and perform end-\\nto-end outlier detection with the constructed pipeline. TODS provides a\\nGraphical User Interface (GUI), where users can flexibly design a pipeline with\\ndrag-and-drop. Moreover, a data-driven searcher is provided to automatically\\ndiscover the most suitable pipelines given a dataset. TODS is released under\\nApache 2.0 license at https://github.com/datamllab/tods.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 18, 15, 36, 43, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Selectivity Estimation with Attribute Value Dependencies using Linked Bayesian Networks',\n",
       "  'authors': ['Max Halford', 'Philippe Saint-Pierre', 'Franck Morvan'],\n",
       "  'summary': 'Relational query optimisers rely on cost models to choose between different\\nquery execution plans. Selectivity estimates are known to be a crucial input to\\nthe cost model. In practice, standard selectivity estimation procedures are\\nprone to large errors. This is mostly because they rely on the so-called\\nattribute value independence and join uniformity assumptions. Therefore,\\nmultidimensional methods have been proposed to capture dependencies between two\\nor more attributes both within and across relations. However, these methods\\nrequire a large computational cost which makes them unusable in practice. We\\npropose a method based on Bayesian networks that is able to capture\\ncross-relation attribute value dependencies with little overhead. Our proposal\\nis based on the assumption that dependencies between attributes are preserved\\nwhen joins are involved. Furthermore, we introduce a parameter for trading\\nbetween estimation accuracy and computational cost. We validate our work by\\ncomparing it with other relevant methods on a large workload derived from the\\nJOB and TPC-DS benchmarks. Our results show that our method is an order of\\nmagnitude more efficient than existing methods, whilst maintaining a high level\\nof accuracy.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 21, 14, 5, 5, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Selectivity correction with online machine learning',\n",
       "  'authors': ['Max Halford', 'Philippe Saint-Pierre', 'Franck Morvan'],\n",
       "  'summary': \"Computer systems are full of heuristic rules which drive the decisions they\\nmake. These rules of thumb are designed to work well on average, but ignore\\nspecific information about the available context, and are thus sub-optimal. The\\nemerging field of machine learning for systems attempts to learn decision rules\\nwith machine learning algorithms. In the database community, many recent\\nproposals have been made to improve selectivity estimation with batch machine\\nlearning methods. Such methods are all batch methods which require retraining\\nand cannot handle concept drift, such as workload changes and schema\\nmodifications. We present online machine learning as an alternative approach.\\nOnline models learn on the fly and do not require storing data, they are more\\nlightweight than batch models, and finally may adapt to concept drift. As an\\nexperiment, we teach models to improve the selectivity estimates made by\\nPostgreSQL's cost model. Our experiments make the case that simple online\\nmodels are able to compete with a recently proposed deep learning method.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 21, 14, 5, 29, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Scalable Data Series Subsequence Matching with ULISSE',\n",
       "  'authors': ['Michele Linardi', 'Themis Palpanas'],\n",
       "  'summary': 'Data series similarity search is an important operation and at the core of\\nseveral analysis tasks and applications related to data series collections.\\nDespite the fact that data series indexes enable fast similarity search, all\\nexisting indexes can only answer queries of a single length (fixed at index\\nconstruction time), which is a severe limitation. In this work, we propose\\nULISSE, the first data series index structure designed for answering similarity\\nsearch queries of variable length (within some range). Our contribution is\\ntwo-fold. First, we introduce a novel representation technique, which\\neffectively and succinctly summarizes multiple sequences of different length.\\nBased on the proposed index, we describe efficient algorithms for approximate\\nand exact similarity search, combining disk based index visits and in-memory\\nsequential scans. Our approach supports non Z-normalized and Z-normalized\\nsequences, and can be used with no changes with both Euclidean Distance and\\nDynamic Time Warping, for answering both k-NN and epsilon-range queries. We\\nexperimentally evaluate our approach using several synthetic and real datasets.\\nThe results show that ULISSE is several times, and up to orders of magnitude\\nmore efficient in terms of both space and time cost, when compared to competing\\napproaches. (Paper published in VLDBJ 2020)',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 22, 8, 4, 20, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Segmented Pairwise Distance for Time Series with Large Discontinuities',\n",
       "  'authors': ['Jiabo He',\n",
       "   'Sarah Erfani',\n",
       "   'Sudanthi Wijewickrema',\n",
       "   \"Stephen O'Leary\",\n",
       "   'Kotagiri Ramamohanarao'],\n",
       "  'summary': 'Time series with large discontinuities are common in many scenarios. However,\\nexisting distance-based algorithms (e.g., DTW and its derivative algorithms)\\nmay perform poorly in measuring distances between these time series pairs. In\\nthis paper, we propose the segmented pairwise distance (SPD) algorithm to\\nmeasure distances between time series with large discontinuities. SPD is\\northogonal to distance-based algorithms and can be embedded in them. We\\nvalidate advantages of SPD-embedded algorithms over corresponding\\ndistance-based ones on both open datasets and a proprietary dataset of surgical\\ntime series (of surgeons performing a temporal bone surgery in a virtual\\nreality surgery simulator). Experimental results demonstrate that SPD-embedded\\nalgorithms outperform corresponding distance-based ones in distance measurement\\nbetween time series with large discontinuities, measured by the Silhouette\\nindex (SI).',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 23, 9, 17, 57, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Effective and Efficient Variable-Length Data Series Analytics',\n",
       "  'authors': ['Michele Linardi'],\n",
       "  'summary': 'In the last twenty years, data series similarity search has emerged as a\\nfundamental operation at the core of several analysis tasks and applications\\nrelated to data series collections. Many solutions to different mining problems\\nwork by means of similarity search. In this regard, all the proposed solutions\\nrequire the prior knowledge of the series length on which similarity search is\\nperformed. In several cases, the choice of the length is critical and sensibly\\ninfluences the quality of the expected outcome. Unfortunately, the obvious\\nbrute-force solution, which provides an outcome for all lengths within a given\\nrange is computationally untenable. In this Ph.D. work, we present the first\\nsolutions that inherently support scalable and variable-length similarity\\nsearch in data series, applied to sequence/subsequences matching, motif and\\ndiscord discovery problems.The experimental results show that our approaches\\nare up to orders of magnitude faster than the alternatives. They also\\ndemonstrate that we can remove the unrealistic constraint of performing\\nanalytics using a predefined length, leading to more intuitive and actionable\\nresults, which would have otherwise been missed.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 22, 9, 59, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'In-Order Sliding-Window Aggregation in Worst-Case Constant Time',\n",
       "  'authors': ['Kanat Tangwongsan', 'Martin Hirzel', 'Scott Schneider'],\n",
       "  'summary': 'Sliding-window aggregation is a widely-used approach for extracting insights\\nfrom the most recent portion of a data stream. The aggregations of interest can\\nusually be expressed as binary operators that are associative but not\\nnecessarily commutative nor invertible. Non-invertible operators, however, are\\ndifficult to support efficiently. In a 2017 conference paper, we introduced\\nDABA, the first algorithm for sliding-window aggregation with worst-case\\nconstant time. Before DABA, if a window had size $n$, the best published\\nalgorithms would require $O(\\\\log n)$ aggregation steps per window\\noperation---and while for strictly in-order streams, this bound could be\\nimproved to $O(1)$ aggregation steps on average, it was not known how to\\nachieve an $O(1)$ bound for the worst-case, which is critical for\\nlatency-sensitive applications.\\n  This article is an extended version of our 2017 paper. Besides describing\\nDABA in more detail, this article introduces a new variant, DABA Lite, which\\nachieves the same time bounds in less memory. Whereas DABA requires space for\\nstoring $2n$ partial aggregates, DABA Lite only requires space for $n+2$\\npartial aggregates. Our experiments on synthetic and real data support the\\ntheoretical findings.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 29, 4, 11, 13, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The Shapley Value of Inconsistency Measures for Functional Dependencies',\n",
       "  'authors': ['Ester Livshits', 'Benny Kimelfeld'],\n",
       "  'summary': 'Quantifying the inconsistency of a database is motivated by various goals\\nincluding reliability estimation for new datasets and progress indication in\\ndata cleaning. Another goal is to attribute to individual tuples a level of\\nresponsibility to the overall inconsistency, and thereby prioritize tuples in\\nthe explanation or inspection of dirt. Therefore, inconsistency quantification\\nand attribution have been a subject of much research in Knowledge\\nRepresentation and, more recently, in Databases. As in many other fields, a\\nconventional responsibility sharing mechanism is the Shapley value from\\ncooperative game theory. In this paper, we carry out a systematic investigation\\nof the complexity of the Shapley value in common inconsistency measures for\\nfunctional-dependency (FD) violations. For several measures we establish a full\\nclassification of the FD sets into tractable and intractable classes with\\nrespect to Shapley-value computation. We also study the complexity of\\napproximation in intractable cases.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 29, 7, 10, 36, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Database Repairing with Soft Functional Dependencies',\n",
       "  'authors': ['Nofar Carmeli',\n",
       "   'Martin Grohe',\n",
       "   'Benny Kimelfeld',\n",
       "   'Ester Livshits',\n",
       "   'Muhammad Tibi'],\n",
       "  'summary': 'A common interpretation of soft constraints penalizes the database for every\\nviolation of every constraint, where the penalty is the cost (weight) of the\\nconstraint. A computational challenge is that of finding an optimal subset: a\\ncollection of database tuples that minimizes the total penalty when each tuple\\nhas a cost of being excluded. When the constraints are strict (i.e., have an\\ninfinite cost), this subset is a \"cardinality repair\" of an inconsistent\\ndatabase; in soft interpretations, this subset corresponds to a \"most probable\\nworld\" of a probabilistic database, a \"most likely intention\" of a\\nprobabilistic unclean database, and so on. Within the class of functional\\ndependencies, the complexity of finding a cardinality repair is thoroughly\\nunderstood. Yet, very little is known about the complexity of this problem in\\nthe more general soft semantics. This paper makes a significant progress in\\nthis direction. In addition to general insights about the hardness and\\napproximability of the problem, we present algorithms for two special cases: a\\nsingle functional dependency, and a bipartite matching. The latter is the\\nproblem of finding an optimal \"almost matching\" of a bipartite graph where a\\npenalty is paid for every lost edge and every violation of monogamy.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 29, 7, 17, 16, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Alignment Approximation for Process Trees',\n",
       "  'authors': ['Daniel Schuster',\n",
       "   'Sebastiaan van Zelst',\n",
       "   'Wil M. P. van der Aalst'],\n",
       "  'summary': 'Comparing observed behavior (event data generated during process executions)\\nwith modeled behavior (process models), is an essential step in process mining\\nanalyses. Alignments are the de-facto standard technique for calculating\\nconformance checking statistics. However, the calculation of alignments is\\ncomputationally complex since a shortest path problem must be solved on a state\\nspace which grows non-linearly with the size of the model and the observed\\nbehavior, leading to the well-known state space explosion problem. In this\\npaper, we present a novel framework to approximate alignments on process trees\\nby exploiting their hierarchical structure. Process trees are an important\\nprocess model formalism used by state-of-the-art process mining techniques such\\nas the inductive mining approaches. Our approach exploits structural properties\\nof a given process tree and splits the alignment computation problem into\\nsmaller sub-problems. Finally, sub-results are composed to obtain an alignment.\\nOur experiments show that our approach provides a good balance between accuracy\\nand computation time.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 9, 29, 15, 24, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'All You Need Is CONSTRUCT',\n",
       "  'authors': ['Dominique Duval', 'Rachid Echahed', 'Frederic Prost'],\n",
       "  'summary': 'In SPARQL, the query forms SELECT and CONSTRUCT have been the subject of\\nseveral studies, both theoretical and practical. However, the composition of\\nsuch queries and their interweaving when forming involved nested queries has\\nnot yet received much interest in the literature. We mainly tackle the problem\\nof composing such queries. For this purpose, we introduce a language close to\\nSPARQL where queries can be nested at will, involving either CONSTRUCT or\\nSELECT query forms and provide a formal semantics for it. This semantics is\\nbased on a uniform interpretation of queries. This uniformity is due to an\\nextension of the notion of RDF graphs to include isolated items such as\\nvariables. As a key feature of this work, we show how classical SELECT queries\\ncan be easily encoded as a particular case of CONSTRUCT queries.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 2, 8, 10, 32, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Trajectory-Based Spatiotemporal Entity Linking',\n",
       "  'authors': ['Fengmei Jin',\n",
       "   'Wen Hua',\n",
       "   'Thomas Zhou',\n",
       "   'Jiajie Xu',\n",
       "   'Matteo Francia',\n",
       "   'Maria E Orlowska',\n",
       "   'Xiaofang Zhou'],\n",
       "  'summary': 'Trajectory-based spatiotemporal entity linking is to match the same moving\\nobject in different datasets based on their movement traces. It is a\\nfundamental step to support spatiotemporal data integration and analysis. In\\nthis paper, we study the problem of spatiotemporal entity linking using\\neffective and concise signatures extracted from their trajectories. This\\nlinking problem is formalized as a k-nearest neighbor (k-NN) query on the\\nsignatures. Four representation strategies (sequential, temporal, spatial, and\\nspatiotemporal) and two quantitative criteria (commonality and unicity) are\\ninvestigated for signature construction. A simple yet effective dimension\\nreduction strategy is developed together with a novel indexing structure called\\nthe WR-tree to speed up the search. A number of optimization methods are\\nproposed to improve the accuracy and robustness of the linking. Our extensive\\nexperiments on real-world datasets verify the superiority of our approach over\\nthe state-of-the-art solutions in terms of both accuracy and efficiency.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 4, 8, 45, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'LEAPME: Learning-based Property Matching with Embeddings',\n",
       "  'authors': ['Daniel Ayala', 'Inma Hernández', 'David Ruiz', 'Erhard Rahm'],\n",
       "  'summary': 'Data integration tasks such as the creation and extension of knowledge graphs\\ninvolve the fusion of heterogeneous entities from many sources. Matching and\\nfusion of such entities require to also match and combine their properties\\n(attributes). However, previous schema matching approaches mostly focus on two\\nsources only and often rely on simple similarity measurements. They thus face\\nproblems in challenging use cases such as the integration of heterogeneous\\nproduct entities from many sources.\\n  We therefore present a new machine learning-based property matching approach\\ncalled LEAPME (LEArning-based Property Matching with Embeddings) that utilizes\\nnumerous features of both property names and instance values. The approach\\nheavily makes use of word embeddings to better utilize the domain-specific\\nsemantics of both property names and instance values. The use of supervised\\nmachine learning helps exploit the predictive power of word embeddings.\\n  Our comparative evaluation against five baselines for several multi-source\\ndatasets with real-world data shows the high effectiveness of LEAPME. We also\\nshow that our approach is even effective when training data from another domain\\n(transfer learning) is used.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 5, 12, 42, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Event Trend Aggregation Under Rich Event Matching Semantics',\n",
       "  'authors': ['Olga Poppe',\n",
       "   'Chuan Lei',\n",
       "   'Elke A. Rundensteiner',\n",
       "   'David Maier'],\n",
       "  'summary': 'Streaming applications from health care analytics to algorithmic trading\\ndeploy Kleene queries to detect and aggregate event trends. Rich event matching\\nsemantics determine how to compose events into trends. The expressive power of\\nstate-of-the-art systems remains limited in that they do not support the rich\\nvariety of these semantics. Worse yet, they suffer from long delays and high\\nmemory costs because they opt to maintain aggregates at a fine granularity. To\\novercome these limitations, our Coarse-Grained Event Trend Aggregation (Cogra)\\napproach supports this rich diversity of event matching semantics within one\\nsystem. Better yet, Cogra incrementally maintains aggregates at the coarsest\\ngranularity possible for each of these semantics. In this way, Cogra minimizes\\nthe number of aggregates -- reducing both time and space complexity. Our\\nexperiments demonstrate that Cogra achieves up to four orders of magnitude\\nspeed-up and up to eight orders of magnitude memory reduction compared to\\nstate-of-the-art approaches.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 6, 19, 26, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Sharon: Shared Online Event Sequence Aggregation',\n",
       "  'authors': ['Olga Poppe',\n",
       "   'Allison Rozet',\n",
       "   'Chuan Lei',\n",
       "   'Elke A. Rundensteiner',\n",
       "   'David Maier'],\n",
       "  'summary': 'Streaming systems evaluate massive workloads of event sequence aggregation\\nqueries. State-of-the-art approaches suffer from long delays caused by not\\nsharing intermediate results of similar queries and by constructing event\\nsequences prior to their aggregation. To overcome these limitations, our Shared\\nOnline Event Sequence Aggregation (Sharon) approach shares intermediate\\naggregates among multiple queries while avoiding the expensive construction of\\nevent sequences. Our Sharon optimizer faces two challenges. One, a sharing\\ndecision is not always beneficial. Two, a sharing decision may exclude other\\nsharing opportunities. To guide our Sharon optimizer, we compactly encode\\nsharing candidates, their benefits, and conflicts among candidates into the\\nSharon graph. Based on the graph, we map our problem of finding an optimal\\nsharing plan to the Maximum Weight Independent Set (MWIS) problem. We then use\\nthe guaranteed weight of a greedy algorithm for the MWIS problem to prune the\\nsearch of our sharing plan finder without sacrificing its optimality. The\\nSharon optimizer is shown to produce sharing plans that achieve up to an\\n18-fold speed-up compared to state-of-the-art approaches.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 6, 19, 27, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Query Rewriting On Path Views Without Integrity Constraints',\n",
       "  'authors': ['Julien Romero', 'Nicoleta Preda', 'Fabian Suchanek'],\n",
       "  'summary': 'A view with a binding pattern is a parameterised query on a database. Such\\nviews are used, e.g., to model Web services. To answer a query on such views,\\none has to orchestrate the views together in execution plans. The goal is\\nusually to find equivalent rewritings, which deliver precisely the same results\\nas the query on all databases. However, such rewritings are usually possible\\nonly in the presence of integrity constraints - and not all databases have such\\nconstraints. In this paper, we describe a class of plans that give practical\\nguarantees about their result even if there are no integrity constraints. We\\nprovide a characterisation of such plans and a complete and correct algorithm\\nto enumerate them. Finally, we show that our method can find plans on\\nreal-world Web Services.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 7, 17, 12, 43, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Anomaly Detection in Large Labeled Multi-Graph Databases',\n",
       "  'authors': ['Hung T. Nguyen', 'Pierre J. Liang', 'Leman Akoglu'],\n",
       "  'summary': 'Within a large database G containing graphs with labeled nodes and directed,\\nmulti-edges; how can we detect the anomalous graphs? Most existing work are\\ndesigned for plain (unlabeled) and/or simple (unweighted) graphs. We introduce\\nCODETECT, the first approach that addresses the anomaly detection task for\\ngraph databases with such complex nature. To this end, it identifies a small\\nrepresentative set S of structural patterns (i.e., node-labeled network motifs)\\nthat losslessly compress database G as concisely as possible. Graphs that do\\nnot compress well are flagged as anomalous. CODETECT exhibits two novel\\nbuilding blocks: (i) a motif-based lossless graph encoding scheme, and (ii)\\nfast memory-efficient search algorithms for S. We show the effectiveness of\\nCODETECT on transaction graph databases from three different corporations,\\nwhere existing baselines adjusted for the task fall behind significantly,\\nacross different types of anomalies and performance metrics.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 7, 18, 41, 33, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An Experimental Analysis of Indoor Spatial Queries: Modeling, Indexing, and Processing',\n",
       "  'authors': ['Tiantian Liu',\n",
       "   'Huan Li',\n",
       "   'Hua Lu',\n",
       "   'Muhammad Aamir Cheema',\n",
       "   'Lidan Shou'],\n",
       "  'summary': 'Indoor location-based services (LBS), such as POI search and routing, are\\noften built on top of typical indoor spatial queries. To support such queries\\nand indoor LBS, multiple techniques including model/indexes and search\\nalgorithms have been proposed. In this work, we conduct an extensive\\nexperimental study on existing proposals for indoor spatial queries. We survey\\nfive model/indexes, compare their algorithmic characteristics, and analyze\\ntheir space and time complexities. We also design an in-depth benchmark with\\nreal and synthetic datasets, evaluation tasks and performance metrics. Enabled\\nby the benchmark, we obtain and report the performance results of all\\nmodel/indexes under investigation. By analyzing the results, we summarize the\\npros and cons of all techniques and suggest the best choice for typical\\nscenarios.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 8, 11, 48, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'PolyFrame: A Retargetable Query-based Approach to Scaling DataFrames (Extended Version)',\n",
       "  'authors': ['Phanwadee Sinthong', 'Michael J. Carey'],\n",
       "  'summary': \"In the last few years, the field of data science has been growing rapidly as\\nvarious businesses have adopted statistical and machine learning techniques to\\nempower their decision making and applications. Scaling data analysis, possibly\\nincluding the application of custom machine learning models, to large volumes\\nof data requires the utilization of distributed frameworks. This can lead to\\nserious technical challenges for data analysts and reduce their productivity.\\nAFrame, a Python data analytics library, is implemented as a layer on top of\\nApache AsterixDB, addressing these issues by incorporating the data scientists'\\ndevelopment environment and transparently scaling out the evaluation of\\nanalytical operations through a Big Data management system. While AFrame is\\nable to leverage data management facilities (e.g., indexes and query\\noptimization) and allows users to interact with a very large volume of data,\\nthe initial version only generated SQL++ queries and only operated against\\nApache AsterixDB. In this work, we describe a new design that retargets\\nAFrame's incremental query formation to other query-based database systems as\\nwell, making it more flexible for deployment against other data management\\nsystems with composable query languages.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 12, 8, 37, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Streaming enumeration on nested documents',\n",
       "  'authors': ['Martín Muñoz', 'Cristian Riveros'],\n",
       "  'summary': 'Some of the most relevant document schemas used online, such as XML and JSON,\\nhave a nested format. In the last decade, the task of extracting data from\\nnested documents over streams has become especially relevant. We focus on the\\nstreaming evaluation of queries with outputs of varied sizes over nested\\ndocuments. We model queries of this kind as Visibly Pushdown Transducers (VPT),\\na computational model that extends visibly pushdown automata with outputs and\\nhas the same expressive power as MSO over nested documents. Since processing a\\ndocument through a VPT can generate a massive number of results, we are\\ninterested in reading the input in a streaming fashion and enumerating the\\noutputs one after another as efficiently as possible, namely, with\\nconstant-delay. This paper presents an algorithm that enumerates these elements\\nwith constant-delay after processing the document stream in a single pass.\\nFurthermore, we show that this algorithm is worst-case optimal in terms of\\nupdate-time per symbol and memory usage.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 12, 21, 26, 31, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Raptor Zonal Statistics: Fully Distributed Zonal Statistics of Big Raster + Vector Data [Pre-Print]',\n",
       "  'authors': ['Samriddhi Singla', 'Ahmed Eldawy'],\n",
       "  'summary': 'Recent advancements in remote sensing technology have resulted in petabytes\\nof data in raster format. This data is often processed in combination with high\\nresolution vector data that represents, for example, city boundaries. One of\\nthe common operations that combine big raster and vector data is the zonal\\nstatistics which computes some statistics for each polygon in the vector\\ndataset. This paper models the zonal statistics problem as a join problem and\\nproposes a novel distributed system that can scale to petabytes of raster and\\nvector data. The proposed method does not require any preprocessing or indexing\\nwhich makes it perfect for ad-hoc queries that scientists usually want to run.\\nWe devise a theoretical cost model that proves the efficiency of our algorithm\\nover the baseline method. Furthermore, we run an extensive experimental\\nevaluation on large scale satellite data with up-to a trillion pixels, and big\\nvector data with up-to hundreds of millions of edges, and we show that our\\nmethod can perfectly scale to big data with up-to two orders of magnitude\\nperformance gain over Rasdaman and Google Earth Engine.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 13, 19, 15, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'On the Efficiency of K-Means Clustering: Evaluation, Optimization, and Algorithm Selection',\n",
       "  'authors': ['Sheng Wang', 'Yuan Sun', 'Zhifeng Bao'],\n",
       "  'summary': \"This paper presents a thorough evaluation of the existing methods that\\naccelerate Lloyd's algorithm for fast k-means clustering. To do so, we analyze\\nthe pruning mechanisms of existing methods, and summarize their common pipeline\\ninto a unified evaluation framework UniK. UniK embraces a class of well-known\\nmethods and enables a fine-grained performance breakdown. Within UniK, we\\nthoroughly evaluate the pros and cons of existing methods using multiple\\nperformance metrics on a number of datasets. Furthermore, we derive an\\noptimized algorithm over UniK, which effectively hybridizes multiple existing\\nmethods for more aggressive pruning. To take this further, we investigate\\nwhether the most efficient method for a given clustering task can be\\nautomatically selected by machine learning, to benefit practitioners and\\nresearchers.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 13, 19, 45, 30, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Taurus: Lightweight Parallel Logging for In-Memory Database Management Systems (Extended Version)',\n",
       "  'authors': ['Yu Xia', 'Xiangyao Yu', 'Andrew Pavlo', 'Srinivas Devadas'],\n",
       "  'summary': \"Existing single-stream logging schemes are unsuitable for in-memory database\\nmanagement systems (DBMSs) as the single log is often a performance bottleneck.\\nTo overcome this problem, we present Taurus, an efficient parallel logging\\nscheme that uses multiple log streams, and is compatible with both data and\\ncommand logging. Taurus tracks and encodes transaction dependencies using a\\nvector of log sequence numbers (LSNs). These vectors ensure that the\\ndependencies are fully captured in logging and correctly enforced in recovery.\\nOur experimental evaluation with an in-memory DBMS shows that Taurus's parallel\\nlogging achieves up to 9.9x and 2.9x speedups over single-streamed data logging\\nand command logging, respectively. It also enables the DBMS to recover up to\\n22.9x and 75.6x faster than these baselines for data and command logging,\\nrespectively. We also compare Taurus with two state-of-the-art parallel logging\\nschemes and show that the DBMS achieves up to 2.8x better performance on NVMe\\ndrives and 9.2x on HDDs.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 14, 1, 20, 54, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data Readiness Report',\n",
       "  'authors': ['Shazia Afzal',\n",
       "   'Rajmohan C',\n",
       "   'Manish Kesarwani',\n",
       "   'Sameep Mehta',\n",
       "   'Hima Patel'],\n",
       "  'summary': 'Data exploration and quality analysis is an important yet tedious process in\\nthe AI pipeline. Current practices of data cleaning and data readiness\\nassessment for machine learning tasks are mostly conducted in an arbitrary\\nmanner which limits their reuse and results in loss of productivity. We\\nintroduce the concept of a Data Readiness Report as an accompanying\\ndocumentation to a dataset that allows data consumers to get detailed insights\\ninto the quality of input data. Data characteristics and challenges on various\\nquality dimensions are identified and documented keeping in mind the principles\\nof transparency and explainability. The Data Readiness Report also serves as a\\nrecord of all data assessment operations including applied transformations.\\nThis provides a detailed lineage for the purpose of data governance and\\nmanagement. In effect, the report captures and documents the actions taken by\\nvarious personas in a data readiness and assessment workflow. Overtime this\\nbecomes a repository of best practices and can potentially drive a\\nrecommendation system for building automated data readiness workflows on the\\nlines of AutoML [8]. We anticipate that together with the Datasheets [9],\\nDataset Nutrition Label [11], FactSheets [1] and Model Cards [15], the Data\\nReadiness Report makes significant progress towards Data and AI lifecycle\\ndocumentation.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 14, 16, 26, 29, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Survive the Schema Changes: Integration of Unmanaged Data Using Deep Learning',\n",
       "  'authors': ['Zijie Wang',\n",
       "   'Lixi Zhou',\n",
       "   'Amitabh Das',\n",
       "   'Valay Dave',\n",
       "   'Zhanpeng Jin',\n",
       "   'Jia Zou'],\n",
       "  'summary': 'Data is the king in the age of AI. However data integration is often a\\nlaborious task that is hard to automate. Schema change is one significant\\nobstacle to the automation of the end-to-end data integration process. Although\\nthere exist mechanisms such as query discovery and schema modification language\\nto handle the problem, these approaches can only work with the assumption that\\nthe schema is maintained by a database. However, we observe diversified schema\\nchanges in heterogeneous data and open data, most of which has no schema\\ndefined. In this work, we propose to use deep learning to automatically deal\\nwith schema changes through a super cell representation and automatic injection\\nof perturbations to the training data to make the model robust to schema\\nchanges. Our experimental results demonstrate that our proposed approach is\\neffective for two real-world data integration scenarios: coronavirus data\\nintegration, and machine log integration.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 15, 8, 10, 37, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Discovering Hierarchical Processes Using Flexible Activity Trees for Event Abstraction',\n",
       "  'authors': ['Xixi Lu', 'Avigdor Gal', 'Hajo A. Reijers'],\n",
       "  'summary': 'Processes, such as patient pathways, can be very complex, comprising of\\nhundreds of activities and dozens of interleaved subprocesses. While existing\\nprocess discovery algorithms have proven to construct models of high quality on\\nclean logs of structured processes, it still remains a challenge when the\\nalgorithms are being applied to logs of complex processes. The creation of a\\nmulti-level, hierarchical representation of a process can help to manage this\\ncomplexity. However, current approaches that pursue this idea suffer from a\\nvariety of weaknesses. In particular, they do not deal well with interleaving\\nsubprocesses. In this paper, we propose FlexHMiner, a three-step approach to\\ndiscover processes with multi-level interleaved subprocesses. We implemented\\nFlexHMiner in the open source Process Mining toolkit ProM. We used seven\\nreal-life logs to compare the qualities of hierarchical models discovered using\\ndomain knowledge, random clustering, and flat approaches. Our results indicate\\nthat the hierarchical process models that the FlexHMiner generates compare\\nfavorably to approaches that do not exploit hierarchy.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 16, 10, 50, 41, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Enumerating Answers to First-Order Queries over Databases of Low Degree',\n",
       "  'authors': ['Arnaud Durand', 'Nicole Schweikardt', 'Luc Segoufin'],\n",
       "  'summary': 'A class of relational databases has low degree if for all $\\\\delta>0$, all but\\nfinitely many databases in the class have degree at most $n^{\\\\delta}$, where\\n$n$ is the size of the database. Typical examples are databases of bounded\\ndegree or of degree bounded by $\\\\log n$.\\n  It is known that over a class of databases having low degree, first-order\\nboolean queries can be checked in pseudo-linear time, i.e.\\\\ for all\\n$\\\\epsilon>0$ in time bounded by $n^{1+\\\\epsilon}$. We generalize this result by\\nconsidering query evaluation.\\n  We show that counting the number of answers to a query can be done in\\npseudo-linear time and that after a pseudo-linear time preprocessing we can\\ntest in constant time whether a given tuple is a solution to a query or\\nenumerate the answers to a query with constant delay.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 16, 13, 33, 34, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'LiMITS: An Effective Approach for Trajectory Simplification',\n",
       "  'authors': ['Yunheng Han', 'Hanan Samet'],\n",
       "  'summary': 'Trajectories represent the mobility of moving objects and thus is of great\\nvalue in data mining applications. However, trajectory data is enormous in\\nvolume, so it is expensive to store and process the raw data directly.\\nTrajectories are also redundant so data compression techniques can be applied.\\nIn this paper, we propose effective algorithms to simplify trajectories. We\\nfirst extend existing algorithms by replacing the commonly used $L_2$ metric\\nwith the $L_\\\\infty$ metric so that they can be generalized to high dimensional\\nspace (e.g., 3-space in practice). Next, we propose a novel approach, namely\\nL-infinity Multidimensional Interpolation Trajectory Simplification (LiMITS).\\nLiMITS belongs to weak simplification and takes advantage of the $L_\\\\infty$\\nmetric. It generates simplified trajectories by multidimensional interpolation.\\nIt also allows a new format called compact representation to further improve\\nthe compression ratio. Finally, We demonstrate the performance of LiMITS\\nthrough experiments on real-world datasets, which show that it is more\\neffective than other existing methods.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 16, 20, 46, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Aggregated Deletion Propagation for Counting Conjunctive Query Answers',\n",
       "  'authors': ['Xiao Hu',\n",
       "   'Shouzhuo Sun',\n",
       "   'Shweta Patwa',\n",
       "   'Debmalya Panigrahi',\n",
       "   'Sudeepa Roy'],\n",
       "  'summary': 'We investigate the computational complexity of minimizing the source\\nside-effect in order to remove a given number of tuples from the output of a\\nconjunctive query. This is a variant of the well-studied {\\\\em deletion\\npropagation} problem, the difference being that we are interested in removing\\nthe smallest subset of input tuples to remove a given number of output tuples}\\nwhile deletion propagation focuses on removing a specific output tuple. We call\\nthis the {\\\\em Aggregated Deletion Propagation} problem. We completely\\ncharacterize the poly-time solvability of this problem for arbitrary\\nconjunctive queries without self-joins. This includes a poly-time algorithm to\\ndecide solvability, as well as an exact structural characterization of NP-hard\\ninstances. We also provide a practical algorithm for this problem (a heuristic\\nfor NP-hard instances) and evaluate its experimental performance on real and\\nsynthetic datasets.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 17, 1, 50, 27, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Construction and Application of Teaching System Based on Crowdsourcing Knowledge Graph',\n",
       "  'authors': ['Jinta Weng',\n",
       "   'Ying Gao',\n",
       "   'Jing Qiu',\n",
       "   'Guozhu Ding',\n",
       "   'Huanqin Zheng'],\n",
       "  'summary': \"Through the combination of crowdsourcing knowledge graph and teaching system,\\nresearch methods to generate knowledge graph and its applications. Using two\\ncrowdsourcing approaches, crowdsourcing task distribution and reverse captcha\\ngeneration, to construct knowledge graph in the field of teaching system.\\nGenerating a complete hierarchical knowledge graph of the teaching domain by\\nnodes of school, student, teacher, course, knowledge point and exercise type.\\nThe knowledge graph constructed in a crowdsourcing manner requires many users\\nto participate collaboratively with fully consideration of teachers' guidance\\nand users' mobilization issues. Based on the three subgraphs of knowledge\\ngraph, prominent teacher, student learning situation and suitable learning\\nroute could be visualized. Personalized exercises recommendation model is used\\nto formulate the personalized exercise by algorithm based on the knowledge\\ngraph. Collaborative creation model is developed to realize the crowdsourcing\\nconstruction mechanism. Though unfamiliarity with the learning mode of\\nknowledge graph and learners' less attention to the knowledge structure, system\\nbased on Crowdsourcing Knowledge Graph can still get high acceptance around\\nstudents and teachers\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 18, 14, 26, 10, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'uARMSolver: A framework for Association Rule Mining',\n",
       "  'authors': ['Iztok Fister', 'Iztok Fister Jr'],\n",
       "  'summary': 'The paper presents a novel software framework for Association Rule Mining\\nnamed uARMSolver. The framework is written fully in C++ and runs on all\\nplatforms. It allows users to preprocess their data in a transaction database,\\nto make discretization of data, to search for association rules and to guide a\\npresentation/visualization of the best rules found using external tools. As\\nopposed to the existing software packages or frameworks, this also supports\\nnumerical and real-valued types of attributes besides the categorical ones.\\nMining the association rules is defined as an optimization and solved using the\\nnature-inspired algorithms that can be incorporated easily. Because the\\nalgorithms normally discover a huge amount of association rules, the framework\\nenables a modular inclusion of so-called visual guiders for extracting the\\nknowledge hidden in data, and visualize these using external tools.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 21, 10, 36, 31, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Neural Networks for Entity Matching: A Survey',\n",
       "  'authors': ['Nils Barlaug', 'Jon Atle Gulla'],\n",
       "  'summary': 'Entity matching is the problem of identifying which records refer to the same\\nreal-world entity. It has been actively researched for decades, and a variety\\nof different approaches have been developed. Even today, it remains a\\nchallenging problem, and there is still generous room for improvement. In\\nrecent years we have seen new methods based upon deep learning techniques for\\nnatural language processing emerge.\\n  In this survey, we present how neural networks have been used for entity\\nmatching. Specifically, we identify which steps of the entity matching process\\nexisting work have targeted using neural networks, and provide an overview of\\nthe different techniques used at each step. We also discuss contributions from\\ndeep learning in entity matching compared to traditional methods, and propose a\\ntaxonomy of deep neural networks for entity matching.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 21, 15, 36, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Cluster-and-Conquer: When Randomness Meets Graph Locality',\n",
       "  'authors': ['George Giakkoupis',\n",
       "   'Anne-Marie Kermarrec',\n",
       "   'Olivier Ruas',\n",
       "   'François Taïani'],\n",
       "  'summary': 'K-Nearest-Neighbors (KNN) graphs are central to many emblematic data mining\\nand machine-learning applications. Some of the most efficient KNN graph\\nalgorithms are incremental and local: they start from a random graph, which\\nthey incrementally improve by traversing neighbors-of-neighbors links.\\nParadoxically, this random start is also one of the key weaknesses of these\\nalgorithms: nodes are initially connected to dissimilar neighbors, that lie far\\naway according to the similarity metric. As a result, incremental algorithms\\nmust first laboriously explore spurious potential neighbors before they can\\nidentify similar nodes, and start converging. In this paper, we remove this\\ndrawback with Cluster-and-Conquer (C 2 for short). Cluster-and-Conquer boosts\\nthe starting configuration of greedy algorithms thanks to a novel lightweight\\nclustering mechanism, dubbed FastRandomHash. FastRandomHash leverages\\nrandom-ness and recursion to pre-cluster similar nodes at a very low cost. Our\\nextensive evaluation on real datasets shows that Cluster-and-Conquer\\nsignificantly outperforms existing approaches, including LSH, yielding\\nspeed-ups of up to x4.42 while incurring only a negligible loss in terms of KNN\\nquality.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 22, 7, 31, 12, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Efficient RDF Graph Storage based on Reinforcement Learning',\n",
       "  'authors': ['Lei Zheng', 'Ziming Shen', 'Hongzhi Wang'],\n",
       "  'summary': 'Knowledge graph is an important cornerstone of artificial intelligence. The\\nconstruction and release of large-scale knowledge graphs in various fields pose\\nnew challenges to knowledge graph data management. Due to the maturity and\\nstability, relational database is also suitable for RDF data storage. However,\\nthe complex structure of RDF graph brings challenges to storage structure\\ndesign for RDF graph in the relational database. To address the difficult\\nproblem, this paper adopts reinforcement learning (RL) to optimize the storage\\npartition method of RDF graph based on the relational database. We transform\\nthe graph storage into a Markov decision process, and develop the reinforcement\\nlearning algorithm for graph storage design. For effective RL-based storage\\ndesign, we propose the data feature extraction method of RDF tables and the\\nquery rewriting priority policy during model training. The extensive\\nexperimental results demonstrate that our approach outperforms existing RDF\\nstorage design methods.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 22, 8, 57, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Automated Metadata Harmonization Using Entity Resolution & Contextual Embedding',\n",
       "  'authors': ['Kunal Sawarkar', 'Meenkakshi Kodati'],\n",
       "  'summary': \"ML Data Curation process typically consist of heterogeneous & federated\\nsource systems with varied schema structures; requiring curation process to\\nstandardize metadata from different schemas to an inter-operable schema. This\\nmanual process of Metadata Harmonization & cataloging slows efficiency of\\nML-Ops lifecycle. We demonstrate automation of this step with the help of\\nentity resolution methods & also by using Cogntive Database's Db2Vec embedding\\napproach to capture hidden inter-column & intra-column relationships which\\ndetect similarity of metadata and then predict metadata columns from source\\nschemas to any standardized schemas. Apart from matching schemas, we\\ndemonstrate that it can also infer the correct ontological structure of the\\ntarget data model.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 17, 2, 14, 15, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'REMIX: Efficient Range Query for LSM-trees',\n",
       "  'authors': ['Wenshao Zhong', 'Chen Chen', 'Xingbo Wu', 'Song Jiang'],\n",
       "  'summary': 'LSM-tree based key-value (KV) stores organize data in a multi-level structure\\nfor high-speed writes. Range queries on traditional LSM-trees must seek and\\nsort-merge data from multiple table files on the fly, which is expensive and\\noften leads to mediocre read performance. To improve range query efficiency on\\nLSM-trees, we introduce a space-efficient KV index data structure, named REMIX,\\nthat records a globally sorted view of KV data spanning multiple table files. A\\nrange query on multiple REMIX-indexed data files can quickly locate the target\\nkey using a binary search, and retrieve subsequent keys in sorted order without\\nkey comparisons. We build RemixDB, an LSM-tree based KV-store that adopts a\\nwrite-efficient compaction strategy and employs REMIXes for fast point and\\nrange queries. Experimental results show that REMIXes can substantially improve\\nrange query performance in a write-optimized LSM-tree based KV-store.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 24, 1, 21, 21, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Purely Regular Approach to Non-Regular Core Spanners',\n",
       "  'authors': ['Markus L. Schmid', 'Nicole Schweikardt'],\n",
       "  'summary': \"The regular spanners (characterised by vset-automata) are closed under the\\nalgebraic operations of union, join and projection, and have desirable\\nalgorithmic properties. The core spanners (introduced by Fagin, Kimelfeld,\\nReiss, and Vansummeren (PODS 2013, JACM 2015) as a formalisation of the core\\nfunctionality of the query language AQL used in IBM's SystemT) additionally\\nneed string equality selections and it has been shown by Freydenberger and\\nHolldack (ICDT 2016, Theory of Computing Systems 2018) that this leads to high\\ncomplexity and even undecidability of the typical problems in static analysis\\nand query evaluation. We propose an alternative approach to core spanners: by\\nincorporating the string-equality selections directly into the regular language\\nthat represents the underlying regular spanner (instead of treating it as an\\nalgebraic operation on the table extracted by the regular spanner), we obtain a\\nfragment of core spanners that, while having slightly weaker expressive power\\nthan the full class of core spanners, arguably still covers the intuitive\\napplications of string equality selections for information extraction and has\\nmuch better upper complexity bounds of the typical problems in static analysis\\nand query evaluation.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 26, 9, 27, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Exploring Memory Access Patterns for Graph Processing Accelerators',\n",
       "  'authors': ['Jonas Dann', 'Daniel Ritter', 'Holger Fröning'],\n",
       "  'summary': 'Recent trends in business and technology (e.g., machine learning, social\\nnetwork analysis) benefit from storing and processing growing amounts of\\ngraph-structured data in databases and data science platforms. FPGAs as\\naccelerators for graph processing with a customizable memory hierarchy promise\\nsolving performance problems caused by inherent irregular memory access\\npatterns on traditional hardware (e.g., CPU). However, developing such hardware\\naccelerators is yet time-consuming and difficult and benchmarking is\\nnon-standardized, hindering comprehension of the impact of memory access\\npattern changes and systematic engineering of graph processing accelerators.\\n  In this work, we propose a simulation environment for the analysis of graph\\nprocessing accelerators based on simulating their memory access patterns.\\nFurther, we evaluate our approach on two state-of-the-art FPGA graph processing\\naccelerators and show reproducibility, comparablity, as well as the shortened\\ndevelopment process by an example. Not implementing the cycle-accurate internal\\ndata flow on accelerator hardware like FPGAs significantly reduces the\\nimplementation time, increases the benchmark parameter transparency, and allows\\ncomparison of graph processing approaches.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 26, 14, 34, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'PPQ-Trajectory: Spatio-temporal Quantization for Querying in Large Trajectory Repositories',\n",
       "  'authors': ['Shuang Wang', 'Hakan Ferhatosmanoglu'],\n",
       "  'summary': 'We present PPQ-trajectory, a spatio-temporal quantization based solution for\\nquerying large dynamic trajectory data. PPQ-trajectory includes a\\npartition-wise predictive quantizer (PPQ) that generates an error-bounded\\ncodebook with autocorrelation and spatial proximity-based partitions. The\\ncodebook is indexed to run approximate and exact spatio-temporal queries over\\ncompressed trajectories. PPQ-trajectory includes a coordinate quadtree coding\\nfor the codebook with support for exact queries. An incremental temporal\\npartition-based index is utilised to avoid full reconstruction of trajectories\\nduring queries. An extensive set of experimental results for spatio-temporal\\nqueries on real trajectory datasets is presented. PPQ-trajectory shows\\nsignificant improvements over the alternatives with respect to several\\nperformance measures, including the accuracy of results when the summary is\\nused directly to provide approximate query results, the spatial deviation with\\nwhich spatio-temporal path queries can be answered when the summary is used as\\nan index, and the time taken to construct the summary. Superior results on the\\nquality of the summary and the compression ratio are also demonstrated.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 26, 17, 2, 52, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'CoroBase: Coroutine-Oriented Main-Memory Database Engine',\n",
       "  'authors': ['Yongjun He', 'Jiacheng Lu', 'Tianzheng Wang'],\n",
       "  'summary': 'Data stalls are a major overhead in main-memory database engines due to the\\nuse of pointer-rich data structures. Lightweight coroutines ease the\\nimplementation of software prefetching to hide data stalls by overlapping\\ncomputation and asynchronous data prefetching. Prior solutions, however, mainly\\nfocused on (1) individual components and operations and (2) intra-transaction\\nbatching that requires interface changes, breaking backward compatibility. It\\nwas not clear how they apply to a full database engine and how much end-to-end\\nbenefit they bring under various workloads.\\n  This paper presents \\\\corobase, a main-memory database engine that tackles\\nthese challenges with a new coroutine-to-transaction paradigm.\\nCoroutine-to-transaction models transactions as coroutines and thus enables\\ninter-transaction batching, avoiding application changes but retaining the\\nbenefits of prefetching. We show that on a 48-core server, CoroBase can perform\\nclose to 2x better for read-intensive workloads and remain competitive for\\nworkloads that inherently do not benefit from software prefetching.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 29, 22, 54, 52, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Patterns Count-Based Labels for Datasets',\n",
       "  'authors': ['Yuval Moskovitch', 'H. V. Jagadish'],\n",
       "  'summary': 'Counts of attribute-value combinations are central to the profiling of a\\ndataset, particularly in determining fitness for use and in eliminating bias\\nand unfairness. While counts of individual attribute values may be stored in\\nsome dataset profiles, there are too many combinations of attributes for it to\\nbe practical to store counts for each combination. In this paper, we develop\\nthe notion of storing a \"label\" of limited size that can be used to obtain good\\nestimates for these counts. A label, in this paper, contains information\\nregarding the count of selected patterns--attributes values combinations--in\\nthe data. We define an estimation function, that uses this label to estimate\\nthe count of every pattern. We present the problem of finding the optimal label\\ngiven a bound on its size and propose a heuristic algorithm for generating\\noptimal labels. We experimentally show the accuracy of count estimates derived\\nfrom the resulting labels and the efficiency of our algorithm.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 30, 16, 0, 34, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Independence in Infinite Probabilistic Databases',\n",
       "  'authors': ['Martin Grohe', 'Peter Lindner'],\n",
       "  'summary': 'Probabilistic databases (PDBs) model uncertainty in data. The current\\nstandard is to view PDBs as finite probability spaces over relational database\\ninstances. Since many attributes in typical databases have infinite domains,\\nsuch as integers, strings, or real numbers, it is often more natural to view\\nPDBs as infinite probability spaces over database instances. In this paper, we\\nlay the mathematical foundations of infinite probabilistic databases. Our focus\\nthen is on independence assumptions. Tuple-independent PDBs play a central role\\nin theory and practice of PDBs. Here, we study infinite tuple-independent PDBs\\nas well as related models such as infinite block-independent disjoint PDBs.\\nWhile the standard model of PDBs focuses on a set-based semantics, we also\\nstudy tuple-independent PDBs with a bag semantics and independence in PDBs over\\nuncountable fact spaces.\\n  We also propose a new approach to PDBs with an open-world assumption,\\naddressing issues raised by Ceylan et al. (Proc. KR 2016) and generalizing\\ntheir work, which is still rooted in finite tuple-independent PDBs.\\n  Moreover, for countable PDBs we propose an approximate query answering\\nalgorithm.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 30, 20, 34, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Domain-specific Knowledge Graphs: A survey',\n",
       "  'authors': ['Bilal Abu-Salih'],\n",
       "  'summary': 'Knowledge Graphs (KGs) have made a qualitative leap and effected a real\\nrevolution in knowledge representation. This is leveraged by the underlying\\nstructure of the KG which underpins a better comprehension, reasoning and\\ninterpretation of knowledge for both human and machine. Therefore, KGs continue\\nto be used as the main means of tackling a plethora of real-life problems in\\nvarious domains. However, there is no consensus in regard to a plausible and\\ninclusive definition of a domain-specific KG. Further, in conjunction with\\nseveral limitations and deficiencies, various domain-specific KG construction\\napproaches are far from perfect. This survey is the first to offer a\\ncomprehensive definition of a domain-specific KG. Also, the paper presents a\\nthorough review of the state-of-the-art approaches drawn from academic works\\nrelevant to seven domains of knowledge. An examination of current approaches\\nreveals a range of limitations and deficiencies. At the same time, uncharted\\nterritories on the research map are highlighted to tackle extant issues in the\\nliterature and point to directions for future research.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 10, 31, 10, 39, 53, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Memory-Efficient RkNN Retrieval by Nonlinear k-Distance Approximation',\n",
       "  'authors': ['Sandra Obermeier', 'Max Berrendorf', 'Peer Kröger'],\n",
       "  'summary': 'The reverse k-nearest neighbor (RkNN) query is an established query type with\\nvarious applications reaching from identifying highly influential objects over\\nincrementally updating kNN graphs to optimizing sensor communication and\\noutlier detection. State-of-the-art solutions exploit that the k-distances in\\nreal-world datasets often follow the power-law distribution, and bound them\\nwith linear lines in log-log space. In this work, we investigate this\\nassumption and uncover that it is violated in regions of changing density,\\nwhich we show are typical for real-life datasets. Towards a generic solution,\\nwe pose the estimation of k-distances as a regression problem. Thereby, we\\nenable harnessing the power of the abundance of available Machine Learning\\nmodels and profiting from their advancement. We propose a flexible approach\\nwhich allows steering the performance-memory consumption trade-off, and in\\nparticular to find good solutions with a fixed memory budget crucial in the\\ncontext of edge computing. Moreover, we show how to obtain and improve\\nguaranteed bounds essential to exact query processing. In experiments on\\nreal-world datasets, we demonstrate how this framework can significantly reduce\\nthe index memory consumption, and strongly reduce the candidate set size. We\\npublish our code at https://github.com/sobermeier/nonlinear-kdist.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 3, 15, 13, 5, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Predict and Write: Using K-Means Clustering to Extend the Lifetime of NVM Storage',\n",
       "  'authors': ['Saeed Kargar', 'Heiner Litz', 'Faisal Nawab'],\n",
       "  'summary': 'Non-volatile memory (NVM) technologies suffer from limited write endurance.\\nTo address this challenge, we propose Predict and Write (PNW), a K/V-store that\\nuses a clustering-based machine learning approach to extend the lifetime of\\nNVMs. PNW decreases the number of bit flips for PUT/UPDATE operations by\\ndetermining the best memory location an updated value should be written to. PNW\\nleverages the indirection level of K/V-stores to freely choose the target\\nmemory location for any given write based on its value. PNW organizes NVM\\naddresses in a dynamic address pool clustered by the similarity of the data\\nvalues they refer to. We show that, by choosing the right target memory\\nlocation for a given PUT/UPDATE operation, the number of total bit flips and\\ncache lines can be reduced by up to 85% and 56% over the state of the art.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 4, 22, 3, 9, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Characterizing Transactional Databases for Frequent Itemset Mining',\n",
       "  'authors': ['Christian Lezcano', 'Marta Arias'],\n",
       "  'summary': \"This paper presents a study of the characteristics of transactional databases\\nused in frequent itemset mining. Such characterizations have typically been\\nused to benchmark and understand the data mining algorithms working on these\\ndatabases. The aim of our study is to give a picture of how diverse and\\nrepresentative these benchmarking databases are, both in general but also in\\nthe context of particular empirical studies found in the literature. Our\\nproposed list of metrics contains many of the existing metrics found in the\\nliterature, as well as new ones. Our study shows that our list of metrics is\\nable to capture much of the datasets' inner complexity and thus provides a good\\nbasis for the characterization of transactional datasets. Finally, we provide a\\nset of representative datasets based on our characterization that may be used\\nas a benchmark safely.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 9, 12, 26, 14, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Batchwise Probabilistic Incremental Data Cleaning',\n",
       "  'authors': ['Paulo H. Oliveira',\n",
       "   'Daniel S. Kaster',\n",
       "   'Caetano Traina-Jr.',\n",
       "   'Ihab F. Ilyas'],\n",
       "  'summary': 'Lack of data and data quality issues are among the main bottlenecks that\\nprevent further artificial intelligence adoption within many organizations,\\npushing data scientists to spend most of their time cleaning data before being\\nable to answer analytical questions. Hence, there is a need for more effective\\nand efficient data cleaning solutions, which, not surprisingly, is rife with\\ntheoretical and engineering problems. This report addresses the problem of\\nperforming holistic data cleaning incrementally, given a fixed rule set and an\\nevolving categorical relational dataset acquired in sequential batches. To the\\nbest of our knowledge, our contributions compose the first incremental\\nframework that cleans data (i) independently of user interventions, (ii)\\nwithout requiring knowledge about the incoming dataset, such as the number of\\nclasses per attribute, and (iii) holistically, enabling multiple error types to\\nbe repaired simultaneously, and thus avoiding conflicting repairs. Extensive\\nexperiments show that our approach outperforms the competitors with respect to\\nrepair quality, execution time, and memory consumption.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 9, 20, 15, 2, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Comprehensive and Efficient Workload Compression',\n",
       "  'authors': ['Shaleen Deep',\n",
       "   'Anja Gruenheid',\n",
       "   'Paraschos Koutris',\n",
       "   'Jeffrey Naughton',\n",
       "   'Stratis Viglas'],\n",
       "  'summary': 'This work studies the problem of constructing a representative workload from\\na given input analytical query workload where the former serves as an\\napproximation with guarantees of the latter. We discuss our work in the context\\nof workload analysis and monitoring. As an example, evolving system usage\\npatterns in a database system can cause load imbalance and performance\\nregressions which can be controlled by monitoring system usage patterns,\\ni.e.,~a representative workload, over time. To construct such a workload in a\\nprincipled manner, we formalize the notions of workload {\\\\em representativity}\\nand {\\\\em coverage}. These metrics capture the intuition that the distribution\\nof features in a compressed workload should match a target distribution,\\nincreasing representativity, and include common queries as well as outliers,\\nincreasing coverage. We show that solving this problem optimally is NP-hard and\\npresent a novel greedy algorithm that provides approximation guarantees. We\\ncompare our techniques to established algorithms in this problem space such as\\nsampling and clustering, and demonstrate advantages and key trade-offs of our\\ntechniques.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 11, 5, 16, 30, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Scalable Querying of Nested Data',\n",
       "  'authors': ['Jaclyn Smith',\n",
       "   'Michael Benedikt',\n",
       "   'Milos Nikolic',\n",
       "   'Amir Shaikhha'],\n",
       "  'summary': 'While large-scale distributed data processing platforms have become an\\nattractive target for query processing, these systems are problematic for\\napplications that deal with nested collections. Programmers are forced either\\nto perform non-trivial translations of collection programs or to employ\\nautomated flattening procedures, both of which lead to performance problems.\\nThese challenges only worsen for nested collections with skewed cardinalities,\\nwhere both handcrafted rewriting and automated flattening are unable to enforce\\nload balancing across partitions.\\n  In this work, we propose a framework that translates a program manipulating\\nnested collections into a set of semantically equivalent shredded queries that\\ncan be efficiently evaluated. The framework employs a combination of query\\ncompilation techniques, an efficient data representation for nested\\ncollections, and automated skew-handling. We provide an extensive experimental\\nevaluation, demonstrating significant improvements provided by the framework in\\ndiverse scenarios for nested collection programs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 12, 13, 45, 8, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Turning Transport Data to Comply with EU Standards while Enabling a Multimodal Transport Knowledge Graph',\n",
       "  'authors': ['Mario Scrocca',\n",
       "   'Marco Comerio',\n",
       "   'Alessio Carenini',\n",
       "   'Irene Celino'],\n",
       "  'summary': 'Complying with the EU Regulation on multimodal transportation services\\nrequires sharing data on the National Access Points in one of the standards\\n(e.g., NeTEx and SIRI) indicated by the European Commission. These standards\\nare complex and of limited practical adoption. This means that datasets are\\nnatively expressed in other formats and require a data translation process for\\nfull compliance.\\n  This paper describes the solution to turn the authoritative data of three\\ndifferent transport stakeholders from Italy and Spain into a format compliant\\nwith EU standards by means of Semantic Web technologies. Our solution addresses\\nthe challenge and also contributes to build a multi-modal transport Knowledge\\nGraph of interlinked and interoperable information that enables intelligent\\nquerying and exploration, as well as facilitates the design of added-value\\nservices.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 12, 14, 56, 15, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Bridging the Technology Gap Between Industry and Semantic Web: Generating Databases and Server Code From RDF',\n",
       "  'authors': ['Markus Schröder',\n",
       "   'Michael Schulze',\n",
       "   'Christian Jilek',\n",
       "   'Andreas Dengel'],\n",
       "  'summary': 'Despite great advances in the area of Semantic Web, industry rather seldom\\nadopts Semantic Web technologies and their storage and query concepts. Instead,\\nrelational databases (RDB) are often deployed to store business-critical data,\\nwhich are accessed via REST interfaces. Yet, some enterprises would greatly\\nbenefit from Semantic Web related datasets which are usually represented with\\nthe Resource Description Framework (RDF). To bridge this technology gap, we\\npropose a fully automatic approach that generates suitable RDB models with REST\\nAPIs to access them. In our evaluation, generated databases from different RDF\\ndatasets are examined and compared. Our findings show that the databases\\nsufficiently reflect their counterparts while the API is able to reproduce\\nrather simple SPARQL queries. Potentials for improvements are identified, for\\nexample, the reduction of data redundancies in generated databases.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 16, 13, 50, 43, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The Problem with XSD Binary Floating Point Datatypes in RDF',\n",
       "  'authors': ['Jan Martin Keil', 'Merle Gänßinger'],\n",
       "  'summary': 'The XSD binary floating point datatypes are regularly used for precise\\nnumeric values in RDF. However, the use of these datatypes for knowledge\\nrepresentation can systematically impair the quality of data and, compared to\\nthe XSD decimal datatype, increases the probability of data processing\\nproducing false results. We argue why in most cases the XSD decimal datatype is\\nbetter suited to represent numeric values in RDF. A survey of the actual usage\\nof datatypes on the relevant subset of the December 2020 Web Data Commons\\ndataset, containing 19453060341 literals from real web data, substantiates the\\npractical relevancy of the described problem: 29 %-68 % of binary floating\\npoint values are distorted due to the datatype.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 13, 14, 27, 50, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Multi-SQL: An extensible multi-model data query language',\n",
       "  'authors': ['Yu Yan',\n",
       "   'Nan Jiang',\n",
       "   'Hongzhi Wang',\n",
       "   'Yutong Wang',\n",
       "   'Chang Liu',\n",
       "   'Yuzhuo Wang'],\n",
       "  'summary': 'Big data management aims to establish data hubs that support data in multiple\\nmodels and types in an all-around way. Thus, the multi-model database system is\\na promising architecture for building such a multi-model data store. For an\\nintegrated data hub, a unified and flexible query language is incredibly\\nnecessary. In this paper, an extensible and practical query language--Multi-SQL\\nis proposed to realize the unified management of multi-model data considering\\nthe co-processing of multi-model data. To the best of our knowledge, Multi-SQL\\nis the first query language based on various data models. Multi-SQL can also be\\nexpanded to suit more complicated scenarios as it is flexible to support other\\ndata models. Moreover, we provide a formal semantic definition of the core\\nfeatures of Multi-SQL, including the multi-model definition, multi-model\\nfilters, multi-model joins, etc. Furthermore, we propose a two-level query\\nimplementation method to totally exploit the existing query optimization\\ncapabilities of the underlying engines which could largely improve the query\\nexcution efficiency.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 17, 15, 54, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Query Expressibility and Verification in Ontology-Based Data Access',\n",
       "  'authors': ['Carsten Lutz', 'Johannes Marti', 'Leif Sabellek'],\n",
       "  'summary': \"In ontology-based data access, multiple data sources are integrated using an\\nontology and mappings. In practice, this is often achieved by a bootstrapping\\nprocess, that is, the ontology and mappings are first designed to support only\\nthe most important queries over the sources and then gradually extended to\\nenable additional queries. In this paper, we study two reasoning problems that\\nsupport such an approach. The expressibility problem asks whether a given\\nsource query $q_s$ is expressible as a target query (that is, over the\\nontology's vocabulary) and the verification problem asks, additionally given a\\ncandidate target query $q_t$, whether $q_t$ expresses $q_s$. We consider (U)CQs\\nas source and target queries and GAV mappings, showing that both problems are\\n$\\\\Pi^p_2$-complete in DL-Lite, coNExpTime-complete between EL and ELHI when\\nsource queries are rooted, and 2ExpTime-complete for unrestricted source\\nqueries.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 18, 9, 50, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Survey and Open Problems in Privacy Preserving Knowledge Graph: Merging, Query, Representation, Completion and Applications',\n",
       "  'authors': ['Chaochao Chen',\n",
       "   'Jamie Cui',\n",
       "   'Guanfeng Liu',\n",
       "   'Jia Wu',\n",
       "   'Li Wang'],\n",
       "  'summary': \"Knowledge Graph (KG) has attracted more and more companies' attention for its\\nability to connect different types of data in meaningful ways and support rich\\ndata services. However, the data isolation problem limits the performance of KG\\nand prevents its further development. That is, multiple parties have their own\\nKGs but they cannot share with each other due to regulation or competition\\nreasons. Therefore, how to conduct privacy preserving KG becomes an important\\nresearch question to answer. That is, multiple parties conduct KG related tasks\\ncollaboratively on the basis of protecting the privacy of multiple KGs. To\\ndate, there is few work on solving the above KG isolation problem. In this\\npaper, to fill this gap, we summarize the open problems for privacy preserving\\nKG in data isolation setting and propose possible solutions for them.\\nSpecifically, we summarize the open problems in privacy preserving KG from four\\naspects, i.e., merging, query, representation, and completion. We present these\\nproblems in details and propose possible technical solutions for them.\\nMoreover, we present three privacy preserving KG-aware applications and simply\\ndescribe how can our proposed techniques be applied into these applications.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 20, 2, 35, 47, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'OAK: Ontology-Based Knowledge Map Model for Digital Agriculture',\n",
       "  'authors': ['Quoc Hung Ngo', 'Tahar Kechadi', 'Nhien-An Le-Khac'],\n",
       "  'summary': 'Nowadays, a huge amount of knowledge has been amassed in digital agriculture.\\nThis knowledge and know-how information are collected from various sources,\\nhence the question is how to organise this knowledge so that it can be\\nefficiently exploited. Although this knowledge about agriculture practices can\\nbe represented using ontology, rule-based expert systems, or knowledge model\\nbuilt from data mining processes, the scalability still remains an open issue.\\nIn this study, we propose a knowledge representation model, called an\\nontology-based knowledge map, which can collect knowledge from different\\nsources, store it, and exploit either directly by stakeholders or as an input\\nto the knowledge discovery process (Data Mining). The proposed model consists\\nof two stages, 1) build an ontology as a knowledge base for a specific domain\\nand data mining concepts, and 2) build the ontology-based knowledge map model\\nfor representing and storing the knowledge mined on the crop datasets. A\\nframework of the proposed model has been implemented in agriculture domain. It\\nis an efficient and scalable model, and it can be used as knowledge repository\\na digital agriculture.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 20, 14, 16, 12, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Verifying the Correctness of Analytic Query Results',\n",
       "  'authors': ['Masoud Nosrati', 'Ying Cai'],\n",
       "  'summary': 'Data outsourcing is a cost-effective solution for data owners to tackle\\nissues such as large volumes of data, huge number of users, and intensive\\ncomputation needed for data analysis. They can simply upload their databases to\\na cloud and let it perform all management works, including query processing.\\nOne problem with this service model is how query issuers can verify the query\\nresults they receive are indeed correct. This concern is legitimate because, as\\na third party, clouds may not be fully trustworthy, and as a large data center,\\nclouds are ideal targets for hackers. There has been significant work on query\\nresult verification, but most consider only simple queries where query results\\ncan be attained by checking the raw data against the query conditions directly.\\nIn this paper, we consider the problem of enabling users to verify the\\ncorrectness of the results of analytic queries. Unlike simple queries, analytic\\nqueries involve ranking functions to score a database, which makes it difficult\\nto build data structures for verification purposes. We propose two approaches,\\nnamely one-signature and multi-signature, and show that they work well on three\\nrepresentative types of analytic queries, including top-k, range, and KNN\\nqueries, through both analysis and experiments.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 19, 14, 9, 12, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Efficient Approximate Nearest Neighbor Search for Multiple Weighted $l_{p\\\\leq2}$ Distance Functions',\n",
       "  'authors': ['Huan Hu', 'Jianzhong Li'],\n",
       "  'summary': 'Nearest neighbor search is fundamental to a wide range of applications. Since\\nthe exact nearest neighbor search suffers from the \"curse of dimensionality\",\\napproximate approaches, such as Locality-Sensitive Hashing (LSH), are widely\\nused to trade a little query accuracy for a much higher query efficiency. In\\nmany scenarios, it is necessary to perform nearest neighbor search under\\nmultiple weighted distance functions in high-dimensional spaces. This paper\\nconsiders the important problem of supporting efficient approximate nearest\\nneighbor search for multiple weighted distance functions in high-dimensional\\nspaces. To the best of our knowledge, prior work can only solve the problem for\\nthe $l_2$ distance. However, numerous studies have shown that the $l_p$\\ndistance with $p\\\\in(0,2)$ could be more effective than the $l_2$ distance in\\nhigh-dimensional spaces. We propose a novel method, WLSH, to address the\\nproblem for the $l_p$ distance for $p\\\\in(0,2]$. WLSH takes the LSH approach and\\ncan theoretically guarantee both the efficiency of processing queries and the\\naccuracy of query results while minimizing the required total number of hash\\ntables. We conduct extensive experiments on synthetic and real data sets, and\\nthe results show that WLSH achieves high performance in terms of query\\nefficiency, query accuracy and space consumption.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 24, 5, 56, 48, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'TKUS: Mining Top-K High-Utility Sequential Patterns',\n",
       "  'authors': ['Chunkai Zhang', 'Zilin Du', 'Wensheng Gan', 'Philip S. Yu'],\n",
       "  'summary': 'High-utility sequential pattern mining (HUSPM) has recently emerged as a\\nfocus of intense research interest. The main task of HUSPM is to find all\\nsubsequences, within a quantitative sequential database, that have high utility\\nwith respect to a user-defined minimum utility threshold. However, it is\\ndifficult to specify the minimum utility threshold, especially when database\\nfeatures, which are invisible in most cases, are not understood. To handle this\\nproblem, top-k HUSPM was proposed. Up to now, only very preliminary work has\\nbeen conducted to capture top-k HUSPs, and existing strategies require\\nimprovement in terms of running time, memory consumption, unpromising candidate\\nfiltering, and scalability. Moreover, no systematic problem statement has been\\ndefined. In this paper, we formulate the problem of top-k HUSPM and propose a\\nnovel algorithm called TKUS. To improve efficiency, TKUS adopts a projection\\nand local search mechanism and employs several schemes, including the Sequence\\nUtility Raising, Terminate Descendants Early, and Eliminate Unpromising Items\\nstrategies, which allow it to greatly reduce the search space. Finally,\\nexperimental results demonstrate that TKUS can achieve sufficiently good top-k\\nHUSPM performance compared to state-of-the-art algorithm TKHUS-Span.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 26, 19, 36, 20, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Recursive Algorithm for Mining Association Rules',\n",
       "  'authors': ['Abdelkader Mokkadem', 'Mariane Pelletier', 'Louis Raimbault'],\n",
       "  'summary': 'Mining frequent itemsets and association rules is an essential task within\\ndata mining and data analysis. In this paper, we introduce PrefRec, a recursive\\nalgorithm for finding frequent itemsets and association rules. Its main\\nadvantage is its recursiveness with respect to the items. It is particularly\\nefficient for updating the mining process when new items are added to the\\ndatabase or when some are excluded. We present in a complete way the logic of\\nthe algorithm, and give some of its applications. After that, we carry out an\\nexperimental study on the effectiveness of PrefRec. We first compare the\\nexecution times with some very popular frequent itemset mining algorithms.\\nThen, we do experiments to test the updating capabilities of our algorithm.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 28, 18, 53, 3, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Near-Optimal Parallel Algorithm for Joining Binary Relations',\n",
       "  'authors': ['Bas Ketsman', 'Dan Suciu', 'Yufei Tao'],\n",
       "  'summary': 'We present a constant-round algorithm in the massively parallel computation\\n(MPC) model for evaluating a natural join where every input relation has two\\nattributes. Our algorithm achieves a load of $\\\\tilde{O}(m/p^{1/\\\\rho})$ where\\n$m$ is the total size of the input relations, $p$ is the number of machines,\\n$\\\\rho$ is the join\\'s fractional edge covering number, and $\\\\tilde{O}(.)$ hides\\na polylogarithmic factor. The load matches a known lower bound up to a\\npolylogarithmic factor. At the core of the proposed algorithm is a new theorem\\n(which we name the \"isolated cartesian product theorem\") that provides fresh\\ninsight into the problem\\'s mathematical structure. Our result implies that the\\nsubgraph enumeration problem, where the goal is to report all the occurrences\\nof a constant-sized subgraph pattern, can be settled optimally (up to a\\npolylogarithmic factor) in the MPC model.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 30, 0, 48, 35, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Mint: MDL-based approach for Mining INTeresting Numerical Pattern Sets',\n",
       "  'authors': ['Tatiana Makhalova', 'Sergei O. Kuznetsov', 'Amedeo Napoli'],\n",
       "  'summary': 'Pattern mining is well established in data mining research, especially for\\nmining binary datasets. Surprisingly, there is much less work about numerical\\npattern mining and this research area remains under-explored. In this paper, we\\npropose Mint, an efficient MDL-based algorithm for mining numerical datasets.\\nThe MDL principle is a robust and reliable framework widely used in pattern\\nmining, and as well in subgroup discovery. In Mint we reuse MDL for discovering\\nuseful patterns and returning a set of non-redundant overlapping patterns with\\nwell-defined boundaries and covering meaningful groups of objects. Mint is not\\nalone in the category of numerical pattern miners based on MDL. In the\\nexperiments presented in the paper we show that Mint outperforms competitors\\namong which Slim and RealKrimp.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 30, 14, 36, 29, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Optimizing Fitness-For-Use of Differentially Private Linear Queries',\n",
       "  'authors': ['Yingtai Xiao',\n",
       "   'Zeyu Ding',\n",
       "   'Yuxin Wang',\n",
       "   'Danfeng Zhang',\n",
       "   'Daniel Kifer'],\n",
       "  'summary': 'In practice, differentially private data releases are designed to support a\\nvariety of applications. A data release is fit for use if it meets target\\naccuracy requirements for each application. In this paper, we consider the\\nproblem of answering linear queries under differential privacy subject to\\nper-query accuracy constraints. Existing practical frameworks like the matrix\\nmechanism do not provide such fine-grained control (they optimize total error,\\nwhich allows some query answers to be more accurate than necessary, at the\\nexpense of other queries that become no longer useful). Thus, we design a\\nfitness-for-use strategy that adds privacy-preserving Gaussian noise to query\\nanswers. The covariance structure of the noise is optimized to meet the\\nfine-grained accuracy requirements while minimizing the cost to privacy.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 11, 30, 22, 10, 21, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Learning to Characterize Matching Experts',\n",
       "  'authors': ['Roee Shraga', 'Ofra Amir', 'Avigdor Gal'],\n",
       "  'summary': 'Matching is a task at the heart of any data integration process, aimed at\\nidentifying correspondences among data elements. Matching problems were\\ntraditionally solved in a semi-automatic manner, with correspondences being\\ngenerated by matching algorithms and outcomes subsequently validated by human\\nexperts. Human-in-the-loop data integration has been recently challenged by the\\nintroduction of big data and recent studies have analyzed obstacles to\\neffective human matching and validation. In this work we characterize human\\nmatching experts, those humans whose proposed correspondences can mostly be\\ntrusted to be valid. We provide a novel framework for characterizing matching\\nexperts that, accompanied with a novel set of features, can be used to identify\\nreliable and valuable human experts. We demonstrate the usefulness of our\\napproach using an extensive empirical evaluation. In particular, we show that\\nour approach can improve matching results by filtering out inexpert matchers.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 2, 14, 16, 38, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Free Gap Estimates from the Exponential Mechanism, Sparse Vector, Noisy Max and Related Algorithms',\n",
       "  'authors': ['Zeyu Ding',\n",
       "   'Yuxin Wang',\n",
       "   'Yingtai Xiao',\n",
       "   'Guanhong Wang',\n",
       "   'Danfeng Zhang',\n",
       "   'Daniel Kifer'],\n",
       "  'summary': 'Private selection algorithms, such as the Exponential Mechanism, Noisy Max\\nand Sparse Vector, are used to select items (such as queries with large\\nanswers) from a set of candidates, while controlling privacy leakage in the\\nunderlying data. Such algorithms serve as building blocks for more complex\\ndifferentially private algorithms. In this paper we show that these algorithms\\ncan release additional information related to the gaps between the selected\\nitems and the other candidates for free (i.e., at no additional privacy cost).\\nThis free gap information can improve the accuracy of certain follow-up\\ncounting queries by up to 66%. We obtain these results from a careful privacy\\nanalysis of these algorithms. Based on this analysis, we further propose novel\\nhybrid algorithms that can dynamically save additional privacy budget.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 2, 23, 28, 27, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'An Algebraic Graph Transformation Approach for RDF and SPARQL',\n",
       "  'authors': ['Dominique Duval', 'Rachid Echahed', 'Frédéric Prost'],\n",
       "  'summary': 'We consider the recommendations of the World Wide Web Consortium (W3C) about\\nRDF framework and its associated query language SPARQL. We propose a new formal\\nframework based on category theory which provides clear and concise formal\\ndefinitions of the main basic features of RDF and SPARQL. We define RDF graphs\\nas well as SPARQL basic graph patterns as objects of some nested categories.\\nThis allows one to clarify, in particular, the role of blank nodes.\\nFurthermore, we consider basic SPARQL CONSTRUCT and SELECT queries and\\nformalize their operational semantics following a novel algebraic graph\\ntransformation approach called POIM.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 3, 2, 27, 57, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'WedgeChain: A Trusted Edge-Cloud Store With Asynchronous (Lazy) Trust',\n",
       "  'authors': ['Faisal Nawab'],\n",
       "  'summary': 'We propose WedgeChain, a data store that spans both edge and cloud nodes (an\\nedge-cloud system). WedgeChain consists of a logging layer and a data indexing\\nlayer. In this study, we encounter two challenges: (1) edge nodes are untrusted\\nand potentially malicious, and (2) edge-cloud coordination is expensive.\\nWedgeChain tackles these challenges by the following proposals: (1) Lazy\\n(asynchronous) certification: where data is committed at the untrusted edge and\\nthen lazily certified at the cloud node. This lazy certification method takes\\nadvantage of the observation that an untrusted edge node is unlikely to act\\nmaliciously if it knows it will be detected (and punished) eventually. Our lazy\\ncertification method guarantees that malicious acts (i.e., lying) are\\neventually detected. (2) Data-free certification: our lazy certification method\\nonly needs to send digests of data to the cloud, instead of sending all data to\\nthe cloud, which enables saving network and cloud resources and reduce costs.\\n(3) LSMerkle: we extend a trusted index (mLSM) to enable indexing data at the\\nedge while utilizing lazy and data-free certification.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 3, 20, 50, 22, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Hiperfact: In-Memory High Performance Fact Processing -- Rethinking the Rete Inference Algorithm',\n",
       "  'authors': ['Conrad Indiono', 'Stefanie Rinderle-Ma'],\n",
       "  'summary': 'The Rete forward inference algorithm forms the basis for many rule engines\\ndeployed today, but it exhibits the following problems: (1) the caching of all\\nintermediate join results, (2) the processing of all rules regardless of the\\nnecessity to do so (stemming from the underlying forward inference approach),\\n(3) not defining the join order of rules and its conditions, significantly\\naffecting the final run-time performance, and finally (4) pointer chasing due\\nto the overall network structure, leading to inefficient usage of the CPU\\ncaches caused by random access patterns. The Hiperfact approach aims to\\novercome these shortcomings by (1) choosing cache efficient data structures on\\nthe primary rank 1 fact index storage and intermediate join result storage\\nlevels, (2) introducing island fact processing for determining the join order\\nby ensuring minimal intermediate join result construction, and (3) introducing\\nderivation trees to allow for parallel read/write access and lazy rule\\nevaluation. The experimental evaluations show that the Hiperfact prototype\\nengine implementing the approach achieves significant improvements in respect\\nto both inference and query performance. Moreover, the proposed Hiperfact\\nengine is compared to existing engines in the context of a comprehensive\\nbenchmark.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 4, 16, 38, 1, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'From Data Harvesting to Querying for Making Urban Territories Smart',\n",
       "  'authors': ['Genoveva Vargas-Solar',\n",
       "   'Ana-Sagrario Castillo-Camporro',\n",
       "   'José Zechinelli-Martini',\n",
       "   'Javier Espinosa-Oviedo'],\n",
       "  'summary': 'This chapter provides a summarized, critical and analytical point of view of\\nthe data-centric solutions that are currently applied for addressing urban\\nproblems in cities. These solutions lead to the use of urban computing\\ntechniques to address their daily life issues. Data-centric solutions have\\nbecome popular due to the emergence of data science. The chapter describes and\\ndiscusses the type of urban challenges and how data science in urban computing\\ncan face them. Current solutions address a spectrum that goes from data\\nharvesting techniques to decision making support. Finally, the chapter also\\nputs in perspective families of strategies developed in the state of the art\\nfor addressing urban problems and exhibits guidelines that can lead to a\\nmethodological understanding of these strategies.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 8, 11, 15, 48, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'SEGSys: A mapping system for segmentation analysis in energy',\n",
       "  'authors': ['Xiufeng Liu', 'Rongling Li', 'Yi Wang', 'Per Sieverts Nielsen'],\n",
       "  'summary': 'Customer segmentation analysis can give valuable insights into the energy\\nefficiency of residential buildings. This paper presents a mapping system,\\nSEGSys that enables segmentation analysis at the individual and the\\nneighborhood levels. SEGSys supports the online and offline classification of\\ncustomers based on their daily consumption patterns and consumption intensity.\\nIt also supports the segmentation analysis according to the social\\ncharacteristics of customers of individual households or neighborhoods, as well\\nas spatial geometries. SEGSys uses a three-layer architecture to model the\\nsegmentation system, including the data layer, the service layer, and the\\npresentation layer. The data layer models data into a star schema within a data\\nwarehouse, the service layer provides data service through a RESTful interface,\\nand the presentation layer interacts with users through a visual map. This\\npaper showcases the system on the segmentation analysis using an electricity\\nconsumption data set and validates the effectiveness of the system.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 11, 16, 13, 24, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Cortex: Harnessing Correlations to Boost Query Performance',\n",
       "  'authors': ['Vikram Nathan',\n",
       "   'Jialin Ding',\n",
       "   'Tim Kraska',\n",
       "   'Mohammad Alizadeh'],\n",
       "  'summary': 'Databases employ indexes to filter out irrelevant records, which reduces scan\\noverhead and speeds up query execution. However, this optimization is only\\navailable to queries that filter on the indexed attribute. To extend these\\nspeedups to queries on other attributes, database systems have turned to\\nsecondary and multi-dimensional indexes. Unfortunately, these approaches are\\nrestrictive: secondary indexes have a large memory footprint and can only speed\\nup queries that access a small number of records, and multi-dimensional indexes\\ncannot scale to more than a handful of columns. We present Cortex, an approach\\nthat takes advantage of correlations to extend the reach of primary indexes to\\nmore attributes. Unlike prior work, Cortex can adapt itself to any existing\\nprimary index, whether single or multi-dimensional, to harness a broad variety\\nof correlations, such as those that exist between more than two attributes or\\nhave a large number of outliers. We demonstrate that on real datasets\\nexhibiting these diverse types of correlations, Cortex matches or outperforms\\ntraditional secondary indexes with $5\\\\times$ less space, and it is $2-8\\\\times$\\nfaster than existing approaches to indexing correlations.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 12, 0, 22, 51, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Deep Analysis on Subgraph Isomorphism',\n",
       "  'authors': ['Li Zeng', 'Yan Jiang', 'Weixin Lu', 'Lei Zou'],\n",
       "  'summary': 'Subgraph isomorphism is a well-known NP-hard problem which is widely used in\\nmany applications, such as social network analysis and knowledge graph query.\\nIts performance is often limited by the inherent hardness. Several insightful\\nworks have been done since 2012, mainly optimizing pruning rules and matching\\norders to accelerate enumerating all isomorphic subgraphs. Nevertheless, their\\ncorrectness and performance are not well studied. First, different languages\\nare used in implementation with different compilation flags. Second,\\nexperiments are not done on the same platform and the same datasets. Third,\\nsome ideas of different works are even complementary. Last but not least, there\\nexist errors when applying some algorithms. In this paper, we address these\\nproblems by re-implementing seven representative subgraph isomorphism\\nalgorithms as well as their improved versions, and conducting comprehensive\\nexperiments on various graphs. The results show pros and cons of\\nstate-of-the-art solutions and explore new approaches to optimization.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 12, 12, 29, 38, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Dual-Store Structure for Knowledge Graphs',\n",
       "  'authors': ['Zhixin Qi', 'Hongzhi Wang', 'Haoran Zhang'],\n",
       "  'summary': 'To effectively manage increasing knowledge graphs in various domains, a hot\\nresearch topic, knowledge graph storage management, has emerged. Existing\\nmethods are classified to relational stores and native graph stores. Relational\\nstores are able to store large-scale knowledge graphs and convenient in\\nupdating knowledge, but the query performance weakens obviously when the\\nselectivity of a knowledge graph query is large. Native graph stores are\\nefficient in processing complex knowledge graph queries due to its index-free\\nadjacent property, but they are inapplicable to manage a large-scale knowledge\\ngraph due to limited storage budgets or inflexible updating process. Motivated\\nby this, we propose a dual-store structure which leverages a graph store to\\naccelerate the complex query process in the relational store. However, it is\\nchallenging to determine what data to transfer from relational store to graph\\nstore at what time. To address this problem, we formulate it as a Markov\\nDecision Process and derive a physical design tuner DOTIL based on\\nreinforcement learning. With DOTIL, the dual-store structure is adaptive to\\ndynamic changing workloads. Experimental results on real knowledge graphs\\ndemonstrate that our proposed dual-store structure improves query performance\\nup to average 43.72% compared with the most commonly used relational stores.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 13, 5, 33, 19, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Schema Extraction on Semi-structured Data',\n",
       "  'authors': ['Panpan Li', 'Yikun Gong', 'Chen Wang'],\n",
       "  'summary': \"With the continuous development of NoSQL databases, more and more developers\\nchoose to use semi-structured data for development and data management, which\\nputs forward requirements for schema management of semi-structured data stored\\nin NoSQL databases. Schema extraction plays an important role in understanding\\nschemas, optimizing queries, and validating data consistency. Therefore, in\\nthis survey we investigate structural methods based on tree and graph and\\nstatistical methods based on distributed architecture and machine learning to\\nextract schemas. The schemas obtained by the structural methods are more\\ninterpretable, and the statistical methods have better applicability and\\ngeneralization ability. Moreover, we also investigate tools and systems for\\nschemas extraction. Schema extraction tools are mainly used for spark or NoSQL\\ndatabases, and are suitable for small datasets or simple application\\nenvironments. The system mainly focuses on the extraction and management of\\nschemas in large data sets and complex application scenarios. Furthermore, we\\nalso compare these techniques to facilitate data managers' choice.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 15, 5, 57, 41, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Graph integration of structured, semistructured and unstructured data for data journalism',\n",
       "  'authors': ['Angelos-Christos Anadiotis',\n",
       "   'Oana Balalau',\n",
       "   'Catarina Conceicao',\n",
       "   'Helena Galhardas',\n",
       "   'Mhd Yamen Haddad',\n",
       "   'Ioana Manolescu',\n",
       "   'Tayeb Merabti',\n",
       "   'Jingmao You'],\n",
       "  'summary': 'Digital data is a gold mine for modern journalism. However, datasets which\\ninterest journalists are extremely heterogeneous, ranging from highly\\nstructured (relational databases), semi-structured (JSON, XML, HTML), graphs\\n(e.g., RDF), and text. Journalists (and other classes of users lacking advanced\\nIT expertise, such as most non-governmental-organizations, or small public\\nadministrations) need to be able to make sense of such heterogeneous corpora,\\neven if they lack the ability to define and deploy custom\\nextract-transform-load workflows, especially for dynamically varying sets of\\ndata sources.\\n  We describe a complete approach for integrating dynamic sets of heterogeneous\\ndatasets along the lines described above: the challenges we faced to make such\\ngraphs useful, allow their integration to scale, and the solutions we proposed\\nfor these problems. Our approach is implemented within the ConnectionLens\\nsystem; we validate it through a set of experiments.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 16, 9, 59, 27, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Clique: Spatiotemporal Object Re-identification at the City Scale',\n",
       "  'authors': ['Tiantu Xu',\n",
       "   'Kaiwen Shen',\n",
       "   'Yang Fu',\n",
       "   'Humphrey Shi',\n",
       "   'Felix Xiaozhu Lin'],\n",
       "  'summary': 'Object re-identification (ReID) is a key application of city-scale cameras.\\nWhile classic ReID tasks are often considered as image retrieval, we treat them\\nas spatiotemporal queries for locations and times in which the target object\\nappeared. Spatiotemporal reID is challenged by the accuracy limitation in\\ncomputer vision algorithms and the colossal videos from city cameras. We\\npresent Clique, a practical ReID engine that builds upon two new techniques:\\n(1) Clique assesses target occurrences by clustering fuzzy object features\\nextracted by ReID algorithms, with each cluster representing the general\\nimpression of a distinct object to be matched against the input; (2) to search\\nin videos, Clique samples cameras to maximize the spatiotemporal coverage and\\nincrementally adds cameras for processing on demand. Through evaluation on 25\\nhours of videos from 25 cameras, Clique reached a high accuracy of 0.87 (recall\\nat 5) across 70 queries and runs at 830x of video realtime in achieving high\\naccuracy.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 17, 0, 5, 50, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'SARS-CoV-2 Coronavirus Data Compression Benchmark',\n",
       "  'authors': ['Innar Liiv'],\n",
       "  'summary': \"This paper introduces a lossless data compression competition that benchmarks\\nsolutions (computer programs) by the compressed size of the 44,981 concatenated\\nSARS-CoV-2 sequences, with a total uncompressed size of 1,339,868,341 bytes.\\nThe data, downloaded on 13 December 2020, from the severe acute respiratory\\nsyndrome coronavirus 2 data hub of ncbi.nlm.nih.gov is presented in FASTA and\\n2Bit format. The aim of this competition is to encourage multidisciplinary\\nresearch to find the shortest lossless description for the sequences and to\\ndemonstrate that data compression can serve as an objective and repeatable\\nmeasure to align scientific breakthroughs across disciplines. The shortest\\ndescription of the data is the best model; therefore, further reducing the size\\nof this description requires a fundamental understanding of the underlying\\ncontext and data. This paper presents preliminary results with multiple\\nwell-known compression algorithms for baseline measurements, and insights\\nregarding promising research avenues. The competition's progress will be\\nreported at \\\\url{https://coronavirus.innar.com}, and the benchmark is open for\\nall to participate and contribute.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 21, 16, 41, 59, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Data Validation',\n",
       "  'authors': ['Mark P. J. van der Loo', 'Edwin de Jonge'],\n",
       "  'summary': 'Data validation is the activity where one decides whether or not a particular\\ndata set is fit for a given purpose. Formalizing the requirements that drive\\nthis decision process allows for unambiguous communication of the requirements,\\nautomation of the decision process, and opens up ways to maintain and\\ninvestigate the decision process itself. The purpose of this article is to\\nformalize the definition of data validation and to demonstrate some of the\\nproperties that can be derived from this definition. In particular, it is shown\\nhow a formal view of the concept permits a classification of data quality\\nrequirements, allowing them to be ordered in increasing levels of complexity.\\nSome subtleties arising from combining possibly many such requirements are\\npointed out as well.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 21, 12, 58, 13, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Towards Quantifying Privacy in Process Mining',\n",
       "  'authors': ['Majid Rafiei', 'Wil M. P. van der Aalst'],\n",
       "  'summary': 'Process mining employs event logs to provide insights into the actual\\nprocesses. Event logs are recorded by information systems and contain valuable\\ninformation helping organizations to improve their processes. However, these\\ndata also include highly sensitive private information which is a major concern\\nwhen applying process mining. Therefore, privacy preservation in process mining\\nis growing in importance, and new techniques are being introduced. The\\neffectiveness of the proposed privacy preservation techniques needs to be\\nevaluated. It is important to measure both sensitive data protection and data\\nutility preservation. In this paper, we propose an approach to quantify the\\neffectiveness of privacy preservation techniques. We introduce two measures for\\nquantifying disclosure risks to evaluate the sensitive data protection aspect.\\nMoreover, a measure is proposed to quantify data utility preservation for the\\nmain process mining activities. The proposed measures have been tested using\\nvarious real-life event logs.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 21, 10, 4, 54, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Learned Indexes for a Google-scale Disk-based Database',\n",
       "  'authors': ['Hussam Abu-Libdeh',\n",
       "   'Deniz Altınbüken',\n",
       "   'Alex Beutel',\n",
       "   'Ed H. Chi',\n",
       "   'Lyric Doshi',\n",
       "   'Tim Kraska',\n",
       "   'Xiaozhou',\n",
       "   'Li',\n",
       "   'Andy Ly',\n",
       "   'Christopher Olston'],\n",
       "  'summary': \"There is great excitement about learned index structures, but understandable\\nskepticism about the practicality of a new method uprooting decades of research\\non B-Trees. In this paper, we work to remove some of that uncertainty by\\ndemonstrating how a learned index can be integrated in a distributed,\\ndisk-based database system: Google's Bigtable. We detail several design\\ndecisions we made to integrate learned indexes in Bigtable. Our results show\\nthat integrating learned index significantly improves the end-to-end read\\nlatency and throughput for Bigtable.\",\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 23, 5, 56, 45, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Discovering Closed and Maximal Embedded Patterns from Large Tree Data',\n",
       "  'authors': ['Xiaoying Wu', 'Dimitri Theodoratos', 'Nikos Mamoulis'],\n",
       "  'summary': 'We address the problem of summarizing embedded tree patterns extracted from\\nlarge data trees. We do so by defining and mining closed and maximal embedded\\nunordered tree patterns from a single large data tree. We design an embedded\\nfrequent pattern mining algorithm extended with a local closedness checking\\ntechnique. This algorithm is called {\\\\em closedEmbTM-prune} as it eagerly\\neliminates non-closed patterns. To mitigate the generation of intermediate\\npatterns, we devise pattern search space pruning rules to proactively detect\\nand prune branches in the pattern search space which do not correspond to\\nclosed patterns. The pruning rules are accommodated into the extended embedded\\npattern miner to produce a new algorithm, called {\\\\em closedEmbTM-prune}, for\\nmining all the closed and maximal embedded frequent patterns from large data\\ntrees. Our extensive experiments on synthetic and real large-tree datasets\\ndemonstrate that, on dense datasets, {\\\\em closedEmbTM-prune} not only generates\\na complete closed and maximal pattern set which is substantially smaller than\\nthat generated by the embedded pattern miner, but also runs much faster with\\nnegligible overhead on pattern pruning.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 26, 7, 8, 42, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Recommending Courses in MOOCs for Jobs: An Auto Weak Supervision Approach',\n",
       "  'authors': ['Bowen Hao',\n",
       "   'Jing Zhang',\n",
       "   'Cuiping Li',\n",
       "   'Hong Chen',\n",
       "   'Hongzhi Yin'],\n",
       "  'summary': 'The proliferation of massive open online courses (MOOCs) demands an effective\\nway of course recommendation for jobs posted in recruitment websites,\\nespecially for the people who take MOOCs to find new jobs. Despite the advances\\nof supervised ranking models, the lack of enough supervised signals prevents us\\nfrom directly learning a supervised ranking model. This paper proposes a\\ngeneral automated weak supervision framework AutoWeakS via reinforcement\\nlearning to solve the problem. On the one hand, the framework enables training\\nmultiple supervised ranking models upon the pseudo labels produced by multiple\\nunsupervised ranking models. On the other hand, the framework enables\\nautomatically searching the optimal combination of these supervised and\\nunsupervised models. Systematically, we evaluate the proposed model on several\\ndatasets of jobs from different recruitment websites and courses from a MOOCs\\nplatform. Experiments show that our model significantly outperforms the\\nclassical unsupervised, supervised and weak supervision baselines.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 28, 14, 3, 18, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Misplaced Subsequences Repairing with Application to Multivariate Industrial Time Series Data',\n",
       "  'authors': ['Xiaoou Ding', 'Hongzhi Wang', 'Jiaxuan Su', 'Chen Wang'],\n",
       "  'summary': 'Both the volume and the collection velocity of time series generated by\\nmonitoring sensors are increasing in the Internet of Things (IoT). Data\\nmanagement and analysis requires high quality and applicability of the IoT\\ndata. However, errors are prevalent in original time series data. Inconsistency\\nin time series is a serious data quality problem existing widely in IoT. Such\\nproblem could be hardly solved by existing techniques. Motivated by this, we\\ndefine an inconsistent subsequences problem in multivariate time series, and\\npropose an integrity data repair approach to solve inconsistent problems. Our\\nproposed repairing method consists of two parts: (1) we design effective\\nanomaly detection method to discover latent inconsistent subsequences in the\\nIoT time series; and (2) we develop repair algorithms to precisely locate the\\nstart and finish time of inconsistent intervals, and provide reliable repairing\\nstrategies. A thorough experiment on two real-life datasets verifies the\\nsuperiority of our method compared to other practical approaches. Experimental\\nresults also show that our method captures and repairs inconsistency problems\\neffectively in industrial time series in complex IIoT scenarios.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 29, 1, 26, 26, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'On the importance of functions in data modeling',\n",
       "  'authors': ['Alexandr Savinov'],\n",
       "  'summary': 'In this paper we argue that representing entity properties by tuple\\nattributes, as evangelized in most set-oriented data models, is a controversial\\nmethod conflicting with the principle of tuple immutability. As a principled\\nsolution to this problem of tuple immutability on one hand and the need to\\nmodify tuple attributes on the other hand, we propose to use mathematical\\nfunctions for representing entity properties. In this approach, immutable\\ntuples are intended for representing the existence of entities while mutable\\nfunctions (mappings between sets) are used for representing entity properties.\\nIn this model, called the concept-oriented model (COM), functions are made\\nfirst-class elements along with sets, and both functions and sets are used to\\nrepresent and process data in a simpler and more natural way in comparison to\\npurely set-oriented models.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 31, 12, 9, 54, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'bloomRF: On Performing Range-Queries with Bloom-Filters based on Piecewise-Monotone Hash Functions and Dyadic Trace-Trees',\n",
       "  'authors': ['Christian Riegger',\n",
       "   'Arthur Bernhardt',\n",
       "   'Bernhard Moessner',\n",
       "   'Ilia Petrov'],\n",
       "  'summary': 'We introduce bloomRF as a unified method for approximate membership testing\\nthat supports both point- and range-queries on a single data structure. bloomRF\\nextends Bloom-Filters with range query support and may replace them. The core\\nidea is to employ a dyadic interval scheme to determine the set of dyadic\\nintervals covering a data point, which are then encoded and inserted. bloomRF\\nintroduces Dyadic Trace-Trees as novel data structure that represents those\\ncovering intervals implicitly. A Trace-Tree encoding scheme represents the set\\nof covering intervals efficiently, in a compact bit representation.\\nFurthermore, bloomRF introduces novel piecewise-monotone hash functions that\\nare locally order-preserving and thus support range querying. We present an\\nefficient membership computation method for range-queries. Although, bloomRF is\\ndesigned for integers it also supports string and floating-point data types. It\\ncan also handle multiple attributes and serve as multi-attribute filter.\\n  We evaluate bloomRF in RocksDB and in a standalone library. bloomRF is more\\nefficient and outperforms existing point-range-filters by up to 4x across a\\nrange of settings.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2020, 12, 31, 13, 17, 37, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'To Share, or not to Share Online Event Trend Aggregation Over Bursty Event Streams',\n",
       "  'authors': ['Olga Poppe',\n",
       "   'Chuan Lei',\n",
       "   'Lei Ma',\n",
       "   'Allison Rozet',\n",
       "   'Elke A. Rundensteiner'],\n",
       "  'summary': 'Complex event processing (CEP) systems continuously evaluate large workloads\\nof pattern queries under tight time constraints. Event trend aggregation\\nqueries with Kleene patterns are commonly used to retrieve summarized insights\\nabout the recent trends in event streams. State-of-art methods are limited\\neither due to repetitive computations or unnecessary trend construction.\\nExisting shared approaches are guided by statically selected and hence rigid\\nsharing plans that are often sub-optimal under stream fluctuations. In this\\nwork, we propose a novel framework Hamlet that is the first to overcome these\\nlimitations. Hamlet introduces two key innovations. First, Hamlet adaptively\\ndecides whether to share or not to share computations depending on the current\\nstream properties at run time to harvest the maximum sharing benefit. Second,\\nHamlet is equipped with a highly efficient shared trend aggregation strategy\\nthat avoids trend construction. Our experimental study on both real and\\nsynthetic data sets demonstrates that Hamlet consistently reduces query latency\\nby up to five orders of magnitude compared to the state-of-the-art approaches.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 1, 2, 3, 21, 32, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'Exploring Data and Knowledge combined Anomaly Explanation of Multivariate Industrial Data',\n",
       "  'authors': ['Xiaoou Ding',\n",
       "   'Hongzhi Wang',\n",
       "   'Chen Wang',\n",
       "   'Zijue Li',\n",
       "   'Zheng Liang'],\n",
       "  'summary': 'The demand for high-performance anomaly detection techniques of IoT data\\nbecomes urgent, especially in industry field. The anomaly identification and\\nexplanation in time series data is one essential task in IoT data mining. Since\\nthat the existing anomaly detection techniques focus on the identification of\\nanomalies, the explanation of anomalies is not well-solved. We address the\\nanomaly explanation problem for multivariate IoT data and propose a 3-step\\nself-contained method in this paper. We formalize and utilize the domain\\nknowledge in our method, and identify the anomalies by the violation of\\nconstraints. We propose set-cover-based anomaly explanation algorithms to\\ndiscover the anomaly events reflected by violation features, and further\\ndevelop knowledge update algorithms to improve the original knowledge set.\\nExperimental results on real datasets from large-scale IoT systems verify that\\nour method computes high-quality explanation solutions of anomalies. Our work\\nprovides a guide to navigate the explicable anomaly detection in both IoT fault\\ndiagnosis and temporal data cleaning.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 1, 5, 6, 19, 39, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'A Survey on Advancing the DBMS Query Optimizer: Cardinality Estimation, Cost Model, and Plan Enumeration',\n",
       "  'authors': ['Hai Lan', 'Zhifeng Bao', 'Yuwei Peng'],\n",
       "  'summary': 'Query optimizer is at the heart of the database systems. Cost-based optimizer\\nstudied in this paper is adopted in almost all current database systems. A\\ncost-based optimizer introduces a plan enumeration algorithm to find a\\n(sub)plan, and then uses a cost model to obtain the cost of that plan, and\\nselects the plan with the lowest cost. In the cost model, cardinality, the\\nnumber of tuples through an operator, plays a crucial role. Due to the\\ninaccuracy in cardinality estimation, errors in cost model, and the huge plan\\nspace, the optimizer cannot find the optimal execution plan for a complex query\\nin a reasonable time. In this paper, we first deeply study the causes behind\\nthe limitations above. Next, we review the techniques used to improve the\\nquality of the three key components in the cost-based optimizer, cardinality\\nestimation, cost model, and plan enumeration. We also provide our insights on\\nthe future directions for each of the above aspects.',\n",
       "  'primary_category': 'cs.DB',\n",
       "  'published_date': datetime.datetime(2021, 1, 5, 13, 47, 45, tzinfo=datetime.timezone.utc)},\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the final output\n",
    "final_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd5f2b1",
   "metadata": {},
   "source": [
    "*Assigning the metadata to a dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21f50f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame(final_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ed42db",
   "metadata": {},
   "source": [
    "*Storing the data from dataframe to csv file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1aef218",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('arxiv_metadata.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aa0cc4",
   "metadata": {},
   "source": [
    ">> ### *Task 2*\n",
    "Display the 5 lines of each category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344ef649",
   "metadata": {},
   "source": [
    "Importing the librearies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf69cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e16a59d",
   "metadata": {},
   "source": [
    "Getting the data from csv file to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1ab62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('arxiv_metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32178888",
   "metadata": {},
   "source": [
    "Separating the data of cs.ET and display the first 5 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50303021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>summary</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>A self-contained and self-explanatory DNA stor...</td>\n",
       "      <td>['Min Li', 'Jiashu Wu', 'Junbiao Dai', 'Qingsh...</td>\n",
       "      <td>Current research on DNA storage usually focuse...</td>\n",
       "      <td>cs.ET</td>\n",
       "      <td>2022-07-19 10:54:54+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>Efficiently Supporting Hierarchy and Data Upda...</td>\n",
       "      <td>['Puru Sharma', 'Cheng-Kai Lim', 'Dehui Lin', ...</td>\n",
       "      <td>We propose a novel and flexible DNA-storage ar...</td>\n",
       "      <td>cs.ET</td>\n",
       "      <td>2022-12-27 11:02:13+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>DataBright: Towards a Global Exchange for Dece...</td>\n",
       "      <td>['David Dao', 'Dan Alistarh', 'Claudiu Musat',...</td>\n",
       "      <td>It is safe to assume that, for the foreseeable...</td>\n",
       "      <td>cs.ET</td>\n",
       "      <td>2018-02-13 18:20:07+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16771</th>\n",
       "      <td>Closed-loop spiking control on a neuromorphic ...</td>\n",
       "      <td>['Jingyue Zhao', 'Nicoletta Risi', 'Marco Monf...</td>\n",
       "      <td>Despite neuromorphic engineering promises the ...</td>\n",
       "      <td>cs.ET</td>\n",
       "      <td>2020-09-01 14:17:48+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16780</th>\n",
       "      <td>Extending the D-Wave with support for Higher P...</td>\n",
       "      <td>['John E. Dorband']</td>\n",
       "      <td>D-Wave only guarantees to support coefficients...</td>\n",
       "      <td>cs.ET</td>\n",
       "      <td>2018-07-13 18:25:44+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "2467   A self-contained and self-explanatory DNA stor...   \n",
       "2468   Efficiently Supporting Hierarchy and Data Upda...   \n",
       "2469   DataBright: Towards a Global Exchange for Dece...   \n",
       "16771  Closed-loop spiking control on a neuromorphic ...   \n",
       "16780  Extending the D-Wave with support for Higher P...   \n",
       "\n",
       "                                                 authors  \\\n",
       "2467   ['Min Li', 'Jiashu Wu', 'Junbiao Dai', 'Qingsh...   \n",
       "2468   ['Puru Sharma', 'Cheng-Kai Lim', 'Dehui Lin', ...   \n",
       "2469   ['David Dao', 'Dan Alistarh', 'Claudiu Musat',...   \n",
       "16771  ['Jingyue Zhao', 'Nicoletta Risi', 'Marco Monf...   \n",
       "16780                                ['John E. Dorband']   \n",
       "\n",
       "                                                 summary primary_category  \\\n",
       "2467   Current research on DNA storage usually focuse...            cs.ET   \n",
       "2468   We propose a novel and flexible DNA-storage ar...            cs.ET   \n",
       "2469   It is safe to assume that, for the foreseeable...            cs.ET   \n",
       "16771  Despite neuromorphic engineering promises the ...            cs.ET   \n",
       "16780  D-Wave only guarantees to support coefficients...            cs.ET   \n",
       "\n",
       "                  published_date  \n",
       "2467   2022-07-19 10:54:54+00:00  \n",
       "2468   2022-12-27 11:02:13+00:00  \n",
       "2469   2018-02-13 18:20:07+00:00  \n",
       "16771  2020-09-01 14:17:48+00:00  \n",
       "16780  2018-07-13 18:25:44+00:00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ET_data = df[df['primary_category']=='cs.ET'].copy()\n",
    "\n",
    "ET_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6dc494",
   "metadata": {},
   "source": [
    "Separating the data of cs.GR and display the first 5 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a016836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>summary</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>Representing Whole Slide Cancer Image Features...</td>\n",
       "      <td>['Erich Bremer', 'Jonas Almeida', 'Joel Saltz']</td>\n",
       "      <td>Regions of Interest (ROI) contain morphologica...</td>\n",
       "      <td>cs.GR</td>\n",
       "      <td>2020-05-13 16:38:24+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>HiVision: Rapid Visualization of Large-Scale S...</td>\n",
       "      <td>['Mengyu Ma', 'Ye Wu', 'Xue Ouyang', 'Luo Chen...</td>\n",
       "      <td>Rapid visualization of large-scale spatial vec...</td>\n",
       "      <td>cs.GR</td>\n",
       "      <td>2020-05-26 02:48:40+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>Least-Squares Affine Reflection Using Eigen De...</td>\n",
       "      <td>['Alec Jacobson']</td>\n",
       "      <td>This note summarizes the steps to computing th...</td>\n",
       "      <td>cs.GR</td>\n",
       "      <td>2020-06-10 21:50:02+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>A novel 3D display based on micro-volumetric s...</td>\n",
       "      <td>['Guangjun Wang']</td>\n",
       "      <td>The present study proposes a novel 3D display ...</td>\n",
       "      <td>cs.GR</td>\n",
       "      <td>2018-12-16 13:06:49+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2472</th>\n",
       "      <td>Global Illumination of non-Euclidean spaces</td>\n",
       "      <td>['Tiago Novello', 'Vinicius da Silva', 'Luiz V...</td>\n",
       "      <td>This paper presents a path tracer algorithm to...</td>\n",
       "      <td>cs.GR</td>\n",
       "      <td>2020-03-24 22:21:28+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "2465  Representing Whole Slide Cancer Image Features...   \n",
       "2466  HiVision: Rapid Visualization of Large-Scale S...   \n",
       "2470  Least-Squares Affine Reflection Using Eigen De...   \n",
       "2471  A novel 3D display based on micro-volumetric s...   \n",
       "2472        Global Illumination of non-Euclidean spaces   \n",
       "\n",
       "                                                authors  \\\n",
       "2465    ['Erich Bremer', 'Jonas Almeida', 'Joel Saltz']   \n",
       "2466  ['Mengyu Ma', 'Ye Wu', 'Xue Ouyang', 'Luo Chen...   \n",
       "2470                                  ['Alec Jacobson']   \n",
       "2471                                  ['Guangjun Wang']   \n",
       "2472  ['Tiago Novello', 'Vinicius da Silva', 'Luiz V...   \n",
       "\n",
       "                                                summary primary_category  \\\n",
       "2465  Regions of Interest (ROI) contain morphologica...            cs.GR   \n",
       "2466  Rapid visualization of large-scale spatial vec...            cs.GR   \n",
       "2470  This note summarizes the steps to computing th...            cs.GR   \n",
       "2471  The present study proposes a novel 3D display ...            cs.GR   \n",
       "2472  This paper presents a path tracer algorithm to...            cs.GR   \n",
       "\n",
       "                 published_date  \n",
       "2465  2020-05-13 16:38:24+00:00  \n",
       "2466  2020-05-26 02:48:40+00:00  \n",
       "2470  2020-06-10 21:50:02+00:00  \n",
       "2471  2018-12-16 13:06:49+00:00  \n",
       "2472  2020-03-24 22:21:28+00:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GR_data = df[df['primary_category']=='cs.GR'].copy()\n",
    "\n",
    "GR_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec70e09",
   "metadata": {},
   "source": [
    "Separating the data of cs.RO and display the first 5 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe55eacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>summary</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2459</th>\n",
       "      <td>Automatic generation of ground truth for the e...</td>\n",
       "      <td>['Hatem Hajri', 'Emmanuel Doucet', 'Marc Revil...</td>\n",
       "      <td>As automated vehicles are getting closer to be...</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>2018-07-16 08:20:40+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <td>EU Long-term Dataset with Multiple Sensors for...</td>\n",
       "      <td>['Zhi Yan', 'Li Sun', 'Tomas Krajnik', 'Yassin...</td>\n",
       "      <td>The field of autonomous driving has grown trem...</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>2019-09-07 20:23:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>Scalable Unsupervised Multi-Criteria Trajector...</td>\n",
       "      <td>['Florian Barth', 'Stefan Funke', 'Tobias Skov...</td>\n",
       "      <td>We present analysis techniques for large traje...</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>2020-10-23 12:32:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>Organization and Understanding of a Tactile In...</td>\n",
       "      <td>['Peng Wang', 'Jixiao Liu', 'Funing Hou', 'Dic...</td>\n",
       "      <td>Advanced service robots require superior tacti...</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>2021-08-09 02:02:17+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3332</th>\n",
       "      <td>Autocorrelation, Wigner and Ambiguity Transfor...</td>\n",
       "      <td>['Jacob Mackay', 'David Johnson', 'Graham Broo...</td>\n",
       "      <td>Simulating the radar illumination of large sce...</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>2022-02-06 01:47:32+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "2459  Automatic generation of ground truth for the e...   \n",
       "2460  EU Long-term Dataset with Multiple Sensors for...   \n",
       "2461  Scalable Unsupervised Multi-Criteria Trajector...   \n",
       "2462  Organization and Understanding of a Tactile In...   \n",
       "3332  Autocorrelation, Wigner and Ambiguity Transfor...   \n",
       "\n",
       "                                                authors  \\\n",
       "2459  ['Hatem Hajri', 'Emmanuel Doucet', 'Marc Revil...   \n",
       "2460  ['Zhi Yan', 'Li Sun', 'Tomas Krajnik', 'Yassin...   \n",
       "2461  ['Florian Barth', 'Stefan Funke', 'Tobias Skov...   \n",
       "2462  ['Peng Wang', 'Jixiao Liu', 'Funing Hou', 'Dic...   \n",
       "3332  ['Jacob Mackay', 'David Johnson', 'Graham Broo...   \n",
       "\n",
       "                                                summary primary_category  \\\n",
       "2459  As automated vehicles are getting closer to be...            cs.RO   \n",
       "2460  The field of autonomous driving has grown trem...            cs.RO   \n",
       "2461  We present analysis techniques for large traje...            cs.RO   \n",
       "2462  Advanced service robots require superior tacti...            cs.RO   \n",
       "3332  Simulating the radar illumination of large sce...            cs.RO   \n",
       "\n",
       "                 published_date  \n",
       "2459  2018-07-16 08:20:40+00:00  \n",
       "2460  2019-09-07 20:23:00+00:00  \n",
       "2461  2020-10-23 12:32:26+00:00  \n",
       "2462  2021-08-09 02:02:17+00:00  \n",
       "3332  2022-02-06 01:47:32+00:00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RO_data = df[df['primary_category']=='cs.RO'].copy()\n",
    "\n",
    "RO_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c50c11",
   "metadata": {},
   "source": [
    "Separating the data of cs.DB and display the first 5 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7f652d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>summary</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Corrigendum to \"Counting Database Repairs that...</td>\n",
       "      <td>['Jef Wijsen']</td>\n",
       "      <td>The helping Lemma 7 in [Maslowski and Wijsen, ...</td>\n",
       "      <td>cs.DB</td>\n",
       "      <td>2019-03-29 12:28:28+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Provenance for Interactive Visualizations</td>\n",
       "      <td>['Fotis Psallidas', 'Eugene Wu']</td>\n",
       "      <td>We highlight the connections between data prov...</td>\n",
       "      <td>cs.DB</td>\n",
       "      <td>2018-05-07 17:11:39+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A framework supporting imprecise queries and data</td>\n",
       "      <td>['Giacomo Bergami']</td>\n",
       "      <td>This technical report provides some lightweigh...</td>\n",
       "      <td>cs.DB</td>\n",
       "      <td>2019-12-28 21:58:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Open Data Portal Germany (OPAL) Projektergebnisse</td>\n",
       "      <td>['Adrian Wilke', 'Axel-Cyrille Ngonga Ngomo']</td>\n",
       "      <td>In the Open Data Portal Germany (OPAL) project...</td>\n",
       "      <td>cs.DB</td>\n",
       "      <td>2021-05-07 10:59:16+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Big Data Visualization Tools</td>\n",
       "      <td>['Nikos Bikakis']</td>\n",
       "      <td>Data visualization is the presentation of data...</td>\n",
       "      <td>cs.DB</td>\n",
       "      <td>2018-01-25 10:16:48+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Corrigendum to \"Counting Database Repairs that...   \n",
       "1          Provenance for Interactive Visualizations   \n",
       "2  A framework supporting imprecise queries and data   \n",
       "3  Open Data Portal Germany (OPAL) Projektergebnisse   \n",
       "4                       Big Data Visualization Tools   \n",
       "\n",
       "                                         authors  \\\n",
       "0                                 ['Jef Wijsen']   \n",
       "1               ['Fotis Psallidas', 'Eugene Wu']   \n",
       "2                            ['Giacomo Bergami']   \n",
       "3  ['Adrian Wilke', 'Axel-Cyrille Ngonga Ngomo']   \n",
       "4                              ['Nikos Bikakis']   \n",
       "\n",
       "                                             summary primary_category  \\\n",
       "0  The helping Lemma 7 in [Maslowski and Wijsen, ...            cs.DB   \n",
       "1  We highlight the connections between data prov...            cs.DB   \n",
       "2  This technical report provides some lightweigh...            cs.DB   \n",
       "3  In the Open Data Portal Germany (OPAL) project...            cs.DB   \n",
       "4  Data visualization is the presentation of data...            cs.DB   \n",
       "\n",
       "              published_date  \n",
       "0  2019-03-29 12:28:28+00:00  \n",
       "1  2018-05-07 17:11:39+00:00  \n",
       "2  2019-12-28 21:58:43+00:00  \n",
       "3  2021-05-07 10:59:16+00:00  \n",
       "4  2018-01-25 10:16:48+00:00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_data = df[df['primary_category']=='cs.DB'].copy()\n",
    "\n",
    "DB_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d18a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5efdb298",
   "metadata": {},
   "source": [
    ">> ### *Task 3*\n",
    "For each of the four primary categories considered \n",
    "(2018-2022) , draw a pie chart with slices (%age) for \n",
    "➢single author papers\n",
    "➢two authors papers\n",
    "➢3-4 authors papers\n",
    "➢ More than four authors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa165ca",
   "metadata": {},
   "source": [
    "Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e315fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3198ba36",
   "metadata": {},
   "source": [
    "Read the CSV file and extract required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53dacd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('arxiv_metadata.csv', usecols=['authors', 'primary_category'])\n",
    "#print(df)\n",
    "categories = ['cs.DB', 'cs.GR', 'cs.RO', 'cs.ET'] #caegories that needs be ploted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc43136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for loop segragete the data with primary categories\n",
    "for category in categories:\n",
    "    category_df = df[df['primary_category'] == category]\n",
    "    \n",
    "    # using variable to count the occurences\n",
    "    single_authors = 0\n",
    "    two_authors = 0\n",
    "    three_four_authors = 0\n",
    "    more_than_four_authors = 0\n",
    "    \n",
    "    # checking each cell and incresing the counter as needed using if condition\n",
    "    for index, row in category_df.iterrows():\n",
    "        num_authors = row['authors'].count(',') + 1\n",
    "        \n",
    "        if num_authors > 4:\n",
    "            more_than_four_authors += 1\n",
    "        elif num_authors == 3 or num_authors == 4:\n",
    "            three_four_authors += 1\n",
    "        elif num_authors == 2:\n",
    "            two_authors += 1\n",
    "        elif num_authors == 1:\n",
    "            single_authors += 1\n",
    "    \n",
    "    #print(category, single_authors, two_authors, three_four_authors, more_than_four_authors)\n",
    "    # Create a pie chart\n",
    "    labels = ['Single Author', 'Two Authors', 'Three to Four Authors', 'More than Four Authors']\n",
    "    sizes = [single_authors, two_authors, three_four_authors, more_than_four_authors]\n",
    "    explode = (0.1, 0, 0, 0)  # explode the first slice for emphasis\n",
    "    \n",
    "    plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "    plt.title(f'Authors Distribution for Category: {category}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757fc7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa70b4a8",
   "metadata": {},
   "source": [
    ">> ### *Task 4*\n",
    "Make a bar graph, with years on X axis and Number of papers on Y axis for each category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e69a54",
   "metadata": {},
   "source": [
    "Importing the requird libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8462fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd00990",
   "metadata": {},
   "source": [
    "Read the csv file and adding new column 'published_year' to store the year of publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0d4c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('arxiv_metadata.csv')\n",
    "df['published_date'] = pd.to_datetime(df['published_date'])\n",
    "df['published_year'] = df['published_date'].dt.year\n",
    "\n",
    "# Group the data by year and category and count the number of papers\n",
    "grouped = df.groupby(['published_year', 'primary_category']).size().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d27a90fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique categories and years\n",
    "categories = df['primary_category'].unique()\n",
    "years = sorted(df['published_year'].unique())  # Sort the years in ascending order\n",
    "\n",
    "# Set the width of each bar\n",
    "bar_width = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb157566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the position of each bar on the x-axis\n",
    "r = range(len(years))\n",
    "bar_positions = [list(map(lambda x: x + bar_width*i, r)) for i in range(len(categories))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b51bbb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the bar graph\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Create bars for each category\n",
    "for i, category in enumerate(categories):\n",
    "    ax.bar(bar_positions[i], grouped.loc[:, category], width=bar_width, label=category)\n",
    "\n",
    "# Set labels, title, and legend\n",
    "ax.set_xlabel('Years')\n",
    "ax.set_ylabel('Number of Papers Uploaded')\n",
    "ax.set_title('Number of Papers Uploaded by Year and Category')\n",
    "ax.set_xticks([r + (len(categories) - 1) * bar_width / 2 for r in range(len(years))])\n",
    "ax.set_xticklabels(years)\n",
    "ax.legend()\n",
    "\n",
    "# Display the bar graph\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4662b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "114f2622",
   "metadata": {},
   "source": [
    ">> ### *Bonus Task*\n",
    "Print the top 3 authors of each category who involved in maximum number of papers of that category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1504540a",
   "metadata": {},
   "source": [
    "Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c98d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e716825d",
   "metadata": {},
   "source": [
    "Function to find the top authors of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2b72a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_authors_by_category(csv_file, categories):\n",
    "    top_authors_by_category = {category: [] for category in categories}\n",
    "\n",
    "    with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            category = row['primary_category']\n",
    "            if category in categories:\n",
    "                author_list = ast.literal_eval(row['authors'])\n",
    "                top_authors_by_category[category].extend(author_list)\n",
    "\n",
    "    top_authors = {}\n",
    "    for category, authors in top_authors_by_category.items():\n",
    "        author_counts = Counter(authors)\n",
    "        top_authors[category] = author_counts.most_common(3)\n",
    "\n",
    "    return top_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7858968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "csv_file = 'arxiv_metadata.csv'\n",
    "categories = ['cs.DB', 'cs.RO', 'cs.GR']  #primary categories that are to be analysed\n",
    "top_authors = find_top_authors_by_category(csv_file, categories)   #calling the function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c7fa2",
   "metadata": {},
   "source": [
    "Printing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f15b9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 authors in category 'cs.DB':\n",
      "Wensheng Gan: 38\n",
      "Philip S. Yu: 27\n",
      "Tim Kraska: 26\n",
      "\n",
      "Top 3 authors in category 'cs.RO':\n",
      "Roland Siegwart: 121\n",
      "Dieter Fox: 90\n",
      "Dinesh Manocha: 80\n",
      "\n",
      "Top 3 authors in category 'cs.GR':\n",
      "Daniel Cohen-Or: 26\n",
      "Lin Gao: 23\n",
      "Alec Jacobson: 21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for category, authors in top_authors.items():\n",
    "    print(f\"Top 3 authors in category '{category}':\")\n",
    "    for author, count in authors:\n",
    "        print(f\"{author}: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa1cff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
